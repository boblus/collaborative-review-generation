{
    "0": {
        "paper": "Refiner : Restructure Retrieval Content Efficiently to Advance\nQuestion-Answering Capabilities\nAnonymous ACL submission\nAbstract\nLarge Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the \"lost-inthe-middle\" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose Refiner , an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. Refiner leverages a single decoder-only LLM to adaptively extract queryrelevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained Refiner (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, Refiner achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. Refiner is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks. 1 Introduction Recent Large Language Models (LLMs) (OpenAI et al., 2024) have demonstrated their remarkable abilities in understanding query (Brown et al., 2020) (Ouyang et al., 2022) and generating human language texts. Nonetheless, LLMs 042still face significant limitations, particularly in domain-specific (Zhang et al., 2024) or knowledgeintensive (Kandpal et al., 2023) tasks, where they often hallucinate (Zhang et al., 2023) when handling queries beyond their parametric knowledge (Muhlgay et al., 2024). To address this issue, Retrieval-augmented generation (RAG) (Lewis et al., 2021) has been rapidly developed (Gao et al., 2024), leveraging external knowledge base to retrieve document chunks semantically similar to the query and incorporate them into LLMs, thereby generating more faithful (Khandelwal et al., 2020) and generalizable (Kamalloo et al., 2023) outputs. However, generating answers from the top retrieved document chunks (Karpukhin et al., 2020) indiscriminately has various shortcomings. Due to the potential ambiguity or insufficiency of information in the query, the chunks could become contradictory and distracting, leading to downstream LLMs (Izacard et al., 2022) to hallucinate (Zhang et al., 2023) (Ji et al., 2023). To mitigate this issue, researchers have proposed advanced RAG approaches, such as adopting query rewriting mechanism (Chan et al., 2024) or self-reflection tokens (Asai et al., 2023) during the generation process, which enhances the performance of RAG by enabling on-demand retrieval and selecting optimal answer generated from each document chunk. Meanwhile, others focus on compressing input prompt by employing a language model (LM) (Anderson et al., 2022) (Yang et al., 2023) (Jiang et al., 2023a) to exclude irrelevant content, thereby reduces computational costs and alleviate \"lost-inthe-middle\" (Wang et al., 2023) syndrome. Despite these advancements, LMs are still predisposed to neglect key information scattered across multiple chunks. Moreover, the document chunks, retrieved based on semantic proximity to a query, often pertain to identical subjects and occasionally contain similar yet distinguished information. Failing to maintain context and naively concatenat1\nFigure 1: The example shows that misleading document chunks are prone to impeding downstream LM from\ncomprehension and leading to hallucination. While our Refiner mitigates the problem by extracting and structuring\nquery-relevant contents by sectioning distinct information, making downstream LM disseminate them effortlessly.\ning these text chunks, can induce confusion and biased comprehension in downstream LM (See examples in Appendix F), exacerbating \"lost-in-themiddle\" syndrome. This is particularly significant in multi-hop question answering (QA)(Yang et al., 2018) (Ho et al., 2020) task, which requires LM to synthesize information from multiple contents to arrive at a correct answer. Therefore, we typically need to structure the information for downstream LM to differentiate the key information. We propose Refiner , a novel end-to-end extractand-restructure paradigm that takes into account the relatedness of information across document chunks. The proposal is realized from two approaches: (1). In order for downstream LMs to align with the original context, we propose to keep query-relevant contents verbatim, while also maintain context around those contents on-demand. (2). To emphasize information relatedness among document chunks, we propose to structure the output, as illustrated in the bottom right of Figure 1: The Refiner servers as grouping extractive contents by different information using hierarchical section, wherein contents with similar information are grouped in the same section, while contents with disparate information are segregated into separate sections. This approach aims at further clarifying chunk-level information relatedness for downstream LMs to easily comprehend. A case study of 112how Refiner resolves the relatedness information in practice, is illustrated in Table 17. The Refiner leverages an arbitrary pre-trained LLM to generate targeted content, on the assumption that LLM possesses a broad foundation of world knowledge (Zhou et al., 2024) (Li et al., 2024), enabling it to effectively section the intricate, nuanced content-wise relationships. The Refiner is trained with knowledge distillation technique (Jung et al., 2024) to adaptively determine the amount of context to retain around key contents, and output with the proposed format. Empirical results on several QA tasks, including single-hop and multi-hop tasks, demonstrate that Refiner significantly improves the answer accuracy of downstream LMs, surpassing the previous stateof-the-art (SOTA) advanced RAG solution and concurrent prompt compressing work by a margin of 2.2%-7.0% on two multi-hop QA datasets, and attains comparable accuracy on three single-hop QA datasets with a token reduction of 77.8% on average, compared to the second-best solution. TheRefiner ’s plug-and-play nature make it ideal for API-based models without parameter access. Once trained, the Refiner can be applied across different upstream retrieval systems and downstream LLMs. The structured output, containing sections, titles and contents, could be further disassembled, leading to a easier source document retrospection 2\nor citation integration for industry application. 2 Related Work Retrieval Augmented Generation The Retrieval Augmented Generation (RAG) (Lewis et al., 2021) (Gao et al., 2024) fundamentally enhances the capabilities of LLMs by retrieving pertinent documents from an external knowledge base, which is made possible through the calculation of semantic similarity between user’s query and documents. Notably, the de facto approach is to segment documents into chunks and construct text representation embedding upon these chunks. By leveraging external knowledge sources like Wikipedia, RAG effectively mitigates the issue of generating factually inaccurate content, thereby greatly enhancing the reliability of generated text. RAG typically involves a retrieval system for query-relevant knowledge indexing from a knowledge base, and a LM that serves as question answering interface. Advanced RAG Advanced RAG model incorporates multiple optimization strategies that span both pre-retrieval and post-retrieval processes. For example, Self-RAG (Asai et al., 2023) introduces a selective knowledge retrieval mechanism, which utilizes a retrieve token to determine the necessity of retrieval and employs multiple critic tokens to assess answer quality. Building upon this, RQRAG (Chan et al., 2024) is trained to dynamically improve query by integrating rewriting, decomposition, and disambiguation functionalities. Furthermore, chain-of-thought (COT) (Wang et al., 2024) and Tree-of-thought (TOT) (Yao et al., 2023) are incorporated to RAG to tackle certain logicaldemanding tasks, which proved to have significantly enhanced the performance of LLMs. Concurrent Post-Retrieval Work Previous research has explored the application of language model-based approaches to further distill or extract query-relevant key information from retrieved document segments (Lewis et al., 2021). Some studies have developed summarization models (Lennox et al., 2023) (Jung et al., 2024) to derive crucial query-relevant information from retrieved document blocks, RECOMP (Xu et al., 2023) has designed a dual-encoder extractive compressor and a encoder-decoder abstractive model to expunge irrelevant context and summarize upon the remainder. Additionally, LongLLMLingua (Jiang et al., 2023a) proposes a question-aware compression 190method with a configurable compression rate, allowing them to reduce computational cost and latency, while LLMLingua-2 (Pan et al., 2024) formulates prompt compression as a token classification problem, thereby retains the faithfulness of the compressed prompt. Compared with recent studies (Xu et al., 2023) (Yoran et al., 2024) (Jiang et al., 2023a) that are similar to our Refiner , major differences are worth mentioning: These methodologies aim at effective and efficient summarization/extraction without considering the semantic relatedness among them, while we dedicate to retain relatedness information by explicitly sectioning them to form a more structured content. Furthermore, we take contextual completeness into account for downstream LMs to align with the original context effortlessly. 3 Problem Formulation Following previous work (Xu et al., 2023), we useX={q1,D1, ...., q i,D i}to represent Refiner ’s input sequence set, where qidenotes query and D i=\b\nd1\ni, ...., dK\ni\t\ndenotes the corresponding topKretrieved documents at step i. The Refiner is designated to extract from D iwith respect to qito generate output sequence yi. The output sequences of the model yishould satisfy the following principles: (1) Concise : The output should be verbatim and context-completed w.r.t retrieved documents, such that yi⊆ D i. (2) Sectioned : contents extracted from different document chunks should all be numeric sectioned. Contextually related contents should be grouped into the same section. Conversely, contents with different information should be grouped to different sections. (3)Succinct : The output length should be significantly shorter than total length of the retrieved documents D i, such that |yi| ≪ |D i|. All contents should be helpful to response to the query. If none of the documents dis relevant, then |yi|= 0. 4 Proposed Method: Refiner The primary objective of our research is to develop a model that leverages intrinsic knowledge to structure content relevant to the question in the postretrieval process. Empirical study (Hu et al., 2024) shows that models with larger parameters tend to better conform their output format to instructions and exhibit enhanced capabilities in comprehension. However, larger models are accompanied by a quadratic increase in computational cost pro3\nFigure 2: Knowledge distillation for Refiner . The curated training data is generated by multiple teacher models,\nwhich undergoes parsing, filtering, voting to improve section and context expediency.\nportional to the length of the prompt. In our case, where the model is required to respond with brief content from a discursive, lengthy context, the computational cost necessitates particular attention. Therefore, we propose to fine-tune a smaller model in order to reign the computational costs and enhance its capabilities to conform to our proposed structured output format. 4.1 Knowledge Distillation Accurate extraction of query-relevant content with pertinent context is crucial for the Refiner model. To achieve this, we develop a training dataset that teaches Refiner to determine the optimal scope of context. While a simple approach to maintain context might involve selecting a few sentences surrounding the key content from the original document chunk, this risks introducing irrelevant context. Instead, we rely on larger LLMs to adaptively determine the extent of context to retain. Specifically, we employ knowledge distillation (Jung et al., 2024) technique to train Refiner , where a simpler student model approximates the behavior of more intricate teacher models (Zuchniak, 2023). However, single teacher model can be imperfect, occasionally producing unstructured output, forgetting to provide context or grouping with erroneous section. To address this, we utilize multi-teacher knowledge distillation (Zuchniak, 2023), where multiple teacher models generate outputs from the same input, and the optimal output segment is selected to create a curated dataset. As depicted in Figure 2, we utilize five teacher models, which are prompted to section contextcompleted query-relevant content from provided document chunks. The generated sentences are rigorously verified against the original document chunks to ensure verbatimness. The section parts are voted on and rearranged, and only the sentences generated by a majority of the models are consid- 278ered relevant context. The Refiner is subsequently trained on concatenated sentences with section restoration. We design diverse prompts on various QA datasets for teacher models to consistently provide the proposed extraction. See detailed data preparation procedure in Figure 4, composition of teacher model and hyperparameters in Table 6, and task-specific prompt templates in Table 8. 4.2 Model Training Now that we have constructed the training dataset, we use Supervised Fine-Tuning (SFT) (Sun, 2024) to train a pre-trained Llama-2-7B-Chat (Touvron et al., 2023) model. In particular, given a curated high-quality train dataset SSFT={Xi, D i,Yi}N\ni=1, where Ndenotes number of samples in the dataset, the goal is to gain a model parameters Mθwith the following training objective function: minimize\nθL(θ;SSFT) =−EX,Y∼SSFT[logMθ(Y|X)]\n(1) where Ldenotes the negative likelihood objective function to minimize, Mθrepresents the model parameters. The expectation EX,Y∼S SFTaverages over our dataset. Note that negative likelihood is adopted to formulate the loss function. Following previous works (Lewis et al., 2021) (Asai et al., 2023) (Yan et al., 2024) (Xu et al., 2023), we train Refiner Mθto generate a textual sequence {yi}N\ni=1that extracts core information and context in Drelevant to qwith fewer tokens than D. To ensure our model can effectively handle training datasets of varying sizes, we employ a progressive training approach (Wu et al., 2024) for the Refiner . More training details are in the Appendix B. 5 Experiments We assess the effectiveness of our paradigm on two distinct criteria: its potential to enhance the performance of downstream LMs and its relative efficacy 4\ncompared to other RAG approaches. We integrate it with various downstream LMs and open-domain question answering datasets, building upon the methodologies established in prior researches (Asai et al., 2023) (Yan et al., 2024) (Xu et al., 2023). 5.1 Evaluate Tasks and Datasets We evaluate our Refiner and downstream LMs on a diverse open-domain QA tasks, including shortform QA, long-form QA and multi-hop QA tasks. Throughout these experiments, we conduct zeroshot evaluations, where the prompts only provide instructions on tasks and outputs in contrast to few-shot examples (Wei et al., 2022) (Sanh et al., 2022). Detailed experiment environments, including prompt templates for Refiner and downstream LMs, are listed in the Appendix C. Following previous work (Asai et al., 2023), we use accuracy as evaluation metric on all test dataset. Notably, the applied accuracy metric is reported to not always reflects the performance of the RAG system, as it looks up the existence of answer string in the generated output, omitting the semantic coherence which results in biased evaluation. In ARC Challenge task, we construct a regular expression-based calculation, which may influence the results compared with other reported scores from advanced RAG solutions. See subsection C.6 for our motivation and methodology for evaluating ARC Challenge task specifically. Single-Hop QA Dataset includes three datasets, PopQA (Mallen et al., 2023), TriviaQAunfiltered (Joshi et al., 2017) and Reasoning Challenge (ARC-Challenge) (Clark et al., 2018). The RAG system needs to answer arbitrary questions about factual knowledge for PopQA and TriviaQA, and multiple-choice questions task about scientific event that occurs in daily life for ARC Challenge. For PopQA, we utilize long-tail subset, consisting of 1,399 rare entity queries with monthly Wikipedia page views of less than 100. The ARC Challenge consists of 1,172 QA samples. For TriviaQA, we employ the established validation and test split methodology adopted in prior research (Asai et al., 2023) (Min et al., 2019) (Guu et al., 2020), using 11,313 test queries for evaluation. We employ Contriever-MS MARCO (Izacard et al., 2021) as our retriever and retrieve ten documents for each query by default. Multi-hop QA Dataset includes HotpotQA-devdistractor (Yang et al., 2018) dataset with 7,405 samples, and 2Wiki-dev (Ho et al., 2020) dataset 365with 12,576 samples. In contrast to the singlehop datasets, where document chunks are retrieved using our default retriever, we directly utilize the relevant content provided by these datasets. 5.2 Advanced RAG and Concurrent Work We compare Refiner -augmented LMs with other advanced RAG solutions, including Self-RAG (Asai et al., 2023) and RQ-RAG (Chan et al., 2024). Additionally, we assess the performance of concurrent compressors, including RECOMP abstractive compressor (Xu et al., 2023) and LongLLMLingua (Jiang et al., 2023a). Hyperparameter settings are detailed in the subsection C.1 and subsection C.2. To facilitate a fair comparison, we maintain consistency with the experimental frameworks, utilizing the same retriever framework and downstream LMs as those used in Refiner . 6 Results 6.1 Refiner consistently outperforms other approaches on multi-hop QA task As shown in Table 1, our Refiner significantly outperforms existing advanced RAG variants and concurrent compressors on multi-hop QA tasks, while demonstrating competitive performance on singlehop QA tasks. Compared to the next best solutions, ourRefiner -augmented system registers a performance margin of 7.0% on HotpotQA task, and 6.4% on 2WikiMultihop task. Furthermore, the Refiner-augmented LMs bridges the performance gap between small-scale and extreme-scale LLM on TriviaQA and HotpotQA tasks, surpasses GPT-3.5Turbo model on PopQA, HotpotQA and 2WikiMultihop task, highlighting the substantial contribution of our Refiner component to RAG. As anticipated, the performance gains fall short on single-hop tasks compared to LongLLMLingua, revealing that the structured output that distinguishes information disparity produced by Refiner has a less pronounced impact on single-hop tasks than on multi-hop tasks. 6.2 Refiner ’s output is highly utilized by LMs To quantify the efficiency among compressors, we compare their compression rates by dividing the average number of output tokens generated by the compressors by the average number of input tokens in the document chunks, then subtracting the value from one. (see Table 2). We use Llama2-7B Chat tokenizer throughout the experiment. The results reveal that the RECOMP abstractive compressor 5\nSingle-Hop Multi-Hop\nPOPQA TQA ARC-C HOTPOT 2WIKI\nLM (acc) (acc) (acc) (acc) (acc)\nProprietary LLM\nGPT-3.5-Turbo ∗ 29.3 74.3 75.3 57.3 54.3\nAdvanced RAG\nSelf-RAG 7B∗ 54.9 66.4 47.3 41.6 39.1\nSelf-RAG 13B∗ 55.8 69.3 53.1 47.5 45.7\nRQ-RAG ∗ 57.1 - 57.4 62.6 44.8\nWith Downstream LM: Llama2-Chat 7B\nNo Compressor 55.4 67.5 50.5 60.7 57.9\nRECOMP abstractive 52.6 65.0 18.7 47.3 58.5\nLongLLMLingua 60.3 70.5 51.6 61.3 55.3\nRefiner 59.3 68.9 51.8 65.1 61.0\nWith Downstream LM: Llama2-Chat 13B\nNo Compressor 54.8 71.8 57.3 63.8 58.4\nRECOMP abstractive 53.4 65.8 49.2 49.2 58.7\nLongLLMLingua 58.3 71.6 57.8 60.1 56.2\nRefiner 59.4 70.6 55.1 67.1 61.6\nWith Downstream LM: Meta-Llama3-Instruct 8B\nNo Compressor 64.2 74.4 77.1 70.4 67.3\nRECOMP abstractive 50.1 63.3 48.0 46.2 56.1\nLongLLMLingua 59.3 71.2 72.0 64.9 54.2\nRefiner 59.7 69.3 71.1 67.2 63.7\nTable 1: Accuracy on single-hop and multi-hop tasks. We report results of proprietary LLM, advanced RAG that\nworks without Refiner , compared to off-the-shelf Llama2-(7,13)B-Chat and Meta-Llama3-Instruct-8B augmented by\nRefiner .Bold numbers indicate the best performance among compressors. * indicates results reported by concurrent\nwork. - indicates the corresponding task is not reported by the original paper.\nRECOMP LongLLMLingua Refiner\nTask (avg #token) (avg #token) (avg #token)\nPOPQA 21.0 764.6 143.1\nTQA 144.1 1,049.0 206.7\nARC-C 20.7 1,022.2 280.2\nHOTPOT 28.0 757.9 130.0\n2WIKI 64.6 578.6 131.2\nCompress Rate 96.6% 48.9% 89.1%\nTable 2: Average number of output tokens and compress\nrates among tasks and compressors.\nachieves the highest compression rate, at 96.6%, followed by Refiner , which compresses 89.1% of input tokens. LongLLMLingua’s compression rate, set as a hyperparameter to 50%, results in an actual compression rate of approximately 48.9%. OurRefiner achieves a comparable performance to LongLLMLingua in single-hop QA tasks, with providing only 22.2% of the average number of tokens, and surpasses it in multi-hop QA tasks with 19.5% of tokens, demonstrating high utilization of Refiner ’s output by downstream LMs, attributable to its structured and context-completed output. 6.3 Refiner -augmented system is less susceptible to noisy and lengthy content Recent study (Xu et al., 2023) reveals that appending irrelevant documents can negatively impact the performance of RAG, our Refiner has successfully 430helped downstream LMs to mitigate the limitation by significantly reducing prompt length while keeping the performance of downstream LMs. Notably, our experimental results show that even when five additional irrelevant document chunks are appended, the performance of downstream LMs remains steady. In fact, as illustrated in Table 3, the accuracy of downstream LMs in both single-hop and multi-hop tasks has actually experienced an upswing. This content length tolerance exhibited byRefiner -augmented system could alleviate the lost-in-the-middle (Wang et al., 2023) phenomenon, commonly observed in downstream LMs. Such tolerance can be attributed to our training data, which deliberately organizes input document chunks in random order, thereby forcing the model to search and extract key information from full context. 6.4 The Refiner is resilient to RAG system To conduct a thorough investigation of the effectiveness coherency of Refiner within different RAG settings, we undertook evaluation tasks using a range of Refiner -augmented LMs and varied top retrieval settings, as illustrated in Table 3. Notwithstanding the variations in content lengths and downstream LMs, the in-task performance exhibits its stability. As shown in Table 3, the standard deviation of in-task accuracy remains relatively constant, 6\nSingle-Hop Multi-Hop\nPOPQA TQA ARC-C Hotpot 2Wiki\nLM (acc) (acc) (acc) (acc) (acc)\nRefiner of top 5 document chunks\nRefiner +Llama2-Chat 7B 59.3 68.9 51.1 47.4 46.0\nRefiner +Llama2-Chat 13B 59.0 70.5 54.2 49.7 45.7\nRefiner +Llama2-Chat 70B 58.3 72.2 55.9 52.3 50.1\nRefiner +Meta-Llama3-Instruct 8B 58.3 69.1 71.8 49.0 48.0\nRefiner +Meta-Llama3-Instruct 70B 57.9 70.9 84.1 49.2 46.7\nSTD 0.57 1.36 14.06 1.77 1.80\nRefiner of top 10 document chunks\nRefiner +Llama2-Chat 7B 59.3 68.9 51.8 65.1 61.0\nRefiner +Llama2-Chat 13B 59.4 70.6 55.1 67.1 61.6\nRefiner +Llama2-Chat 70B 60.8 72.1 57.0 69.8 68.3\nRefiner +Meta-Llama3-Instruct 8B 59.7 69.3 71.1 67.2 63.7\nRefiner +Meta-Llama3-Instruct 70B 59.8 71.1 84.7 68.6 61.8\nSTD 0.61 1.31 13.75 1.77 2.98\nTable 3: Results of various downstream LMs in Meta Llama family augmented by Refiner with top 5 and top 10\nretrieved document chunks. A low standard deviation of accuracy score across downstream LMs in-task, indicating\nthat the Refiner is agnostic to both the downstream LM and the upstream retrieval content length.\ndemonstrating its ubiquitous effectiveness, with the exception of the ARC Challenge task. This is due to the retrieved content’s limited assistance with challenging questions, which necessitates downstream LMs to utilize intrinsic knowledge for deduction and extrapolation, eventually highlights the superiority of more sophisticated models. 7 Ablation Study and Analysis To assess the efficacy of the proposed structured output, an ablation study is implemented by eliminating the structural components from the Refiner ’s output, resulting in naively concatenating the extracted content without any structural organization. As illustrated in Table 4, we evaluate the impact of the removed structure on downstream tasks by testing models equipped with the expunged outputs, This includes using Llama2-(7B, 13B, 70B)-Chat, Meta-Llama3-(8B, 70B)-Instruct models, with both the top 5 and top 10 retrieval configurations. More detailed table from which data in Table 4 is derived, are presented in Table 12 and Table 13. Furthermore, we have conducted various revisions of structure format, the format and corresponding performance statistics are attached in Appendix E. 7.1 Task-Level Analysis Our analysis of the decay in accuracy resulting from the removal of sections reveals significant variations across tasks. As shown in Table 4, the mean accuracy decay calculated for the top 5 and top 10 retrieval blocks indicates that sections make a substantial contribution to accuracy in most tasks, with most accuracy decays being positive. One notable exception is the ARC Challenge task, which 490experiences a decline in accuracy when sections are added. Further investigation reveals that this phenomenon stems from the retrieval system’s propensity to retrieve documents related to incorrect options, leading to confusion for Refiner and a bias towards incorrect information. Encouragingly, the importance of sections varies significantly across tasks, with the overall trend remaining consistent between top retrieval settings. 7.2 Model-Level Analysis Another phenomenon has caught our attention: all downstream LMs under the top 10 retrieval setting have experienced a greater decay in accuracy compared to those under the top 5 retrieval settings. Our explanation is that Refiner tends to extract more content from the extra document chunks that are remotely correlated to the query, and separates them from those that are closely related using sections. Without these sections, downstream LMs struggle to disseminate crucial information from the lengthy contents, leading to a degraded accuracy. More detailed statistical data in the Table 4, are attached in the Appendix D. 7.3 How Verbatim is Refiner ’s Output? Like many other decoder-only models, Refiner is a probabilistic LM (Operationnelle et al., 2001) that sequentially generates the probability distribution of the next token given the preceding tokens. Although our objective is to train the model to produce output that selectively replicates the source content, there is always a possibility of non-conformity, wherein the model fails to reproduce the content verbatim from the given document 7\nSingle-Hop Multi-Hop\nPOPQA TQA ARC-C Hotpot 2Wiki\nLM (acc decay w/o. structure)\nTop 5 document chunks\nRefiner +Llama2-Chat 7B 1.6 2.4 -0.6 0.1 -0.3\nRefiner +Llama2-Chat 13B 0.5 1.1 -0.2 0.2 -1.5\nRefiner +Llama2-Chat 70B 0.3 0.8 -2.8 0.1 -0.1\nRefiner +Meta-Llama3-Instruct 8B 0.4 2.3 -2.6 -0.2 0.0\nRefiner +Meta-Llama3-Instruct 70B 0.3 2.0 -1.4 0.5 -0.1\nMEAN 0.6 1.7 -1.5 0.1 -0.3\nTop 10 document chunks\nRefiner +Llama2-Chat 7B 0.9 2.7 1.4 1.9 3.4\nRefiner +Llama2-Chat 13B 1.5 2.3 0.1 0.8 0.2\nRefiner +Llama2-Chat 70B 2.6 1.2 -2.8 0.8 0.2\nRefiner +Meta-Llama3 Instruct8B 1.9 2.1 -2.2 -0.2 0.1\nRefiner +Meta-Llama3 Instruct70B 1.3 1.7 0.3 0.3 0.0\nMEAN 1.6 2.0 -0.6 0.7 0.8\nTable 4: Ablation analysis on various downstream LMs in Meta Llama family augmented by Refiner with top 5 and\ntop 10 retrieved document chunks. Accuracy decay and task-level statistic are presented by removing section parts\nfrom Refiner ’s structured output.\nFigure 3: The ratio of contents extracted by Refiner that\nare exactly originate from respective document chunks\nunder various top retrieval settings and tasks.\nchunks. The ratio of authentic output generated by Refiner for each top retrieval setting and evaluation task dataset is calculated and illustrated in Figure 3. Overall, the model exhibits a range of conformity from 87.4% to 96.8%, whereupon we proceed to investigate its malformed output and discuss the underlying causes of this phenomenon. Temptation to Answering The major predicament that Refiner encounters, is its compulsion to answer to the query rather than extracting relevant information. This is often exacerbated when the query lacks challenge and the information in the document chunk directly answers to the query. Under such circumstances, the model is inclined to repeat the query and the information, rather than engaging in meaningful information extraction. 539Temptation to Extract from Query Additionally, Refiner is also prone to extracting informative content from the query, a problem that is particularly evident in the ARC-Challenge task. This task is characterized by a query comprising a question and four or more potential answers. The model is more inclined to extract content from these options, which, although potentially incorrect, are nonetheless relevant to the question at hand, thus tempting theRefiner model to extract. 8 Conclusion This work presents a novel paradigm for RetrievalAugmented Generation (RAG), introducing Refiner, a novel end-to-end paradigm that extracts and restructures document chunks, organizing query-relevant context-completed content into sections, thereby enhancing the performance of downstream language models. Thorough evaluations on five open-domain question-answering tasks conclusively demonstrate that Refiner outperforms stateof-the-art RAG solutions that employ more complex enhancements, rivaling and surpassing concurrent prompt compress approaches with a significantly higher token compress rate. Our Refiner has been tested in collaboration with various downstream language models and retrieval settings, proving to be resilient to the challenges posed by lengthy and noisy document chunks and consistently effective. Finally, Refiner is a plug-andplay solution that can be seamlessly integrated with modular RAG systems, facilitating its application across diverse open-source frameworks. 8\n9 Limitations This study seeks to enhance the quality of Large Language Model (LLM) outputs, a deficiency that continues to have significant real-world consequences, including spread of misinformation and provision of incorrect and dangerous advice. Notwithstanding our method and evaluation have demonstrated efficacy, they are conducted in a relatively indirect way, as how to evaluate the correctness of generated structural output from the model still requires further study. Moreover, our model is trained to extract and section information from open domain document chunks, its robustness has not been tested on alternative input structures, such as table data or domain-specific documents, which raises our concerns about potential misuse. Additionally, due to the nature of Refiner being a probabilistic model, there is always a probability of extracting content that does not exactly derive from the document chunks. Therefore, when using them, it is still necessary to exercise caution when distinguishing and verifying the output. 10 Broader Impact Current RAG system has been found to hinder LLMs from producing misleading (Kwon et al., 2023) or incorrect information (Yan et al., 2024). Although there exist concurrent studies that adopt extractive and summarize models to distill information from retrieved documents in post-retrieval process, none have considered the relatedness of document-wise information, potentially resulting in a loss of contextual information that sometimes critical for downstream LMs to arrive at a correct answer. We believe our Refiner is an important step to fill this gap. Furthermore, in contrast to the pre-retrieval and retrieval processes, the postretrieval process has not been fully explored (Gao et al., 2024) by academia and, in our opinion, holds significant potential. Therefore, we hope that our work will shed light on this process and inspire further research in this area. The Refiner dataset, code and model files will be released under the CC BY-NC 4.0 license. 614References Nathan Anderson, Caleb Wilson, and Stephen D. Richardson. 2022. Lingua: Addressing scenarios for live interpretation and automatic dubbing. In Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track) , pages 202–209, Orlando, USA. Association for Machine Translation in the Americas. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. Preprint , arXiv:2310.11511. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Preprint , arXiv:2005.14165. Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. Rq-rag: Learning to refine queries for retrieval augmented generation. Preprint , arXiv:2404.00610. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv , abs/1803.05457. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: A survey. Preprint , arXiv:2312.10997. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training. Preprint , arXiv:2002.08909. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics , pages 6609–6625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint , arXiv:2106.09685. Hanxu Hu, Pinzhen Chen, and Edoardo M. Ponti. 2024. Finetuning large language models with sequential instructions. Preprint , arXiv:2403.07794. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models. Preprint , arXiv:2208.03299. 9\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys , 55(12):1–38. Huiqiang Jiang, Qianhui Wu, , Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023a. LongLLMLingua: Accelerating and enhancing llms in long context scenarios via prompt compression. ArXiv preprint , abs/2310.06839. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b. LLMLingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 13358–13376. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Preprint , arXiv:1705.03551. Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. 2024. Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing. Preprint , arXiv:2305.16635. Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. Preprint , arXiv:2305.06984. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. Preprint , arXiv:2211.08411. Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. Preprint , arXiv:2004.04906. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. Preprint , arXiv:1911.00172. Neema Kotonya and Francesca Toni. 2020. Explainable automated fact-checking for public health claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7740–7754, Online. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Preprint , arXiv:2309.06180. Connor Lennox, Sumanta Kashyapi, and Laura Dietz. 2023. Retrieve-cluster-summarize: An alternative to end-to-end training for query-specific article generation. Preprint , arXiv:2310.12361. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledge-intensive nlp tasks. Preprint , arXiv:2005.11401. 734Guozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji, Ziyu Shang, Jiajun Liu, and Zijie Xu. 2024. Recall, retrieve and reason: Towards better in-context relation extraction. Preprint , arXiv:2404.17809. Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. 2020. Pytorch distributed: Experiences on accelerating data parallel training. Preprint , arXiv:2006.15704. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. Preprint , arXiv:1711.05101. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 9802– 9822, Toronto, Canada. Association for Computational Linguistics. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2851–2864, Hong Kong, China. Association for Computational Linguistics. Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2024. Generating benchmarks for factuality evaluation of language models. Preprint , arXiv:2307.06908. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, 10\nŁukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea V oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. Preprint , arXiv:2303.08774. Departement Operationnelle, Y . Bengio, R Ducharme, Pascal Vincent, and Centre Mathematiques. 2001. A neural probabilistic language model. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint , arXiv:2203.02155. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. 2024. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. ArXiv preprint , abs/2403.12968. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , SC ’20. IEEE Press. 866Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. Preprint , arXiv:2110.08207. Hao Sun. 2024. Supervised fine-tuning as inverse reinforcement learning. Preprint , arXiv:2403.12017. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instructionfollowing llama model. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint , arXiv:2307.09288. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context for retrieval-augmented generation. Preprint , arXiv:2311.08377. Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. Preprint , arXiv:2403.05313. Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le. 2022. Finetuned language models are zero-shot learners. Preprint , arXiv:2109.01652. Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, ThuyTrang Vu, and Gholamreza Haffari. 2024. Continual learning for large language models: A survey. Preprint , arXiv:2402.01364. Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. Preprint , arXiv:2310.04408. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. Preprint , arXiv:2401.15884. 11\nHaoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023. PRCA: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 5364–5375, Singapore. Association for Computational Linguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Preprint , arXiv:2305.10601. Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2024. Making retrieval-augmented language models robust to irrelevant context. Preprint , arXiv:2310.01558. Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024. Raft: Adapting language model to domain specific rag. Preprint , arXiv:2403.10131. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren’s song in the ai ocean: A survey on hallucination in large language models. Preprint , arXiv:2309.01219. Sizhe Zhou, Yu Meng, Bowen Jin, and Jiawei Han. 2024. Grasping the essentials: Tailoring large language models for zero-shot relation extraction. Preprint , arXiv:2402.11142. Konrad Zuchniak. 2023. Multi-teacher knowledge distillation as an effective method for compressing ensembles of neural networks. Preprint , arXiv:2302.07215. 12\nPart I Appendix Table of ContentsA Model Cards 14 B Training Details 15 B.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.2 Teacher Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.3 Structure Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.4 Environment and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 B.5 Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 C Evaluation Details 18 C.1 Hyperparameters for Adavanced RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.2 Hyperparameters for Concurrent Compressor . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.3 Environment and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.4 Prompt Template for Refiner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.5 Prompt Template for Downstream LM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.6 Evaluation Metric for ARC Challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D Ablation Analysis Details 22 E More Analysis on Output Structure 24 F Case Study 26 98898913\nA Model Cards Below illustrates models inclusive in our work, all usages of models are under the compliance of their intended use. Model Name Description\nGPT-3.5-TurboDeveloped by OpenAI, GPT-3.5-Turbo, or ChatGPT, is an advanced NLP model in the GPT\nseries, featuring enhanced context understanding and text generation capabilities. Trained on\na vast array of internet text data, it excels in tasks such as text generation, question answering,\ntranslation, and programming assistance, finding use in sectors like customer service, content\ncreation, and education (URL: https://openai.com/chatgpt ).\nMeta-\nLlama3-\nInstruct\n(8B, 70B )Meta-Llama3-Instruct, a cutting-edge language model designed to excel in understanding and\nresponding to natural language instructions. This innovative AI model is capable of compre-\nhending complex directives and generating human-like responses that are both accurate and\ninformative. By leveraging advanced machine learning algorithms and large-scale training\ndatasets, Meta-Llama3-Instruct has achieved unprecedented levels of language understanding\nand generation capabilities, making it an invaluable tool for a wide range of applications.\nLlama2-Chat\n(7, 13, 70B)Llama2-Chat (Touvron et al., 2023), developed and publicly released by Meta, is a refined\nversion of LLMs, optimized specifically for conversational purposes. The Llama2-Chat belongs\nto the Llama 2 family, a series of pre-trained and fine-tuned generative text models with 7 to 70\nbillion parameters. Being a fine-tuned LLM, Llama2-Chat excels in dialogue scenarios.\nAlpaca\n(7B)Alpaca (Taori et al., 2023) is a language model that has been fine-tuned from Meta’s LLaMA 7B\nmodel for instruction-following tasks. Using 52,000 self-instruct style demonstrations, it was\ntrained with OpenAI’s text-davinci-003 model for instruction-following tasks. In evaluations,\nAlpaca has shown behaviors akin to OpenAI’s text-davinci-003. Notably, Alpaca is distinguished\nby its small size, ease of replication, and low cost, making it an efficient, accessible model.\nRECOMP\nAbstractive\nCompressorRECOMP (Xu et al., 2023) Abstractive Compressor is a part of the RECOMP (Retrieve, Com-\npress, Prepend) method, which involving retrieving documents and compressing them into textual\nsummaries before they are integrated into the context for the language model. This compression\nnot only reduces computational costs but also makes it easier for the language model to identify\nrelevant information in long retrieved documents. The RECOMP Abstractive Compressor is one\nof the two types of compressors used in this method, which generates summaries by synthesizing\ninformation from multiple documents. This allows the compressor to create a concise summary\nthat contains the key information from the retrieved documents. The Abstractive Compressor\nis trained to improve the performance of the language model on end tasks when the generated\nsummaries are prepended to the language model’s input, while keeping the summary concise.\nIf the retrieved documents are irrelevant to the input or offer no additional information to the\nlanguage model, the compressor can return an empty string.\nLongLLMLinguaLongLLMLingua (Jiang et al., 2023a) is a method designed to compress prompts, thereby en-\nhances the performance of LLMs, reduce computational and financial costs, and decrease latency.\nBy compressing the prompt on token level, it reduces costs and boosts efficiency, improving\nperformance by up to 21.4% using only 1/4 of the tokens. Built upon LLMLingua (Jiang et al.,\n2023b), the contributions of this paradigm are: (1) a question-aware coarse-to-fine compression\nmethod to enhance key information density in the prompt; (2) a document reordering mechanism\nto minimize information loss; (3) dynamic compression ratios to integrate coarse-grained and\nfine-grained compression for adaptive granular control; (4) a post-compression subsequence\nrecovery strategy to improve key information integrity.\nSelf-RAGSelf-Reflective Retrieval-Augmented Generation, or Self-RAG (Asai et al., 2023), is a framework\ndesigned to enhance the capabilities of LLMs. It does this by integrating retrieval and self-\ncritique mechanisms into the model’s generation process. The framework trains a single arbitrary\nlanguage model to adaptively retrieve passages on-demand, generate text, and reflect on retrieved\npassages and its own generations using special tokens, called reflection tokens. Reflection tokens\nare categorized into retrieval and critique tokens. Retrieval tokens indicate the need for retrieval,\nwhile critique tokens assess the quality of the generation1. Generating reflection tokens makes\nthe language model controllable during the inference phase, enabling it to tailor its behavior to\ndiverse task requirements.\nRQ-RAGFollowing Self-RAG, RQ-RAG, or Learning to Refine Queries for Retrieval Augmented Genera-\ntion (Chan et al., 2024), is a paradigm to refine queries with capabilities for explicit rewriting,\ndecomposition, and disambiguation. By regenerating responses based on search results from\nexternal search engine, RQ-RAG surpasses the previous state-of-the-art by an average of 1.9%\nacross three single-hop QA datasets, and also demonstrates enhanced performance in handling\ncomplex, multi-hop QA datasets.\nTable 5: Model Cards.\n14\nB Training Details B.1 Dataset Single-hop QA Dataset consists of ARC Challenge (Clark et al., 2018) train, a set of natural, gradeschool science questions, PubHealth (Kotonya and Toni, 2020) train, a fact-checking for claims provided contents that support or refute to the claims, and TriviaQA (Joshi et al., 2017) train, a challenging reading comprehension QA dataset. Since ARC Challenge train and TriviaQA train dataset do not contain document candidates, we use off-the-shelf Contriever-MS MARCO to retrieve top document chunks from 12020 English Wikipedia using official Wikipedia embeddings. 1Multi-hop QA Dataset includes HotpotQA (Yang et al., 2018) train dataset. The dataset contains 90447 1pairs of QA and a set of supporting contents randomly mingled with irrelevant or misleading contents. 1We further clean datasets by removing answers that are absent in the contents or in the outputs of 1teacher models. Please refer to Table 7 for dataset statistics. 1B.2 Teacher Models 1We employ five teacher models to generate the proposed structured output for each query-document pair, 1including Llama2-70B-Chat (Touvron et al., 2023), Meta-Llama3-(8B,70B)-Instruct and Alpaca-7B (Taori 1et al., 2023). We design four prompts to tackle with diverse QA tasks. For each output generated by each 1teacher model, we utilize regular expressions to parse and resolve sections, titles, and contents. We then 1exclude any contents that are not present in the provided document chunks. The remaining contents are 1subject to a voting process among the teacher models, whereby only those contents that receive a majority 1vote from the teacher models are retained. Finally, the filtered contents are concatenated to formulate the 1final training dataset. The detailed task-specific prompts are illustrated in Table 8, and hyperparameters of 1the teacher models are listed in Table 6. 1Hyperparameters Meta-Llama3-70B-Instruct Meta-Llama3-8B-Instruct\ntemperature None None\nrepetition-penalty 1.0 1.0\nmax-length 2048 2do-sample False False\nHyperparameters Llama2-70B-Chat Llama2-13B-Chat Alpaca-7B\ntemperature None None None\nrepetition-penalty 1.0 1.0 1.0\nmax-length 2048 2048 2do-sample False False False\nTable 6: Hyper-parameters of teacher models.\nB.3 Structure Correction 1Once outputs are generated by teacher models, we make a series of cleaning and filtering procedures so 1as to satisfy the proposed principles. Firstly, we filter out contents that are absent in retrieved document 1chunks, store legitimate sections voted by teacher models, then remove sentences that are vetoed by 1teacher models. Finally, the selected sections are unified by re-aligning section numbers, titles and 1extractive contents to form the Refiner ’s training data as illustrated in Figure 4. 1B.4 Environment and Hyperparameters 1We use 32GB memory and 4 Nvidia A100 with 80GB memory to train our model. We adopt distributed 1data parallel (Li et al., 2020) and Deepspeed stage 3 (Rajbhandari et al., 2020) for multi-GPU distributed 1training. We apply parameter-efficient funetuning with LoRA (Hu et al., 2021) rank of 64, alpha of 16 on 1Llama2-7B-Chat. AdamW (Loshchilov and Hutter, 2019) is adopted as optimizer. All seeds are set to 1633. The total train steps reaches 6 hours in the specified environment. 115\nInput: Given nTeacher LM {Mj}n\n1,Refiner Mθ, query set {qi}N\n1, where qrepresents the user’s query, Rdenotes\nretrieval system by default, Cdenotes corpus set, Dis a set of corresponding retrieved documents such that D ∈C,Vis a\ncollection of parsed contents in sentence level generated by teacher LM. Tdenotes temporary content collection.\nOutput: An updated Refiner Mθ\n1:S ← ∅\n2:fori∈ {1, . . . , N }do\n3:Di=TopN (R(qi, C))\n4: forj∈ {1, . . . , n }do\n5: sj=Generate (Mj,[qi,Di])\n6: V=ParseGeneratedContent (sj)\n7: for content ∈Vdo\n8: if content ̸∈Dithen\n9: V.remove (content )\n10: T ← ∅\n11: c=Counter (V)\n12: for content ,count∈cdo\n13: // Iterate over result from counter to get\n// number of occurrence for each content\n14: if count > n/ 2then\n15: content =assignSection (content )\n16: T ← T ∪ content\n17: Yi←Concatenate (T)\n18: S←S∪[qi,Di;Yi]\n19:Mθ=Finetune (Mθ, S)\nFigure 4: Prepare curated train dataset for Refiner .\nB.5 Training Process 1As shown in Table 7, the cleaned datasets are highly imbalanced in number of samples. Thus, in order to 1train the model effectively, we practice progressive training: First, we form training dataset with TriviaQA 1and HotpotQA dataset combined, the model is trained for 3 epochs with a batch size of 128, a peak 1learning rate of 4e-5 with 3% warmup steps, and linear decay afterward. Subsequently, we continue 1training the model combining Arc Challenge and PubHealth datasets, with 5 epoch and a batch size of 1128, a peak learning rate of 2e-5 with 5% warmup steps. 1Dataset Initial Cleaned\nArc Challenge 1119 1PubHealth 9513 9TriviaQA 61888 56HotpotQA 90447 79Table 7: Training data statistics.\n16\nDataset Prompts\nTriviaQA\n&\nHotpotQASystem : You are an expert research assistant. Your job is to find the quotes\nfrom the markdown documents that are relevant to a question. \\nPlease mark\nquotes with sections and titles of documents to group quotes by different\ninformation only from relevant documents. \\nRule of labelling sections: if the\nfact of the first dummy quote \"aaa\" from a document with title \"## AAA\" is\nconsistent with that of the second dummy quote \"bbb\" from a document with\ntitle \"## BBB\", and they contradict to that of the third dummy quote \"ccc\" from\na document with title \"## CCC\", then label them as: \\n“‘\\n1.1. ## AAA \\n\naaa\\n\\n 1.2. ## BBB \\nbbb\\n\\n 2.1. ## CCC \\nccc\\n\\n “‘\\nQuotes should\nbe verbatim and context completed. \\nPlease respond without any explanation.\nUser : Question: { query }\\nDocument (multiple documents are sepa-\nrated by \"—\"): \\n{context }\nARC\nChal-\nlengeSystem : You are an expert research assistant. Your job is to find the quotes\nfrom the markdown documents that are relevant to a question. \\nPlease mark\nquotes with sections and titles of documents to group quotes by different\ninformation only from relevant documents that either support or contradict\nto the question. \\nRule of labelling sections: if the fact of the first dummy\nquote \"aaa\" from a document with title \"## AAA\" is consistent with that of the\nsecond dummy quote \"bbb\" from a document with title \"## BBB\", and they\ncontradict to that of the third dummy quote \"ccc\" from a document with title\n\"## CCC\", then label them as: \\n“‘\\n1.1. ## AAA \\naaa\\n\\n 1.2. ## BBB \\n\nbbb\\n\\n 2.1. ## CCC \\nccc\\n\\n “‘\\nQuotes should be verbatim and context\ncompleted. \\nPlease respond without any explanation.\nUser : Question: { query }\\nDocument (multiple documents are sepa-\nrated by \"—\"): \\n{context }\nPubHealth System : You are an expert research assistant. Your job is to find the quotes from\nthe markdown documents that either support or contradict to a statement. \\n\nPlease mark quotes with sections and titles of documents to group quotes\nby different information only from relevant documents that are helpful with\nanswering to the statement. \\nRule of labelling sections: if the fact of the first\ndummy quote \"aaa\" from a document with title \"## AAA\" is consistent with\nthat of the second dummy quote \"bbb\" from a document with title \"## BBB\",\nand they contradict to that of the third dummy quote \"ccc\" from a document\nwith title \"## CCC\", then label them as: \\n“‘\\n1.1. ## AAA \\naaa\\n\\n 1.2. ##\nBBB \\nbbb\\n\\n 2.1. ## CCC \\nccc\\n\\n “‘\\nQuotes should be verbatim and\ncontext completed. \\nPlease respond without any explanation.\nUser : Statement: { query }\\nDocument (multiple documents are sepa-\nrated by \"—\"): \\n{context }\nTable 8: Prompts used to generated structured outputs from teacher models { query } and { context } represent the\nactual input query and retrieved documents.\n17\nC Evaluation Details 1C.1 Hyperparameters for Adavanced RAG 1For Self-RAG, we adopt the default inference settings. Specifically, we set the weight terms IsRel ,IsSup , 1IsUse values of 1.0, 1.0 and 0.5, respectively. The retrieval threshold is set to 0.2 to encourage frequent 1retrieval. 1C.2 Hyperparameters for Concurrent Compressor 1We assess the performance of concurrent compressors encompassing RECOMP abstractive compres- 1sor (Xu et al., 2023) and LongLLMLingua (Jiang et al., 2023a). Notably, the RECOMP abstractive 1compressor has two versions, one trained on TriviaQA dataset and the other on HotpotQA dataset. We 1evaluate the former version on the PopQA, TriviaQA, and ARC Challenge datasets, while the latter 1version was tested on the HotpotQA and 2WikiMultihop datasets. Regarding the hyperparameters of 1LongLLMLingua, we utilized a default compression rate of 0.5 and a dynamic context compression ratio 1of 0.3. 1C.3 Environment and Hyperparameters 1We use 32GB memory and 4 Nvidia A100 with 80GB memory for inference. we adopt vllm (Kwon et al., 12023) to speed up inference. All models, including downstream LMs and Refiner , use greedy algorithm to 1sample their next tokens, and max new tokens is set to 2048. 1C.4 Prompt Template for Refiner 1To efficiently train Refiner and accelerate convergence speed, we revise its base model, Llama2-Chat’s 1chat template, which is illustrated in Table 9. 1Prompt Template\nforRefiner[INST]«SYS»[MONITOR]{ documents }«/SYS»{ query }[/INST]\nPrompt Template\nfor { documents }## {title }\n{content }\n## {title }\n{content }\n## {title }\n{content }\n...\nTable 9: Prompt template for refiner where { query } represents the actual user input query, { documents } represents\ndocument chunks retrieved for extraction, { content } represents content in document and { title } represents title\nof the document from which the content derives.\nC.5 Prompt Template for Downstream LM 1To simulate real-world application of Refiner , we prompt downstream LMs to work with Refiner ’s output 1as illustrated in the Table 10. 118\nPrompt\nSystem : You are an AI assistant backboned by selective content from different\ndocuments, answer user’s question helpfully and precisely, with the guidance of the\nfollowing steps:\n* If there are no content provided: determine whether it is still possible to answer\nprecisely to the question.\n* If is possible, offer a helpful answer. Otherwise, offer the reason of impossibility.\n* If there exists contents: determine whether the necessary information to an-\nswer the question is either directly mentioned or can be inferred from the documents.\n* When there exists different information that can answer to the question, determine\nwhether it is the question too opaque that causes the problem.\n* If not, answer with a summarized information. Otherwise, also provide advice or\nask question to disambiguate.\n* When summarizing, ensure to include contents that are relevant to the question.\nHere is the content:\n{refiner }\nUser : {query }\nTable 10: Prompts used to generated structured outputs from teacher models { query } represent the actual input\nquery, and { refiner } represent the actual generated output from Refiner .\n19\nC.6 Evaluation Metric for ARC Challenge 1C.6.1 Motivation 1To deliver our motivation to revising evaluation metric for ARC Challenge task, we provide a case from 1the ARC Challenge test subset.In this case, the model is prompted to answer question based on the A, 1B, C and D choices given, while through reading the context we can easily find out that the model is 1answering with the choice B. However, the generated content contains all of the four choices, as previous 1work does not take such cases into account, the performance of the model could be over-evaluated. 1Method Case in Arc Challenge test subset Result Comment\nPrevious\nWorkQuestion : The morning temperature in a\ncity is 41°F. If a sunny, mild day is\nforecast, which temperature is most likely\nfor 2:00 p.m.?\nA. 32° F\nB. 41° F\nC. 78° F\nD. 98° F\nGround Truth : C\nLLM outputs : Based on the given\ninformation, the best answer choice is (B)\n41°F.\nThe given information states that the\nmorning temperature is 41°F, which\nsuggests that the day will be sunny and\nmild. As the temperature drops gradually\nthroughout the day, it is unlikely that the\ntemperature at 2:00 p.m. will be above\n78°F or 98°F, as these temperatures are\nmuch higher than the morning temperature.\nTherefore, the most likely temperature for\n2:00 p.m. is 41 °F, which is the same as the\nmorning temperature.\nAnswer choice (A) 32 °F is too low, as it is\nbelow the morning temperature and\nunlikely to be the temperature at 2:00 p.m.\nin a sunny and mild day. Answer choice\n(C) 78°F is also unlikely, as it is higher\nthan the morning temperature and may not\nbe reached in the afternoon. Answer\nchoice (D) 98 °F is the highest temperature\nmentioned in the passage and is therefore\nunlikely to be the temperature at 2:00 p.m.Matches ground\ntruth C in generated\nsentence: \"Answer\nchoice (C) 78°F is\nalso unlikely\"Wrong\nevaluation\nresult\nOursGround truth C\nmismatches with\npredicted B in\nsentence : \"Based on\nthe given information,\nthe best answer choice\nis (B) 41°F.\"Correct\nevaluation\nresult\nTable 11: The result of regular-expression-based accuracy compared to previous work.\n20\nC.6.2 Code Comparison 1For a concise convey of our evaluation metric, we disclose previous work and our pseudo-python code in 1the following list. 111 # Previous Methods 12 def match(prediction: str, ground_truth: list[str]): 13 # Argument 'prediction 'represents generated output from downstream LM, 14 # and 'ground_truth 'represents a list of answer key strings. 15 # For example, if the ground truth choice for a question is A, 16 # then [ 'A'] will be passed to ground_truth. 17 18 for gt in ground truth: 19 if gt in prediction: 110 return 1 111 112 return 0 1080111 # Our Methods 12 import re 13 14 15 def arc_c_acc(prediction: str, ground_truth: list[str]): 16 # The presence of 'is'preceding the letter 'A'to'E' 17 # is a necessary condition for a match to occur. 18 regex_answer_key = re.compile(r\"is[^\\w]+([A-E])[^\\w]+\", re.DOTALL) 19 110 # re.search only looks for the first match occurence. 111 if len(ground_truth) == 0: 112 return re.search(regex_answer_key, prediction) is None 113 114 lst_res = [] 115 for key in ground_truth: 116 search = re.search(regex_answer_key, prediction) 117 if search is None: 118 lst_res.append(False) 119 continue 120 121 if key.lower() == search.groups()[0].lower(): 122 lst_res.append(True) 123 else: 124 lst_res.append(False) 125 126 # only when all ground truths are matched should we return True. 127 return all(lst_res) 1109121\nD Ablation Analysis Details 1We present a comprehensive disclosure of all ablation analysis results, with specific sections omitted from 1theRefiner ’s output. The accuracy outcomes under the top 5 retrieval setting are detailed in Table 12, 1while the accuracy outcomes under the top 10 retrieval setting are presented in Table 13. 1Task Refiner acc. w/o. Section acc. Decay\nLlama2-7B-Chat\nPopQA 59.3 57.7 1.6\nTriviaQA 68.9 66.5 2.4\nARC Challenge 51.1 51.7 -0.6\nHotpotQA 47.4 47.3 0.1\n2WikiMultihop 46.0 46.3 -0.2\nLlama2-13B-Chat\nPopQA 59.0 58.5 0.5\nTriviaQA 70.5 69.4 1.1\nARC Challenge 54.2 54.4 -0.2\nHotpotQA 49.7 49.5 0.2\n2WikiMultihop 49.7 47.0 -1.3\nLlama2-70B-Chat\nPopQA 58.3 58.0 0.3\nTriviaQA 72.2 71.4 0.8\nARC Challenge 55.9 58.7 -2.8\nHotpotQA 52.3 52.2 0.1\n2WikiMultihop 50.1 50.2 -0.1\nMeta-Llama3-8B-Instruct\nPopQA 58.3 57.9 0.4\nTriviaQA 69.1 66.8 2.3\nARC Challenge 71.8 74.4 -2.6\nHotpotQA 49.0 49.2 -0.2\n2WikiMultihop 48.0 48.0 0.0\nMeta-Llama3-70B-Instruct\nPopQA 57.9 57.6 0.3\nTriviaQA 70.9 68.9 2.0\nARC Challenge 84.1 85.5 -1.4\nHotpotQA 49.2 48.7 0.5\n2WikiMultihop 46.7 46.8 -0.1\nTable 12: Refiner -augmented model accuracy under top 5 retrieval setting.\n22\nTask Refiner acc. w/o. Section acc. Decay\nLlama2-7B-Chat\nPopQA 59.3 58.4 0.9\nTriviaQA 68.9 66.2 2.7\nARC Challenge 51.8 50.4 1.4\nHotpotQA 65.1 63.2 1.9\n2WikiMultihop 61.0 57.6 3.4\nLlama2-13B-Chat\nPopQA 59.4 57.8 1.5\nTriviaQA 70.6 68.3 2.3\nARC Challenge 55.1 55.0 0.1\nHotpotQA 67.1 66.3 0.8\n2WikiMultihop 61.6 61.4 0.2\nLlama2-70B-Chat\nPopQA 60.8 58.2 2.6\nTriviaQA 72.1 70.9 1.2\nARC Challenge 57.0 59.8 -2.8\nHotpotQA 69.8 69.0 0.8\n2WikiMultihop 68.3 68.1 0.2\nMeta-Llama3-8B-Instruct\nPopQA 59.7 57.8 1.9\nTriviaQA 69.3 67.2 2.1\nARC Challenge 71.1 73.3 -2.2\nHotpotQA 67.2 67.4 -0.2\n2WikiMultihop 63.7 63.6 0.1\nMeta-Llama3-70B-Instruct\nPopQA 59.8 58.5 1.3\nTriviaQA 71.1 69.4 1.7\nARC Challenge 84.7 84.4 0.3\nHotpotQA 68.6 68.3 0.3\n2WikiMultihop 61.8 61.8 0.0\nTable 13: Refiner -augmented model accuracy under top 10 retrieval setting.\n23\nE More Analysis on Output Structure 1Our Refiner comes with a structured output, this allows for seamless dissemble of section, titles and 1contents (see Figure 5) using regular expressions, enables us to easily convert them into numerous 1alternative structures. In this context, we will explore a total of 16 additional different structure options, 1as illustrated in Table 14. 1Figure 5: The structure of output generated by Refiner consists of three parts: section, Title and content, whereSection\nelucidates relatedness information among extracted contents, Title represents the Title of the Original document,\ncontent represents the question-relevant content extracted from that document chunk.\nContent Structure Section Structure Title Structure Template\nOriginal 1.1.[Title ]\\n[Content ]\nHierarchy Markdown 1.1. ## [Title ]\\n[Content ]\nQuote 1.1.”[Title ]”\\n[Content ]\nOriginal 1.[Title ]\\n[Content ]\nOriginal Numbered Markdown 1. ## [Title ]\\n[Content ]\nQuote 1.”[Title ]”\\n[Content ]\nOriginal *[Title ]\\n[Content ]\nStar Markdown * ##[Title ]\\n[Content ]\nQuote *”[Title ]”\\n[Content ]\nOriginal 1.1.[Title ]\\n”[Content ]”\nHierarchy Markdown 1.1. ## [Title ]\\n”[Content ]”\nQuote 1.1.”[Title ]”\\n”[Content ]”\nOriginal 1.[Title ]\\n”[Content ]”\nQuote Numbered Markdown 1. ## [Title ]\\n”[Content ]”\nQuote 1.”[Title ]”\\n”[Content ]”\nOriginal *[Title ]\\n”[Content ]”\nStar Markdown * ##[Title ]\\n”[Content ]”\nQuote *”[Title ]”\\n”[Content ]”\nTable 14: Several restructure templates are utilized to assess the impact on the performance of downstream LMs. In\nthese templates, [Title ]denotes the actual Title of the Original document, while [Content ]represents the actual\ncontent extracted from the corresponding document chunk. A comparative analysis is conducted between the\nOriginal hierarchicalSection structure and two alternative structures: a Numbered edSection structure and a Star\nSection structure. Furthermore, the Original Title is compared in its Original form and when formatted using\nmarkdown or quotation marks, while the Original content is compared in its Original form and when decorated with\nquotation marks.\n24\nOutput StructurePOPQA TQA ARC-C\nmean std mean std mean std\nOriginal Content\nNumbered Section Markdown Title 1.0% 0.9% 1.7% 0.5% -0.8% 2.5%\nNumbered Section Original Title 1.2% 0.8% 1.9% 0.5% -0.1% 1.4%\nNumbered Section Quote Title 1.1% 0.8% 1.9% 0.6% -0.4% 2.2%\nHierarchy Section Markdown Title 1.1% 0.7% 1.7% 0.5% -1.2% 2.6%\nHierarchy Section Quote Title 1.0% 0.9% 1.8% 0.7% -0.5% 2.8%\nStar Section Markdown Title 1.4% 0.7% 1.7% 0.6% -0.9% 2.5%\nStar Section Original Title 1.2% 0.7% 1.9% 0.5% 0.1% 1.4%\nStar Section Quote Title 0.9% 0.9% 1.9% 0.7% -0.5% 1.9%\nQuote Content\nNumbered Section Markdown Title 1.3% 0.7% 1.7% 0.6% -0.9% 2.6%\nNumbered Section Original Title 1.2% 0.6% 1.8% 0.5% -0.3% 2.0%\nNumbered Section Quote Title 1.4% 0.7% 1.8% 0.6% -0.9% 2.9%\nHierarchy Section Markdown Title 1.3% 0.7% 1.6% 0.7% -1.4% 3.2%\nHierarchy Section Quote Title 1.1% 0.7% 1.7% 0.7% -0.5% 2.9%\nStar Section Markdown Title 1.2% 0.6% 1.7% 0.6% -1.2% 1.9%\nStar Section Original Title 1.2% 0.6% 1.8% 0.6% -0.6% 1.7%\nStar Section Quote Title 1.1% 0.7% 1.8% 0.7% -0.6% 2.4%\nTable 15: Statistics of improved accuracy margin from various downstream LMs augmented by restructured outputs\nfrom Refiner , compared with no structured output in single-hop tasks.\nOutput StructureHOTPOT 2WIKI\nmean std mean std\nOriginal Content\nNumbered Section Markdown Title 0.5% 0.8% -0.5% 1.2%\nNumbered Section Original Title 0.5% 0.8% -0.3% 1.2%\nNumbered Section Quote Title 0.7% 1.1% -0.1% 1.5%\nHierarchy Section Markdown Title 0.5% 0.9% -0.4% 1.6%\nHierarchy Section Quote Title 0.7% 0.9% -0.3% 1.5%\nStar Section Markdown Title 0.6% 0.7% -0.2% 1.4%\nStar Section Original Title 0.7% 0.8% -0.1% 1.4%\nStar Section Quote Title 0.7% 1.0% 0.2% 1.5%\nQuote Content\nNumbered Section Markdown Title 0.9% 1.2% 0.0% 1.5%\nNumbered Section Original Title 1.0% 1.2% 0.1% 1.5%\nNumbered Section Quote Title 0.9% 1.3% 0.5% 2.0%\nHierarchy Section Markdown Title 0.7% 1.2% -0.2% 1.8%\nHierarchy Section Quote Title 1.0% 1.4% 0.2% 1.7%\nStar Section Markdown Title 0.8% 1.2% 0.2% 1.8%\nStar Section Original Title 1.1% 1.2% 0.2% 1.7%\nStar Section Quote Title 1.1% 1.5% 0.7% 2.1%\nTable 16: Statistics of improved accuracy margin from various downstream LMs augmented by restructured outputs\nfrom Refiner , compared with no structured output in multi-hop tasks.\n25\nF Case Study 1Below, we will provide several exemplary cases illustrating how our model assists downstream models in 1further extracting information to enhance accuracy. We select one case from PopQA and two cases from 1TriviaQA subset under top 5 retrieval setting. For case under PopQA, Refiner successfully disseminates 1similar yet different information using section, improves the quality of answer of downstream LM. While 1in both of the cases under TriviaQA, the Refiner -augmented models successfully answer with ground 1truths which are only mentioned indirectly. 1Question What is George Lewis’s occupation?\nGround Truths ’politician’, ’political leader’, ’political figure’, ’polit.’, ’pol’\nRetrieved Document Chunks Title : George Lewis (footballer, born 1913)\nContent : Thomas George Lewis (20 October 1913 – 6 August 1981) was a Welsh footballer who\nplayed as a centre-forward for Watford in the late 1930s, before joining Southampton for a brief period\nafter World War II....Lewis spent the 1947–48 season back in the Third Division South with Brighton &\nHove Albion before joining Dartford, where he remained until 1952, when he took up a position as\ngroundsman.\nTitle : George Lewis (footballer, born 2000)\nContent : George Lewis Igaba-Ishimwe Maniraguha (born 16 June 2000) is a professional footballer\nwho plays as a forward for club Arsenal. Born in Kigali, Rwanda, Lewis moved to Tanzania at the age\nof one. He then moved to Norway the age of four, starting his career at local side Stakkevollan IF,\nbefore moving to Tromsdalen, and later Tromsø in 2015. He mostly played for the reserve teams of\nboth clubs, notably notching 32 goals in 35 appearances over three seasons for Tromsdalen’s reserve\nteam.\nTitle : George Robert Lewis\nContent : George Robert Lewis (1782–1871) was a versatile English painter of landscapes and\nportraits....Attribution\nTitle : George E. Lewis\nContent : George Emanuel Lewis (born July 14, 1952) is an American composer, performer, and\nscholar of experimental music. He has been a member of the Association for the Advancement of\nCreative Musicians (AACM) since 1971, when he joined the organization at the age of 19. He is\nrenowned for his work as an improvising trombonist and considered a pioneer of computer music,\nwhich he began pursuing in the late 1970s; in the 1980s he created V oyager, an improvising software\nhe has used in interactive performances. Lewis’s many honors include a MacArthur Fellowship and\na Guggenheim Fellowship, and his book A Power Stronger Than Itself: The AACM and American\nExperimental Music received the American Book Award. Lewis is the Edwin H. Case Professor of\nAmerican Music, Composition & Historical Musicology at Columbia University.\nTitle : George Lewis (rugby)\nContent : George Lewis (9 August 1897 – death unknown) was a Welsh rugby union and professional\nrugby league footballer who played in the 1920s and 1930s. He played club level rugby union (RU) for\nPontypool RFC, as a centre, i.e. number 12 or 13, and representative level rugby league (RL) for Wales\nand Monmouthshire, and at club level for St. Helens, as a, i.e. number 1, 3 or 4, 6, or 7....George Lewis\nwas the brother of the rugby union footballer for Pontypool RFC, and the rugby league for St. Helens;\nStanley \"Stan\" Lewis.\nRefiner1.1. George Lewis (footballer, born 1913)\nThomas George Lewis (20 October 1913 – 6 August 1981) was a Welsh footballer...\n1.2. George Lewis (footballer, born 2000)\nGeorge Lewis Igaba-Ishimwe Maniraguha (born 16 June 2000) is a professional footballer...\n2.1. George E. Lewis\nGeorge Emanuel Lewis (born July 14, 1952) is an American composer, performer, and scholar of\nexperimental music.\nLlama2-7B-Chat George Lewis’s occupation is footballer. He played as a centre-forward for Watford in the late 1930s\nand joined Southampton for a brief period after World War II.\nRefiner + Llama2-7B-Chat Based on the content provided, there are three individuals with the name George Lewis:\n1. George Lewis (footballer, born 1913) - The content mentions that he was a Welsh foot-\nballer. Therefore, his occupation was a footballer.\n2. George Lewis (footballer, born 2000) - The content mentions that he is a professional footballer.\nTherefore, his occupation is also a footballer.\n3. George E. Lewis - The content does not mention his occupation directly, but based on the fact that he\nis a composer, performer, and scholar of experimental music, it can be inferred that his occupation is\nrelated to music, possibly as a composer or musician.\nTherefore, the answer to the question \"What is George Lewis’s occupation?\" would be:\n* For George Lewis (footballer, born 1913) and George Lewis (footballer, born 2000): foot-\nballer.\n* For George E. Lewis: composer, performer, and scholar of experimental music.\"\nTable 17: Case study of how Refiner helps downstream LM to disseminate ambiguous information from PopQA\nevaluation set.\n26\nQuestion For which team did Babe Ruth blast his last Major League home run?\nGround Truths ’boston braves’, ’boston braves disambiguation’, ’Boston Braves (disambiguation)’, ’Boston\nBraves’\nRetrieved Document Chunks Title : Lou Chiozza\nContent : Braves, had been through for years but was still being exploited for his name. Just a\nfew days before the Philadelphia series, the Babe hit three home runs in Pittsburgh. Chiozza, who\nhad just been brought up from the Memphis Chicks, hit a short fly ball down the left field line that\nwould have ordinarily at best been a double. Due to his advanced age and decreased mobility, Ruth\nstumbled after the ball in the outfield. The shortstop ran out, retrieved the ball and threw the ball\nhome to barely stop Chiozza from an inside-the-park home run. Many in the park believed that the\numpire felt sorry for the aging star and called an obviously safe Chiozza out at home plate. After\nRuth realized that he was so slow that Lou almost made a home run on an ordinary base hit, he\nstood for a minute, folded his glove and walked off the field into the clubhouse. The Babe knew\nhe was done and he officially retired a few days later on June 2, 1935. Chiozza recalled in his\nlater life that he had wished Ruth had retired on a high note after hitting the three home runs in Pittsburgh\nTitle : Guy Bush\nContent : #714, and was a mammoth of a shot. It was the first home run to clear the right field\ngrandstands at Forbes Field and was reportedly measured as a bomb. It was the last home run – in fact,\nthe last hit of any kind – in Ruth’s major league career, which ended less than a week later. Despite\nRuth’s goliath performance, Waite Hoyt, Ruth’s former Yankee teammate, came in relief for Bush and\nwon the game 11-7, albeit after initially blowing the save. The Pirates sent the 34-year-old Bush to the\nbullpen for the season. The transition did not\nTitle : Babe Ruth\nContent : As part of the Yankees’ vaunted \"Murderers’ Row\" lineup of 1927, Ruth hit 60 home\nruns, which extended his MLB single-season record by a single home run. Ruth’s last season\nwith the Yankees was 1934; he retired from the game the following year, after a short stint with\nthe Boston Braves. During his career, Ruth led the AL in home runs during a season twelve\ntimes. Ruth’s legendary power and charismatic personality made him a larger-than-life figure\nduring the Roaring Twenties. During his career, he was the target of intense press and public\nattention for his baseball exploits and off-field...a long fly caught by Paul Waner probably would\nhave been a home run in any other ballpark besides Forbes Field. Ruth played in the third\ngame of the Pittsburgh series on May 25, 1935, and added one more tale to his playing legend.\nRuth went 4-for-4, including three home runs, though the Braves lost the game 11–7. The last\ntwo were off Ruth’s old Cubs nemesis, Guy Bush. The final home run, both of the game and of\nRuth’s career, sailed out of the park over the right field upper deck–the first time anyone had hit a fair ball\nTitle : Atlanta Braves\nContent : Brave before asking to be traded to the Milwaukee Brewers, while Ruth finished his\ncareer as a Boston Brave. In fact, until Barry Bonds eclipsed the 714 home runs hit by Babe\nRuth in 2006, the top two home run hitters in Major League history had at one time been Braves.\nIn 1976, the team was purchased by media magnate Ted Turner, owner of superstation WTBS,\nas a means to keep the team (and one of his main programming staples) in Atlanta. The finan-\ncially strapped Turner used money already paid to the team for their broadcast rights as a down-payment.\nTitle : 1935 Boston Braves season\nContent : the team due to the attention he commanded. He also discovered that rather than give him a\nshare of the Braves’ profits, Fuchs expected him to invest some of \"his\" money in the team. Seeing a\nteam in utter collapse and realizing he was finished even as a part-time player, Ruth retired on June\n1, six days after he had what remains one of the most memorable afternoons in baseball history. He\nclouted what turned out to be the last three home runs of his career in a game at Forbes Field while\nplaying the Pittsburgh Pirates. He’d wanted to\nRefiner 1.1. Babe Ruth Ruth played in the third game of the Pittsburgh series on May 25, 1935, and added one\nmore tale to his playing legend. Ruth went 4-for-4, including three home runs, though the Braves lost\nthe game 11–7. The last two were off Ruth’s old Cubs nemesis, Guy Bush.\n1.2. Atlanta Braves\nBrave before asking to be traded to the Milwaukee Brewers, while Ruth finished his career as a Boston\nBrave.\nLlama2-13B-Chat According to the text, Babe Ruth’s last Major League home run was hit off Guy Bush of the Pittsburgh\nPirates at Forbes Field on May 25, 1935.\nRefiner + Llama2-13B-Chat Sure, I’d be happy to help! Based on the content provided, Babe Ruth played for the Boston Braves\nwhen he hit his last Major League home run. The content states that Ruth finished his career as a Boston\nBrave. Therefore, the answer to your question is the Boston Braves.\n127\nQuestion Apimania is an obsession with which creatures?\nGround Truths ’Ground bees’, ’bees’, ’Solitary bee’, ’Apiformes’, ’Bee flight’, ’ground bee’, ’bee flight’, ’an-\nthophila’, ’solitary bee’, ’apiforme’, ’Solitary bees’, ’ground bees’, ’Anthophila’, ’Bee’, ’Bees’,\n’apiformes’, ’Apiforme’, ’Ground bee’, ’solitary bees’, ’bee’\nRetrieved Document Chunks Title : Pedro Reyes (artist)\nContent : Assembly of \"pUN\" took place in December 2015 at the 21st Century Museum of\nContemporary Art in Kanazawa, Japan. Entomofagia (Spanish for entomophagy–the practice of eating\ninsects) is a series of culinary experiments. The first happened in 2013 at Casa do Vidro in São Paulo,\nwhere a small snack cart offered visitors a Brazilian street food specialty: giant ants. The project\nfocuses on the enormous ecological advantages of shifting protein consumption to insects. As an\nalternative to the ubiquitous fast-food staple, the beef hamburger, Reyes created the Grasswhopper, a\nburger with a patty made of crickets, a common snack in\nTitle : Insects in literature\nContent : Grasshopper\" by Samson Samsonov (1955) and Jerry Paris (1970) based on that story. The\nAncient Greek playwright Aeschylus has a gadfly pursue and torment Io, a maiden associated with the\nmoon, watched constantly by the eyes of the herdsman Argus, associated with all the stars: \"Io: Ah!\nHah! Again the prick, the stab of gadfly-sting! O earth, earth, hide, the hollow shape—Argus—that evil\nthing—the hundred-eyed.\" William Shakespeare, inspired by Aeschylus, has Tom o’Bedlam in \"King\nLear\", \"Whom the foul fiend hath led through fire and through flame, through ford and whirlpool, o’er\nbog and quagmire\", driven mad by the constant\nTitle : Dromomania\nContent : sometimes been clinical and pathologizing, and other times been descriptive of unusual\nenthusiasm without negative or medicalizing connotations, reflecting the diverse uses of the term\nmania itself. In the 17th century, that the term \"mania\" came to be used to describe any show of great\nenthusiasm for a specific activity or object. Later, it came to be used as a suffix for Greek words to refer\nto an irrational obsession, such as in the words \"guanaikomania\", \"hippomania\", and \"nymphomania\".\nAt the same time emerged the French , such as in which was borrowed in English as \"bibliomania\". The\noriginal sense\n—\nTitle : Erich Hoyt\nContent : entomologist Ted Schultz) broke new ground. In Hoyt’s \"delightful. . . multi-layered\" \"The\nEarth Dwellers\", the \"ant’s eye view of life works spectacularly\" as Hoyt \"fashions the ants into\nenchanting characters\" charting \"an insect’s course through sex, aggression and foreign policy\". \"Insect\nLives\", an American Library Association \"Outstanding Book for the College Bound\" is a \"potpourri\nof fascinating excerpts written by some of the finest insect biologists and naturalists spanning many\ncenturies.\" A deep sea book called \"Creatures of the Deep\" (Firefly, 2001) won the American Society\nof Journalists & Authors, Inc. Outstanding Book Award, General Nonfiction. A second deep sea book,\n\"Weird\nTitle : Obsession (novel)\nContent : Obsession (novel) Obsession is a mystery novel by American author Jonathan Kellerman.\nIt is the 21st novel in the Alex Delaware series. Tanya Bigelow, a former patient, comes to Alex\nrequesting help. She wants him and his friend Milo to investigate something her mother said on her\ndeath bed. Her mother told her that she did something terrible. No one believes that Tanya’s mother\nPatty, who worked with Milo’s partner at the hospital as a nurse, could have done anything terrible, but\nas the reader learns her past, it contains dark secrets. Alex has a new dog, Blanche, that Robin bought\nhim after Spike died. Robin is living with Alex again.\nRefiner 1.1. Erich Hoyt entomologist Ted Schultz) broke new ground. In Hoyt’s \"delightful. . . multi-layered\"\n\"The Earth Dwellers\", the \"ant’s eye view of life works spectacularly\" as Hoyt \"fashions the ants into\nenchanting characters\" charting \"an insect’s course through sex, aggression and foreign policy\".\n1.2. Insects in literature\n\"Io: Ah! Hah! Again the prick, the stab of gadfly-sting! O earth, earth, hide, the hollow\nshape—Argus—that evil thing—the hundred-eyed.\"\n2.1. Obsession (novel)\nHer mother told her that she did something terrible. No one believes that Tanya’s mother Patty, who\nworked with Milo’s partner at the hospital as a nurse, could have done anything terrible, but as the\nreader learns her past, it contains dark secrets.\nLlama2-13B-Chat The content provided does not directly mention an obsession with a specific creature related to the\nterm \"Apimania\". However, it is possible to infer that the term \"entomophagy\" mentioned in the first\nparagraph refers to an interest or obsession with insects as a food source. The term \"entomophagy\" is\nderived from the Greek words \"entomon\" meaning insect and \"phagein\" meaning to eat. This practice\nhas been explored in Pedro Reyes’ art project \"Entomofagia\" which focuses on the advantages of\nshifting protein consumption to insects. Therefore, Apimania could refer to an obsession with insects.\nRefiner + Llama2-13B-Chat Based on the content provided, I can answer your question precisely.\nAccording to the content, Apimania is an obsession with bees. This information can be inferred from\nthe text \"Obsession (novel)\" and the quote \"Io: Ah! Hah! Again the prick, the stab of gadfly-sting! O\nearth, earth, hide, the hollow shape—Argus—that evil thing—the hundred-eyed.\" which mentions bees\nand their sting.\nTherefore, the answer to your question is: Bees.\nTable 18: Two cases from TriviaQA evaluation set illustrating how Refiner helps downstream LM to highlight the\ncorrect information.\n28\n",
        "aspect_review": "The paper introduces a novel task-specific paradigm, Refiner, which significantly enhances the performance of LLMs in question-answering tasks by restructuring and extracting relevant content from document chunks. This task-oriented approach addresses the 'lost-in-the-middle' syndrome effectively, demonstrating its utility across various QA datasets. Comprehensive ablation studies are conducted to validate the effectiveness of the proposed structured output format. The study shows that the structured output significantly contributes to the performance improvement of downstream LMs, especially in multi-hop QA tasks. The paper presents a robust methodology by leveraging a single decoder-only LLM for adaptive extraction and restructuring of query-relevant contents. The use of knowledge distillation with multiple teacher models ensures high-quality training data, enhancing the model's ability to maintain context and relatedness information. The paper provides a detailed explanation of the Refiner's architecture and its integration with RAG systems. It thoroughly discusses the challenges of current RAG approaches and how Refiner addresses these issues, offering clear insights into its operational mechanics and benefits.\n\nThe analysis could benefit from a more detailed exploration of the impact of different retrieval settings on the performance of Refiner-augmented systems. While the paper focuses on open-domain QA tasks, it does not explore the robustness of Refiner on domain-specific datasets or alternative input structures, which could limit its perceived applicability. The ablation study, while comprehensive, could include more variations of the structured output to further validate the optimal configuration. The paper could highlight more explicitly how the Refiner's approach compares to other state-of-the-art methods in terms of computational efficiency and scalability. Some figures and tables in the paper could be formatted more clearly to enhance readability and accessibility of the information. The methodology section could benefit from additional details on the hyperparameter settings and training processes to facilitate reproducibility.",
        "general_review": "The paper introduces Refiner, a novel and highly effective end-to-end extract-and-restructure paradigm that significantly enhances the performance of downstream Large Language Models (LLMs) in question-answering tasks. This work is groundbreaking in its approach to addressing the 'lost-in-the-middle' syndrome by adaptively extracting and restructuring query-relevant content, which is a notable novelty. The technical depth and rigor are evident through the comprehensive experimental validation across multiple datasets, where Refiner consistently outperforms state-of-the-art Retrieval-Augmented Generation (RAG) and concurrent compressing approaches. The plug-and-play nature of Refiner demonstrates significant practical impact, making it applicable to diverse open-source frameworks and enhancing the efficiency of RAG systems with impressive token reduction rates.\n\n1. The paper could benefit from minor writing or clarity improvements to enhance the overall readability and flow of information. 2. A few small details regarding hyperparameters or implementation specifics are not fully elaborated, which could aid in reproducibility. 3. Minor formatting adjustments on figures might improve visual clarity. 4. There is a slight need for clarification on the availability of the code to ensure that other researchers can easily implement and test the proposed method."
    },
    "1": {
        "paper": "You Make me Feel like a Natural Question:\nTraining QA Systems on Transformed Trivia Questions\nAnonymous ACL submission\nAbstract\nTraining question answering ( QA) and information retrieval systems for web queries require large, expensive datasets that are difficult to annotate and time-consuming to gather. Moreover, while natural datasets of informationseeking questions are often prone to ambiguity or ill-formed, there are troves of freely available, carefully crafted question datasets for many languages. Thus, we automatically generate shorter, information-seeking questions, resembling web queries in the style of the Natural Questions ( NQ) dataset from longer trivia data. Training a QAsystem on these transformed questions is a viable strategy for alternating to more expensive training setups showing the F1 score difference of less than 6%and contrasting the final systems. 1 Introduction Question answering is a central problem in AIresearch. One way of understanding whypeople ask questions was explained in Rogers et al. (2023): questions come from either an information-seeking paradigm (V oorhees, 2019, henceforth informationseeking) or a probing, evaluative paradigm (Turing, 1950, probing). While it is easy to get questions in the information-seeking paradigm because the asker creates questions that they do not know the answer to, additional annotations to find these answers are expensive. For example, Natural Questions (Kwiatkowski et al., 2019), a benchmark dataset collected by Google from questions people asked online, critically does not include the correct answers . Annotating answers could be more expensive than their probing counterparts, mostly written by QA writing experts (e.g., trivia members). Moreover, while large corporations can collect large-scale natural information-seeking questions at no cost , these questions lack in quality for their ambiguity (Min et al., 2020) and false presuppositions (Yu et al., 2022). Due to these downfalls, 041Boyd-Graber and Börschinger (2020) argue that probing questions are more useful for building QA systems. Thus, we utilize the Quiz Bowl ( QB) samples, a probing QA dataset, created by trivia experts (Section 2).1This paper investigates whether and how we can transform the probing QBsamples into questions that resemble natural, information-seeking questions. To this end, we propose a syntactic transformation technique NATURALIZATION that converts QBelicitations into QB-TRANS questions that resemble NQ(Section 3). To validate the quality of QB-TRANS for training QA systems, we consider two experimental settings: zero-shot and supervised. The zero-shot setting examines whether QB-TRANS is an effective training data for a QA system when compared toNQ(Section 4). We train QA systems with QBTRANS training data and compare the two systems on the NQtest set. Average F1 scores on NQtest set vary by less than 6%, which implies that QBTRANS can replace NQtraining data. We also combine NQwith QB-TRANS as training data in our supervised setting (Section 5), improving F1 (tested on NQtest set) by 10% compared to training on only NQ.QB-TRANS lacks issues that plague NQ: presupposition and ambiguity (Section 7). Moreover, NATURALIZATION generalizes to other datasets. Our contributions are naturalizing of probing QBdataset into information-seeking QB-TRANS while retaining the positive traits of QB samples, thereby improving QA performance with a more affordable process. Section 9 shows how this can ensure a cheaper and more up-to-date alternative to NQdata which benefits different models and datasets. 1QBwriters are particularly known for understanding what\nmakes for a good QA pair; QBdataset avoids the ambiguity\nand false presuppositions that are often in NQ.\n1\n2 Artful but Arcane QBdataset This section discusses why we use QBdata and how different they are from NQquestions. The next section explains NATURALIZATION (Section 3). Elicitations from QBdataset Consider this QB sample example: A radio mast named for this city was the world’s tallest structure until the mast collapsed in 1991. This capital contains a skyscraper formerly known as the Joseph Stalin Palace of Culture and Science. A landmark called Sigismund’s Column commemorates Sigismund III Vasa, who moved his capital from Kraków to this city on the Vistula River. A 1943 Jewish ghetto uprising occurred in—for 10 points—what Polish capital? Here, clues are introduced pyramidally—harder, more obscure clues about Warsaw are sorted to appear at the first sentence (Rodriguez et al., 2021)— so that whoever knows the most about Warsaw should be able to answer the question sooner.2However, we do not need this complexity. Instead, we extract the series of clues that an expert author thought was noteworthy about Warsaw (e.g., key sites that commemorate its history and rulers who made it the capital). We define the source text paragraph as elicitation. As they are combined pieces of clues in multiple sentences, they are not grammatical or natural. Thus, we turn each clue extracted from elicitation into multiple NQ-like questions, which are short and simple. Ultimately, our goal is NATURALIZ INGthese clues into information-seeking, natural questions. Comparison with NQdatasets For each QBelicitation, we extract an average of seven clue sentences. Each sentence is 22words on average. On the other hand, in NQ, the average sentence length is eight words (Kwiatkowski et al., 2019). The NQquestions were harvested from Google queries based on specific heuristics.3The number of samples from QBand NQare comparable ( QB:112,927 elicitations and answers and NQ:307,373samples); however, there exists a substantial difference in cost, quality, and quantity. 2For example, deciding it “moved his capital from Kraków\nto this city on the Vistula” requires the ability to decide not\njust what to answer, enough to answer but also when to answer\nin the quiz bowl tournament (He et al., 2016).\n3For example, the questions start with “who”, “when”\nor “where” followed by a finite form of “do” or a modal\nverb (Kwiatkowski et al., 2019)For cost comparison, while the QBelicitations have answers unambiguously created by trivia authors, answers to NQquestions must be laboriously annotated by paid workers. While Google has not officially released costs, the convoluted process and the lack of reproduction since 2019 suggests that its price is high. From the QAresearcher’s perspective, the elicitation process is free. For quality comparison, trivia authors who created QBelicitations understand the importance of discouraging ambiguity and false suppositions in their clues (Boyd-Graber and Börschinger, 2020) while they are prevalent in NQ. Thus, if we can faithfully elicit these clues from QB, the resulting questions may be of higher quality than NQquestions (Detail analysis is in Section 7). Finally, for quantity comparison, because each QBelicitation contains many clues, the the size of a transformed dataset is three-fold larger than NQ. Also, while the NQdataset may only ask a single question about a rare entity, this is not likely the case for QB: a single elicitation would produce several clues about an entity, allowing a model to understand more about each potential answer. 3 NATURALIZATION This section outlines NATURALIZATION : converting the elicitations into multiple NQ-like questions (Figure 1). 3.1 Generating Candidates Many of the transformations depend on an initial dependency parse (Nivre, 2010). Some parsed elicitations are statements about a target entity that do not resemble how questions are asked (e.g statements about the target entity “she was the last Queen of Hawaii” or “this element is mined from bauxite”). To transform these into questions, we find mentions coreferent with the answer. Conjunction and Removing Clauses Given these candidates, we then extract the minimal facts that would form the basis of a question. For example, if the QBelicitation had “he wrote Animal Farm and 1984”, this can become two facts: “he wrote Animal Farm ” and “he wrote 1984 ”. Thus, we construct independent clauses by extracting spans that contain the mention (“he”), a verb (“wrote”), and one member of a conjunction (either of the two works). Similarly, we can sometimes remove clauses: “this author who graduated Eton 2\nThis city on the Bay of Bothnia is home to Nylund’s Three Smiths and Takanen and Walter Runeberg’s statues of Alexander II.Original:\n3. Select Lexical Answer Type (over all elicitations with same answer)1. Parse Sentence (simpliﬁed for diagram)\nNylund’s Three SmithsNPTakanen and Walter Runeberg’s statues of Alexander IINPandis home toThis capital on the Bay of BothniaNP\ncityFinnish citycapitalmunicipality2. Generate Variations: Alternate Independent Clauses and Remove Optional Clauses This capital is home to Nylund’s Three SmithsThis capital on the Bay of Bothnia is home to Nylund’s Three SmithsThis capital is home to Takanen and Walter Runeberg’s statues of Alexander II\n4. Convert to QuestionWhat city is home to Nylund’s Three SmithsWhat city on the Bay of Bothnia is home to Nylund’s Three SmithsWhat city is home to Takanen and Walter Runeberg’s statues of Alexander II5. Run Classiﬁer, Rank by Similarity to Natural QuestionsWhat city is home to Nylund’s Three Smiths?Length: 8Bigram: home toBigram: What city\n=0.8Figure 1: In the process of creating information-seeking\nstyle questions from probing elicitations, (1) we take\neach clue sentence from the paragraph-long QBquestion,\nand parse it. (2-3) The parsed sentences are transformed\ninto variants, (4) that are finally turned into information-\nseeking questions.\nCollege wrote Homage to Catalonia ” can be simplified to “this author wrote Homage to Catalonia ” (Details in Appendix, Algorithm 2). Canonical Answer Type Next, we identify what kind of answer the question is looking for. This is important because sometimes questions written inQB’s pyramidal style uses oblique references, particularly at the beginning of the question: “substance” for zinc, “creator” for Chinua Achebe , or “polity” for Bangladesh . However, these are rarer than the most straightforward and direct references. For example, zinc is most often asked about using “what element”, Chinua Achebe with “what playwright”, and Bangladesh with “what nation”. Thus, we group all QBelicitations that have the same answer and for each answer find the most frequent string used to ask about the answer. These canonical answer types then replace the mentions in the original question. Imperative to Interrogative The most obvious difference between QBelicitations and NQques- 190tions is that QBelicitations are not grammatical questions: rather, they are declarative statements about the answer. For imperative statements such as “name this first prime minister of Canada”, we generate a synthetic mention that makes the object of the imperative verb the question: “who was the first prime minister of Canada” by mapping the canonical answer type to its WORDNET(Fellbaum, 1998) hypernym and applying the appropriate question word (e.g., person.n.01 maps to “who”, time_period.n.01 maps to “when”). The whole pseudocode is given in Algorithm 4 and 5. Additional Heuristics Through observation of the linguistic and grammatical style of NQwe add additional heuristics to further improve the candidates such as removing punctuation andadding subject (full list in Appendix A). 3.2 LLM Transformation: Llama2 and GPT As a baseline, we transform QBclues into questions by prompting GPT and LLAMA 2(Touvron et al., 2023), generative text models.4For a fair comparison, we follow a similar process of generating NQ-like question from QB. We extract clue sentences from the QBelicitations, then prompt GPT and LLAMA 2to produce a natural question. We first identify the lexical answer type (e.g., “this person”) and ask GPT and LLAMA 2to formulate a query that could be used as a Google search. This is an example prompt for GPT and LLAMA 2(text in bold is specific to answer): This “sea Prince of Wales” leaves his home by hiding out in a canoe near a coral reef, and he is mistakenly called “Hedgehog” by a character who offers him a ninetieth lay, a partner of Bildad named Peleg. This is a trivia question. Turn this into shorter question of fewer than 20 words that start with “what sea”, and ask about this “sea prince” in the short question. The questions should be natural as a Google query to find out what the answer to the long question is. The shorter questions you write should not include the answer, Queequeg and not be confusable with other answers. . LLAMA 2and GPT transformations do not contain all the clues or hallucinate some information (Table 1). For example, LLAMA 2did not capture the entity of interest and asked about the “sea” instead of a “character”. LLAMA 2andGPT have similar generated questions (Examples in Table 8). 4https://huggingface.co/meta-llama/\nLlama-2-7b-hf\n3\nOriginal QB QB -TRANS LLAMA 2\nThis “sea Prince of\nWales” leaves his\nhome by hiding out\nin a canoe near a\ncoral reef, and he\nis mistakenly called\n“Hedgehog” by a\ncharacter who offers\nhim a ninetieth lay,\na partner of Bildad\nnamed Peleg.which “sea prince\nof wales” leaves his\nhome by hiding out\nin a canoe near a\ncoral reef, and he\nis mistakenly called\n“hedgehog” by a char-\nacter who offers him\na ninetieth lay, a part-\nner of bildad named\npeleg.What sea did the \"sea\nPrince of Wales\" hide\nout in a canoe near a\ncoral reef?\nThis substance moves\nto the liver where it is\nturned into glucose in\nthe Cori cycle.which substance\nmoves to the liver\nwhere it is turned into\nglucose in the Cori\ncycle.What substance is\nproduced when the\nbody breaks down\ncarbohydrates during\nintense exercise?\nIn one of this man’s\npaintings, one char-\nacter oddly uses her\nleft hand to grasp\nthe red-cloaked char-\nacter’s chin while her\nright hand sits at his\nkneeIn one of which\nman’s paintings,\none character oddly\nuses her left hand to\ngrasp the red-cloaked\ncharacter’s chin while\nher right hand sits at\nhis knee?What is the signif-\nicance of the left-\nhand grasp and the\nright-hand placement\nin Jean Auguste Do-\nminique Ingres’ paint-\ning featuring a red-\ncloaked character and\nanother woman?\nTable 1: LLAMA 2converted question can introduce er-\nror in question. For example, conversion of QBclues\nusing rule-based and large language model-based tech-\nniques ( LLAMA 2). In the case of LLAMA 2of the first\nexample, the question changed the entity the QBclue\nis asked about. In the second example, words were re-\nmoved from the question and additional random words\nwere added, implying LLAMA 2transformation is worse\nthan that of QB-TRANS .\n4 Zero-shot QA with QB-TRANS training We ensure we use no NQdata and evaluate on NQ test set (which disadvantages our approach). 4.1 Challenges in Zero-shot QA System There are challenges in comparing models for zeroshot QA because some models are based on large language models (LLMs) that do not disclose training data. Thus do not know whether some zero-shot systems use NQin their pretraining process (Shi et al., 2023a). For example, Oscar Sainz (2023); Narayanan (2023); Magar and Schwartz (2022); Sainz et al. (2023a,b) suggest that GPT -3.5 is contaminated with NQtraining and development set. One sign that these models train on NQis that they give an abnormal probability for tokens in NQ as measured by Min K% probability (Shi et al., 2023a). The state-of-the-art LLMs have an average probability of 63% (Detail of the results in Appendix, Table 11). This indicates that these stateof-the-art LLMs has a high probability of having NQin the training data. Another clue that these models have used NQfor training is that they repeat NQanswers to questions even when NQis wrong (manually detected) (Ta- 265ble 2); this is the clearest signal that the model has seen the NQdata’s answers, as annotation errors are less likely to be by coincidence. For example, we probe GPT with time-sensitive questions that have answers no longer valid. We observe that GPT incorrectly answers those questions, with the answers included in the NQ dataset. We infer that it is likely for GPT ’s training data to be contaminated (Sainz et al., 2023a; Cotton et al., 2024) and can no longer be a fair candidate for the zero-shot setting experiments. 4.2 Zero-shot QA systems Thus, we select two systems with high accuracy on traditional NQtraining: Deep Passage Retrieval (Karpukhin et al., 2020b, DPR) and RetrievalAugmented Language Modeling Framework (Shi et al., 2023b, REPLUG ).These systems are trained from the ground up. DPR (Karpukhin et al., 2020a) extracts the answer from a context which is extracted using passage retriever models. We train DPR on the questions, answers, and context passages for the NQ-like generated QB-TRANS questions dataset (ours). In training, we generate the positive context by collecting passages that contain answer string, and negative context otherwise (Example in Appendix, Table 9). In REPLUG (Shi et al., 2023b), the retrieval model finds the most appropriate passage from a large corpus; then the model produces more accurate answers by augmenting retrieved information to the input context. 4.3 Training Data We compare all of our generated datasets with the original NQdataset ( NQ). Our goal is to create a QAsystem with the same accuracy as the original NQdataset while training on the QB-TRANS dataset, so this is an upper bound. In this zeroshot experiment, we used different percentages of QB-generated questions for training the model. We compare this traditional training regime with several training sets derived from QB-TRANS . The full results are given in Appendix, Figure 6. We compare against all transformed sentences from our syntactic-based method ( QB-TRANS ) to the LLM baseline (QB-GPT and QB-L LAMA 2). We used multiple passes when difference in dataset size. For example when the dataset size for NQis307k, we used multiple passes to compare against QB-TRANS dataset of size 800k. 4\nNQquestion NQanswer (wrong) Gold answer GPT answer Comment\nwho won the Oscar for best pic-\nture in 1976?Rocky One Flew Over The\nCuckoo’s NestRocky Rocky won the best picture in 1977 (osc, 2023).\nwhere was held the first session\nof Muslim leagueDhaka, Bangladesh Karachi Dhaka, Bangladesh The AIME Conference in 1906, held at Dhaka, Bangladesh, laid the\nfoundation of the Muslim League. (mus, 2023)\nTotal number of death row in-\nmates in the us2,718 2 ,331 Over 2,400people This information is changed over periods.\nWho is next in line to be the\nmonarch of EnglandCharles, Prince of Wales Prince William Charles, Prince of\nWalesThe answer is outdated.\nTable 2: To determine whether NQis in the training data of GPT , we take the answers given by GPT 3.5. If the\nanswer is the same as given in NQdataset, we can assume it has seen those datasets.\n0 200000 400000 600000010203040DPR\n0 200000 400000 600000REPLUG\nNumber of data pointsF1 score\nDatasets\nNQ QB-GPT QB-Llama2 QB-Trans\nFigure 2: QB-Trans can replace NQin training QA sys-\ntem and achieve accuracy close to NQtraining system.\nDPR : As expected, QB-TRANS without any NQdata\ncomes within 5% of a model trained on NQ. Training\non the full QB-TRANS and evaluating it produces the\nhighest F1 score system with DPR. This does better\nthan transformations created by prompting a GPT and\nLLAMA .REPLUG : Again, QB-TRANS without any\nNQdata comes within 7% of a model trained on NQ.\n4.4 Results and Analysis Our transformations lag behind a model trained directly on NQby only about 6%on average, while the LLM lags by over 10%.QB-TRANS data can be applied to different QA systems and achieve comparable performance (Figure 2). LLM-based transformation ( QB-GPT and QBLlama2) performs worse than syntactic NATURAL IZATION . This happens because even the worst transformed questions from the QB-TRANS dataset are better than many of the questions produced by the LLM (Table 1). Not only does the desired answer change in LLM-based transformation (it is not clear that there is a correct answer), but the answer also appears in the question (despite prompt instructions). 5 Supervised QA System with QB-NQ training data We compare all of the naturalized datasets with the original NQdataset ( NQ), with the goal of having the largest NQ-like dataset. 3345.1 Supervised QA systems As the baseline, we use the top model in the NQ challenge leaderboard ReflectionNet (Wang et al., 2020): a MRC model for answer prediction and Reflection model for answer confidence. We also use the state-of-the-art GENREAD (Yu et al., 2023), which is a generate-then-retrieve pipeline QA system that directly generates the contextual documents by using clustering document representations. This method outperforms traditional retrievethen-read methods. We also use the two retrievalbased systems DPR(Karpukhin et al., 2020b) and REPLUG (Shi et al., 2023b) from the previous section, but this time trained with QB-TRANS data along with NQdataset. 5.2 Training Data We train the supervised QA systems with our QBNQdataset, the combination of original NQand QB-TRANS questions. We replace the QA systems’ training data with QB-NQdataset to see how our dataset performs when merged with the NQdataset and whether our dataset can be used as an expansion of the NQdataset. Here, QB-NQ-20, represents all of the filtered and transformed QB-TRANS dataset and 20% percent of the original NQdata. NQexamples are selected uniformly at random. We also used the same multiple passes when differences in dataset size like zero-shot setting. More detail on the formation of training questions and answers is in Appendix E. 5.3 Supervised Classifier The generation process results in many questions that insufficiently resemble the informationseeking questions we want to emulate: some are too short or long, do not make sense, or still look too much like a probing QBelicitations. Like how Goodfellow et al. (2014) uses a classifier to filter the outputs of an automatic generative process, we identify the best examples from the above process. We use a simple logistic regression classifier (Cox, 1958) trained on the generated NQ-like examples 5\n020406080DPR GenRead\n0 200000 400000 600000020406080REPLUG\n0 200000 400000 600000ReflectionNet\nNumber of data pointsF1 score\nDatasets\nNQ QB-NQ-100 QB-NQ-20 QB-NQ-50Figure 3: QB-Trans adding with NQin training QA\nsystem can achieve F1 much higher ( 10% on average)\ntoNQtraining system. DPR : Supervised training on\nQB-NQ-100 and evaluating on NQtest set produces\nthe highest F1 score system with DPR. However, the\ncheaper datasets from our systematic conversion ( QB-\nNQ-50 ), with a noisier but larger dataset, reached a sub-\nstantial fraction of the F1 score. Similarly, REPLUG,\nReflectionNet and GenRead : Again, in a supervised\nsetting, QB-NQ-100 data crosses the NQby10points\nof a model trained on NQ, and adding just 50% of the\nNQdata ( QB-NQ-50) allows the model to reach within\n12% of the F1 score of the model trained on the whole\nNQdataset.\n(through the process described in the previous section) as negative examples and with real NQexamples as positive examples. To make use of the answers provided in the dataset, we designed the classifier with the answers included as a feature in the dataset. Nonetheless, our features identify question topics and formats that occur frequently in NQ. For example, the bigram “who played”, reflects NQ’s emphasis on popular culture; starting questions with “how”, “when”, or “where” recapitulates the process for harvesting NQ; and short questions have the highest feature weight, emphasizing that NQ questions are short. We also use early stopping with the classifier to find the optimum number of data points needed for each model. For that, we add 50kdata at each iteration based on the classifier and test it on NQ dev set until the F1 score continues to increase. When the score starts to drop we continue it for five more iterations to avoid local minima. If F1 again starts to increase, we continue. Otherwise, the data number that has the best F1 score on the 398ModelsDatasets\nNQ QB -NQ-No classifier With classifier\nno early stopping early stopping\nDPR 39.23 43.54 46.21 49.12\nREPLUG 45.75 55.29 49.12 57.56\nReflectionNet 64.01 68.36 73.89 75.87\nGenRead 74.31 79.56 80.03 78.01\nTable 3: The best F1-score is reported here. The classi-\nfier with early stopping helps us to find out the optimal\nnumber of data points needed for the model.\ndev set is chosen as the optimal train set. 5.4 Result and Analysis We argued that using transformed QB-TRANS data would be cheaper than using NQdata (which is expensive) to gather answers. What if we have access to a fraction of the NQdata? Finally, given the best configuration of the previous experiment, we add a small amounts of NQdata to see how much is needed to recreate the best NQresult. Adding half of the NQbrings parity to the result. Therefore, our experiments show the effectiveness of QB-TRANS dataset as an alternative of NQdataset in the zero-shot setting and an expansion of NQ dataset in supervised QA systems. Similar results can be seen in all the systems (Figure 3). ReflectionNet and GenRead have higher F1 score than DPR and REPLUG because of their usage of large language models and ensemble models in training. No data in the training process is changed. The result is summarised in Table 3. 6 Answer Equivalence in Zero-shot and Supervised Training Thus far, we focused on ensuring that the transformed questions resemble the target NQdata as much as possible but did not consider the answers. To fully emulate NQdata, the answers need to be comparable. Thus, we expand the answer set provided in the QBdataset (which typically is more formal and verbose than NQ) with the WikiData answer equivalence sets from Si et al. (2021) for both training and evaluation. For example, NQhas a question “Where do the greasers live in the outsiders?” with the correct answer set comprised of {“Tulsa”, “Oklahoma”}. However, if the QA system answers “tulsa”, “Oklahoma”, it will be considered as incorrect in the exact match. Thus, we apply an answer equivalence system to change the answer set to {“Tulsa”, “Oklahoma”, “ttown”, “Tulsa”, “tulsa oklahoma”, 6\n0255075DPR GenRead\n0 200000 400000 6000000255075REPLUG\n0 200000 400000 600000ReflectionNet\nNumber of data pointsF1 score\nDatasets\nNQ QB-NQ-100 QB-NQ-50 QB-Trans-100Figure 4: With answer equivalence : Again, QB-NQ-\n100data crosses by 12% on average of a model trained\nonNQ, and adding just 50% of the NQdata allows the\nmodel to reach within 7%of the whole NQwith answer\nequivalence. QB-TRANS -100 comes within 5%points\nof model trained on NQ.\n“wagoner county Tulsa city”}. After adding answer equivalence in the supervised setting, the F1 score for QB-NQincreased by 12% from NQwhich is3%more than systems without answer equivalence. Moreover, the F1 score for QB-NQ-50 is much closer ( 2%improvement) to NQthan they were without answer equivalence. In zero-shot setting, the F1 score for QB-TRANS is5%less than the F1 score for NQ(without answer equivalence F1 score was 6%less than NQ) (consistent with results in Si et al. (2021)) (Figure 4). 7 Analysis of Transformed Questions 7.1 Quality of Generated Data To analyze the quality of our dataset, we use CREPE (Yu et al., 2022) to identify false presuppositions (Table 4). The percentage of presuppositions present in our dataset is less than NQ. NQhas has more ambiguous questions detected using Min et al. (2020)’s AmbigQA binary classifier and GPT-3.5 (Table 4). An example of an ambiguous question from NQ,“How many nominations does Game of Thrones have?” This question can ask about the number of nominations “Game of Thrones” has across all its seasons, or it can ask about any particular season or award ceremony. Therefore, no precise answer can be given without additional context. On the other hand, QBelicitation ensures each clue points to a unique object without any ambiguity. 466Dataset Size % of Presupposition% of Ambiguity\nusing GPT-3.5 using AmbigQA\nNQ 307373 21 63 68\nQB-Trans 800000 27 27 25\nTable 4: The percentage of harmful presupposition and\nambiguous questions in NQand QBTrans dataset. QB-\nTrans has fewer presuppositions and significantly fewer\nambiguities than NQ.\n7.2 Transformation Error Analysis Not all of the original elicitations are transformed correctly. Consider this original clue from elicitation: This author created a character who smokes a cigarette before the body of his dead mother, and who vacations with his friend Raymond and shoots an Arab on the beach. The heuristic \"split conjunction\" and “no wh-word” are applied and generate questions “This author created a character who smokes a cigarette before the body of his dead mother,”, “what author vacations with his friend Raymond” and “what author shoots an Arab on the beach”. The 2nd and 3rd questions are incorrect. This happens because there is an error in finding relative clauses when splitting via conjunction. In the future, we will detect these sorts of questions earlier where the transform technique will not be directly applicable via the dependency parse tree. 7.3 Cost of Heuristics and Generalization Our process took several iterations to refine the heuristics. It took less than a hundred hours. However, all these heuristics can be directly applied to other pyramidal and clue-based questionanswering datasets and generate NQ-like data at a cheaper cost without going through each clue manually. To show the generalization of our heuristics, we apply the heuristics to different datasets. For example,Jeopardy! has an elicitation: This small, red summer fruit develops tiny seeds on the outside and often tops shortcake. After applying the heuristics described in Section 3.1 the question becomes Which small, red summer fruit develops tiny seeds on the outside and often tops shortcake? We apply these heuristics to similar clue-based datasets Jeopardy! (Jeo, 2024), TriviaQA (Joshi et al., 2017a), HotpotQA (Yang et al., 2018) and Japanese dataset AI King (AIk, 2024). Examples of the original questions from these datasets and transformed questions after applying our heuristics are in Appendix Table 12 and 13. Figure 5 shows 7\n0 500000 1000000 150000030405060No classifier\n0 500000 1000000 1500000With classifier\nNumber of data pointsF1 score\nDatasets\nCombined-NQ-100 NQ QB-NQ-100Figure 5: No classifier: The combined dataset shows\nsimilar performance initially with the model trained on\nNQand QB-NQ. However, when we increase the data\npoint, it goes 12% higher than the model trained only on\nNQ. With the classifier , the classifier chose the training\ndata to resemble NQ. Therefore, the data selected earlier\nproduces a better F1 score. However, after 110k data\npoints, the performance starts to deteriorate. That means\nthe data we add does not resemble NQafter that.\nModelsDatasets\nNQ QB -NQ-100-Jeopardy-TriviaQA-AI King-HotpotQA\nNo classifier With classifier\nno early stopping early stopping\nDPR 39.23 52.20 57.48 53.54\nREPLUG 45.75 58.35 57.10 60.92\nReflectionNet 64.01 75.91 77.96 79.89\nGenRead 74.31 80.98 82.90 85.87\nTable 5: The best F1-score on NQtest is reported here.\nThe classifier with early stopping based on NQdev helps\nus to find out the optimal number of data points.\nthe application of heuristics to other datasets can generate larger datasets and this combined dataset (COMBINED -NQ-100 ) can improve the F1 score forDPR . We can significantly increase the size of datasets by applying these heuristics automatically to different language and domain datasets which can increase the system’s F1 score compared to the system solely trained on NQ. The results of these datasets are in Table 5. Table 10 shows the percentage of error our heuristics have while applying to different domain and language datasets is less than 1%. Our heuristics can also detect errors (e.g. ill-formed sentences, ambiguous clues about the entity, etc.) in the datasets. For example, in the Jeopardy! elicitation \"Hits hard\", it is not possible to answer that without more context. Our heuristics can be applied to identify them. 8 Related Work 8.1 Generating Questions Given the expense of gathering these data, an obvious alternative is to generate your data. While we transform one question format into another, Probably Asked Questions (Lewis et al., 2021, PAQ) transforms source documents into questions that could be asked. These questions are more formulaic than the questions carefully crafted by trivia experts in 536the QBdataset, but an obvious extension would be to see if PAQquestions could help augment the results here. Another class of transformed questions are translated questions that convert datasets like SQUAD into multiple languages (Carrino et al., 2020; d’Hoffschmidt et al., 2020). A frequent research thrust has been to create methods to generalize these datasets, either by merging datasets together (Artetxe et al., 2019; Khashabi et al., 2020) or by QA-driven slot-filling (Du et al., 2021b) or event extraction via QA (Lyu et al., 2021) by creating algorithms that explicitly generalize (Munteanu et al., 2004; Munteanu and Marcu, 2005). More related work is in Appendix, Section C. 8.2 Transforming Questions Our approach of transforming the form of QBelicitations is inspired by a long line of research. Machine translation models are used to transform questions to resemble the text where the answer would be found (Wang et al., 2007) or to transform a context-dependent question into a question that more closely resembles NQ(Demszky et al., 2018). 9 Conclusion and Future Work Transformed NQ-like questions from the QBdata is an alternative to expensive datasets like NQ. The transformed data itself is not as good as NQby itself, but is competitive; this is a reasonable option if the resources are not available to curate a dataset like NQ.NQis used text summarization, document retrieval, alignment along with benchmark of QA evaluation. However, the dataset is getting old with absolute questions and out-of-date answers. If there is a budget to create a dataset comparable to NQ, a small amount of this data augmented with transformed data from a dataset like QBcan surpass a model trained on the NQdataset alone. This can act as a continuous flow of new natural questions. Moreover, there are some methods like reinforcement learning from human feedback (RHLF) that uses NQalong with other datasets (Li et al., 2023; Feng et al., 2023) or create new datasets aligning NQwith other datasets for LLMs (Yang, 2023). Our work shows that there are additional sources of information that are cheaper and more recent that can feed into these datasets instead of NQ. For future work, we can apply this conversion technique to other languages’ probing dataset (Han et al., 2023) where transformation heuristics can be learned using human data. 8\n10 Limitations Focus on Natural Questions We focus on NQ, a popular and respected dataset. It contains real user questions from Google on a variety of topics and they are natural queries. This diversity helps in training QA models and is suitable as a benchmark for the evaluation of QA systems. Other datasets are different, and we do not know how well our transformations would generalize to other datasets. However, we suspect that similar transformations would also succeed. Errors hidden by Correct Answers While our transformed data often gets to the right answer, we have not systematically verified that the produced questions are themselves correct. It could be that enough of the necessary contents within the conversions remain that systems can reach the correct answer but that the questions contain errors (either factual or grammatical). From our inspection of the questions, we do not believe this to be the case, but a systematic evaluation would be needed to confirm this. However, this would dramatically raise the cost of the dataset, obviating one of the motivations for this approach. Distribution Shift QBand NQhave very different distributions: QBis more academic, while NQ has more questions about sports and pop culture. Thus, solely evaluating on NQpotentially says little about how well our conversion process works for the topics that are over-represented in QBcompared toNQ. While NQdoes have some questions about literature and science, they are under-represented; it could be that our transformations are particularly brittle on questions about equations or works of fiction but NQevaluation does not expose that weakness. Ethical Considerations The most important ethical consideration of this paper is that we are using the data from the trivia community to train a model. In contrast to datasets like SearchQA (Dunn et al., 2017) or TriviaQA (Joshi et al., 2017b) where it is unclear how the original trivia authors feel about the use of the data, theQBcommunity explicitly welcomes the sharing and dissemination of the data to train QBplayers: datasets are covered by a creative commons license (and the norm of sharing indeed predates the formal creation of creative commons). While computer QAsystems are a different kind of trivia player 634(machine rather than human), we believe that this would be in the spirit of the community. 9",
        "aspect_review": "The paper is well-structured, providing a clear narrative from problem identification to solution proposal and validation. The sections are logically organized, making it easy for readers to follow the progression of ideas. The paper addresses a significant problem in the field of QA systems by utilizing freely available, high-quality trivia datasets to generate information-seeking questions. This approach offers a cost-effective alternative to expensive datasets like NQ, which is a notable contribution. The motivation to transform trivia questions into natural, information-seeking questions is well-justified. The proposed NATURALIZATION technique is innovative, and the validation through both zero-shot and supervised settings demonstrates the practical impact of the approach.\n\nThe evaluation primarily focuses on the NQ dataset, which might not fully capture the effectiveness of the transformation across diverse domains. A broader evaluation could strengthen the claims. While the results are promising, the paper could benefit from a more detailed analysis of the failure cases and transformation errors to provide deeper insights into the limitations of the approach. Some sections, particularly those detailing the transformation heuristics, could benefit from additional clarity and examples to ensure that readers fully understand the processes involved. The paper assumes the transformed questions are equivalent to NQ questions in quality. However, a more rigorous analysis of the quality of the generated questions compared to NQ would be beneficial. The comparison with other transformation techniques, such as those using LLMs, could be expanded to provide a more comprehensive view of the advantages and limitations of the proposed method. The methodology relies heavily on syntactic transformations, which may not capture the semantic nuances of natural questions fully. Exploring semantic-based transformations could enhance the robustness of the approach.",
        "general_review": "The paper presents a novel and cost-effective methodology for transforming probing trivia questions into natural, information-seeking questions suitable for training QA systems. This transformation maintains the quality and reduces the ambiguity and false presuppositions often found in natural datasets. The empirical results are insightful, showing that the transformed dataset, QB-TRANS, can achieve performance close to that of the NQ dataset with less than a 6% difference in F1 scores. The paper is well-organized, clearly explaining the transformation process and providing comprehensive experimental validation across different QA systems. The practical impact is significant, offering a cheaper alternative to expensive datasets like NQ and ensuring a continuous flow of updated data.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the transformation process. 2. Some small details on hyperparameters or implementation specifics are missing, which could aid in reproducibility. 3. The formatting of figures could be slightly improved for better readability. 4. There is a need for slight clarification on the availability of the code used for the transformations and experiments."
    },
    "2": {
        "paper": "Where Visual Speech Meets Language: VSP-LLM Framework\nfor Efficient and Context-Aware Visual Speech Processing\nAnonymous ACL submission\nAbstract\nIn visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of an LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptation (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuA ViC benchmark, we demonstrate that VSP-LLM trained on just 30 hours of labeled data can more effectively translate lip movements compared to the recent model trained with 433 hours of data. 1 Introduction Along with audio, visual speech ( e.g., lip movements) plays a critical role in human communication. With the increasing acknowledgment of the importance of visual speech, a diverse range of visual-based speech processing technologies (Assael et al., 2016; Petridis and Pantic, 2016; Chung and Zisserman, 2017a; Ma et al., 2021a, 2022b; Yemini et al., 2024) is emerging. For instance, Visual Speech Recognition (VSR) (Kim et al., 2021; Ma et al., 2022a; Yeo et al., 2023a) allows for the ∗Equal Contribution.†Corresponding Author.identification of spoken words through the observation of lip movements alone, without the need for audio access. Most recently, the exploration has begun into Visual Speech Translation (VST) (Cheng et al., 2023), which directly generates translated text in the target language from the input lip movements of the source language. One key challenge in visual speech processing is to distinguish homophenes (Kim et al., 2022). Homophenes refer to the words having different sounds but showing the same lip movements. Therefore, a crucial aspect of developing visual speech processing systems is in the modeling of context so that the same lip movements can be mapped into correct different pronunciations (that is distinguishing homophenes). Recently, Large Language Models (LLMs) (Zhang et al., 2022a; Brown et al., 2020; Workshop et al., 2022) are attracting significant attention across various fields (Han et al., 2023; Wu et al., 2023b; Fathullah et al., 2023), thanks to their versatility and strong ability to model context. Motivated by the recent success of LLMs, we try to investigate whether the rich context modeling ability of LLMs can be employed in visual speech processing and can mitigate the ambiguity of homophenes, especially focusing on two tasks, VSR and VST. To this end, in this paper, we propose a new framework named Visual Speech Processing incorporated with LLM (VSP-LLM) that learns the seamless embedding of visual speech into the learned text space of LLMs. VSP-LLM employs a self-supervised visual speech model to embed the input visual speech into phoneme-level representations, where the derived phonetic information can be effectively associated with text (Zhang et al., 2022b). Moreover, to reduce the computational burden in training along with LLMs, we propose a novel deduplication method that reduces the input sequence lengths of LLMs. Concretely, we employ visual speech units, the discretized repre1\nsentations of the features from a self-supervised model, as indicators for overlapped information between sequences. As the visual speech units can be regarded as pseudo-text (Lakhotia et al., 2021), the visual speech features assigned to the same visual speech units are averaged to reduce the processing of redundant information and improve computational efficiency. Through our analysis, we show that the sequence length can be reduced by approximately 50% using the proposed deduplication, with minimal performance degradation. Finally, the proposed VSP-LLM is jointly trained to perform VSR and VST with a single model which is the first explored in this paper. We show that by bringing the powerful context modeling ability into visual speech processing, we achieve state-of-the-art performances in both VSR and VST when using the LRS3 (Afouras et al., 2018) and MuA ViC (Anwar et al., 2023) datasets as training data. Additionally, our VSP-LLM trained with just 30 hours of data outperforms the recent translation model used 433 hours of training data. The key contributions of this paper can be summarized as follows: 1) To the best of our knowledge, this is the first work to incorporate visual speech modeling with LLMs and achieve state-ofthe-art performances in VSR and VST. 2) This is the first to work to develop a unified visual speech processing model that can perform both VSR and VST with a single trained model. 3) We propose a novel visual speech deduplication that significantly improves computational efficiency. 4) We show that the proposed VSP-LLM can perform multitasks with superior performances even in limited training resource situations, just with 30 hours of labeled data by outperforming the recent translation model. 2 Related Work 2.1 Visual Speech Processing Visual speech processing technologies are mainly comprised of two parts, VSR and VST. VSR is a task to recognize the language content by watching lip movements, without any sound. The VSR technologies have greatly progressed with the development of deep learning. Early works (Chung and Zisserman, 2017b; Stafylakis and Tzimiropoulos, 2017; Petridis et al., 2017, 2018) utilize the CNN (He et al., 2016) and the RNN (Chung et al., 2014; Hochreiter and Schmidhuber, 1997) to devise a word-level VSR system. To expand the VSR 131systems into sentence-level, (Chung et al., 2017; Afouras et al., 2018) have utilized a multi-stage pipeline to automatically collect large-scale VSR data. Based on the large-scale VSR datasets, researchers (Serdyuk et al., 2022; Ma et al., 2021b) have developed the VSR systems from the perspective of architecture, especially the Transformer (Vaswani et al., 2017) have greatly improved the performance of VSR by enabling to capture of the context between any two positions of lip sequences. Moreover, the multimodal learning strategies (Zhao et al., 2020; Afouras et al., 2020; Ren et al., 2021; Ma et al., 2021a; Kim et al., 2021, 2022; Yeo et al., 2023b) have attempted to complement the insufficient visual speech representations by utilizing audio information. A recent self-supervised model known as A V-HuBERT (Shi et al., 2022), has significantly improved the visual speech representations by predicting the pseudo-label assigned from clustering audio-visual features, with a mask-prediction task like BERT (Devlin et al., 2019). According to the advancement of the VSR system, we can now recognize lip movements quite accurately through state-of-the-art VSR models such as A V-HuBERT. Building upon this, the exploration for VST has begun by introducing a Multilingual Audio-Visual Corpus (MuA ViC) (Anwar et al., 2023) dataset and constructing a VST (Cheng et al., 2023). Despite these research efforts, the development of visual speech processing systems enabling multitask via a unified model, such as VSR and VST, has never been explored in the previous visual speech processing literature. Hence, the objective of this paper is to develop a unified model to perform multi-tasks, including VSR and VST, by utilizing a rich context modeling ability of LLMs. 2.2 Integration of speech models and LLMs LLMs have shown remarkable success in various tasks due to their extensive linguistic knowledge and contextual understanding. While leveraging such inherent advantages of LLMs, several studies have tried to seamlessly integrate text-based knowledge with other modalities, particularly in the audio speech domain. For example, AudioPaLM (Rubenstein et al., 2023) has been proposed to build a unified model interacting between text language and audio speech. To naturally bridge the gap between the two modalities, AudioPaLM has developed a multimodal vocabulary composed of discrete tokens representing both text and speech. Fathullah et 2\nVisual Speech Unit\nBased D eduplication\nInstructionPredicted Tokens\nFor VST Task\nTranslate this English speech to {TGT LANG }. Input:For VSR Task\nRecognize this speech in English. Input:LLM( QLoRA )\nInput Video\nToken Embedding\nVisual EncoderIwould love to hear from you\nReduced Visual Features\nVisual Features16 7 7 7 9 9\nVisual Speech Unit MappingAveragingFigure 1: Illustration of our VSP-LLM framework. Visual speech representations encoded from the visual encoder\nare mapped to visual speech units. Then the visual speech representations are reduced through averaging based on\nthe mapped visual speech units. These reduced representations are fed into the LLM along with text instructions.\nal. (Fathullah et al., 2023) have employed LLaMA as a speech recognition decoder so that the speech sequence features obtained from a conformer encoder were designed to be directly mapped into text tokens, the domain of LLaMA. Moreover, Wu et al. (Wu et al., 2023a) have tried to address the inherent problem of mismatched sequence lengths between speech signals and text, while taking LLaMA as a speech translation decoder. So, they have compressed the speech sequence feature and matched its sequence length with that of the text. However, while the existing studies have primarily focused on incorporating LLMs with the audio speech modality, the exploration of such integration for visual speech processing remains unexplored. In this paper, we propose a novel framework that integrates visual speech processing with LLM. Specifically, we attempt to mitigate the homophenes problem, one of the key challenges in the field of visual speech processing, by leveraging the rich context modeling capabilities of LLM. Additionally, to address the training load issues arising from the integration of the visual speech model and LLM, we introduce the concept of a visual speech unit. Through the implementation of visual speech units, we propose a novel visual speech deduplication method that compresses redundant representations while preserving contextual information. 3 Method Figure 1 shows the overall framework of the proposed Visual Speech Processing incorporated with LLM (VSP-LLM). It includes a visual encoder that 214embeds the input video into the input space of a pretrained LLM, a visual speech unit based deduplication module that discards redundant information in contiguous frames, and an instruction embedding component that serves as a task specifier. In the following, we describe each component in detail. 3.1 Visual-to-Text Space Mapping Our primary objective is to employ the rich context modeling capability of LLM in our visual speech modeling. To accomplish this, we need to represent the input video in a manner that aligns closely with linguistic information, thereby facilitating the association between visual inputs and the text space of the pre-trained LLM. Motivated by the recent success of the self-supervised speech models (Hsu et al., 2021; Shi et al., 2022) that showed the learned representations are highly correlated with phonetic information ( e.g., phoneme) (Pasad et al., 2023), we employ A V-HuBERT (Shi et al., 2022) for our base visual encoder. Then, a learnable visual-to-text embedding layer is introduced to map the visual representations into the input space of LLM. We name this process as visual-to-text space mapping. To investigate how well the visual representation aligns with the text embedding space of the LLM, we compute the cosine similarity between the visual speech representation and the token embeddings of the LLM, mapping it to the text token with the highest similarity. Figure 2a shows an example of a textualized visual speech representation. An intriguing observation is that, with well-structured visual-text space mapping, textu3\nalized visual speech representations can exhibit pronunciation resembling real words. However, we observe redundant information when mapping entire video frames to text due to the similarity of adjacent frames. For instance, words like ’is’ and ’a’ are repeated multiple times, and the word ’social’ is mapped as a long stretch. This redundancy increases computational load when visual speech representations are fed into LLM. To address this, we propose a novel method called \"Visual Speech Unit-based Deduplication\" to remove redundancy while retaining semantic content. 3.2 Visual Speech Unit based Deduplication Compared to the length of the input video, the length of the text is much shorter. This is similar to the relationships between speech and text in Automatic Speech Recognition (ASR) (Graves and Graves, 2012), where the input speech is almost always longer than the output text. Therefore, when we map visual speech representations into text space through visual-to-text space mapping, the resulting embedded output matches the length of the input video frames. If we directly provide it to the LLM, a large computational burden is inevitable. Here, we note that the video is smooth in temporal and the contiguous frames contain overlapped information, and propose to reduce the length of the embedded representation before feeding it to the LLM. To this end, we first extract the pronunciation cue from the visual representations through discretization. Recent literature (Lakhotia et al., 2021) shows that discretized self-supervised speech features, termed speech units, contain phonetic information while suppressing non-linguistic variations. Motivated by this, we propose to extract a visual version of speech units, namely visual speech units, which can be obtained by performing K-means clustering on the self-supervised visual speech representations. By doing this, we can access the pronunciation information for each video frame without requiring any text input (Lee et al., 2022). Then, by employing the visual speech units as pseudo text, we investigate the overlapped contiguous frames. Finally, the corresponding visual features are averaged out. For instance, if the obtained visual speech units are {7,7,7,16,9,9}as illustrated in Figure 1, then the visual features at positions 1, 2, and 3 are averaged together, and those at positions 5 and 6 are averaged, resulting in 3 frames. We find that GT: race is a social category that has staggering biological consequences\n(a)è R RRran rareaitaitsessits is isisisa aaaSS SSSS so sososoclosedoccialcialual\nALal -Alg CccCAT Manattatter particularacggororryorryy veryringDM 를führteplainbothhalt\nVerein that that that tom have have hass S SSSSST stSt KraastastACAGggeryringinging\nら,>'; occas cham МB by bybiiarierALLL Laronon COVIDGGICICalALALlessAlgK C CCom \nCom ComCONontransSpCicacququQententnesscesssssssiss ementittel\n(b)L R r it itрезультате is isaexpr S Sso social ALC Catacoriendo)'])) entrepregin that have has\na SststStoakagggeringing ,цію occas foisbyionLognGAL CComkon スicqualentceessчis\nis social that has\nstaggering consequencesa social\nthat has staggering\nconsequences\naFigure 2: Textulaization results of the visual speech rep-\nresentations. GT, (a), and (b) indicate the ground truth,\ntextualization without deduplication, and textualization\nwith deduplication, respectively.\nthe proposed visual speech unit based deduplication reduces the sequence lengths by about 46.62% compared to the input video lengths. Most importantly, we observed that the deduplication process does not result in any drop in performance. The reduced visual features, when converted into text (Figure 2b), maintain the meaning of each word while the duplication of each word has been removed. For instance, the recurrence of ’is’ and ’a’, which appeared multiple times in the original feature, is reduced, and the length of ’social’, which has a long stretch, is also drastically reduced. 3.3 Multi-task Learning with Instruction One advantage of bridging LLMs into visual speech processing is that we can leverage the versatility of LLMs as well. To investigate this, we train the proposed VSP-LLM with two tasks, VSR and VST. VSR aims to recognize the input silent speech while VST aims not only to predict the recognized speech but also to translate it into the target language. We design the system so that tasks can be controlled by inputting instructions directly into the LLM. When performing the VSR task the instruction is set to as below, Recognize this speech in English. Input: ${Dedupped_Visual_Feature} where the deduplicated visual features are inserted after the instruction. Otherwise, to perform VST, the following instruction is employed. Translate this English speech to ${TGT LANG}. Input: ${Dedupped_Visual_Feature} where the target language is used for the position ofTGT LANG . The objective function for each task can be written as follows, L=−LX\nl=1logp(yl|X, I, y<l), (1) 4\nwhere Xis input video, Iis instruction used, ylis the l-th text token of the ground truth sentence, y<lis the previous predictions, and Lis the length of ground truth. Please note that this is the first work exploring a unified framework of VSR and VST. For training, we employ the recently proposed QLoRA (Dettmers et al., 2023) to further relieve the computational load in training LLM. 4 Experiment 4.1 Dataset Lip Reading Sentences 3 (LRS3) (Afouras et al., 2018) is the most widely-used dataset for VSR, which comprises 433 hours of English audio-visual speech corpus with transcription data. These corpora are collected from the TED and TEDx talks. We utilize the LRS3 dataset to measure the VSR performance of the proposed unified model. Multilingual Audio-Visual Corpus (MuA ViC) (Anwar et al., 2023) is a multilingual audio-visual dataset designed for speech recognition and speechto-text translation. It includes 1200 hours of audiovisual corpus in 9 languages, providing full transcriptions and covering 6 English-to-X translations, as well as 6 X-to-English translation directions. To evaluate the VST performance of our model, we utilize English-to-X translation data from MuA ViC dataset, where X can be among four languages, Spanish (Es), French (Fr), Portuguese (Pt), and Italian (It). For training our model, we combine the LRS3 dataset and English-to-X translation data of MuA ViC. 4.2 Implementation Details Preprocessing. The video is resampled at 25 fps, and facial landmarks are detected using RetinaFace (Deng et al., 2020). Mouth regions are cropped using bounding boxes of size 96×96and converted to grayscale. During training, we apply data augmentation by randomly cropping the video to 88×88and horizontally flipping it. Architecture. We use the A V-HuBERT large (Shi et al., 2022) pre-trained on LRS3 (Afouras et al., 2018) and V oxCeleb2 English (Chung et al., 2018) as our visual encoder. In all experiments, except the ablation part, we utilize 200 clustered visual speech units. For the LLM, we adopt LLaMA2-7B (Touvron et al., 2023) and fine-tune it using QLoRA (Dettmers et al., 2023) with the rank value of 16 and a dropout rate of 5%. To align the dimensions of the visual representation from the visual encoder 381to the LLaMA input embedding, we use a single linear layer as our visual-to-text embedding layer. Training and evaluation. We follow A V-HuBERT (Ren et al., 2021) except for the number of updates and learning rate. We conduct training with a learning rate of 5e−4and the number of updates is 15K updates for LRS3 1h, 5h, 10h, and 30K updates for LRS3 30h and 433h. For VSP-LLM (FT), the visual encoder is frozen for the first 18K steps and then unfrozen afterward. Adam optimizer is employed for training with β1= 0.9andβ2= 0.98, utilizing a tri-stage learning rate scheduler. The training process is executed on 8 3090 RTX GPUs. For decoding, we use a beam search with a beam width of 20 and a length penalty of 0. We assess the performance of our model using Word Error Rate (WER) for the VSR task and BLEU score (Papineni et al., 2002) for the VST task. We use total FLOPs per epoch as a metric to measure the model operation count during training. 4.3 Experimental Results 4.3.1 Comparison with State-of-the-arts In this subsection, we compare the proposed unified model with state-of-the-art VSR and VST methods. Please note that the proposed model can perform multi-tasks VSR and VST with a single trained model while the other models need a single model per specific task. Table 1 presents the performance comparisons of the proposed method with state-of-the-art VSR methods on the LRS3 dataset. The top section of Table 1 outlines the performance of current supervised approaches that depend on extensive labeled training data, while the lower section presents a comparison with other self-supervised methods. Table 1 demonstrates that our approach achieves performance on par with others by employing just 30 hours of labeled data, despite the proposed unified model’s ability to handle multiple tasks—VSR and VST—simultaneously. When employing 433 hours of training data, our method achieves a WER of 26.7%. By fine-tuning the VSP-LLM(FT) with an unfrozen visual encoder, we further enhance our performance, achieving a WER of 25.4%, surpassing other self-supervised approaches. Moreover, Table 1’s upper part shows that the existing supervised methods record exceptional performance using (tens of) thousands of labeled data. However, it is important to highlight that the proposed unified model can obtain comparable performances to 5\nMethodPre-training\nData (hrs)Labeled\nTraining Data (hrs)Recognition\nTaskTranslation\nTaskWER(%)\nSupervisedAfouras et al. (2018) - 1,519 ✓ 58.9\nShillingford et al. (2019) - 3,886 ✓ 55.1\nMakino et al. (2019) - 31,000 ✓ 33.6\nPrajwal et al. (2022) - 2,676 ✓ 30.7\nMa et al. (2021b) - 595 ✓ 30.4\nMa et al. (2023) - 3,448 ✓ 19.1\nSerdyuk et al. (2022) - 90,000 ✓ 17.0\nChang et al. (2023) - 100,000 ✓ 12.8\nSelf-supervisedA V-HuBERT (Shi et al., 2022) 1,759 30 ✓ 32.5\nV ATLM (Zhu et al., 2023) 1,759 30 ✓ 31.6\nRA Ven (Haliassos et al., 2022) 1,759 30 ✓ 32.5\nAKVSR (Yeo et al., 2023a) 1,759 30 ✓ 29.1\nVSP-LLM 1,759 30 ✓ ✓ 29.8\nA V-HuBERT (Shi et al., 2022) 1,759 433 ✓ 28.6\nV ATLM (Zhu et al., 2023) 1,759 433 ✓ 28.4\nRA Ven (Haliassos et al., 2022) 1,759 433 ✓ 27.8\nAKVSR (Yeo et al., 2023a) 1,759 433 ✓ 27.6\nVSP-LLM 1,759 433 ✓ ✓ 26.7\nVSP-LLM(FT) 1,759 433 ✓ ✓ 25.4\nTable 1: The performance comparisons with state-of-the-art VSR methods. Compared to the self-supervised\nmethods, the proposed VSP-LLM, which can perform both VSR and VST, achieves state-of-the-art recognition\nperformances. We also evaluate the performance of a fine-tuned VSP-LLM(FT) with an unfrozen visual encoder.\nMethodLabeled\ndata(hrs)BLEU ↑\nEn-It En-Fr En-Pt En-Es Avg\nAnwar et al. (2023) 433 15.1 16.8 15.1 19.2 16.6\nA V-HuBERT 433 16.6 19.4 17.4 21.7 18.8\nCascaded (A V-HuBERT + MT) 433 17.6 19.5 17.4 22.4 19.2\nVSP-LLM 30 16.1 19.3 16.6 20.7 18.2\nVSP-LLM 433 17.9 22.3 18.7 22.7 20.4\nVSP-LLM(FT) 433 17.7 22.2 19.4 22.4 20.4\nTable 2: Experimental results for English to target lan-\nguage (En-X) translation on the MuA ViC benchmark.\nseveral supervised methods. Table 2 presents the comparison results of VST performance. We construct two baseline models for comparison. The first, A V-HuBERT, is trained similarly to our approach, utilizing both VSR and VST datasets. The second model is a cascaded system that incorporates a pre-trained A V-HuBERT for VSR with a neural machine translation model (Fan et al., 2021). Through this comparison, our proposed VSP-LLM demonstrates superior VST performance across four English-to-X translation tasks, achieving BLEU scores of 17.9, 22.3, 18.7, and 22.7 for English to Italian, French, Portuguese, and Spanish, respectively. The VSP-LLM(FT) shows a better performance 19.4 BLUE score on 446translation from English to Portuguese and comparable performances in other languages. Moreover, it is worth noting that the proposed method achieves an 18.2 BLEU score on average with only 30 hours of labeled data, outperforming the bilingual speech translation model (Anwar et al., 2023) trained with 433 hours of labeled data. 4.3.2 Effectiveness of Rich Context Modeling We have developed a unified model incorporating LLMs to leverage their advanced context modeling capabilities. Therefore, in this section, we conduct a qualitative experiment to demonstrate the effectiveness of the proposed VSP-LLM in handling homophenes, a challenging problem that requires substantial context understanding to accurately identify homophenes. Figure 3 shows several transcription examples obtained from A V-HuBERT and our model, illustrating how our proposed method accurately generates words by considering the entire context of a sentence. For instance, in a homophene case, A V-HuBERT incorrectly transcribes \"i\", a word which visually resembles \"eye\" on the lips, but differs in meaning. On the other hand, our method correctly generates \"eye\", successfully completing the idiom \"eye to eye\" to describe mu6\nHomophene Cases Other Cases\nGround Truth : iam fascinated by those times when people do not see eyeto eye Ground Truth : it's a composite view that's constantly changing and being updated\nAV-HuBERT :iam fascinated by their times when people who do not see iand i AV-HuBERT :it's a compositive view that's constantly changing and being updated\nVSP -LLM : iam fascinated by those times when people you do not see eyeto eye VSP -LLM : it's a composite view that's constantly changing and being updated\nGround Truth : it's not like teaching them how to ridea bike Ground Truth :and when italk to judges around the united states which ido all the \ntime now they all say the same\nAV-HuBERT :it's not like teaching them how to write a bike AV-HuBERT :and when italk to just around the united states which ido all the time \nnow they all say the same\nVSP -LLM : it's not like teaching them how to ridea bike VSP -LLM :and when italk to judges around the united states which ido all the \ntime now they all say the same\nGround Truth : it's like a piece of junk mail to be thrown away Ground Truth :if you want this experience to live on as something historic then at the \nreception\nAV-HuBERT :it's like a piece of chunk bear is being thrown away AV-HuBERT :if you want this experience to live on and something is a story that has \na reception\nVSP -LLM : it's like a piece of junk mail being thrown away VSP -LLM :if you want this experience to live on as something historic that's what \nhappened to\nGround Truth : but it's not about fire and brimstone either Ground Truth : so when you're born you can make feelings like calmness and\nAV-HuBERT :but it's not about fire and brip stone either AV-HuBERT :so when you're born you can make feelings like copness and\nVSP -LLM : but it's not about fire and brimstone VSP -LLM : so when you're born you can make feelings like calmness andFigure 3: The qualitative results showing that the contextual modeling ability of LLM, which is adopted in our\nmethod, can improve the homophene problem and other confusing cases. The red and blue words indicate the wrong\npredictions from A V-HuBERT. However, as shown in the examples, the proposed method can generate correct\nwords by considering the entire context ( e.g., ‘i’ to ‘ eye’).\n0510152025303540~2 2~4 4~6 6~Word Error Rate (%)\nVideo length of test sample (sec)\nMa et al. Prajwal et al. AV-HuBERT Proposed Method41.1\n38.036.734.7\n31.6 31.1\n22.524.5\n20.2\n17.121.327.328.6\n16.8 17.0\n12.9\nFigure 4: VSR performance analysis on LRS3 with vary-\ning video length of test samples. Due to the strength of\ncontextual understanding ability of LLM, the proposed\nmethod shows superior performance with longer videos.\ntual understanding between individuals. Similarly, A V-HuBERT’s transcription of \"write\" is contextually inappropriate for a sentence discussing teaching the physical skill of riding a bike. Our method, however, accurately outputs \"ride\" resulting in the correct phrase \"ride a bike\". Also, we can observe similar results in the other cases, not the homophene problem only. For example, the proposed method can generate the word “composite” according to standard English usage, unlike A V-HuBERT, which erroneously outputs \"compositive\". These results corroborate that our approach can more effectively comprehend contextual clues and generate more precise and natural answers, due to the integration of LLM. Additionally, we evaluate the VSR performance 487Number of\nClustersBLEU ↑ Length of\nsequenceFLOPs (P)\nEn-It En-Fr En-Pt En-Es Avg\n- 12.3 15.8 13.7 16.7 14.6 1.00 62.4\n2000 11.2 15.9 13.8 16.5 14.4 0.70 53.8 (13.8%)\n200 12.1 15.4 13.6 16.8 14.5 0.53 45.6 (26.9%)\n50 12.1 14.9 13.3 16.9 14.3 0.45 41.0 (34.3%)\nTable 3: Analysis on computational efficiency with vary-\ning number of visual speech unit clusters. When the\ndeduplication strategy is adopted, the proposed method\nobtains comparable performances with greatly reduced\nsequence length and training FLOPs.\nacross various video length segments to explore the effectiveness of LLM in handling long speech. Figure 4 shows that WER decreases as video length increases. Notably, our proposed method exhibits outstanding recognition performance, with a WER of 12.9% on videos longer than 6 seconds. Furthermore, our method demonstrates consistent performance improvements as the length of the video increases, compared to other methods. It indicates the effectiveness of LLM’s context modeling in longer video utterances, which demand a more comprehensive understanding of context. 4.3.3 Effectiveness of Deduplication We conduct experiments to assess the effectiveness of our deduplication strategy. For the deduplication process, the number of clusters for visual speech units is required to be determined, and we show the effectiveness according to the number of clus7\n“What do you”\n(a)\n43 79 89 124 (b)“So we did”\n141 186 138 (c)“I don’t  know”\n46 171 0 96 181 112Figure 5: Visualization results showing how video frame\nfeatures are deduplicated and mapped into visual speech\nunits. By doing so, the redundant frame features can be\nreduced efficiently.\nters. Table 3 presents these results, and the first row shows the performance of the baseline which does not utilize the deduplication. The baseline obtains an average BLEU score of 14.6 with 62.4 peta FLOPs per training epoch. By applying the proposed deduplication, our method acquires comparable performance, while significantly reducing the sequence length and computational resources (FLOPs). Specifically, with 200 clusters for visual speech units, our method not only maintains a similar performance level with a 14.5 average BLEU score but also cuts the sequence length by 53%. Consequently, the FLOPs are greatly reduced to 45.6, marking a 26.9% decrease. These experiments confirm that deduplication, applied to visual speech units, effectively eliminates redundant information. Moreover, we delve into the deduplication process by examining it at the video frame level to check whether consecutive visual features, characterized by similar lip movements, are grouped into the same visual speech unit. Figure 5 provides several visual examples alongside their corresponding phrases and video frames. In Figure 5 (a), as a speaker articulates “ What do you ”, it’s noted that 11 video frames can be expressed by 5 visual speech units. For instance, the visual sequences for the sound “wha” belong to the same 43rd unit. Similarly, Figure 5 (c) illustrates that the four frames corresponding to “I” can be efficiently represented by the 46th and 171st visual speech units. Through this analysis, we confirm that visual features with similar lip shapes can be effectively deduplicated, significantly reducing the visual sequence’s length. 4.3.4 VSP-LLM in Data-limited Situation Leveraging the contextual understanding capabilities of LLM, which are pre-trained on vast text 542MethodLabeled\nData(hrs)BLEU ↑\nWER(%) ↓\nEn-It En-Fr En-Pt En-Es Avg\nA V-HuBERT 1 0.0 0.0 0.1 0.1 0.5 100.2\nVSP-LLM 1 1.0 2.8 2.0 1.7 1.8 84.84\nA V-HuBERT 5 1.4 3.8 2.0 1.7 2.2 71.9\nVSP-LLM 5 10.6 14.0 11.5 15.1 12.8 36.2\nA V-HuBERT 10 3.0 5.1 3.9 4.5 4.1 56.7\nVSP-LLM 10 12.1 15.4 13.6 16 8 12.8 34.3\nA V-HuBERT 15 3.4 7.1 5.5 8.7 6.2 52.4\nVSP-LLM 15 13.5 16.9 14.2 17.0 15.4 32.8\nTable 4: Impact of the amount of labeled data. It shows\nthat a small amount of labeled data is sufficient to con-\nstruct a unified VSR and VST model by leveraging\ncontextual understanding capabilities of LLM.\ncorpora, we suppose that a small amount of labeled data is sufficient for constructing a unified VSR and VST model. This is because the proposed VSPLLM endeavors to establish visual-to-text mapping while entrusting the task of language modeling to the LLM. To validate it, we train VSP-LLM on the MuA ViC dataset with different amounts of labeled data; 1 hour ,5 hours ,10 hours , and 15 hours . For comparison, we also develop A VHuBERT on the same data. Table 4 displays the VSR and VST performances. In all experimental conditions, regardless of the amount of data used, our proposed method significantly outperforms A VHuBERT. Moreover, when using only 15 hours of labeled data, our unified method achieves a WER of 32.8%. This is a noteworthy achievement, particularly when compared to the previous VSR (Makino et al., 2019) model achieving a WER of 33.6%, by using 31k hours of labeled data for training. 5 Conclusion In this paper, we proposed a novel framework, Visual Speech Processing with LLMs (VSP-LLM), designed to leverage the context modeling ability of LLMs. Through this framework, we built a unified model that can perform multi-tasks, VSR, and VST, with a single model. Moreover, the proposed deduplication strategy reduces the redundant information of visual speech representations based on pronunciation information modeled from visual speech units. Through extensive experiments, we verified that the proposed deduplication method can reduce the visual sequence length by about 50% with minimal performance degradation. In addition, we validated the effectiveness of the VSPLLM by achieving a superior performance in the MuA ViC benchmark with only 30 hours of labeled data. 8\n6 Limitations We have proposed a powerful visual speech processing method that incorporates LLMs to recognize and translate lip movements into other languages, leveraging the rich context modeling ability of LLMs. Despite the impressive improvement in the performance of this proposed method, the utilization of LLMs has been limited to VSR and VST tasks. We expect that the proposed VSPLLM framework can be expanded to in real-world communication scenarios by utilizing additional non-verbal cues such as facial expressions and gestures. Especially, the VSP-LLM combined with non-verbal cues is expected to perform various tasks such as emotional recognition and dialog generation, starting with this paper as a foundation. 7 Broader impact and ethics The integration of Large Language Models (LLMs) within our framework plays a pivotal role in its ability to handle the complexities of visual speech across different languages. LLM brings a deep understanding of contextual and linguistic information, which is critical for accurately interpreting and translating visual speech cues. This capacity for nuanced language processing underpins our confidence in the framework’s potential for broader linguistic applicability. Moreover, our experiments have demonstrated exceptional data efficiency and significant performance gains with relatively small amounts of labeled data for each language. This efficiency is crucial for scalability to other languages and dialects, particularly those for which extensive labeled datasets may not be readily available. The ability to achieve robust performance with limited data is indicative of the framework’s adaptability and its potential for expansion to a wider linguistic range. 616",
        "aspect_review": "The paper introduces a novel framework, VSP-LLM, which effectively incorporates large language models (LLMs) into visual speech processing, demonstrating strong technical depth and rigor. The methodology includes a unique deduplication process that significantly reduces computational load while maintaining performance. The experimental results are comprehensive and demonstrate state-of-the-art performance in both visual speech recognition (VSR) and visual speech translation (VST) tasks, even with limited training data. The framework shows superior performance over existing models, highlighting its significant practical impact.\n\nThe related work section could be more comprehensive by including a broader range of recent studies in visual speech processing and LLM integration. While the integration of LLMs into visual speech processing is novel, the paper could further emphasize the distinct differences from closely related works. Some additional experiments could be conducted to explore the framework's performance under different conditions or with other datasets. The evaluation could be expanded to include more qualitative analyses to provide deeper insights into the model's performance. Though the results are impressive, further analysis on error cases could provide more clarity on the framework's limitations. The paper could benefit from a discussion on the potential biases in the datasets used and how they might affect the results. The presentation of the results could be improved with clearer tables and figures, ensuring all data is easily interpretable. Some aspects of the methodology, such as the choice of hyperparameters, could be detailed more thoroughly.",
        "general_review": "The paper introduces a groundbreaking framework, VSP-LLM, that incorporates Large Language Models (LLMs) into visual speech processing, achieving state-of-the-art performances in Visual Speech Recognition (VSR) and Visual Speech Translation (VST). This novel approach effectively addresses the challenge of homophenes by leveraging the context modeling capabilities of LLMs, a significant technical advancement. The proposed deduplication method significantly enhances computational efficiency by reducing visual sequence lengths by approximately 50% without performance loss, showcasing strong technical depth and rigor. The framework's ability to perform multi-tasks with a single model, using only 30 hours of labeled data, demonstrates comprehensive experimental validation and significant practical impact, especially in data-limited scenarios.\n\n1. Minor improvements in writing clarity could enhance the paper's readability, particularly in the explanation of the deduplication process. 2. Small details on hyperparameters and implementation specifics could be elaborated to aid reproducibility. 3. Very minor formatting adjustments on figures would improve visual clarity. 4. Slight clarification on the availability of code would help in assessing the reproducibility of the results."
    },
    "3": {
        "paper": "Generative Subgraph Retrieval for\nKnowledge Graph–Grounded Dialog Generation\nAnonymous ACL submission\nAbstract\nKnowledge graph–grounded dialog generation requires retrieving a dialog-relevant subgraph from the given graph and then seamlessly integrating it with the dialog history. Previous works typically represent a graph using either an external encoder such as graph neural networks and retrieve relevant triplets by similarity between single-vector representations of triplets and a dialog history. However, the external encoders cannot leverage the knowledge from trained language models, and the retrieval procedures are also suboptimal since the single-vector summarization of dialog history causes an information bottleneck. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two methods: structure-aware knowledge graph linearization with self-supervised graph-specific tokens and graph-constrained decoding with graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR demonstrates the best performance in knowledge graph–grounded dialog generation, as evaluated on OpenDialKG and KOMODIS datasets. 1 Introduction Dialog generation models aim to generate an informative and appropriate response given a dialog. Pretrained Language Models (PLMs) have shown promising performance on the task (Roberts et al., 2020; Touvron et al., 2023; Achiam et al., 2023). However, they often generate irrelevant, factually incorrect, or hallucinating responses since the generation process relies on the language models’ internal parameters (Lewis et al., 2020; Shuster et al., 2021). To address this issue, many works have explored knowledge-grounded dialog generation 041models, which generate responses using external knowledge (Wang et al., 2020; Zhao et al., 2020). Some works consider unstructured texts such as Wikipedia articles (Dinan et al., 2019) and internet web pages (Ghazvininejad et al., 2018) as external knowledge sources. The other line of works uses external knowledge graphs (KGs) to leverage their structural and semantic information for generating dialog responses grounded on the knowledge graphs (Moon et al., 2019; Galetzka et al., 2021; Tuan et al., 2022; Kang et al., 2023). Recent knowledge graph–grounded dialog generation models such as (Tuan et al., 2022; Kang et al., 2023) first retrieve context-relevant subgraphs from the given knowledge graph to filter out the irrelevant information before generating a response. Many works (Tuan et al., 2022; Kang et al., 2023) encode the dialog history into a single vector and use it on another encoder ( e.g., bi-encoder) to retrieve relevant triplets from the knowledge graph. However, it may lead to information bottleneck since a single vector has a limited capacity to represent a long multi-turn dialog (Humeau et al., 2020; Cao et al., 2021; Lee et al., 2022). They also require separate knowledge graph embeddings or models, such as graph neural networks (GNNs), to represent the knowledge graphs (Galetzka et al., 2021; Tuan et al., 2022; Kang et al., 2023), which cannot effectively leverage the knowledge from PLMs. Other works such as Luo et al. (2024); Xu et al. (2023) address the information bottleneck issue by applying generative retrieval (Lee et al., 2022; Sun et al., 2023). It casts retrieval as an autoregressive generation to facilitate direct interactions between contexts and knowledge paragraphs. However, since they only targets on natural language contexts, they simply linearize knowledge triplets with conventional token representations and decoding strategies, which do not properly account for given graph’s structure and properties. To address the aforementioned issues, we pro1\npose a Dialog Generation model with Generative Subgraph Retrieval ( DialogGSR ), consisting of generative subgraph retriever and response generator. The proposed generative subgraph retrieval uses two graph-specialized methods: a structureaware knowledge graph linearization for representing the graph and graph-constrained decoding for effective generative subgraph retrieval. Our knowledge graph linearization approach adds a small number of specific token embeddings to consider both the structural position of knowledge entities and reverse relations between entities. By selfsupervising the special tokens with knowledge graph reconstruction loss, it effectively represents the knowledge graph. The graph-constrained decoding facilitates autoregressively retrieving the knowledge considering the graph structural information, thereby generating valid and relevant knowledge subgraphs. Since DialogGSR only uses language models for both subgraph retrieval and dialog generation, it can effectively leverage pretrained language models’ knowledge for both tasks. We evaluate DialogGSR on two KG–grounded dialog generation datasets: OpenDialKG (Moon et al., 2019) and KOMODIS (Galetzka et al., 2020). Our proposed method shows the best performance on both benchmark datasets. Our contributions are three-fold as follows: •We propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves the relevant knowledge subgraphs by generating their token sequences. •We design knowledge graph linearization for effective graph representations and graphconstrained decoding for retrieving valid and relevant subgraphs. •We show the state-of-the-art response generation performance on two benchmark datasets, OpenDialKG and KOMODIS. 2 Related Works 2.1 Generative Retrieval Retrieving relevant information from a large corpus such as a text corpus or a knowledge base is crucial in many tasks (Lewis et al., 2020; Chen et al., 2017; Izacard and Grave, 2021; Thorne et al., 2018). Recent studies have demonstrated that generative retrieval models can be more effective than conventional encoder-based retrieval models (Cao et al., 1302021; Lee et al., 2022, 2023; Bevilacqua et al., 2022; Wang et al., 2022). They cast retrieval tasks as generation tasks, where relevant sequences are generated rather than retrieved given input queries. Several studies (Chen et al., 2022a; Thorne, 2022; Lee et al., 2022; Yu et al., 2023; Xu et al., 2023; Luo et al., 2024) have shown the effectiveness of generative retrieval in various knowledge-intensive natural language processing tasks. Motivated by these works, we propose a generative subgraph retrieval model with knowledge graph linearization and graph-constrained decoding for effective graph representation and generation. 2.2 Knowledge-Grounded Dialog Generation Many language generation approaches use pretrained language models (PLMs) (Radford et al., 2019; Devlin et al., 2019; Roberts et al., 2020; Thoppilan et al., 2022; Touvron et al., 2023; Achiam et al., 2023), showing strong performance. However, they often suffer from the hallucination issue (Dušek et al., 2018; Balakrishnan et al., 2019; Dušek et al., 2020), which generates plausible but factually wrong responses since they rely on the models’ internal parameters. To address this problem, recent works have proposed to augment the models with external knowledge sources (Moon et al., 2019; Dinan et al., 2019; Lian et al., 2019). This approach is effective for generating factually accurate results in various language generation tasks (Fernandes et al., 2019; Huang et al., 2020; Yasunaga et al., 2021; Yu et al., 2022; Zhang et al., 2022b). Regarding dialog generation, various works incorporate external knowledge graph into the generation (Moon et al., 2019; Zhou et al., 2021, 2018; Tuan et al., 2019; Zhang et al., 2020). For instance, Space Efficient (Galetzka et al., 2021) proposes an efficient method to encode knowledge triplets. RHO (Ji et al., 2023) generates responses with the dialog history and knowledge graph represented by graph embedding methods ( e.g., TransE (Bordes et al., 2013)). It directly uses gold knowledge, which is practically inapplicable, thereby difficult to fairly compare with the other approaches. DiffKG (Tuan et al., 2022) uses a graph reasoning encoder on top of sparse matrices for graph representations. SURGE (Kang et al., 2023) applies GNNs to retrieve context-relevant subgraphs. Different from these works, our work autoregressively retrieves the context-relevant subgraphs and then generates knowledge-grounded dialogs without requiring separate knowledge graph modules. 2\nFigure 1: The overall inference process of DialogGSR. DialogGSR consists of a generative subgraph retriever\nand response generator. (a) Generative subgraph retrieval autoregressively retrieves subgraphs via generative\nsubgraph retriever with graph-constrained decoding based on entity informativeness score. In step 0, given the\ndialog, GSR retrieves the most relevant triplets by referring to the graph constraint. In step 1, given the dialog\nand the prompt-augmented triplet, we generatively retrieve the next triplets. (b) Resposne generator generates the\nresponses with the dialog and the prompt-augmented generated subgraph.\n3 Methods We aim to generate knowledge-grounded dialog responses with a retrieval augmented generation approach that retrieves context-relevant subgraphs from given knowledge graphs. Specifically, we propose a Dialog Generation model with Generative Subgraph Retrieval (DialogGSR), which consists of a generative subgraph retriever and a response generator. In this section, we first introduce the task of knowledge graph–grounded dialog generation. Next, we propose Generative Subgraph Retrieval (GSR), which generates token sequences of subgraphs through the generative subgraph retriever. For better retrieval, GSR represents the knowledge graph with structure-aware knowledge graph linearization and autoregressively retrieves the knowledge graph via graph-constrained decoding with entity-informativeness scores. Then, we present a response generator, which conducts subgraph–augmented dialog generation. Finally, we introduce the training details of DialogGSR including our self-supervised knowledge graph reconstruction loss. The overall inference process of DialogGSR is illustrated in Figure 1. 3.1 KG–Grounded Dialog Generation The goal of knowledge graph–grounded dialog generation is to generate a dialog response by jointly reasoning over a dialog history and a knowledge graph. We represent a dialog history as a token se- 210quence, x= [x1, x2, . . . , x n], where xi∈ V is the i-th token of the dialog history and Vdenotes the vocabulary set. A knowledge graph is defined as G= (E,R,T), where Eis the set of entities and R is the set of relations. Tdenotes the set of triplets, (eh, r, e t)∈ T , each of which are composed of a head entity eh∈ E, a tail entity et∈ E, and a relation r∈ R between the two entities. We use k-hop subgraph linked to the entities mentioned in the input dialog as retrieval candidates following previous works (Kang et al., 2023). The example of extracting candidate subgraph is in Figure 3. We formulate knowledge graph–grounded dialog generation as follows: pθ(y|x,G) =tY\nj=1pθ(yj|x,y<j,G), (1) where y= [y1, y2, . . . , y t]is the output response, tis the length of the response, and y<j= [y1, . . . y j−1]denotes the generated sequence at the previous time steps. Since a KG can include a huge number of irrelevant entities and relations, extracting subgraphs related to the dialog context is crucial in KG-grounded dialog generation. 3.2 Generative Subgraph Retrieval In this section, we describe Generative Subgraph Retrieval (GSR), which autoregressively retrieves a knowledge subgraph ˆG. Since a knowledge 3\nsubgraph can be represented as a set of triplets 237\b\nτ(1), . . . , τ(k)\t\n, where kis the number of triplets consisting the subgraph, a retrieval of knowledge triplet sequences is regarded as the subgraph retrieval. Many existing subgraph retrieval methods for dialog generation (Kang et al., 2023; Zhang et al., 2022a) obtain the knowledge triplet by measuring the relevance score between the dialog history and each knowledge triplet and retrieving the triplets with high scores. Since these methods encode the long dialog history into a single fixed length of vector, they have a limited capacity to represent multi-turn dialog. This is called the information bottleneck problem (Izacard et al., 2020; Luan et al., 2021). Due to this problem, they suffer from retrieving accurate knowledge, especially when the dialog has many turns. Also, they require independent knowledge graph encoders to represent knowledge graphs, which cannot fully leverage the knowledge available in PLMs. To address these limitations, generative subgraph retrieval methods are used to cast the graph retrieval as graph generation providing a more direct interaction between a dialog context and a knowledge graph by representing the graph with a token sequence. For effective generative retrieval, our GSR uses the following two methods: (1) Structureaware knowledge graph linearization that converts the knowledge graph into token sequences with learnable special tokens considering the connectivity and reverse relations between entities and (2) Graph-constrained decoding that ensures the language model to generate valid knowledge subgraphs and predicts the next tokens considering not only the language model’s scores but also the proximities among the entities on the graph. 3.3 Structure-Aware Knowledge Graph Linearization The goal of the structure-aware knowledge graph linearization is to convert the knowledge graph into a token sequence comprehensible to language models. Our structure-aware knowledge graph linearization augments a sequence of knowledge graph tokens with graph-specific learnable special tokens to help the model represent the knowledge graph without separate graph encoders. Different from prior graph linearization methods such as Luo et al. (2024); Xu et al. (2023) that do not take into account graph connections and reverse relations, our structure-aware knowledge graph 287linearization considers and more effectively represents the knowledge graph structures. Specifically, if there are connected triplets ( e.g., (e1, r1, e2)and(e2, r2, e3)), we efficiently represent the path as [Head] e1[Int 1]r1[Int 2]e2 [Int 3]r2. . . e l+1[Tail] . To represent multiple disconnected triplets or paths, we insert [SEP] between them. For more expressive representations of special tokens, we use multiple consecutive tokens to represent each of [Int],[Rev] , which improves the performance as described in Section A.2. Additionally, since a knowledge graph can contain reverse relations, representing them is crucial in knowledge graph processing (Feng et al., 2020; Qi et al., 2023; Zhu et al., 2024). Therefore, we introduce another special token [Rev] for representing reverse relations when (1) there is a mentioned entity that is the tail of a triplet because the decoding always starts with one of the mentioned entities, or (2) two triplets are connected with opposite directions ( e.g.,(e1, r1, e2)and(e3, r2, e2)). We effectively represent reverse relations by adding special tokens [Rev 1]and[Rev 2]without modifying the relation tokens. For example, given a triplet (e3, r2, e2), the corresponding triplet with the reverse relation (e2,˜r2, e3)is represented as [Head] e2[Rev 1]r2[Rev 2]e3[Tail] . In sum, we represent the subgraph ˆGas the concatenation of the knowledge paths converted with the special tokens as follows: zˆG=[Head] e1[Int 1]r1. . .\nel+1[Tail][SEP][Head] ek···.(2) All the special tokens are learnable with soft prompting. They are learned with both downstream task loss and knowledge graph reconstruction loss, which will be introduced in Section 3.6. Our structure-aware knowledge graph linearization with the special tokens helps the language model capture knowledge graph information without any separate knowledge graph encoders, which leads to the full utilization of the power of PLMs. 3.4 Graph-Constrained Decoding We introduce a graph-constrained decoding method to generate valid and relevant subgraphs. Without the graph constraints, the language model is prone to generating invalid or irrelevant subgraphs due to its bias, disregarding the graph structures (Chen et al., 2022b; Cao et al., 2021). To address this issue, our proposed graph-constrained decoding 4\nmethod injects the knowledge graph information into the language model in the decoding step. Formally, given the dialog xand the output sequence ˜π<tat the previous time step, the log probability of the next token wis computed with logpvocab(w|x,˜π<t, CM).CMis the prefix tree based on the ego-graph (Zhu et al., 2021) of a set of mentioned entities em∈ M as depicted in Figure 1 (right). The mentioned entities are entities appearing in the input dialog history among the entities of a knowledge graph. (Kang et al., 2023) For example, in Figure 1 given the dialog “Do you know Lionel Messi?\", the entity ‘Messi’ corresponds to the mentioned entity since it is in the knowledge graph. The next token prediction probability pvocab is calculated only on the tokens included in the potential next token set of the constraint CM(i.e.,(˜π<t, w)∈ CM). Therefore, the generation model is forced to generate valid knowledge only. In the graph-constrained decoding process, we also reflect the importance of each entity in the knowledge graph by defining the graph-based nexttoken prediction probability, which is formulated as: log ˜p(w|x,˜π<t, CM) =α·logpvocab(w|x,˜π<t, CM)\n+(1−α)·logpgraph(w|˜π<t, CM),\n(3)where pgraph is the probability of predicting the next token based on graph information, and αis a hyperparameter. If the segment of a sequence (˜π<t, w)is the part of the tokenized entity ei, the graph-based next-token probability is defined as: pgraph(w|˜π<t, CM)∝ S(ei,M), (4) whereSis the entity informativeness score of entity eion the mentioned entity set M. If all entities have the same informative score, the prediction probability pgraph of all the entities becomes equal. The next token is decided only with the probability pvocab produced by the language model. So, the structural information of the knowledge graph is not used when retrieving the subgraph. To address it, we introduce a graph structurebased entity informativeness score. To consider the structural proximity between entity eiand mentioned entities em∈ M on the graph, we define the structure-based entity informativeness score (IS) as IS(ei,M) =1\n|M|X\nem∈Ms(ei, em), (5) 381where s(ei, em)is the graph structural proximity between entity eiandem. The proximity between entities can be measured with structural information related to the node pairs such as degree, shortest path, and common neighbors (Katz, 1953; Gasteiger et al., 2019; Brin, 1998). Typical methods for graph structural proximity are the number of connections of the node pairs, which can be defined as scon(ei, em) =P\nN(em)1(ei=em), where N(e)is the neighborhood set of entity e. Since it cannot capture multi-hop relations, we introduce a Katz index–based entity informativeness score (Katz, 1953), formulated as follows: ISkatz(ei,M) =1\n|M|X\nem∈MKX\nk=1βk(Ak)i,m,\n(6) where Ais the adjacency matrix of graph G. Since Akcan be interpreted as the number of paths between entity eiandem, this Katz index–based entity informativeness score captures multi-hop relationships between the node pair different from the simple connection-based score. 3.5 Response Generation After retrieving the subgraphs, we generate the response given the dialog history and the retrieved subgraphs. We first apply the knowledge graph linearization to the retrieved subgraph ˆGto transform it into a token sequence, zˆG. Then, we compose the input sequence ˆxfor our dialog generation model with the linearized subgraph zˆGand the dialog history x(i.e.,x=h\nzˆG;xi\n, where [; ]denotes concatenation). The knowledge graph–augmented dialog input is fed into DialogGSR to generate the response y. 3.6 Training DialogGSR For training DialogGSR, we first self-supervise the special tokens with knowledge graph reconstruction loss. Then, we train our GSR to find knowledge graphs informative to dialog generation. We also train the dialog generator by minimizing response generation loss. Knowledge graph reconstruction loss. Similar to the training scheme of masked language modeling (Roberts et al., 2020; Devlin et al., 2019), we propose a self-supervised method to learn the special tokens for the linearization by masking either an entity token or a relation token in the token 5\nMethodBLEU ROUGE Unigram KQA\nB-1 B-2 B-3 B-4 R-1 R-2 R-L F1 EM F1\nT5 (w/o KG) 15.79 9.19 5.61 3.43 19.67 7.13 19.02 22.21 12.25 20.69\nSpace Efficient (series) 16.15 10.03 6.66 4.50 21.15 8.56 20.44 24.55 36.60 42.64\nSpace Efficient (parallel) 16.33 10.22 6.81 4.64 21.42 8.85 20.68 24.87 38.54 44.34\nEARL 11.49 6.34 4.06 2.75 15.36 4.37 14.61 16.88 32.47 35.88\nDiffKG 15.68 9.13 5.60 3.46 19.50 7.07 18.84 22.26 12.25 20.99\nSURGE (unsup.) 17.77 11.30 7.69 5.36 21.64 9.14 20.75 25.24 48.49 55.77\nSURGE (semi-sup.) 17.70 11.21 7.61 5.28 21.43 8.85 20.57 25.07 51.00 57.63\nSURGE (contrastive) 17.29 11.04 7.54 5.28 21.35 8.98 20.48 25.10 50.45 57.70\nDialogGSR (Ours) 19.30 12.10 8.30 5.83 22.32 9.24 21.23 25.50 54.61 60.57\nTable 1: Response generation performance comparison on OpenDialKG dataset.\nMethod BLEU ROUGE F1\nT5 (w/o KG) 7.58 18.54 16.60\nSpace Efficient (series) 8.34 22.36 17.37\nSpace Efficient (parallel) 9.33 22.80 17.72\nSURGE (unsup.) 11.46 23.49 18.70\nSURGE (semi-sup.) 11.28 23.58 18.68\nSURGE (contrastive) 11.51 24.13 19.51\nDialogGSR (Ours) 11.96 24.47 19.60\nTable 2: Experimental results on KOMODIS dataset.\nsequence of each knowledge path and reconstructing it. Specifically, we first sample k-hop path G′from the knowledge source graph Gand convert it into token sequence zG′. Then, we randomly mask out either an entity or a relation of it. The loss is formulated as LGraphRecon =−logp(zG′|ˆzG′), (7) where zG′is the token sequence of a sampled path andˆzG′is its randomly masked sequence. For example, a knowledge triplet zp=⟨‘Scarlet Letter’, ‘written by’, ‘N.Hawthorne’ ⟩can be randomly masked as ⟨<M>, ‘written by’, ‘N.Hawthorne’ ⟩\n⟨‘Scarlet Letter’, <M>, ‘N.Hawthorne’ ⟩\n⟨‘Scarlet Letter’, ‘written by’, <M> ⟩.Note that masking is done at the entity or relation level as done in Roberts et al. (2020). By minimizing the graph reconstruction loss, our framework self-supervise the special tokens [Head],[Int],[Rev],[Tail] in(2), resulting in better knowledge graph representations. All the other parameters are frozen during this tuning. Graph retrieval loss. To train our generative subgraph retriever, our method minimizes the knowledge subgraph retrieval loss defined as: LRet=Ex[−logp(G⋆|x)]\n=Exh\n−logp\u0010\n˜τ⋆(1), . . . , ˜τ⋆(k)|x\u0011i\n,(8) 450Method path@1 path@3\nSeq2Seq 3.1 18.3\nTri-LSTM 3.2 14.2\nEXT-ED 1.9 5.8\nDialKG Walker 13.2 26.1\nAttnFlow 17.37 24.84\nAttnIO 23.72 37.53\nDiffKG 26.12 44.50\nSURGE 16.76 28.64\nDialogGSR (Ours) 28.96 46.76\nTable 3: Retrieval performance on OpenDialKG.\nwhere G⋆is the gold subgraph, and τ⋆(1)···τ⋆(k)are gold triplets. Response generation loss. We generate dialog responses with dialog history xand contextrelevant knowledge subgraphs ˆGretrieved from GSR. The response generation loss is defined as follows: LGen=Exh\n−logp\u0010\ny⋆|x,ˆG\u0011i\n, (9) where y⋆is the golden response. 4 Experiments 4.1 Experimental Setup For fair comparisons with previous works, we use T5-small (Roberts et al., 2020) as the base PLM. For datasets, we use two datasets (OpenDialKG (Moon et al., 2019) and KOMODIS (Galetzka et al., 2020)). OpenDialKG is a dataset that consists of 15K dialogs with 91K turns and 1.12M triplets from Freebase (Bast et al., 2014) knowledge graph. KOMODIS is a dataset that consists of 7.5k dialogs with 103k turns and the corresponding KG, which contains 88K triplets. We follow Galetzka et al. (2020); Kang et al. (2023) to split the dialogs into train (70%), validation (15%), and test (15%) sets for both datasets. More details are in Appendix B. 6\nGraph Const. Special tokens B-1 B-2 B-3 B-4 path@3\nw/o Const. with Soft Prompt (w/o Recon.) 17.66 10.96 7.32 5.14 10.00\nHard Const. w/o Special tokens 18.51 11.68 7.90 5.60 35.83\nHard Const. with Special tokens (w/o Recon.) 18.90 11.91 7.90 5.44 39.53\nHard Const. with Special tokens (with Recon.) 19.09 11.87 7.96 5.44 43.27\nConnection Const. with Special tokens (with Recon.) 19.14 11.87 8.08 5.57 45.85\nKatz Const. with Special tokens (with Recon.) 19.30 12.10 8.30 5.83 46.76\nTable 4: Ablation study of each component in DialogGSR on OpenDialKG dataset.\nMethod B-1 B-2\nBase (w/o KG) 18.68 11.96\nDialogGSR (w/o Const.) 19.60 13.32\nDialogGSR (ours) 21.10 14.44\nTable 5: Experimental results on OpenDialKG dataset\nwith large language model Llama-3-8b under the fine-\ntuning with LoRA (Hu et al., 2022). ‘Const.’ denotes\ngraph-constrained decoding.\n4.2 Experimental Results We compare our DialogGSR with existing knowledge–grounded dialog generation models on OpenDialKG dataset. Table 1 shows that DialogGSR achieves the best performance in all metrics (BLEU, ROUGE, KQA, and F1 score). In particular, DialogGSR outperforms other baselines on KQA metrics by a large margin (4.61 on EM metric), which indicates that the proposed method generates more factually correct responses with relevant knowledge. In addition, our method achieves a 1.53 performance gain on BLEU-1 metric compared to the best baseline method, which is an 8.61% improvement. The performance gain of DialogGSR compared to SURGE, which retrieves the subgraph with a bi-encoder and uses graph neural networks for graph representations, indicates that our generative retrieval is effective in retrieving relevant knowledge and generating more accurate responses based on the retrieved knowledge. We also conduct experiments on KOMODIS (Galetzka et al., 2020) dataset. Similar to the OpenDialKG result, Table 2 demonstrates that our DialogGSR achieves the best performance compared to all the previous approaches. To further validate the effectiveness of our generative subgraph retrieval, we compare the retrieval performance by path@k metrics. Table 3 shows that DialogGSR achieves the best performance compared to the other baselines. This result indicates that our generative subgraph retrieval successfully retrieves context-relevant subgraphs from the knowledge graph by fully utilizing the power of pretrained language models. In addition to the quantitative results above, we 1 2 3 4\nNumber of turns1020304050path@3\nDialogGSR\nDiffKG\nSURGEFigure 2: Retrieval performance according to the num-\nber of turns.\nalso provide human evaluation in Appendix A.1. It shows that our DialogGSR generates more preferable responses than baselines. 4.3 Analysis We analyze DialogGSR to answer the following research questions: [Q1] Does each component of DialogGSR contributes to a performance improvement? [Q2] Are graph-constrained decoding and the entity informativeness score helpful for retrieving context-relevant subgraphs? [Q3] Is GSR robust to the information bottleneck issue? [Q4] Is our DialogGSR effective with large language models (LLMs)? Ablation studies. We provide the ablation studies to answer [Q1], [Q2] by empirically showing the contribution of each component of DialogGSR in Table 4. w/o Const. is generative retrieval without graph-constrained decoding. Hard const. is the retrieval with graph-constrained decoding but not considering entity informativeness score. Connection andKatz use entity informativeness scores based on Connection ( IScon) and Katz metrics (ISKatz) referred in Section 3.4, respectively. with Special tokens (w/o Recon.) uses special tokens to linearize the knowledge graph without graph reconstruction learning while with Special tokens (w/ Recon.) uses prompts learned with graph reconstruction. Table 4 shows that each component contributes to the performance improvement of the model. In particular, graph-constrained decoding is crucial in our generative approach. 7\nDialog Gold response SURGE (Baseline) DialogGSR (Ours)\n(a) Do you like Shaun White?\n(b) I know he’s an Olympic snowboarder he was funny in\nFriends With Benefits.\n(a) Oh, I’ve never seen that movie, isn’t Mila Kunis in it?\nI love her!\n(b) She is. Justin Timberlake and Woody Harrelson were also\nin it. Shaun just played a small part.\n(a) Do you by any chance remember who Mila Kunis is married\ntoo, I totally forgot.She’s married to\nAshton Kutcher.Mila Kunis is married\nto Jennifer Lawrence.Mila Kunis is married\nto Ashton Kutcher.\nKnowledge triplets τretrieved by Baseline\n⟨‘Justin Timberlake’, ‘place musical career began’, ‘Shelby Forest’ ⟩\n⟨‘Justin Timberlake’, ‘place musical career began’, ‘Millington’ ⟩\n⟨‘Justin Timberlake’, ‘romantic relationship (with celebrities)’, ‘Scarlett Johansson’ ⟩\nKnowledge triplets τretrieved by DialogGSR (ours)\n⟨‘Ashton Kutcher’, ‘romantic relationship (with celebrities)’, ‘Mila Kunis’ ⟩\n⟨‘Friends with Benefits’, ‘starred_actors’, ‘Mila Kunis’ ⟩\n⟨‘Friends with Benefits’, ‘starred_actors’, ‘Patricia Clarkson’ ⟩\nTable 6: Comparison on responses generated by SURGE (Baseline) and DialogGSR given a dialog.\nIn addition, the models with graph constraints show improvements compared to the model without the constraints, which means that the graph constraint is important for generative retrieval of knowledge subgraphs. Also, using entity informative score (Connection, Katz) performs better than graph constraints without it since the entity informativeness score reflects graph structural proximity in the decoding process. Information bottleneck issue. Information bottleneck issue (Humeau et al., 2020; Cao et al., 2021; Lee et al., 2022) usually occurs when a long text sequence, such as a dialog history, is encoded into a fixed single vectors. To explore the robustness of DialogGSR to the information bottleneck issue ( [Q3] ), we compare the retrieval performance of DialogGSR with the baselines such as DiffKG and SURGE with respect to the number of turns in dialog histories in Figure 2. The result shows that DialogGSR is robust for long dialog histories whereas the other methods often deteriorate as the number of turns increases. Effectiveness of DialogGSR with LLMs. To assess the effectiveness of our DialogGSR in other LLMs ( [Q4] ), we apply it to LLaMA-3 (Meta, 2024) in Table 5. From the table, the performance gain of DialogGSR compared to the base model is 2.42 in BLEU-1 score. In addition, our proposed graph-constrained decoding is still important in LLMs. These results indicate that DialogGSR is also effective in other LLMs. Qualitative analysis. We perform qualitative analysis by comparing responses generated from SURGE and DialogGSR. Table 6 shows a sam- 575pled Gold response and the responses generated by SURGE ( Baseline response ) and DialogGSR ( DialogGSR response ) given a multi-turn dialog. From the table, DialogGSR retrieves more informative knowledge to generate responses compared to the baseline. Given the last turn “Do you by any chance remember who Mila Kunis is married too, I totally forgot”, our DialogGSR successfully retrieves the knowledge information related to ‘Mila Kunis’ to help provide the appropriate response from the question while the baseline fails to retrieve information related to answer the question. In contrast, the baseline incorrectly retrieves knowledge information related to “Justin Timberlake”, who is mentioned in the past turn (4th turn), which results in a factually incorrect response. This demonstrates that generative retrieval is effective in retrieving informative knowledge and generating knowledge-grounded multi-turn dialogs. More qualitative results are included in Appendix A.3. 5 Conclusion We have presented DialogGSR, a dialog generation model with generative subgraph retrieval. DialogGSR retrieves context-relevant subgraphs, by generating the subgraph token sequences considering both the dialog context and the graph information. We have proposed novel knowledge graph linearization to convert knowledge triplets into token sequences with self-supervised graph-specific tokens to represent knowledge graphs without separate knowledge graph modules. In addition, we design graph-constrained decoding for valid and relevant generative retrieval. Our experiments demonstrate the effectiveness of our proposed method in knowledge grounded dialog generation. 8\nLimitations DialogGSR generatively retrieves token sequences of the subgraph from the knowledge graph and then generates a response with the retrieved subgraph. However, similar to works using graph retrieval on knowledge-grounded dialog generation, our generative subgraph retrieval retrieves only the knowledge contained in the knowledge graph. Second, the benchmark dataset for knowledge graph–grounded dialog generation is limited. Except for OpendialKG (Moon et al., 2019) dataset, there is no dataset on dialog generation with a large-scale knowledge graph. So, new benchmark datasets on dialog generation with knowledge graphs deserve more attention. Ethics Statement Our DialogGSR does not have any direct negative social impacts, it can potentially be used maliciously, similar to other dialog generation models. These models may produce factually incorrect or biased responses, particularly in sensitive areas such as politics, religion, and diplomacy. To address these risks, we advocate for the release of benchmark datasets without private information and emphasize the need for research into the methods that detect the source of texts. These measures are essential for the responsible development and use of dialog generation technologies. 638",
        "aspect_review": "The paper introduces a novel approach called DialogGSR, which leverages generative subgraph retrieval for knowledge graph-grounded dialog generation. This method integrates structure-aware knowledge graph linearization and graph-constrained decoding, allowing for effective graph representation and retrieval without separate graph encoders. DialogGSR demonstrates state-of-the-art performance on benchmark datasets OpenDialKG and KOMODIS, outperforming existing methods in various metrics such as BLEU, ROUGE, and KQA. The experiments are comprehensive and validate the effectiveness of the proposed approach across different setups. The paper provides a clear motivation for addressing the limitations of existing methods, such as the information bottleneck issue and the inability to leverage pretrained language models effectively. The proposed methods are well-justified, and the results validate the intuition behind the approach.\n\nWhile the experiments are thorough, the study is limited to only two datasets, which may not fully represent the diversity of knowledge graph-grounded dialog scenarios. The paper could benefit from improved clarity in certain sections, such as the detailed explanation of the graph-constrained decoding process. Although the results are strong, the paper could include more qualitative analyses to better illustrate the practical impact of the proposed method in real-world dialog scenarios.",
        "general_review": "The paper introduces a novel approach, DialogGSR, for knowledge graph-grounded dialog generation that combines generative subgraph retrieval with language models, addressing the information bottleneck issue present in previous methods. The proposed structure-aware knowledge graph linearization and graph-constrained decoding methods are innovative, allowing for effective retrieval and integration of knowledge graphs into dialog generation. The paper demonstrates strong technical depth and rigor, with comprehensive experimental validation on the OpenDialKG and KOMODIS datasets, showing state-of-the-art performance. The practical impact is significant, as the approach improves the accuracy and relevance of generated dialog responses by leveraging pretrained language models effectively.\n\n1. The paper could benefit from minor improvements in writing and clarity, particularly in the explanation of complex concepts such as the graph-constrained decoding process. 2. Details on hyperparameters and implementation specifics are somewhat limited, which might hinder reproducibility. 3. Figures could be formatted more clearly to enhance understanding. 4. The availability of the code is not explicitly mentioned, which may limit the immediate impact of the research."
    },
    "4": {
        "paper": "Language Models as Compilers: Simulating Pseudocode Execution\nImproves Algorithmic Reasoning in Language Models\nAnonymous ACL submission\nAbstract\nAlgorithmic reasoning tasks that involve complex logical patterns, such as completing Dyck language, pose challenges for large language models (LLMs), despite their recent success. Prior work has used LLMs to generate programming language and applied external compilers for such tasks. Yet, when on the fly, it is hard to generate an executable code with the correct logic for the solution. Even so, code for one instance cannot be reused for others, although they might require the same logic to solve. We present THINK -AND -EXECUTE , a novel framework that improves LLMs’ algorithmic reasoning: (1) In THINK , we discover task-level logic shared across all instances, and express such logic with pseudocode ; (2) In EXECUTE , we tailor the task-level pseudocode to each instance and simulate the execution of it.THINK -AND-EXECUTE outperforms several strong baselines (including CoT and PoT) in diverse algorithmic reasoning tasks. We manifest the advantage of using task-level pseudocode over generating instance-specific solutions one by one. Also, we show that pseudocode can better improve LMs’ reasoning than natural language (NL) guidance, even though they are trained with NL instructions. 1 Introduction Reasoning in large language models (LLMs) typically entails analyzing the logical structure underlying a problem and realizing the logic into a sequence of reasoning steps to derive the final answer (Zhou et al., 2022a,b; Hao et al., 2023). In particular, algorithmic reasoning has long been a formidable challenge for LLMs, as it requires to scrutinize a complicated reasoning pattern and to translate it into a long sequence of reasoning steps (Suzgun et al., 2022; Valmeekam et al., 2022; Pan et al., 2023). To improve the reasoning capabilities of LLMs, prior works have primarily pursued two directions. The first direction includes enhancing the 042reasoning execution step by generating a rationale in natural language ( e.g., Chain-of-Thought (Wei et al., 2022; Kojima et al., 2022)) or a piece of code ( e.g., Program-of-Thought (Chen et al., 2023), Program-Aided LMs (Gao et al., 2023)). However, such approaches perform step-by-step reasoning on-the-fly, without a dedicated phase for planning. This necessitates that the LLM analyze the logic and execute it within a single inference call, which constrains its expressiveness. Moreover, when encountering a similar problem, the LLM should solve it without being able to reuse the logic previously understood. The second direction involves explicitly generating a plan described in natural language with LLMs. The plan describes the logic of the task and the LLM would subsequently concretize it into a sequence of reasoning steps ( e.g., Least-toMost (Zhou et al., 2022b), Plan-and-Solve (Wang et al., 2023)). Yet, as prior works have mentioned, during our preliminary experiments, we find that natural language might not be the optimal medium to describe the logic of the problem (Li et al., 2023). In addition, prior works mostly rely on generating a plan by observing a single instance, which hinders analyzing the core reasoning pattern shared across similar instances within a single task (Zhou et al., 2024). To address these issues, we introduce THINK AND-EXECUTE , an algorithmic framework that discovers a logic that reflects the shared reasoning pattern behind a given task, and conducts reasoning by tailoring the logic into each instance. THINK AND -EXECUTE consists of three distinctive steps; We first ask an LLM to THINK about common reasoning patterns of a task by providing it with a few example questions. Then, the LLM translates the natural language description of the logic in a pseudocode format. The pseudocode format allows more flexibility in applying the logic to each instance compared to programming language such as 1\nFirst, let's analyz e t he stat ement s: \n1 . Vina t ells t he trut h. 2. Helene sa ys \nVina lies. 3 . Kandi sa ys Helene t ells \nt he trut h. 4 . Jame y sa ys Kandi lies. 5 . \nKa sa ys Jame y lies. F r om stat ement \n2, w e can inf er t hat Helene lies, \nwhich means ...... \n...... w e can inf er t hat Ka t ells t he \ntrut h. Ther ef or e, Ka t ells t he trut h.\n \nTher ef or e,  t he answ er is Y es.def solution():\n    vina = T rue\n    helene = not vina\n    kandi = vina\n    jame y = not kandi\n    ka = not jame y\n    \n    r eturn ‘Y es’ if ka else ‘No’\nsolution()\nInstance fr om W eb of LiesT HINK -and-E XECUTE\nChain-of - ThoughtPr ogr am-of - Thought\n>>> Y esApplied t o \nt he gener at ed\npseudocode\npr omptR ead an giv en input t e xt and answ er t he question in t he \ninput t e xt wit h \"Y es\" or \"No\" .\nInput T e xt: \nVina t ells t he trut h. Helene sa ys Vina lies. Kandi sa ys \nHelene t ells t he trut h. Jame y sa ys Kandi lies. Ka sa ys \nJame y lies. Does Ka t ell t he trut h?\nLLMLLM\nLLMVina sa ys t he trut h. Vina t ells t he trut h: T rue\n...... sa ys Kandi lies. Jame y t ells t he trut h: T rue\nKa sa ys Jame y lies. Ka t ells t he trut h: F alse\nFinal answ er: No def\nf or in\nif\nnot\nelse\nif else (input _ t e xt):\n    stat ement s, question = e xtract _ inf ormation(input _ t e xt)\n    stat ement s = stat ement s.split( ) ...\n    ( , question)\n    trut h _ dict = {}\n\n     stat ement  stat ement s:\n        action = get _ action(stat ement)\n        person 1 , person2 = get _ people(stat ement)\n         action == :\n            trut h _ dict [ person 1 ]  =  trut h _ dict [ person2 ]\n        :\n            trut h _ dict [ person 1 ]  = trut h _ dict [ person2 ]\n        (f\"  sa ys  .  t ells t he trut h: \n\" )\n\n    person _ t o _ check = get _ tar get _ person( question)\n    answ er =   trut h _ dict [ person _ t o _ check ]   \n answ erw eb_of _lies\" , \"\n“Q uestion: ”\n\"lies\"\n'Y es' 'No '\n\n    r eturnprint\nprint{ person 1 } { person2 } { action } { person 1 }\n{ trut h _ dict [ person 1 ]}U se a b str act f unctions t o \ne x pr ess t he logic .P rint ()  stat ement s t o output C o T  r ationales .Figure 1: An illustration of THINK -AND -EXECUTE , compared with Zero-shot Chain-of-Thought (Kojima et al.,\n2022) and Program-of-Thoughts (Chen et al., 2023).\nPython. Finally, in EXECUTE step, the LLM simulates the execution of the task-level pseudocode to follow the logic in it and predicts the output result of the pseudocode. Through extensive experiments on 7 algorithmic reasoning tasks from Big-Bench Hard (Suzgun et al., 2022), we show the effectiveness of THINK -AND-EXECUTE over the challenging baselines. The superior performance of THINK -AND EXECUTE over PoT suggests that discovering the common logic for a given task and applying it to each instance would be more helpful than writing instance-specific code for every instance. Noteworthily, simulating the execution of pseudocode is shown to improve LMs’ reasoning more than planning with natural language (NL), even though they are trained to follow NL instructions. Furthermore, we empirically show that the pseudocode prompt discovered by an LLM can be applied to small LMs (SLMs), such as CodeLlama-7B, to boost their reasoning ability. This indicates the efficiency of THINK -AND -EXECUTE over other code prompting methods that require the LLM to generate instance-specific code every time ( e.g., PoT). To summarize, our contributions are as follows: •We introduce THINK -AND -EXECUTE , a framework that performs reasoning with a pseudocode that contains the common logical structure of a given task. •We show that THINK -AND -EXECUTE achieves notable improvements over strong baselines, including Chain-of-Thought and 115Program-of-Thought prompting, across various algorithmic tasks in Big-Bench Hard. •We demonstrate that the pseudocode written by an LLM can be transferred to SLMs, showing the efficiency of our approach. 2 T HINK -AND -EXECUTE In this section, we introduce THINK -AND EXECUTE and provide a detailed explanation of how LLMs perform reasoning with it. We incorporate an Instructor LM Iand a Reasoner LM R, forTHINK andEXECUTE , respectively. Figure 2 shows the overview of our framework. 2.1 T HINK : Describing the Underlying Logic of a Task in a Pseudocode Format The goal for the Instructor LM Iin this phase is to discover the underlying logic for solving a given task t, and generate a prompt describing the logic, which will be further applied to all instances of the task (in EXECUTE ). This prompt is constructed with pseudocode rather than natural language, which is used in prior work to guide the LM to perform step-by-step reasoning (Kojima et al., 2022; Wang et al., 2023). Step 1: Constructing a meta prompt. To prompt the Instructor LM Ito generate a tasklevel pseudocode for the given target task t, we provide Pof other tasks as demonstrations in a meta prompt.1In practice, we construct the meta 1We manually annotate Pfor each task in Tin advance.\nSee Appendix B.1 for examples.\n2\nT HINK : T ask -le v el InstructionE XECUTE : Instance-le v el R easoning\nLegendInstancesAnsw ersIAAnsw er Pseudocode \nPr ompt Question +R easonerLMT ask 1Question * 3\n......AnalysisT ask 2\n...T ask 3Question * 3AnalysisPseudocode Pr ompt Pseudocode Pr ompt T ar get T ask: W eb of LiesMeta pr omptQuestion * 3Pseudocode Pr omptdef\nf or in\nr eturn (input_t e xt):\n   \n     stat ement  stat ement s:\n        action = get_action(stat ement)\n        \n     answ erw eb_of _liesInstruct or LMInstruct or LM...AnalysisBuilding a trut hfulness map\nPr ocessing stat ement s:  \nCr eat e  a  map  or  dictionar y  t o  r epr esent  t he  \nr elationships  betw een  ......\n:  \nF or  each  stat ement,  updat e  t he  \ntrut hfulness  map  ......  Figure 2: An overview of THINK -AND -EXECUTE . InTHINK (Top), an LLM analyzes the given task provided in\nthe meta prompt and generates a pseudocode prompt that describes the necessary logic for solving the task. Then, in\nEXECUTE (Bottom), the LLM conducts reasoning for each instance by simulating the execution of the pseudocode\nprompt.\nprompt with 3 randomly sampled tasks (3 example questions, analysis, and Pfor each task) from T as demonstrations and the target task t(3 example questions without the answers).2Step 2: Analyzing the target task. Given the meta prompt, Igenerates an analysis containing key reasoning logic that is required to solve the target task regardless of the instances (questions). For example, in Figure 2 (Top), the generated analysis points out that building a truthfulness map and updating it by processing statements are needed to solve the task, i.e., Web of Lies. This step guides I to focus on the reasoning process shared among all the instances, which would be crucial in making a task-level prompt. Step 3: Generating a pseudocode prompt based on the analysis. Next, based on the analysis, Iwrites a prompt Pin the form of pseudocode, which breaks down the necessary reasoning steps for solving the target task. We choose to use the pseudocode format over the form of natural language plan (Kojima et al., 2022; Wang et al., 2023) 2We use the questions of the examples instances in the\nfew-shot prompt in Big-Bench Hard.for two main reasons: (1) the efficiency of it in describing the logic behind a task ( e.g., avoid using repetitive instructions via for loop), and (2) the guidance of what and when to generate rationales via the argument in print() statement and the location within the execution of code. For example, in Figure 2, the Pcontains the statement, print(f\"{person1} says {person2} {action}. {person1} tells the truth: {truth_dict[person1]}\"), which instructs the Reasoner LM to generate a rationale that is helpful in keep tracking of the truth map containing the truthfulness of each person, during the execution of P. We provide more examples in Appendix G. 2.2 E XECUTE : Simulating the Execution of Pseudocode Prompt for an Instance The reasoner LM Rthen conducts reasoning with the generated pseudocode prompt P, tailoring the logic in Pfor the given instance. Following Wei et al. (2022), we aim to maximize the reasoning abilities of the LM by instructing them to explicitly generate intermediate reasoning steps, known as chain-of-thought (CoT) reasoning. Ris instructed to predict not only the final output result of the 3\nReasoner/Method DL GS Nav CO TS SO WL Avg\nCodeLlama-7B\nDirect Prompting 0.0 9.0 39.0 24.4 4.4 11.2 47.6 19.4\nZero-shot CoT (Kojima et al., 2022) 0.0 16.8 26.0 10.8 20.0 10.4 44.8 18.4\nNL Planning 0.0 10.0 52.0 0.4 7.6 18.8 50.4 19.9\nZero-shot PoT (Chen et al., 2023) 0.0 10.0 47.2 23.6 4.4 3.2 45.2 19.1\nTHINK -AND -EXECUTE 2.0 13.2 70.8 49.6 19.2 22.0 38.8 30.8\nCodeLlama-13B\nDirect prompting 0.0 3.2 39.0 28.8 0.0 6.8 37.2 16.4\nZero-shot CoT (Kojima et al., 2022) 0.0 24.8 62.4 28.0 21.6 15.6 44.8 28.2\nNL Planning 1.2 8.8 24.8 28.8 7.2 17.6 53.6 20.3\nZero-shot PoT (Chen et al., 2023) 1.2 16.4 45.6 38.8 10.8 35.6 20.4 24.1\nTHINK -AND -EXECUTE 8.0 18.4 70.4 50.4 25.2 32.4 49.6 36.3\nGPT-3.5-Turbo\nDirect prompting 1.0 33.0 57.0 52.4 41.2 20.0 54.0 36.9\nZero-shot CoT (Kojima et al., 2022) 4.4 46.8 73.2 70.4 44.4 37.6 59.2 48.0\nNL Planning 1.2 35.6 58.8 46.8 32.0 40.0 50.4 37.8\nZero-shot PoT (Chen et al., 2023) 0.4 21.2 77.2 45.6 0.4 28.0 54.0 32.4\nChain-of-Code (Li et al., 2023) 2.8 17.6 57.2 26.0 16.8 29.6 46.4 28.1\nPlan-and-Solve (Wang et al., 2023) 4.0 41.2 84.8 74.8 52.4 37.2 58.0 50.3\nTHINK -AND -EXECUTE 6.0 41.6 96.8 72.0 68.0 65.6 72.8 60.4\nTable 1: Zero-shot performance of THINK -AND -EXECUTE and baselines on seven algorithmic reasoning tasks,\nincluding Dyck Languages (DL), Geometric Shapes (GS), Navigate (Nav), Reasoning about Colored Objects (CO),\nTemporal Sequences (TS), Tracking Shuffled Objectives (SO), and Web of Lies (WL) from Big-Bench Hard (Suzgun\net al., 2022).\ncode, but also the intermediate execution outputs as rationales. Specifically, Rpredicts a list of outputs O={o1, o2, ..., o k}of the pseudocode by simulating the execution process of P, where oidenotes thei-th system output from print() statements, and{o1}k−1\n1are CoT rationales toward the final answer ok. We assume that tracking intermediate execution results would benefit Rto keep track of the state of variables while they change over the execution of the code. We enable Rto mimic the behavior of a compiler with a system message “Generate the expected outputs (from all print() functions) of the code. ”. The final answer for a given question is outputted with “ print(\"Final answer:{answer}\") ” command as the last system output ok. 3 Experimental Setup 3.1 Datasets We curate seven algorithmic reasoning tasks from Big-Bench Hard (Suzgun et al., 2022), including: dyck languages; geometric shapes; navigate; reasoning about colored objects; temporal sequence;tracking shuffled objectives; web of lies. 213These are specifically designed to measure the stepby-step reasoning capability of LLMs. Model performance on evaluated in zero-shot settings, where we do not provide demonstrations in the prompt. We provide detailed explanations in Appendix A.5. 3.2 Baselines We consider the following baselines: (1) Direct prompting : Directly predicting the answer without generating any rationales. (2) Zero-shot CoT (Kojima et al., 2022): A setting where LLMs are evoked to generate the reasoning steps with “ Let’s think step by step ”, before the answer. (3) Zeroshot PoT (Chen et al., 2023): A setting where an LLM generates an instance-specific Python code that can be executed with a Python interpreter. Then, the execution result is used as the final answer. (4) NL planning : A variation of THINK AND -EXECUTE , where the task-level instruction is generated in natural language , instead of pseudocode. 3.3 Models For the Reasoner LM R, we adopt GPT-3.5Turbo (OpenAI, 2023), which shows strong perfor4\nmance in various reasoning benchmarks and code generation tasks (Zellers et al., 2019; Cobbe et al., 2021; Muennighoff et al., 2024), as well as the 7B and 13B versions of CodeLlama (Roziere et al., 2023), which are trained on both code and natural language corpora and further fine-tuned to follow natural language instructions. As for the Instructor LMI, we choose GPT-3.5-Turbo. 4 Results 4.1 T HINK -AND -EXECUTE Improves Algorithmic Reasoning We start by comparing our framework with direct prompting and zero-shot CoT (Kojima et al., 2022) in Table 1. We find that zero-shot CoT performs better than direct prompting with average improvements of 11.1% with GPT-3.5-Turbo, respectively, suggesting zero-shot CoT to be a strong baseline. OurTHINK -AND-EXECUTE , however, further outperforms both of them significantly regardless of model sizes, which indicates that explicitly generating a plan is an effective way to improve the LLM’s reasoning capabilities than simply encouraging LLMs to generate their intermediate reasoning steps. 4.2 Task-level Pseudocode Prompts Benefits a Wider Range of Algorithmic Reasoning Tasks than Instance-specific Python Code In Table 1, PoT shows performance gains in some tasks over direct prompting ( e.g., Navigate; Tracking Shuffled Objects) with Python code generated specifically for each instance and the corresponding interpreter output as the answer. However, such improvement is difficult to generalize to all tasks, e.g., 0.4% accuracy in both Dyck Language and Temporal Sequences, with GPT-3.5-Turbo. By contrast, THINK -AND-EXECUTE outperforms PoT and direct prompting in all tasks with GPT-3.5-Turbo. This suggests that making the task-level strategy with pseudocode and applying it to each instance can benefit LLM’s reasoning in a wider range of algorithmic reasoning tasks than generating instancespecific Python codes. 4.3 The Logic Discovered by an LLM can be Transferred to SLMs We further explore if the pseudocode prompt written by an LLM ( i.e., GPT-3.5-Turbo as the instructor) can be applied to smaller LMs: the CodeLlama family in Table 1. When applying 284Method Avg\nw/o Analysis 21.8\nTHINK -AND -EXECUTE 60.4\nTable 2: Ablation on Step2 of T HINK phase.\nthe pseudocode prompts generated by GPT-3.5Turbo, CodeLlama-7B and -13B significantly outperform direct prompting. Moreover, THINK -ANDEXECUTE with CodeLlama-13B shows comparable performance with GPT-3.5-Turbo with PoT and direct prompting. 4.4 Pseudocode Better Describes the Logic for Solving a Task than Natural Language We also compare our approach with NL planning, a variant of ours that utilizes natural language to write the task-level instruction, instead of pseudocode. In practice, we provide human-written NL plans that contain a similar amount of information toPin the meta prompt and use it to generate the task-level NL plan for the given task. Surprisingly, although the LMs are fine-tuned to follow natural language instructions, we find that task-level pseudocode prompts can boost their performance more than NL plans (Table 1). 4.5 Ablation Studies Components of the pseudocode prompt. We conduct an ablation study on each component of the pseudocode prompt. For that, we prepare four types of pseudocode prompts: (1) Human-written pseudocode ; (2) Human-written prompt w/o comments and semantics by removing the comments that explain the code and replacing variable names with meaningless alphabets, such as X, Y , and Z; (3) Human-written prompt w/ for loop and (4) w/ intermediate print() statements. The results are in Figure 3. Model performance decreases significantly when applying prompts w/o comments and semantics, especially in Temporal Sequences. This implies that semantics play an important role in guiding the LLMs to apply the discovered logic and reasoning with it accordingly. Also, we find that printing out the intermediate execution steps withprint() is crucial in reasoning, which is consistent with the finding from Wei et al. (2022). Generating the analysis before the pseudocode prompt. Table 2 shows a notable decrease in model performance when generating pseudocode 5\n020406temporal sequences tracking shuffled \nobjectivesreasoning about \ncolored objectsnavigatew/o intermediate print() w/o comments & semantics w/o for loop Human-written pseudocodeFigure 3: Ablation study of the components of pseudocode prompt using GPT-3.5-Turbo.\nMethod Avg\nSelf-Discover w/ GPT-4 77.9\nTHINK -AND -EXECUTE w/ GPT-4 81.7\nTable 3: Comparison of THINK -AND -EXECUTE and\nSelf-Discover (Zhou et al., 2024) using GPT-4 on Big-\nBench Hard. The results of Self-Discover are obtained\nfrom the original paper, because the code and prompts\nare not provided. The full results are in Appendix A.4.\nprompts without conducting the analysis first. This suggests that explicitly generating analysis on the task can elicit a better pseudocode prompt that contains the necessary logic for solving the task. 4.6 Comparison with other Baselines We further compare THINK -AND -EXECUTE with another three baselines: (1) Plan-and-Solve (Wang et al., 2023), where an LLM sequentially generates a natural language plan for solving the given instance, step-by-step reasoning according to the plan, and the final answer; (2) Chain-of-Code (Li et al., 2023), where Python code is generated as a part of intermediate reasoning steps specifically for a given instance; (3) Self-Discover (Zhou et al., 2024), a concurrent work that devises a task-level reasoning structure in a JSON format before inferencing the instance. First, as presented in Table 3 (Left), we find THINK -AND-EXECUTE largely outperforms Plan-and-Solve and Chain-of-Code by 10.9 and 32.3 percentage points in terms of accuracy, respectively. Second, while Self-Discover also incorporate task-level instruction, in Table 3 (Right), our THINK -AND -EXECUTE with pseudocode prompts shows better performance when 350using GPT-4 (Achiam et al., 2023).3These findings indicate that generating (1) task-level instruction with (2) pseudocode can better represent the necessary logic for solving a task and benefit LLM’s algorithmic ability. 5 Analysis We conduct experiments to address the following research questions: •RQ1 : Is task-level pseudocode more helpful than instance-specific pseudocode? •RQ2 : Does pre-training on code corpora improve reasoning? •RQ3 : How is the quality of the logic discovered by THINK -AND -EXECUTE compared to human-written logic? 5.1 Implementing the Underlying Logic is more Effective than Instance-specific Logic in Pseudocode (RQ1) We conduct an analysis to check if the improvement of THINK -AND -EXECUTE is contributed by our chosen format for the task-level instruction, i.e., pseudocode. We compare THINK -AND EXECUTE with a concurrent work, Chain-of-Code (CoC) (Li et al., 2023). In Table 1, THINK -AND EXECUTE outperforms CoC, showing about 2x improvement in the average score. The main difference between THINK -AND -EXECUTE and CoC is that we use pseudocodes which are generated to express logic shared among the tasks instances, while CoC incorporates pseudocode as part of the intermediate reasoning steps towards the solution of a 3We usegpt-4-0613 for GPT-4.\n6\n020406dyck languages geometric shapes temporal \nsequencestracking shuffled \nobjectivestracking shuffled \nobjectivesreasoning about \ncolored objectsweb of lies navigateLlama-13B Codellama-13BFigure 4: Analysis on the effect of code pre-training on the reasoning capability in applying THINK -AND-EXECUTE .\nWithout pre-training on code corpora the accuracies drop notably.\nReasoner/Method DL GS Nav CO TS SO WL Avg\nCodeLlama-7B\nHuman-written P 2.4 0.0 40.4 29.6 12.0 18.0 52.8 22.2\nTHINK -AND -EXECUTE 2.0 13.2 70.8 49.6 19.2 22.0 38.8 30.8\nCodeLlama-13B\nHuman-written P 2.8 14.8 72.8 40.4 16.8 15.6 49.6 30.4\nTHINK -AND -EXECUTE 8.0 18.4 70.4 50.4 25.2 32.4 49.6 36.3\nGPT-3.5-Turbo\nHuman-written P 12.4 50.0 86.0 50.8 84.0 32.4 74.4 55.7\nTHINK -AND -EXECUTE 6.0 41.6 96.8 72.0 68.0 65.6 72.8 60.4\nTable 4: Comparison between T HINK -AND -EXECUTE and Human-written P.\ngiven instance. Hence, the results indicate the advantages of applying pseudocode for the generation of task-level instruction over solely using them as a part of rationales. 5.2 T HINK -AND -EXECUTE Requires Knowledge in Code (RQ2) To understand whether SLMs acquire the ability to understand the task-level logic written in pseudocode during pre-training on code corpora, we compare the performance of CodeLlama-13B with Llama-13B using THINK -AND -EXECUTE . In Figure 4, CodeLlama-13B shows better reasoning capabilities compared to Llama-13B in all tasks. These results suggest that the improvement from using THINK -AND-EXECUTE could depend on the knowledge of code, which is usually obtained by pre-training with code corpora. Writing code usually involves understanding the logic behind the given problem and expecting the execution results of a code, which resemble the same reasoning process of T HINK -AND -EXECUTE . 4025.3 Models Prefer Pseudocode from THINK -AND -EXECUTE Compared to Human’s (RQ3) To gauge LLMs’ capabilities in discerning the underlying logic of a task, we compare THINK AND -EXECUTE (using GPT-3.5-Turbo as the Instructor) with human-written pseudocode prompts. The results are shown in Table 4. Using the GPT3.5-Turbo the Reasoner, THINK -AND -EXECUTE scores 60.4% in terms of accuracy, which is superior to the human-written P(with an accuracy of 55.7%). Especially, in the tasks of Navigate and Tracking Shuffled Objectives, pseudocode prompts generated by THINK -AND -EXECUTE elicit better performance. This also holds true when adopting CodeLlama-7B and -13B as the Reasoner, further suggesting the effectiveness of our THINK step over human writers. 5.4 Impact of LLMs’ Capability on THINK -AND -EXECUTE In examining the impact of LLMs’ capabilities within our framework, we investigate the influence of both the Reasoner and Instructor compo7\nReasonerInstructor\nCodeLlama-13B CodeLlama-34B GPT-3.5-Turbo\nCodeLlama-13B 30.9 33.0 36.4\nCodeLlama-34B 32.5 34.2 39.1\nGPT-3.5-Turbo 33.9 35.9 60.4\nTable 5: Analysis of the effect of the capability of Reasoner and Instructor on the performance. We report the\naverage performance on the 7 tasks.\nnents on performance, as depicted in Table 5. Notably, higher accuracy scores are observed when utilizing GPT-3.5-Turbo as Reasoners compared to CodeLlama-13B and CodeLlama-34B. Additionally, the effectiveness of the Instructor also plays a crucial role, with GPT-3.5-Turbo exhibiting the highest accuracy scores across all configurations. These results underscore the significance of both the Reasoner and Instructor components in enhancing the performance of T HINK -AND -EXECUTE . 6 Related Work Chain-of-Thought (CoT) prompting. CoT prompting evokes LMs to generate intermediate reasoning steps that guide and explain the solution (Wei et al., 2022; Wang et al., 2022; Wu et al., 2023). One common paradigm of this is zero-shot CoT prompting (Kojima et al., 2022). Without specifically designed question-explanation-answer triplets as demonstrations, zero-shot CoT prompting elicits a plausible reasoning path towards the final answer with simple instruction, such as \"Let’s think step-by-step\" , eliciting better model performance in tasks that require multi-step reasoning. In the context of improving zero-shot CoT, Wang et al. (2023) propose to first generate a plan breaking down the target task into smaller subtasks, and then solve each subtask according to the plan. Similar to our approach, a concurrent work (Zhou et al., 2024) devises a task-level reasoning structure that can be applied to each instance (question) of the target task. The most significant distinction between these prior studies and ours is that our THINK AND -EXECUTE adopts pseudocode (as opposed to natural language) to express the necessary logic for solving the task. We demonstrate that our tasklevel pseudocode prompt empowers LMs with better ability of zero-shot reasoning than natural language plans under various settings in Section 5. 464Incorporation of code in reasoning. With unambiguous syntax and strict structure, programming languages such as Python have been applied to LLM-based systems to improve system performance in solving tasks. For instance, Gao et al. (2023) and Chen et al. (2023) use LLMs to generate Python code for given mathematical questions, and run the generated code on external compilers to obtain/calculate the answers. Besides, there has been a line of work on improving LLMs’ capabilities with pseudocode (Zelikman et al., 2023; Mishra et al., 2023). Concurrently with our work, Li et al. (2023) present chain-of-code (CoC), where pseudocode is also incorporated along with the Python code for solving a given question (instance). While this approach generates instance-specific code as intermediate reasoning steps for each individual instance, our THINK -AND -EXECUTE , by contrast, focus on the task-level pseudocode prompt that can be applied to all instances. We compare CoC and THINK AND -EXECUTE in Section 4. Another concurrent work (Weir et al., 2024), inspired by our study, delves into training LLMs that are specialized to generate task-level pseudocodes. 7 Conclusion In this paper, we present THINK -AND -EXECUTE , an algorithmic reasoning framework that generates a logic for solving the given task into a pseudocode and performs reasoning by simulating the execution of the pseudocode with language models. Through extensive experiments, we show the effectiveness ofTHINK -AND -EXECUTE , over the strong baselines. These results underscore not only the usefulness of pseudocode in eliciting language models’ reasoning capabilities but also the efficiency of our framework in discovering the high-quality logic behind a given task. 8\n8 Limitations and Discussion A possible limitation of our approach is that we focus on algorithmic reasoning, as we believe it is the best setting to assess LLMs’ capabilities in understanding a complex logic and carrying out a sequence of reasoning step, following the logic. However, we believe that THINK -AND -EXECUTE can be applied to other domains of reasoning that require following a long sequence of reasoning steps, such as multi-hop reasoning (Ji et al., 2020) and symbolic reasoning (Madaan and Yazdanbakhsh, 2022). 514",
        "aspect_review": "The THINK-AND-EXECUTE framework introduces a novel approach to algorithmic reasoning by leveraging task-level pseudocode, which significantly enhances the reasoning capabilities of language models over existing methods. The paper demonstrates comprehensive experimental validation across seven diverse algorithmic reasoning tasks from Big-Bench Hard, showcasing the robustness and applicability of the proposed framework. The work is groundbreaking in its use of task-level pseudocode to capture shared logic across instances, which is a departure from traditional instance-specific coding approaches. The framework's motivation is well-justified by the limitations of current LLM reasoning methods, and its effectiveness is validated through extensive experiments showing superior performance over strong baselines.\n\nWhile the experiments are thorough, additional details on hyperparameter settings could enhance reproducibility. The focus on algorithmic reasoning may limit the perceived impact to broader NLP tasks, though the approach has potential applicability beyond the current scope. Minor clarifications on the code availability and implementation specifics could improve the paper's clarity. Some figures could benefit from minor formatting improvements for better readability.",
        "general_review": "The paper presents a novel framework, THINK-AND-EXECUTE, which significantly improves algorithmic reasoning in large language models (LLMs) by leveraging pseudocode to express task-level logic. This approach is notable for its innovation in using pseudocode over natural language, demonstrating strong technical depth and rigor. The framework is validated through comprehensive experimental results, showing superior performance over strong baselines such as Chain-of-Thought and Program-of-Thought prompting. Moreover, the practical impact is significant as the pseudocode can be transferred to smaller language models, enhancing their reasoning capabilities.\n\n1. Minor improvements in writing clarity could enhance the readability of certain sections. 2. Additional details on hyperparameters and implementation specifics would be beneficial for reproducibility. 3. Some figures could benefit from minor formatting adjustments to improve visual clarity. 4. Slight clarification on the availability and accessibility of the code used in experiments would be useful."
    },
    "5": {
        "paper": "Pixology: Probing the Linguistic and Visual Knowledge of\nPixel-based Language Models\nAnonymous EMNLP submission\nAbstract Pixel-based language models (LMs) have emerged as a compelling alternative to subword-based LMs, particularly because they can represent virtually any script. PIXEL , a canonical example of such a model, is a vision transformer that has been pre-trained on rendered text. While PIXEL has shown promising cross-script transfer abilities and robustness to orthographic perturbations, it falls short of outperforming monolingual subword counterparts like BERT in most other contexts. This discrepancy raises questions about the amount of linguistic knowledge learnt by these models and whether their performance in language tasks stems more from their visual capabilities than their linguistic ones. To explore this, we probe PIXEL using a variety of linguistic and visual tasks to assess its position on the vision-to-language spectrum. Our findings reveal a substantial gap between the model’s visual and linguistic understanding. The lower layers of PIXEL predominantly capture superficial visual features, whereas the higher layers gradually learn more syntactic and semantic abstractions. Additionally, we examine variants ofPIXEL trained with different text rendering strategies, discovering that introducing certain orthographic constraints at the input level can facilitate earlier learning of surface-level features. With this study, we hope to provide insights that aid the further development of pixelbased language models. 1 Introduction Subwords are currently the standard units of processing in language modelling (Sennrich et al., 2016). While they have been shown to work well in monolingual models (Devlin et al., 2019; Liu et al., 2019), in a multilingual context they can lead to an inevitable vocabulary bottleneck with each language competing for space in a finite vocabulary (Rust et al., 2023; Liang et al., 2023). 042Characters and byte-based models have been proposed as alternatives to subwords, but they lead to longer input sequences (Raffel et al., 2020; Xue et al., 2022; Tay et al., 2022; Clark et al., 2022). Another proposed solution is pixel-based models where patches of pixels are the main unit of representation. A canonical example of this is the PIXEL (Pixel-based Encoder of Language) model (Rust et al., 2023), where text is rendered as a sequence of fixed-sized patches and passed as input to a vision transformer (ViT) (Dosovitskiy et al., 2021). This approach allows the model to represent virtually any script. Although current versions of the pixel-based language models do not outperform their monolingual subword-based counterparts on most downstream tasks (Rust et al., 2023; Lotz et al., 2023), they are a promising approach to multilingual modelling and offer a unique opportunity to explore modelling language through images. PIXEL is a juxtaposition of a vision and language model: even though it receives image patches as input, the content of those patches is rendered text, making it a visual model of language. With this study, we aim to understand where PIXEL stands on the vision-tolanguage spectrum. To this end, we probe PIXEL on various visual and language tasks and compare performance with BERT (Devlin et al., 2019) – the language model it is most comparable to – and VIT-MAE (He et al., 2022) – the vision model it is most comparable to. We conduct a comprehensive analysis of the linguistic and visual capabilities of PIXEL that can be used to aid further development of pixel-based language models. Concretely: RQ1: How much linguistic knowledge is encoded in PIXEL ? RQ2: How much visual knowledge does PIXEL have? We find that PIXEL learns surface-level linguistic information in the lower layers, resulting in higher1\nlevel syntactic and semantic abstractions appearing in higher layers than BERT (§5.1). When comparing to VIT-MAE ,PIXEL underperforms on image tasks, with visual probing accuracy decreasing in the higher layers (§5.2). Thus, the surface-level information is diluted as it acquires linguistic knowledge in the higher layers. Lotz et al. (2023) trained newer pixel-based language models that add some orthographic constraints to the input that can potentially augment linguistic learning in the lower layers. In this context, we ask the following question: RQ3: Does adding orthographic constraints to the input enhance the linguistic knowledge in PIXEL ? We find that a rendering strategy that makes word boundaries more explicit in the input enables PIXEL to learn surface-level linguistic features earlier in the model, thereby aiding semantic understanding (§5.3). Overall, we take inspiration from BERTology, the study of the linguistic capabilities in BERT (Rogers et al., 2020), and aim for this work to foster future explorations and advancements for PIXEL . 2 Background 2.1 PIXEL The pixel-based language models examined in this study are ViTs (Dosovitskiy et al., 2021) that lie at the confluence of NLP and computer vision. A ViT is an application of the transformer architecture (Vaswani et al., 2017; Devlin et al., 2019) to process images. An image is split into patches that are each flattened into a vector and then projected into a lower-dimensional space through a linear transformation. Positional embeddings are added to retain spatial information before feeding these patch vectors into a transformer encoder. Inspired by the self-supervised masked language modelling paradigm, a variant of ViT is the masked auto-encoder (He et al., 2022), or VIT-MAE , that learns image representations by masking random image patches. A decoder reconstructs the image from the latent representation of the mask tokens. The PIXEL model by Rust et al. (2023) is trained on the VIT-MAE architecture. It takes a rendered image of text sized 16×8464 as input, which is split into patches of 16×16pixels. Instead of randomly masking individual patches, PIXEL randomly masks spans of patches to force the model to learn higher levels of language abstraction. PIXEL 132is pre-trained on a rendered version of the English Wikipedia and the BookCorpus (Zhu et al., 2015). Thus, it is comparable to BERT in terms of pretraining data and VIT-MAE in terms of architecture and parameters. PIXEL follows the idea of visual text representations by Salesky et al. (2021), who embed rendered text using 2D convolutions for continuous openvocabulary machine translation. They demonstrate that visual text representations are more robust to noise and provide the benefits of a tokenization-free text processing pipeline. Lotz et al. (2023) further improved PIXEL by experimenting with different text rendering strategies. Their work provides insights into the semantic modelling capabilities of PIXEL models and correlates that to frequency bias. We include some of these models in our study. 2.2 Model Interpretability The survey by Zhao et al. (2024) categorises model interpretability into local explanations of predictions and global explanations of model behaviour. Global explanations aim to understand the general concepts encoded in the individual components of a language model. The most prominent method for global explanations of linguistic understanding in language models is probing , specifically classifierbased probing (Belinkov, 2022). In this approach, model weights are frozen and for each of its layers, a small classifier is trained to solve a task given a pooled representation of the intermediate embeddings at that layer. The task is designed to isolate an aspect of linguistic understanding that may or may not be present in the embedding (Adi et al., 2016; Hewitt and Manning, 2019; ¸ Sahin et al., 2020; Zhu et al., 2022). The same idea has been used for investigating computer vision models (Alain and Bengio, 2018; Basaj et al., 2021) and, more recently, multi-modal models (Dahlgren Lindström et al., 2020). A standard framework for linguistic probing is SentEval (Conneau and Kiela, 2018), which includes various probing tasks that uncover different levels of linguistic information in sentence embeddings. SentEval has been extensively employed to analyse models for sentence-level semantics (Ma et al., 2019; Krasnowska-Kiera ´s and Wróblewska, 2019; Ravichander et al., 2021), and it is the dataset we adopt in this study. Linguistic probing has been used prominently in BERTology (Rogers et al., 2020) to understand 2\nType Name Predict for a given sentence... Labels\nLinguistic Probing\nSurfaceSentence Length ( SentLen ) the length. 6 bins\nWord Content ( WC) which one of 1000 possible words is in it. 1SyntacticBigram Shift ( BShift ) if the order of two random words was inverted. 2\nTree Depth ( TreeDepth ) the depth of the syntactic tree. 5-12\nTop Constituents ( TopConst )the sequence of top constituents directly below the\nsentence (S) node.20\nSurface SemanticTense ( Tense ) the tense of the main verb. 3\nSubject Number ( SubjNum ) the number of the subject. 2\nObject Number ( ObjNum ) the number of the object. 2\nComplex SemanticSemantic Odd Man Out ( SOMO )if a noun or a verb has been switched out for another. 2\nCoordination Inversion ( CoordInv ) if the two coordinate clauses have been inverted. 2\nVisual Probing\nVisualMax Count ( MaxCount ) the frequency of the character with the max count. 4 bins\nArgmax Count ( ArgmaxCount ) the character that has the max count. 5 bins\nTable 1: Description of probing tasks used in this study.\nthe levels of linguistic information stored in BERT embeddings (Tenney et al., 2019b; Jawahar et al., 2019; Mehrafarin et al., 2022). It has been established that BERT tends to encapsulate more syntactic knowledge in its middle layers, while semantic comprehension is more pronounced in the higher layers (Tenney et al., 2019a). In this context, we aim to gain analogous insights about pixel-based language models. 3 Probing Tasks We now introduce the probing tasks used in our experiments. We probe PIXEL on two levels: linguistic and visual. For linguistic probing we rely on the SentEval framework mentioned above. ViTs have more direct access to surface-level information than subword-based models, since their input is segmented into units of fixed visual size (as opposed to variable-sized tokens) and shown to the model after a continuous linear projection (as opposed to a lookup). Thus, we also employ tasks that are designed to verify whether orthographic information is more easily identifiable throughout PIXEL . 3.1 Linguistic Probing Tasks The SentEval framework contains probes that quantify three levels of linguistic knowledge present in sentence embeddings: surface ,syntactic , and semantic (Conneau et al., 2018). Table 1 presents all the tasks, with their type and description. We evaluate the performance of the models at each layer on these tasks to explain the hierarchy of linguistic 214understanding contained within the model. We note, however, that all tasks falling under the semantic category do not all probe for the same kind of information. Tense ,SubjNum andObjNum can be solved by trivial surface cues like the presence of certain morphemes like the suffixes -edand -es. However, unlike surface tasks, performance on these tasks does not drastically degrade in the upper layers as the model gains semantic understanding (Jawahar et al., 2019), and they can be predictors of downstream semantic performance (Zhu et al., 2022). Thus, we dub these tasks surface semantic . SOMO andCoordInv , on the other hand, need more complex semantic learning to be solved. We therefore term these tasks as complex semantic . The distinction between surface semantic andcomplex semantic can also be justified by the differences in accuracies between human evaluation and model performance for tasks in both these categories as reported by Conneau et al. (2018). Most neural models are able to either match or surpass human evaluation for the surface semantic tasks, but not for the complex semantic tasks. This recategorization also helps to identify consistencies in linguistic understanding, particularly when explaining trends with BERT . 3.2 Visual Probing Tasks We introduce two new tasks to probe for purely visual information – MaxCount andArgmaxCount (see Table 1). Every word in every sentence of theSentLen task is replaced by a random English 3\nword generated with the wonderwords1library to create synthetic datasets. By using random words instead of a sentence, we ensure that the task is purely visual, but does not disadvantage the BERT tokenizer (as opposed to using random characters which could result in single-character tokens). This also distinguishes them from the surface tasks in SentEval since there is no underlying linguistic pattern to this data. The labels are binned to ensure a uniform distribution and we down-sample the labels that occur with a very high frequency (for example, ‘e’ is the most frequent letter in 50% of the dataset). More task details are in Appendix A. MNIST As a final task to probe for purely visual information, we rely on MNIST (Deng, 2012), which consists of white-on-black images of handwritten digits (0 to 9). It is an image classification benchmark dataset and its resemblance to rendered text as well as the simplicity of the task make it suitable for probing.2We do not evaluate BERT on this task since it cannot represent images. 4 Experimental Setup 4.1 Models Our analysis will primarily focus on the PIXEL base model trained by Rust et al. (2023), further termed PIXEL . We also make a comparison with its variants introduced by Lotz et al. (2023) for RQ3 . Specifically, we look at PIXEL -bigrams, pretrained using the bigrams rendering strategy which ensures that every patch contains at most 2 characters, and that no patch overlaps a word boundary, adding extra space where needed. We also look at PIXEL -small-words, trained on the words rendering strategy that merely enforces the second constraint. Since it has no base version released, we additionally probe PIXEL -small-bigrams and PIXEL -small for a fair comparison.3All these are compared against BERT andVIT-MAE . An overview of the model parameters is in Appendix B. 4.2 Probing We follow the same probing setup as defined by Conneau and Kiela (2018). Sentence representations for each example in the datasets are obtained by mean-pooling the token or patch embeddings generated at every hidden layer for each model. 1github.com/mrmaxguns/wonderwordsmodule\n2Each image is 28×28pixels that we resize to 16×16,\nthe image patch size for one patch in PIXEL .\n3huggingface.co/Team-PIXELThese embeddings are passed to a classifier that learns to predict the corresponding class label using a cross-entropy loss. For our experiments, we use the implementation and default hyper-parameters proposed by Araujo et al. (2022) for both linguistic and visual tasks. 4.3 Fine-tuning For a better understanding of the general linguistic abilities of vision models ( RQ1 ), we fine-tune VIT-MAE on universal dependencies (UD) (Nivre et al., 2016) POS-tagging, dependency parsing and GLUE (Wang et al., 2018) using the same hyperparameters as Rust et al. (2023). We re-use PIXEL ’s text rendering configuration, and render text into a square image of 224×224to match the input size ofVIT-MAE . To gauge the general visual abilities ofPIXEL (RQ2 ), we fine-tune PIXEL andVIT-MAE on the CIFAR100 (Krizhevsky and Hinton, 2009) image classification dataset. 5 Results and Analysis 5.1 RQ1: How much linguistic knowledge is encoded in PIXEL ? To investigate this question, we first compare PIXEL andVIT-MAE fine-tuned on language tasks. This is to assess the extent to which PIXEL ’s pre-training regime makes it better at language tasks than a regular vision transformer. Results are in Table 2. Task PIXEL ViT-MAE BERT\nPoS 96.7 93.1 97.2\nDP 88.7 78.2 90.6\nGLUE avg. 74.1 58.1 80.0\nTable 2: Language fine-tuning results for PIXEL ,BERT (taken\nfrom Rust et al. (2023)) and V IT-MAE\nIt is clear that PIXEL has an advantage over VITMAE . Since PIXEL performs substantially better than VIT-MAE on GLUE, it can be argued that PIXEL learns some semantics. This can be explained either by the domain similarity between PIXEL pre-training and the downstream task input or because its pre-training on language actually enables the model to learn linguistic abstractions. To investigate which of the two factors explains the advantage, we run the linguistic probing tasks onPIXEL ,BERT andVIT-MAE , illustrated in Figure 1. Each plot also includes the majority baseline4for that task as a lower bound for each model. 4The accuracy if always predicting the most frequent label.\n4\nFigure 1: Linguistic probing results on PIXEL ,BERT and V IT-MAE , along with the majority baseline.\nFigure 2: Example of \"cool\" being rendered differently in\ndifferent contexts for PIXEL . The red lines represent patch\nboundaries.\nIf the embeddings do not contain any useful information for the task, we would expect the performance to be equivalent to the majority baseline. The performance of BERT is consistent with what is documented in literature. Surface features are encoded in the lower layers, syntactic features are represented in the middle layers, and semantic features are found in the upper layers (Jawahar et al., 2019). The performance for VIT-MAE for all layers, for most tasks, is very close to the majority baseline. For tasks where some visual information can be useful, for example in SentLen , and Tense (the visual presence of morpheme -edcan be associated with label PAST ),VIT-MAE performs better than the majority baseline but does not improve or decline through the layers. The performance of PIXEL , when higher than VIT-MAE , can thus be attributed to its linguistic knowledge and not due to having input that is closer to the downstream task. Across all tasks, PIXEL consistently has an initial monotonic rise in accuracy, starting with a similar performance as VIT-MAE in the lower layers. This indicates that it is using purely visual information in the lower layers, and learns linguistic information in the higher layers. In other words, PIXEL starts as a visual model, and becomes more of a language model through the layers. However, PIXEL never matches the peak performance of BERT in any layer. This is consistent with the results from Rust et al. (2023), where PIXEL underperforms BERT on the English tasks. We can, therefore, hypothesize that much of PIXEL ’s capacity is used in recovering from the performance gap 363between a vision and language model. Does PIXEL learn syntax and semantics? Unlike BERT ,PIXEL does not have a consistent curve across the surface ,syntactic andsemantic tasks. This is most striking in the surface tasks. For BERT , there is an inverse relation between model depth and accuracy. For SentLen , the accuracy curve of PIXEL rises until layer 5 and then stagnates. For WC, on the other hand, it has a steep rise in the initial layers until layer 7, where it starts to drop. The task requires a good knowledge of word-level features and boundaries - something that is encoded in BERT already at the input level, but PIXEL has to learn during training. We illustrate this further in Figure 2. The patches encoding the word \"cool\" differ when used in the context of a sentence compared to when it is rendered alone Thus, it may take more layers for PIXEL to reconcile the two different embeddings as the same word. Lotz et al. (2023) have also commented on this phenomenon and linked it to poor downstream semantic performance. They also found that PIXEL -based language models form better contextualised word representations in the upper layers of the model. We can extrapolate this phenomenon to explain the initial monotonic rise in other tasks. For syntactictasks, PIXEL peaks at layer 9, later than BERT , then stagnates or declines. This delay leads to a delayed learning of higher level abstractions, suggesting that PIXEL needs more layers to match BERT ’s performance. We leave this question to future work. The performance across the surface semantic tasks for PIXEL shows some consistency. There is a steep rise until layer 3, after which the curve has a more gradual rise, crossing BERT accuracy in the higher layers. For complex semantic tasks, both PIXEL and BERT achieve peak performance be5\nFigure 3: Visual probing results.\ntween layers 9 and 12. However, the performance gap between the two is substantial, indicating that PIXEL does not learn semantic abstractions at the same level as BERT . This is also substantiated by the difference in the downstream performance gap between BERT and PIXEL for syntactic and semantic tasks, mentioned in Table 2. PIXEL ’s performance on dependency parsing and POS-tagging is very close to BERT , while its performance on GLUE, which contains tasks requiring more semantic understanding, is about 6% lower. The drop in performance for surface tasks in the higher layers also indicates that PIXEL forgets some surface level information as it learns more linguistic abstractions. We substantiate this further with the results on the visual probing tasks below. 5.2 RQ2: How much visual knowledge does PIXEL have? We investigate this question by first probing PIXEL on the visual tasks introduced in §3.2 to understand whether it is indeed forgetting the surface level information in the higher layers. Results are shown in Figure 3. For both MaxCount andArgmaxCount , we see thatVIT-MAE has the highest performance in the lower layers, followed by PIXEL and then BERT . BERT performance has a steady decline, much like thesurface tasks in Figure 1. PIXEL ’s performance is much closer to VIT-MAE , but it does not have much decline through the layers, leading to a higher performance than VIT-MAE in the higher layers. PIXEL has slight increases in performance in the middle layers, analogous to the performance peaks insurface tasks. The substantially higher performance than BERT , combined with the similarity to VIT-MAE performance, indicates that PIXEL still retains much surface level information in the higher layers. PIXEL ’s high performance on surface semantic tasks in the higher layers also substantiate this since PIXEL has access to both surface and semantic information. 442Model Accuracy\nPIXEL 0.52\nVIT-MAE 0.83\nRandom Model 0.42\nTable 3: Results for PIXEL ,VIT-MAE andVIT-MAE with\nrandomised weights fine-tuned on CIFAR100 for image clas-\nsification.\nFigure 4: MNIST probing results\nCan PIXEL be a vision model? IfPIXEL still retains much surface information in the higher layers, is it able to perform well on vision tasks? To investigate this question, we present fine-tuning results for PIXEL andVIT-MAE in Table 3. If PIXEL performs competitively, it implies that PIXEL is fundamentally a vision model that has acquired some language understanding. We also fine-tune a transformer of the same size with randomized weights as a lower bound baseline. The performance gap between PIXEL andVITMAE on image classification is analogous to the performance gap between the two on the GLUE tasks in Table 2. Thus, even though PIXEL is a vision transformer and it retains much surface level information, its pre-training regime on language has lead to a substantially worse performance on image classification, much closer to the random baseline than to VIT-MAE . It can be argued that PIXEL ’s poorer performance on CIFAR-100 is due to a domain mismatch, stemming from its pre-training on black-and-white text, which offers limited exposure to the color and complexity of the input. To disentangle this, we probe PIXEL on MNIST at every layer. The results are in Figure 4. The curves for PIXEL are consistent with the curves in Figure 3 and surface tasks in Figure 1, in that there is a performance decline through the layers. The difference is that PIXEL performance declines immediately after layer 1, and unlike Figure 1, it is at a lower accuracy than VIT-MAE in the lower layers. Thus, even on input that is similar to the data that PIXEL was pre-trained on, PIXEL does not match V IT-MAE performance. 6\nFigure 5: Selected linguistic probing results for small PIXEL\nvariants, base models are indicated with dotted lines.\n5.3 RQ3: Does adding orthographic constraints to the input enhance the linguistic capabilities in PIXEL ? Results from §5.1 and §5.2 establish that PIXEL learns surface level information in the lower layers, which leads to delayed learning of higher level semantics. This raises the question of how the gap between visual and linguistic understanding in layers 1 - 6 (the layer with peak performance on surface tasks) can be bridged earlier in the model. Encoding words with differing visual patch representations, as shown in Figure 2, can be made easier by ensuring consistent rendering of words across contexts. The added constraints to the rendering in the PIXEL -variants may lead to a faster learning of surface level information and word boundaries in the lower layers, as discussed in §5.1, thereby making PIXEL behave more like BERT . This idea is further justified by the fact that PIXEL -bigrams and PIXEL -small-words have better downstream performance than PIXEL . Probing results on selected tasks for PIXEL -small, PIXEL -small-words, and PIXEL -small-bigrams are in Figure 5. We also include BERT -base and VIT-MAE -base in the graphs for reference. At the small scale, PIXEL suffers an almost catastrophic decline in probing performance, showing no more linguistic understanding than VITMAE . Similarly, PIXEL -small-bigrams also does not demonstrate any meaningful linguistic understanding. PIXEL -small-words, on the other hand, displays probing performance comparable to 508PIXEL -base, even at the small scale. It starts with much higher accuracy than VIT-MAE in layer 1 indicating that there is already linguistic information present in the initial layers due to the imposed structure at the input level. It also achieves peak performance in most tasks earlier than PIXEL -base. Specifically, for WC, the accuracy rises only until layer 4 before it declines. The curves for syntactic tasks are more similar to BERT , with the lower layers achieving scores higher than PIXEL . A combination of visual and some semantic understanding leads to scores for surface semantic tasks being even higher than BERT in the upper layers. For complex semantic tasks, however, the curve rises until layers 7-8 and then plateaus, indicating higher semantic abstractions are still not being learnt by the model. Since PIXEL -small-bigrams and PIXEL -small do not have any meaningful linguistic representations at the small scale, we compare the base versions of the two models on the linguistic probes and find similar trends. PIXEL -bigrams at both the base and small scale performs worse than PIXEL . Specific results and analysis can be found in Appendix C. Why is fine-tuned PIXEL -bigrams better than fine-tuned PIXEL ?The observation above is at odds with the downstream performance of PIXEL bigrams, which Lotz et al. (2023) found to be better than PIXEL . To understand this discrepancy, we run the linguistic probes on fine-tuned versions of the models. We fine-tuned the PIXEL -base-bigrams model on UD parsing (syntactic) and MNLI (semantic) with the same hyper-parameter setup as PIXEL , and compare them to the fine-tuned PIXEL models made available by Rust et al. (2023) on the same tasks. Results are in Figure 6 and Figure 7. We see that across all probing tasks, fine-tuned PIXEL -bigrams demonstrates better performance than fine-tuned PIXEL . Merchant et al. (2020) found that finetuning BERT on dependency parsing shows effects throughout the model, but MNLI only affects the top layers. Moreover, fine-tuning can cause the model to potentially forget some linguistic knowledge. Mehrafarin et al. (2022) also echoed that fine-tuning on tasks with larger data sizes (like MNLI) can lead to loss of linguistic information in the pre-trained encodings. We see this trend in PIXEL , where both UD and MNLI fine-tuning decrease probing performance onBShift andSOMO . There is a slight decline in performance on all other probing tasks with UD 7\nFigure 6: Selected probing results for PIXEL and PIXEL -\nbigrams finetuned on UD.\nfine-tuning, but with MNLI fine-tuning, the performance remains similar to pre-trained PIXEL . We observe the contrary with PIXEL -bigrams. Both UD and MNLI fine-tuning have enhanced the linguistic knowledge encoded in all the layers, with probing performance compared to PIXEL -bigrams pre-trained being much higher. Additionally, UD fine-tuning particularly increases probing performance on syntactic tasks in the top layers, and MNLI fine-tuning similarly increases probing performance on the complex semantic tasks in the top layers. Thus, we can speculate that the inductive bias learnt during fine-tuning creates better linguistic representations in PIXEL -bigrams. 5.4 Summary of Findings On the spectrum of vision and language, it can be concluded from the results of RQ1 andRQ2 that PIXEL is more of a language model than a vision model. The difference in downstream performance between PIXEL andVIT-MAE is much larger than PIXEL and BERT . Although with a lower accuracy, PIXEL ’s behaviour with linguistic probing is more similar to BERT than V IT-MAE . However, much of PIXEL ’s linguistic understanding is surface level. The lower layers in PIXEL learn surface level information, as demonstrated by the visual and linguistic probes. The linguistic knowledge acquired in the upper layers demonstrates some syntactic understanding, but does not capture very strong semantic information. This indicates Figure 7: Selected probing results for PIXEL and PIXEL -\nbigrams finetuned on MNLI.\nthat adding more layers to the model could allow the model to have better semantic representations. While that is a solution on the architecture side, from RQ3 results we can conclude that on the input side PIXEL -words comes out to be the current best solution to bridging the gap between visual and language understanding in the lower layers in the model. Nevertheless, it still lacks in semantic understanding, and as Lotz et al. (2023) have noted it is not very efficient to train. PIXEL -bigrams has worse linguistic probing performance than PIXEL , but the inductive bias it learns during fine-tuning dramatically improves the linguistic knowledge encoded in its layers. PIXEL , on the contrary, forgets some linguistic information during fine-tuning. 6 Conclusion This study is a first step towards understanding the language modelling capabilities of pixel-based models. Although these models exhibit substantial linguistic understanding, the nature of image-text representations leads to a gap in visual and linguistic understanding. Pixel-based models need to learn the discrete representations that subwordbased models already have access to at the input level. Adding orthographic constraints to the input can help bridge this gap, but further architectural modifications could improve these models more, which is a promising direction for future work. 8\n7 Limitations Our main approach to understanding the linguistic information encoded in pixel-based language models is probing. We acknowledge that although this is our primary method of inquiry, it comes with its flaws. Belinkov and Glass (2019) have noted that even though certain information is detected by a probe as being present in the embeddings, it does not necessarily imply that the information is used by the model. They also remark that using a deeper auxiliary classifier for the probe may lead to better results. There are other criticisms of the approach like Hewitt and Liang (2019) that question whether the probe uncovers information encoded in the embedding, or just learns the linguistic task itself that it is trained on. Pimentel et al. (2020) challenge this and present evidence of the former. Zhu and Rudzicz (2020) recommend using a control mechanism to select probes, based on discussions about the dichotomy raised above. Thus, although this does not dismiss the validity of our findings, we note that our results and conclusions should be read with these caveats in mind. 642",
        "aspect_review": "The paper introduces a novel approach by employing pixel-based language models as a compelling alternative to subword-based models, showcasing a unique intersection of NLP and computer vision. The study provides a thorough comparison between PIXEL, BERT, and VIT-MAE, offering insights into the linguistic and visual capabilities of pixel-based models relative to established language and vision models. The findings reveal a substantial gap between visual and linguistic understanding in PIXEL, with lower layers capturing visual features and higher layers learning syntactic and semantic abstractions. This insight is vital for future model development.\n\nThe motivation for choosing specific probing tasks could be elaborated to strengthen the justification for their selection in evaluating linguistic and visual knowledge. Some details on the experimental setup, such as hyperparameter choices, could be more explicitly stated to enhance reproducibility. While the evaluation is comprehensive, the paper could benefit from additional analysis on how different rendering strategies impact performance across diverse languages. The paper's impact could be broadened by discussing potential applications of pixel-based language models beyond the scope of the current study. The methodology could be improved by incorporating more diverse datasets to validate the generalizability of the findings across different linguistic contexts.",
        "general_review": "The paper presents notable novelty by exploring pixel-based language models (LMs), an emerging alternative to subword-based LMs, which can represent any script. It demonstrates strong technical depth and rigor through comprehensive probing of PIXEL across linguistic and visual tasks, revealing insights into its position on the vision-to-language spectrum. The study provides comprehensive experimental validation by comparing PIXEL with BERT and VIT-MAE, showing the model's strengths and weaknesses in various contexts. Additionally, it offers significant practical impact by suggesting improvements in pixel-based models, such as introducing orthographic constraints to enhance linguistic learning.\n\n1. Minor writing or clarity improvements could be made to ensure smoother transitions between sections, particularly in the results and analysis parts. 2. Small details on hyperparameters or implementation specifics are lacking, which could aid in reproducibility. 3. Very minor formatting issues are present in some figures, which could be improved for better readability. 4. Slight clarification on code availability would enhance the paper's utility for practitioners looking to build upon this work."
    },
    "6": {
        "paper": "SPROUT : Green Generative AI with Carbon-Efficient LLM Inference\nAnonymous submission\nAbstract\nThe rapid advancement of generative AI raises significant environmental concerns, notably carbon emissions. Our framework, SPROUT , demonstrates the carbon footprint reduction of generative LLM inference by over 40% in realworld evaluations, marking a critical effort to align NLP with eco-friendly initiatives. 1 Introduction The AI boom, driven by the demand for generative artificial intelligence (GenAI) (Nijkamp et al., 2023; Jumper et al., 2021; Pierce and Goutos, 2023; Chen et al., 2023a), has prompted concerns over its environmental impact, particularly in terms of carbon emissions associated with the datacenters hosting these technologies. OpenAI’s reported pursuit of trillions in investment for AI chips (Fortune, 2024), destined for their datacenter infrastructure, underscores the scale of resource expansion required to support GenAI’s growth. Generative large language models (LLMs) have gained a substantial user base across various scientific fields (Singhal et al., 2023; Lin et al., 2023; Liu et al., 2024, 2023b; Christofidellis et al., 2023). This underscores a critical need for research focused on minimizing LLMs’ environmental impact. Although training these models requires extensive compute cycles and carbon footprint, it is the inference processes of these LLMs that are poised to become the predominant source of emissions, according to various prior studies (Chien et al., 2023; Wu et al., 2022; de Vries, 2023). Unlike traditional natural language understanding models that predict a single masked word or sentiment, generative LLMs are even more carbon-demanding as they perform iterative predictions for each request until reaching a predefined token or iteration limit. Despite the urgency of this issue, there lacks a solution for reducing carbon emissions specifically from the 038LLM inference operations – which is natural given the field is in early stages, but rapidly evolving. In this paper, we design SPROUT as the first work to address the sustainability challenges in running a generative LLM inference service. Various previous works have attempted to reduce the carbon footprint of machine learning (ML) applications (Wu et al., 2022; Acun et al., 2023a; Li et al., 2023a), but none has designed optimizations tailored to LLM inference which is becoming a dominant workload and requires intervention to reduce its carbon footprint. The following summarizes S PROUT ’s insights and contributions. Introduction of generation directives to LLM inference for carbon saving. Previous works have identified the opportunity to manipulate the number of parameters in the model to save energy and cost (Wan et al., 2020; Romero et al., 2021), while SPROUT is the first work to identify that in generative language model inference, its autoregressive generation pattern presents a unique opportunity beyond previous works. SPROUT introduces the concept of “generation directives”, a strategy to indirectly manipulate the number of autoregressive inference iterations while providing high-quality content generation. For example, a directive can guide the model to provide a concise response, saving significant carbon from generating a long sequence while still being accurate. Identifying the variability in the carbon intensity of the electricity generation and the diverse requirements of different tasks, SPROUT can leverage different generation directives to minimize the carbon footprint of LLM inference with a guarantee of generation quality. Design, implementation and evaluation of carbon-friendly generation directive configuration for LLM inference. We present SPROUT , a novel carbon-aware generative language model inference framework designed to reduce carbon footprint through the strategic use of token genera1\ntion directives while maintaining high-quality outputs. From the selection of directive levels based on electricity grid carbon intensity and user behavior variability, SPROUT introduces a linear programming approach for system-level optimization, balancing carbon savings with generation quality. SPROUT identifies the difficulty in retrieving generation quality feedback, and implements an automatic offline and opportunistic quality assessment mechanism to ensure the framework’s decisions are informed by up-to-date generation quality. We evaluate SPROUT using production software setup, state-of-the-art LLM, representative corpus to synthesize user prompts, and real carbon intensity traces from global electricity grid operator regions. Our evaluation confirms SPROUT ’s effectiveness in reducing carbon emissions by more than 40% while still achieving high generation quality. 2 Background and Motivation Carbon footprint of an inference request. The carbon footprint is a metric for quantifying the amount of greenhouse gas emissions (gCO 2) generated. When requesting a service from a datacenter server (e.g., HTTP requests), its carbon footprint comprises the operational carbon andembodied carbon . The operational carbon comes from electricity generation to power the datacenter, which powers the hardware (e.g., GPUs) that serves the request (carbon intensity ×energy). The carbon intensity (denoted as COIntensity\n2 ) of the grid, representing the amount of CO 2emission per unit of energy usage (gCO 2/kWh), reflects the “greenness” of the energy source. For example, wind turbines have lower carbon intensity than coal power plants. Due to the temporal difference in availability of renewable energy, carbon intensity varies over time. Embodied carbon (denoted as COEmbed\n2 ) represents the carbon emissions associated with the manufacturing and packaging of computer components, effectively “embodied\" within the device itself. We follow the methodology in (Gupta et al., 2021, 2022) to model the embodied carbon. For an inference request processed by a computing device, its share of embodied carbon is proportional to the execution time relative to the device’s overall lifespan. The total carbon footprint of serving an inference request, Creq, can be formally expressed as: Creq=COIntensity\n2 ·Ereq+COEmbed\n2\nTlife·Treq (1) Llama2\n7BLlama2\n13B0.000.020.040.06CO2 per\nRequest (g)\n0 500 1000 1Number of Generated Tokens0.00.10.20.3CO2 Emission\nof Generated\nTokens (g)\nLlama2 7B\nLlama2 13B\nLinear Fit\n(a) (b)Figure 1: Quantifying the impact of factors on an infer-\nence request’s carbon footprint: (a) the number of model\nparameters and (b) the number of generated tokens.\nHere, EreqandTreqrepresent the energy consumption and execution time for the request, respectively, withTlifeindicating the assumed device lifespan, set to five years for this analysis. Given that the lifespan of the device significantly exceeds any single request’s execution time, operational carbon dictates the total carbon footprint, except in scenarios where COIntensity\n2 approaches zero. Motivational Empirical Study and Opportunities. We make three major observations. Takeaway 1. The LLM inference carbon footprint depends on not only the model size but also the number of tokens generated, presenting a new opportunity to reduce carbon without being forced to choose a smaller model size. In Fig. 1 (a), we demonstrate how the carbon footprint of LLM inference changes with model size, showcasing examples with the Llama2 model at 7 billion (smaller model) and 13 billion parameters (larger model). In Fig. 1 (b), we execute a series of input prompts on the Llama2 7B and 13B model and observe that there is a strong linear correlation between the total carbon emission and the volume of tokens generated from request. The autoregressive token generation iteratively predicts the subsequent token until an end-ofsequence (EOS) token emerges or a predefined limit is reached. Despite initial computations to pre-fill the KV cache with key and value vectors from the input prompt, we show that the overall carbon emission of a request is largely dictated by the quantity of generated tokens . Our experimental results show that rather than naively relying on smaller models and potentially compromising the contextual understanding capabilities, we can potentially infer from a larger size model but focus on generating fewer tokens (Fig. 1 (b)). Takeaway 2. Incorporating generation directives into prompts can significantly reduce the carbon footprint by enabling concise yet accurate responses. To control the LLM token generation length, we introduce “generation directive\". 2\n\u0007\u0006\f\u0007\u0007\u0006\f\n\u0007\u0006\r\u0007\u0007\u0006\r\n\b\u0006\u0007\u0007\u000f\u001b\u001c\u001c\u0015\u0013\u001e\u001a\u0015\u001d\u001d\u0003\u0004\u0010\u001b\u001c\u0019\u0006\u0005\u0007\u0006\u0007\u0007\u0007\u0006\t\n\u0007\u0006\n\u0007\u0007\u0006\u000b\n\b\u0006\u0007\u0007\u000e\u001f\u0015\u001c\u0012\u0016\u0015\u0003\u000f\u0011\t\u0004\u0010\u001b\u001c\u0019\u0012\u0018\u0017 \u0015\u0014\u0005<prompt> How old is the Earth approximately? (A) 50,000 years (B) 300 million years (C) 4.5 billion years (D) no one knows<generation directive L0 (default)> Based on a variety of geological and astronomical evidence, including …. While …, the scientific consensus is (C): 4.5 billion years old.<generation directive L1 (brief)> (C). The Earth is approximately 4.5 billion years old.(a)(b)(13B, L0)(7B, L0)(13B, L1)BetterFigure 2: (a) Example of applying generation directive.\n(b) Hosting larger models (e.g., Llama2 13B) with gen-\neration directives can outperform smaller models (e.g.,\nLlama2 7B) in both carbon emission and correctness.\n\u0018\u001d#\u001f'\u001d\u001f\u0014\u0014\u0013\u001a\u0019)#-#\u001b\b\u0007\b\b\u0007\t\b\u0007\n\b\u0007\u000b\b\u0007\f\u000f-\u001f)\u001b!\u001f\u0003\u0011\u0016\n\u0004\u0015()&\u001b%#.\u001f\u001e\u0005\u0011\u001b)\u001c('\u0003\u0012&#**#('\u0013\b\u0003\u0004\u001e\u001f \u001b,%+\u0005\u0013\t\u0003\u0004\u001c)#\u001f \u0005\u0013\n\u0003\u0004\u001c)#\u001f \u0006\u0006\u0005\n\u0018\u001d#\u001f'\u001d\u001f\u0014\u0014\u0013\u001a\u0019)#-#\u001b\b\u0007\r\b\u0007\u000e\t\u0007\b\t\u0007\t\t\u0007\n\u0011())\u001f\u001d+'\u001f**\u0003\u0017\u001b+\u001f\u0004\u0015()&\u001b%#.\u001f\u001e\u0005\u0010\u001f'\u001d\"&\u001b)$\u0003\u0018\u001d()\u001f1.01.01.0\nFigure 3: Applying generation directives across differ-\nent tasks reveals varied sensitivity to these directives.\nDefinition 1 Ageneration directive is an instruction (e.g., “respond concisely”) that guides the model to generate tokens. Each generation directive level specifies a pre-defined text sequence that acts as this guiding instruction. In Fig. 2 (a), we show a prompt from the MMLU task (Hendrycks et al., 2020). Without using any specific directives (level L0), the Llama2 13B model defaults to generating an extensive number of tokens. However, such detailed background information may not always align with user preferences. Applying a generation directive (level L1) ensures both brevity and correctness. This practice demonstrates the potential to reduce carbon emissions from token generation. Fig. 2 (b) demonstrates such potential quantitatively by measuring the CO 2emission and MMLU correctness rate. It shows that employing generation directives with a larger model (13B, L1) significantly outperforms smaller models (7B, L0) in both carbon and the accuracy of generated content. This is attributed to the larger model’s superior contextual understanding, which, when combined with concise generation directives, retains its comprehensive knowledge base without unnecessary verbosity, highlighting the advantage of optimizing response generation instead of model sizes. Takeaway 3: The impact of employing generation directives on carbon emissions and accuracy differs across user tasks, presenting an interesting challenge in optimally utilizing these directives, particularly in the context of fluctuating carbon intensity. In Fig. 3, we show the impacts of different generation directives (L0, L1, L2) 202on different tasks including science knowledge (Lu et al., 2022) and trivia knowledge (Joshi et al., 2017). We observe that both the amount of carbon emission and the generation’s correctness rate vary with the task. Responding to these challenges, we design SPROUT , a generative LLM inference framework that takes advantage of generation directives to dynamically optimize the carbon footprint while guaranteeing high-quality generations. 3 S PROUT Design 3.1 System Overview and Key Ideas SPROUT is designed as the first carbon-aware generative language model inference framework, utilizing token generation directives to minimize carbon footprint while ensuring high-quality content generation. Fig. 4 shows a brief design overview of SPROUT . Once the user prompts are assigned to an inference server by the load balancer, they are tokenized into numerical representations. In this phase, a generation directive selector 1assigns a directive level to each prompt, integrating it into the tokenized input. The policy for assigning directive levels is established by SPROUT ’s token generation directive optimizer 2, as detailed in Sec. 3.2. This optimizer systematically considers the carbon intensity of the local grid and the feedback on both the quality and carbon footprint of token generation. To retrieve the local carbon intensity, we access third-party API endpoints such as Electricity Maps (Maps, 2024). To enable inference carbon feedback, SPROUT monitors the datacenter PUE and device energy with tools such as nvidia-smi to record the GPU power and processing time of requests and save the logs to the database. However, obtaining the token generation quality feedback is a different process from the above metrics. After autoregressive inference concludes on the inference server 3, the generated tokens are detokenized and sent back to the user clients, while simultaneously, the request and node monitoring logs are archived in the database. A generation quality evaluator 4then extracts a sample of prompts from the database, generates responses for each at all generation directive levels, and identifies the directive level that yields the best response for each request. However, determining the directive level that yields the best response presents a challenge due to the subjective nature of preference and the absence of a definitive best response. Since manual 3\nUsersUserPromptsLoad Balancer\nGeneration Directive Selector\nInference Server\nDatabaseUserResponseRequest LogsGeneration Quality Evaluator\nAuto-Evaluation LLM\nRegional Carbon IntensityOpportunistic Invoker\nGeneration Directive Level Quality and Carbon ProﬁlesAPISample6\nRegional Carbon IntensityAPI2\nGenerationDirective Optimizer3ConsultFigure 4: Overview of S PROUT ’s Carbon-Friendly Inference System.\nevaluation by humans is impractical, following a methodology from recent research (Dubois et al., 2024), SPROUT employs an LLM-based automatic evaluator, rather than human evaluators, to provide generation quality feedback, aligning with common academic and industry practices (Liu et al., 2023a; Bai et al., 2024; MistralAI, 2024). SPROUT ’s evaluator consults an auto-evaluation LLM 5to gauge its preference for the responses, logging them back into the database. The whole process happens offline, and since the evaluation process also incurs carbon emission, SPROUT ’s opportunistic evaluation invoker 6(Sec. 3.3) ensures the evaluations are carried out only as necessary and during low carbon intensity periods. 3.2 Generation Directive Optimizer While employing generation directives to reduce token output in the autoregressive process is beneficial for lowering carbon emissions, it poses a risk to content quality. Two key external factors further complicate this balance: the regional carbon intensity powering the datacenter, which directly affects the efficacy of carbon savings, and the nature of user prompts, which influences the impact of generation directives on both emissions and content quality. To address these challenges, SPROUT ’s optimizer is designed to dynamically adjust to fluctuations in carbon intensity and the variability of user prompt tasks. In scenarios of low carbon intensity, SPROUT prioritizes directives that enhance content quality, leveraging the carbon discount in generating new tokens. Under high carbon intensity, it opts for directives that may slightly compromise quality but significantly reduce emissions. This strategy underpins the mathematical formulation of the SPROUT optimizer, ensuring that it targets both carbon footprint and content quality. Optimization variable. Optimizing directive levels for each request introduces several practical complications: (i) Dimensionality challenge: the number of dimensions equals the number of received requests (user prompts) at each optimization 294step. (ii) Computational overhead: the optimization is in the critical path before the autoregressive inference starts, delaying the time to first token (TTFT). (iii) Predictability issues: anticipating the impact of each directive level on carbon emissions and content quality for individual requests is challenging. We can only infer general trends from historical data, which do not apply to specific future prompts. Considering these challenges, SPROUT adopts a system-level optimization strategy for generation directive levels, rather than an impractical per-request optimization. It achieves this by determining the probability of selecting each directive level for all user requests (prompts). Let n denote the number of available generation directive levels. The optimization variable, represented asx= [x0, x1, . . . , x n−1]T, defines xi∈[0,1]as the probability of applying the i-th directive level to any request, with x0representing the baseline directive L0 (indicating no directive). To ensure every request receives a directive level, the condition 315Pn−1\ni=0xi= 1must be satisfied. This system-wide probabilistic approach to directive selection, while not optimizing for individual prompts, is shown to achieve carbon savings close to those of an impractical per-request Oracle optimizer (Sec. 5). Objective function. Following Eq.1, we design the objective function f(x)to encapsulate the expected carbon footprint of an inference request. f(x) =k0·eTx+k1·pTx (2) where xdenotes the probabilities of selecting each directive level across all user prompts. It incorporates (i) the current regional carbon intensity ( k0in gCO 2/kWh ), obtained via API; (ii) the prorated per-second embodied carbon of the inference hardware through its device lifetime ( k1ingCO 2/s); and (iii) the profiles of energy consumption ( e) and processing time ( p) for requests employing various generation directive levels. The vectors e= [e0, e1, . . . , e n−1]Tandp= [p0, p1, . . . , p n−1]Trepresent the average energy (in kWh) and processing time (in seconds), respectively, for recent 4\nrequests guided by each directive level. Generation quality constraints. The optimizer also requires feedback from the generation quality evaluator, which reports the auto-evaluation LLM’s preference on which directive level is the best for all sampled requests. Let q= [q0, q1, . . . , q n−1]Twhere qi∈[0,1]denote the preference rate of each directive level reported by the evaluator. For example, if q= [0.5,0.3,0.2]T, it means 50% of the time, the auto-evaluator prefers the response generated using directive L0, 30% of the time by L1 and 20% of the time by L2. We can denote the expected generation quality as qTx. During the optimization, we need to make sure the preference rate does not deviate beyond a threshold of ξ∈[0,1]away from the q0generation baseline using directive L0. In addition, SPROUT designs the actual quality deviation from q0to vary based on the current carbon intensity – when the carbon intensity is low, the constraint should be more strictly enforced (deviation closer to 0) since renewable energy is abundant in the grid to support high-quality generation, and vice versa, during high carbon intensity periods, the deviation should be closer to ξ. This can be formulated as an inequality constraint: qTx≥(1−k0−kmin\n0\nkmax\n0−kmin\n0·ξ)·q0 (3) where kmin\n0andkmax\n0are the known historical minimum and maximum carbon intensities, respectively, and are used for min-max normalization ofk0. The parameter ξ, adjustable according to system requirements, facilitates a balance between carbon footprint and content quality. For SPROUT ’s evaluation (detailed in Sec. 5), we set ξto 0.1. Problem formulation. The overall optimization is min\nx∈Rnf(x) (4) s.t.qTx≥qlb, (5) ∀i,0≤xi≤1, (6) n−1X\ni=0xi= 1 (7) For simplicity, we replace the right-hand side of Eq. 3 with scalar qlbto represent the quality lower bound. Eq. 6 indicates that the probability of each level is within the range of 0 to 1, and Eq. 7 indicates that all probabilities sum to 1. Note that f(x)is linear because both eTandpTare constants to the optimization variable x(Eq. 2), and 381all the constraints in Eq. 5, 6 and 7 are linear to x. Therefore, we have mapped the optimal generation directive level configuration problem to a linear programming problem and we can use the HiGHS dual simplex solver (Huangfu and Hall, 2018) to find the optimal solution for x. 3.3 Opportunistic Offline Quality Assessment In Eq. 5, SPROUT relies on the qTvector to impose the quality constraint. As a carbon-friendly generative LLM inference framework, SPROUT not only cares about the carbon footprint of the inference server but also the quality evaluation process, especially when the auto-evaluation LLM can have >10×number of parameters than the inference model (e.g., GPT-4 compared with Llama model). Note that the quality evaluation is not in the critical path of online inference serving and thus can be done offline opportunistically in a different server. SPROUT triggers the offline quality evaluation based on specific carbon intensity thresholds of the evaluation server. When deciding on whether to evaluate at the current time t, it is critical to weigh the carbon intensity of the evaluator LLM at the current moment, denoted as k(t)\n2, against the time elapsed since the last evaluation at t0. Direct and frequent evaluations can lead to unnecessary carbon emissions without significant benefit, whereas delayed evaluations can undermine the optimizer’s reliability, as the qTvector becomes outdated (Sec. 3.2). To mitigate these issues, we first enforce a grace period to ensure the evaluation does not occur too frequently, then introduce an urgency multiplier to the carbon intensity to capture the increasing need for re-evaluation as time progresses. The urgency-adjusted carbon intensity k′(t)\n2is expressed as k′(t)\n2=e−β(t−t0)·k(t)\n2 (8) The urgency parameter, β, determines the rate at which the evaluation interval incurs penalties over time, ensuring that the value of immediate evaluation – offering a timely update to the qTvector in Sec. 3.2 – is weighed against waiting for potentially lower future carbon intensities. By default, we set βso that the urgency-adjusted carbon intensity k′(t)\n2becomes 1/2of the actual carbon intensity after 24 hours without evaluation. An offline evaluation starts under conditions of: (i) ts represents a local minimum for k′(t)\n2, indicating a positive second-order derivative at that point; (ii) 5\n\u0007\u000b\b\u0007\b\u000b\t\u0007\u000f\u0016\u0018\u0013\u0003\u0004\u0015\u001a\u001e\u001b\u0005\u0007\b\u0007\u0007\t\u0007\u0007\n\u0007\u0007\f\u0011\u001b\u0012\u001a\u0019\u0003\r\u0019\u001d\u0013\u0019\u001c\u0016\u001d\u001f\u0004\u0014\f\u000e\t\u0006\u0017\u0010\u0015\u0005\u0017\u0004\u0004\u0005\t\u0017\u0004\t\u0004\u0004\u0005\u0007\u000b\b\u0007\b\u000b\t\u0007\u000f\u0016\u0018\u0013\u0003\u0004\u0015\u001a\u001e\u001b\u0005\u0007\b\u0007\u0007\t\u0007\u0007\n\u0007\u0007\f\u0011\u001b\u0012\u001a\u0019\u0003\r\u0019\u001d\u0013\u0019\u001c\u0016\u001d\u001f\u0004\u0014\f\u000e\t\u0006\u0017\u0010\u0015\u0005InvocationInvocation(a)(b)∇2 >= 0Figure 5: Process to select the opportunity to invoke\nquality evaluation (golden star). (a) The urgency-\nadjusted k′(t)\n2must fall within the green zone after the\ngrace period (red area) and below the carbon intensity\nthreshold (green line). The red crosses, despite showing\na positive second-order derivative, do not qualify for\nevaluation. (b) Even if carbon intensity stays high all\nthe time, the increasing evaluation urgency ensures that\noffline evaluation always occurs.\na grace period has elapsed since the last evaluation; (iii) the urgency-adjusted carbon intensity at ts,k′(ts)\n2, falls below a predefined threshold, such as 50% of the historical maximum carbon intensity. This evaluative mechanism, illustrated in Fig. 5, highlights moments of evaluation marked by stars in two different cases, underlining SPROUT ’s consideration for both carbon intensity and the need for timely quality feedback. We have implemented SPROUT ’s generation directives as system prompts, implemented the inference server and monitoring framework following industry standards, and developed an automatic quality evaluation mechanism for Sec. 3.3. More details are provided in Appendix A.2. 4 Methodology Experiment setup. We conduct experiments on a testbed comprising two nodes, each equipped with two NVIDIA A100 40GB GPUs and two AMD EPYC 7542 CPUs. We use Meta Llama2 13B (Touvron et al., 2023) to establish the inference server, with each GPU hosting a model instance within its 40GB HBM memory. To assess SPROUT ’s efficiency, three levels of generation directives are implemented: L0 as the default baseline with no directives, L1 for “brief\" generation, and L2 for “very brief\" generation. GPT-4, accessed via the OpenAI API, serves as the auto-evaluation LLM for offline quality assessments. Each of our quality evaluation requests to OpenAI’s gpt-4-0613 API costs about $0.01 on average. SPROUT is evaluated using a diverse set of NLP tasks across five real-world electricity grid operation regions of US Texas (TX), US California (CA), South Australia (SA), Netherlands (NL), and Great Britain (GB) in February 2023, detailed in Appendix A.3. TX CA SA NL GB0102030405060Carbon Saving (%)Carbon Emission\nTX CA SA NL GB020406080100Preference\n(Normalized %)Generation QualityFigure 6: SPROUT significantly saves carbon while pre-\nserving quality across all geographical regions.\nCompeting schemes. SPROUT is evaluated alongside five distinct strategies, detailed as follows: BASE is the baseline strategy that represents a vanilla LLM inference system, it does not explore the opportunity of generation directives discussed in Sec. 2. CO 2_OPTrepresents a scheme that aggressively minimizes CO 2emissions without considering the generation quality. It will always use the generation directive level that yields the lowest carbon footprint. MODEL _OPTis an implementation of the idea to adjust the underlying model parameters to achieve optimization goals from previous works (Romero et al., 2021; Wan et al., 2020). Unaware of the generation directives, this scheme uses inference model variants (i.e., Llama2 7B and 13B) as optimization variables since model variants also introduce the trade-offs between carbon and generation quality. It represents the optimal model variant selection for the user prompts. SPROUT _STAis a static version of SPROUT , applying a single, month-long optimal generation directive configuration identified through offline configuration sweeping, without dynamic adjustments based on real-time carbon intensity and generation feedback. ORACLE is an impractical scheme based on oracle information. It assumes the inference carbon emission on every generation directive level is known ahead of time for all user prompts, and knows the exact generation quality feedback for future prompts instead of relying on sampling. Metrics. The two primary metrics are the inference carbon footprint and the text generation quality. The carbon footprint metric accounts for the CO 2 emissions associated with each inference, averaged for comparison against the default operation represented by BASE. The generation quality is measured from the auto-evaluation LLM’s preference, normalized against B ASE as a percentage. 5 Evaluation Effectiveness of SPROUT .SPROUT consistently achieves substantial carbon savings while maintaining high generation quality in diverse geo6\n\u000b\u0007\f\u0007\r\u0007\b\u0007\u0007\u0007\t\n\n\u0007\u000b\n\u0010\u001e+\u001f*)\u0003\u001a\u001e/&)$\u0003\u0005\u0004\u0006\u001b\"0\u001e,\u0003\u0005\u001c\u001a\u0006\u000f\u000e\u001a\u0012\u0010\u0017\u0004\u001d\u0017\u0018\u001b\u0015\u0017\u0011\u0012\u0014\u001d\u0017\u0018\u001b\u001a\u0018\u0019\u0017\u001c\u001b\u001d\u001a\u001b\u000e\u001a\u0018\u0019\u0017\u001c\u001b\u0017\u0019\u000e\u0010\u0014\u0012\n\u000b\u0007\f\u0007\r\u0007\b\u0007\u0007\u0010\u001e'&#*+)&\u001e\u0003\u0005\u001c\u001a\u0006\n\u000b\u0007\f\u0007\r\u0007\b\u0007\u0007\u001a*.-%\u0003\u000e.,-+\u001e'&\u001e\n\u000b\u0007\f\u0007\r\u0007\b\u0007\u0007\u0016\"-%\"+'\u001e)!\n\u000b\u0007\f\u0007\r\u0007\b\u0007\u0007\u0013+\"\u001e-\u0003\u000f+&-\u001e&)\n\u0013\")\"+\u001e-&*)\u0003\u0018+\"#\"+\") \"\u0003\u0016*+(\u001e'&1\"!\u0003-*\u0003\u000f\u000e\u001a\u0012\u0003\u0005\u0004\u0006BetterFigure 7: SPROUT excels when competing against com-\npetitive strategies and is closest to O RACLE\ngraphical regions in Table 2. As shown in Fig. 6, SPROUT ’s application of optimized generation directives can reduce carbon emissions by up to 60%. The normalized generation preferences across all regions remain above the 90% mark, notably reaching over 95% in South Australia (SA) alongside a carbon saving exceeding 40%. Below, we contextualize the magnitude of potential savings for easier interpretation, but do not claim that SPROUT directly achieves them. For example, from an inference service provider perspective, according to a recent survey (de Vries, 2023), deploying OpenAI’s ChatGPT service necessitates around 29K NVIDIA A100 GPUs, equating to an energy consumption of 564 MWh daily. In the Azure West US region of California (Microsoft, 2024), this translates to monthly CO 2emissions of 3,266 tonnes. Adopting SPROUT -like solution could result in a monthly carbon reduction of 1,903 tonnes – equivalent to offsetting the carbon footprint of flying 6,358 passengers from New York City to London (ICAO, 2024). SPROUT outperforms competing methods, closely aligning with the ORACLE standard. Fig. 7 illustrates SPROUT ’s performance against competing strategies outlined in Sec. 4, showcasing its proximity to the ideal ORACLE in both carbon savings and normalized generation preference across all regions. Here, vertical lines denote the upper bound of generation preference in our evaluation, while horizontal lines indicate the upper bound of carbon savings. Unlike CO 2_OPT, which prioritizes carbon reduction at the expense of generation quality, SPROUT maintains a balance closer to BASE quality. While MODEL _OPT,SPROUT _STA, and SPROUT exhibit similar preferences, MODEL _OPTfalls short in carbon savings, highlighting the limitations of optimizing solely based on inference model variants (Romero et al., 2021; Wan et al., 2020). In contrast to its static version SPROUT _STA, SPROUT demonstrates that its dynamic approach to generation directives yields results nearer 0.00.20.40.60.81.0020406080100Empirical CDF (%)200 gCO2/kWhCO2_OPT MODEL_OPT SPROUT_STA SPROUT ORACLE\n0.00.20.40.60.81.0300 gCO2/kWh\n0.00.20.40.60.81.0400 gCO2/kWh\nCO2 per Request (Normalized)Figure 8: Cumulative distribution function (CDF) of per-\nrequest CO 2emission normalized to BASE as carbon\nintensity varies.\nto the ORACLE benchmark, underscoring the effectiveness of adaptive configurations. Analysis of the Sources of SPROUT ’s Effectiveness and Evaluation Overhead. First, we show thatSPROUT dynamically adapts when carbon intensity varies. Fig. 8 presents the empirical cumulative distribution function (CDF) for 10K inference requests across three environmental carbon intensities: 200, 300, and 400 gCO 2/kWh. The x-axis scales the CO 2emissions of each request relative to its execution on the BASEsystem. Since we only show CO 2per request, as expected, CO 2_OPTis the best among all the schemes – 80% of requests have used less than 30% of the BASE carbon emission. When carbon intensity increases, SPROUT ’s CDF moves closer and closer to CO 2_OPT, indicating that SPROUT ’s optimizer is adapting to the regional carbon intensity since the gain from using more concise directives gets amplified at higher carbon intensities. Specifically, when carbon intensity is 200 gCO 2/kWh, 40% of SPROUT ’s requests have used less than 40% of the carbon footprint than BASE; when it increases to 400 gCO 2/kWh, about 75% of SPROUT ’s requests have less than 40% of SPROUT ’s carbon footprint. Unlike CO 2_OPTand SPROUT _STA, which do not adjust based on carbon intensity and thus maintain constant CDF curves, SPROUT exhibits a dynamic adaptation that aligns closely with O RACLE in a request-level analysis. The offline quality evaluator is key to SPROUT ’s effectiveness. In Fig. 9, we select SPROUT -friendly prompts which are prompts whose shorter responses are on average more preferred by the autoevaluator than their default responses, and mix them with unfriendly prompts (shorter responses are less preferred by auto-evaluator than default responses). Over time, we vary the proportion of these two types of prompts, and observe that when the portion of friendly is high, SPROUT without the evaluator will miss out on the opportunity to save more carbon while achieving higher evaluator preference at the same time. As we can see 7\n\b\t\b\b\u0016\u001a)!%\u0005\u0004\u0006\u0018$\u001e'!\u001d$\u001c\",\u0003+(\u0007\u0003\u0011'!\u001d$\u001c\",\u0003\u0014'%#&)\u0003\u0016\u001a)!%\n\b\r\b\t\b\b\u0017\u001a+!$\u001f(\u0005\u0004\u0006\u000f\u001a'\u001b%$\u0003\u0010#!((!%$\u0003\u0017\u001a+!$\u001f(\n\b\r\t\b\t\r\n\b\n\r\u000b\b\u000b\r\f\b\u000e\b\t\b\r\t\n\b\u0014'\u001d\u001e\u0007\u0005\u0004\u0006\u0012\u001d$\u001d'\u001a)!%$\u0003\u0015*\u001a\"!),\u0019!) \u0019!) %*)\u0013%*'UnfriendlyFriendlyMissed Carbon SavingMissed Preference ImprovementFigure 9: Without its offline evaluator, SPROUT misses\nthe opportunity to leverage requests friendly to concise\ndirective levels, thus forfeiting potential benefits in car-\nbon savings and generation quality simultaneously.\nTXCASANLGB0.00.40.81.21.62.0Overhead (%)Evaluator Carbon\nTX CA SA NL GB0100200300400500Carbon Intensity\n(gCO2/kWh)Invocation Carbon Intensity\n(a) (b)\nFigure 10: (a) Carbon overhead of SPROUT ’s offline\nevaluator. (b) Violin plot of evaluated region’s carbon\nintensity distribution, and the carbon intensity where\nSPROUT invokes offline evaluation (marked as red line).\naround hour 22, the normalized preference is above 100%, meaning the auto-evaluation LLM prefers SPROUT ’s generation over the default generation more than 50% of the time. The offline evaluator’s low carbon overhead is also a key contributor to SPROUT ’s carbon savings. In Fig. 10 (a), we show the carbon overhead of SPROUT ’s offline evaluator. Since GPT-4 is only accessible from third-party API, we use the following numbers to estimate the offline evaluation carbon footprint. GPT-4 is speculated to use a mixture-ofexperts (MoE) architecture, and during inference, only one expert is active. Thus, the model size is equivalent to one expert that has 220B parameters, which can be hosted on 16 A100 GPUs. With the measured average API accessing time of 500ms, we assume all 16 GPUs are running at max power (250W), under no network delay and no batched processing. Despite our conservative estimation where in reality the GPU generation time is much shorter than 500ms (network latency, pre- and postprocessing) and multiple requests can be processed simultaneously in a batch, the overhead in Fig. 10 (a) serving 30 requests per second (RPS) (Kwon et al., 2023) is still well below 1% for all regions. The minimal carbon impact stems from (i) strategically timing evaluations to coincide with periods of low carbon intensity as shown in Fig. 10 (b), and (ii) designing the request to the auto-evaluation LLM such that it generates only a minimal number 623of assessment tokens, as detailed in Appendix A.2. We further show that SPROUT is robust across different seasons, and show the Pareto front of the carbon and quality trade-off in Appendix A.4. 6 Related Work Sustainable AI (Wu et al., 2022) and Sustainable HPC (Li et al., 2023a) have explored various carbon trade-offs in ML infrastructure. Various works have analyzed the AI development’s impact on carbon emission (Patterson et al., 2021, 2022; Schwartz et al., 2020; Acun et al., 2023b; Strubell et al., 2019; Anderson et al., 2023). SPROUT is motivated by these works and takes the effort a step further to LLM inference application. While systems like Carbon Explorer (Acun et al., 2023a), Ecovisor (Souza et al., 2023), Clover (Li et al., 2023b), and Dodge et al.(Dodge et al., 2022) have been designed to adapt to varying carbon intensities, they have not been specifically optimized for generative LLM inference workloads. Previous works have explored pre-training and fine-tuning algorithms for controllable text generation, steering the generation towards specific lexical choices or sentiments (Zhang et al., 2023; Zhou et al., 2023; Dinu et al., 2019; Keskar et al., 2019). SPROUT suggests a promising new direction – controlling LLMs generation toward carbon efficiency. Various works have focused on performance and memory optimization, exploring strategies like sparsity and pruning (Liu et al., 2023c; Frantar and Alistarh, 2023), speculative decoding (Leviathan et al., 2023; Chen et al., 2023b), GPU kernel tiling and fusion (Dao, 2023; Zheng et al., 2023). These advancements are crucial for facilitating the deployment of larger LLMs to a broader audience. However, the environmental implications of these technologies are equally important. Carburacy (Moro et al., 2023) and LLMCarbon (Faiz et al., 2023) offer carbon footprint evaluations to help researchers gauge the environmental impact of LLM training, while SPROUT is the first work to tackle the carbon footprint challenge of generative LLM inference. 7 Conclusion This paper introduced SPROUT , a framework to enhance the sustainability of generative language models. SPROUT can reduce the carbon footprint of LLM inference by over 40%, indicating a greener future for natural language generation. 8\n8 Limitation SPROUT may not be useful for requests that generate very short responses. In this case, adding a generation directive to the prompt may incur more carbon than not using directives. However, note that the extra carbon to process a longer input sequence that includes a generation directive is very limited as modern LLM serving systems maintain a KV cache, which stores key and value vectors from previously processed tokens without recomputing their KV vectors. The generation directive will be maintained in the KV cache after the initial pre-filing phase during LLM inference. SPROUT is not evaluated on serving commercial LLMs such as ChatGPT and Gemini because these models are close-sourced. Our evaluation requires local deployment to perform carbon measurements. Expert LLM users may send API requests and specify the system prompt. SPROUT will conservatively not apply generation directives to such requests as the directive may conflict user’s system prompt (e.g., if the user explicitly asks for detailed responses). 9 Ethical Considerations This work does not raise ethical concerns as we aim to align natural language generation with environmental sustainability. 699",
        "aspect_review": "SPROUT introduces a pioneering approach to reduce carbon emissions specifically from LLM inference, marking a significant advancement in sustainable AI. The practical impact of reducing carbon emissions by over 40% during LLM inference is substantial, aligning AI development with eco-friendly initiatives. Comprehensive experimental validation across various geographical regions demonstrates the effectiveness and adaptability of SPROUT. SPROUT outperforms existing methods and approaches the performance of an oracle, showcasing its superiority in balancing carbon savings and generation quality. The use of generation directives and a linear programming approach for optimizing token generation is innovative and well-executed.\n\nThe novelty could be further enhanced by exploring additional applications beyond LLM inference. A more detailed comparison with similar frameworks in other domains could strengthen the related work section. The results could benefit from additional statistical analysis to further validate the findings. Some figures could be improved for clarity and better visual representation of data. While SPROUT performs well, a deeper analysis of why certain methods underperform could provide more insights. Details on hyperparameters and implementation specifics could be expanded to aid reproducibility.",
        "general_review": "The paper presents a novel and impactful framework, SPROUT, which addresses the critical issue of carbon emissions in generative LLM inference, a growing concern in the AI community. The methodology introduced, such as generation directives, is innovative and provides a fresh perspective on reducing carbon footprints while maintaining high-quality outputs. The technical depth and rigor are evident in the comprehensive system design and the use of linear programming for optimization. Extensive experimental validation demonstrates SPROUT's effectiveness in reducing emissions by over 40% across various real-world settings, highlighting its significant practical impact. The paper is well-organized, with a clear exposition of related literature and a strong alignment with eco-friendly initiatives.\n\n1. Minor improvements in writing clarity could enhance the paper's readability, particularly in the explanation of complex technical concepts. 2. Additional details on hyperparameters or implementation specifics would benefit readers looking to replicate the study. 3. Some figures could be formatted more clearly to improve interpretability. 4. Clarification on the availability of code would be helpful for researchers interested in building on this work."
    },
    "7": {
        "paper": "NeBuLa: A discourse aware Minecraft Builder\nAnonymous EMNLP submission\nAbstract\nWhen engaging in collaborative tasks, humans efficiently exploit the semantic structure of a conversation to optimize verbal and nonverbal interactions. But in recent “language to code\" or “language to action\" models, this information is lacking. We show how incorporating the prior discourse and nonlinguistic context of a conversation situated in a nonlinguistic environment can improve the “language to action\" component of such interactions. We fine tune an LLM to predict actions based on prior context; our model, NeBuLa, doubles the net-action F1 score over the baseline on this task of Jayannavar et al. (2020). We also investigate our model’s ability to construct shapes and understand location descriptions using a synthetic dataset. 1 Introduction High level building agents use conversation in a collaborative task to combine information about the extant conversation, the world, and prior actions to execute new instructions. Such agents interpret messy or vague language, produce actions, then reassess the situation, ask questions or take in corrections from other agents to optimize their actions. Successful collaborative conversations are vital for efficiently performing complex interactive tasks. In this paper, we study the messy language of ordinary human collaborative conversation, and how a large language model can learn to execute instructions from such conversations. We isolate several factors that affect this task. The first factor is the interactions between linguistic and nonlinguistic contexts. Previous work has shown that at least some context is needed to understand and carry out conversationally given instructions (Jayannavar et al., 2020). We improve on that work by first establishing a baseline by using the entire exchange up to an instruction ias a context for an LLM to interpret 040i. Our LLM model, NeBuLa ( NeuralBuilder with Llama), trained on the Minecraft Dialogue Corpus (MDC) (Narayan-Chen et al., 2019), achieves netaction F1 scores that is almost double of Jayannavar et al. (2020). Using the Minecraft Structured Dialogue dataset (MSDC) (Thompson et al., 2024), which provides semantic relations between MDC dialogue moves and nonlinguistic actions, we show that particular discursive components of the linguistic and nonlinguistic context are necessary and sufficient for the LLM to understand an instruction to the degree provided by the baseline. Analysing NeBuLa’s performance revealed two other factors that importantly adversely affect its performance. An instruction in the MSDC has two basic components: a description of a shape in terms of four parameters—numbers of components, colors, arrangement and orientation— and the description of a location where the shape should be placed. Human Architects often use analogies to everyday objects that may be challenging to process, and in addition shape descriptions are often underspecified, meaning that one could perform the instruction correctly in various ways. Location descriptions in the Minecraft world are also quite difficult to process and highly underspecified. For example, put a tower in a corner could be correctly located in any of the four corners of the Minecraft board. We address this problem in two ways: first by further finetuning NeBuLa on a synthetic dataset to improve its performance in building basic shapes and locating them appropriately; and secondly, and more importantly, by revising the evaluation metric used by Jayannavar et al. (2020) to reflect more realistically the semantics of location expressions. We show that, on our synthetic dataset, NeBuLa achieves high accuracy as per our intuitive metric in performing basic instructions. After some preliminaries and discussion of prior work (Section 2), we present the NeBuLa model 1\nand its baseline performance in Section 3, and then a necessary and sufficient discourse feature to get scores equivalent to the baseline in Section 4. In Section 5, we explain several issues associated with Minecraft corpus. We try to address these issues in Section 6. In this section, we explain our evaluation metric for underspecified instructions, as well as experiments on our synthetic datasets. 2 Related Work MDC Narayan-Chen et al. (2019) introduced a corpus of two person dialogues situated in a simulated Minecraft environment. The dialogues record conversations about collaborative tasks, in which an Architect and a Builder cooperate to build sometimes complex 3-dimensional shapes out of blocks of six different colors. The Architect provides instructions, while the Builder is tasked with translating these instructions into actions. The Builder sometimes asks questions, and the Architect may correct themselves or the Builder, or both, concerning both linguistic and nonlinguistic moves. The corpus accurately reflects the variety and complexity of actual cooperative conversation. Details on the MDC are in Table 1. Instructions to code: Neural Builder and variants The MDC (Jayannavar et al., 2020) incentivized the development of an algorithm that could predict sequences of actions from instructions. The actions involved basic moves of placing or removing blocks from certain positions in the environment. Jayannavar et al. (2020) trained a model consisting of a GRU (Cho et al., 2014) to handle textual input coupled with a CNN to integrate information from the current state and a GRU to predicted an action sequence. Although they experimented with several training regimes, the best performance came from one in which a sequence of conversational moves after some action sequence, assumed to be instructions are given to the model, are followed by the next action sequence of the Builder, followed by the next sequence of linguistic moves are input to the model to predict the subsequent action sequence. (See Figure 1). The net-action F1 metric evaluates a model’s prediction based on the exact color and coordinate match between the model’s predicted sequence and Builder’s gold action sequence. In general, Jayannavar et al. (2020) showed that the problem of predicting action sequences from natural language instructions in naturally occurring Figure 1: The Neural Builder (Jayannavar et al., 2020)\ntakes as input the sequence i_n a_n i_n+ 1and the\nworld-state to predict the subsequent action sequence.\ndialogue remains extremely challenging. Their Neural Builder had net action f1 of 0.20on the MDC test set. Shi et al. (2022) propose a somewhat different task from Jayannavar et al. (2020); they try to predict when the Builder should execute an action and when they should instead ask for a clarification question. To this end, they annotated all Builder dialogue moves with a taxonomy of dialogue acts. They then specified a single specific action under the execution label instead of a sequence of actions. Thus, their set-up is not directly comparable to that of Jayannavar et al. (2020). Bonial et al. (2020, 2021) added dialogue acts to Minecraft utterances, but they did not evaluate the effect of these dialogue acts on the Neural Builder’s predictions of actions. Dialogue acts are a partial step towards a full discourse structure; they provide labels for various dialogue moves, but the full discourse structure that we propose to use involves relations between moves. These relations are important as they tell us how to link different parts of, for instance, an instruction into a coherent whole. As we aim to demonstrate in this paper, discourse structure can help to clean up datasets for training and thereby improve training. MSDC Thompson et al. (2024) provided full discourse annotations for the Minecraft corpus, known as the Minecraft Structured Dialogue Corpus (MSDC), using the discourse theory and annotation principles of SDRT (Asher, 1993; Asher and Lascarides, 2003) extended to a multimodal environment, in which both nonlinguistic actions and discourse moves can 2\nTrain+Val Test Total\nOriginal MDC\n# Dialogues 410 137 MSDC\n# Dialogues 407 133 # EDUs 17135 5402 22# EEUs 25555 7258 32# EEUs\nsquished 4687 1473 6# Relation\ninstances 26279 8250 34Table 1: MDC and MSDC characteristics.\nenter into semantic relations like Elaboration, Correction, and Narration (Hunter et al., 2018; Asher et al., 2020). They followed annotation practices given for the STAC corpus (Asher et al., 2016). Thompson et al. (2024) also adapted the parser from Bennis et al. (2023) to predict discourse structures for the Minecraft corpus with relatively high reliability. Statistics on the MSDC are in Table 1. LLMs in robotics Parallel to this work, there has been an increasing amount of research in aiding virtual or real robots with tasks by using LLMs to provide translations from natural language instruction to code that programs the robot to perform the relevant actions (Liang et al., 2023; Singh et al., 2023; Yu et al., 2023). This research is directly relevant to our work, as we use LLMs to go from natural language to a pseudo-code of pick and place statements. However, whereas Liang et al. (2023); Singh et al. (2023); Yu et al. (2023) focus on optimizing the translation from instructions, typically one instruction, to various different coding paradigms, we focus on how linguistic and nonlinguistic interactions affect the resulting action sequence. As our results and previous results on the MDC show, producing actions from interactive conversation with frequently underspecified instructions, which are also dependent upon the discourse and nonlinguistic contexts for proper interpretation, is a much more challenging task than translating well crafted unambiguous instructions into code. In addition, we show that to predict a relevant action from the instruction in+1in the MDC environment, Figure 2: An excerpt from Minecraft Corpus. The\nBuilder interrupts the action sequence by asking a\nquestion.\nit is not sufficient to use a context with just the penultimate instruction inand previous action sequence an. 3NeBuLa: an LLM for Predicting Action Sequences We’ve seen that Jayannavar et al. (2020)’s evaluation method for neural agents gives rather poor results. Observations of the results of neural Builder anecdotally yielded no ending configurations that matched those in the gold. The training scheme of Jayannavar et al. (2020) assumes, in effect, that Architect instructions and Builder actions follow one another with regularity. An unfortunate consequence of this assumption is that actions are individuated by the conversational turns that immediately precede and follow them. Jayannavar et al. (2020) initiate a new action sequence whenever there is a linguistic move of any kind. But that’s not realistic, as bits of text don’t always yield a well-formed or even an underspecified instruction. You might have a clarification question from the Builder in between two action sequences that are in fact carrying out one and the same action as in Figure 2. Builders in the MDC frequently ask questions with respect to the initial instruction about the actions they are simultaneously making; answers to those questions may affect the actions, but it doesn’t mean that there are two distinct series of actions pertaining to two distinct instructions, one before the question and its response and one after. In addition, the Builder sometimes starts to build before the instruction sequence is complete; intuitively, the initial actions form a coherent action sequence with the actions that are subsequent to the 3\nfurther instruction. These observations show that the assumptions of Jayannavar et al. (2020) about how actions are individuated are too simple. Different conversational moves will change and make more precise the shape and position of the structure intended by the initial instruction. Hunter et al. (2018) note that different conversational moves can help conceptualize actions differently. For example, in many Minecraft sessions, an initial instruction gives the Builder an action type that might be realized in many different ways. Something like build a tower of 5 blocks is an action type for which a concrete realization would have to specify the color, perhaps the nature of the building blocks, and a location. As the conversation evolves and unless the Architect corrects their instruction, the type of action to be performed becomes more and more specified. A simple baseline alternative to the scheme proposed by Jayannavar et al. (2020) that addresses the difficulties we just mentioned is to see how a model performs with the complete prior conversation and action sequences up to the predicted action. This was not an option for Jayannavar et al. (2020)’s model, but more recent LLMs are capable of doing this. We used Llama-2-7B, Llama-2-13B and Llama3-8B models to take as context all the conversation and action sequences up to action sequence an to predict an. We fine-tuned Llama on the MDC’s (Jayannavar et al., 2020) training set. All the models were finetuned for 3epochs using QLoRA method (Dettmers et al., 2023). Table 8 in the appendix provides details of computing resources and the hyperparameters for finetuning. Table 2 shows the net-action F1 scores on the validation and test set of MDC. All the finetuned LLMs significantly improved scores in comparison with the F1 0.20score of Neural Builder (Jayannavar et al., 2020). Llama-3-8B essentially doubled the baseline score of 0.20. In the rest of the paper, we refer to Llama-3-8B finetuned on MDC as NeBuLa. The finetuned model, NeBuLa, will be made publicly available. 4 Using Discourse Structure to Improve NeBuLa The ideal way to model the instructional interactions is to have two ongoing, interleaved processes that interact and influence each other. On the one hand, there is the evolving conversational 284Dataset Llama-2-7b Llama-2-13b Llama-3-8b\nValidation 0.292 0.323 0.Test 0.326 0.338 0.Table 2: Net-Action F1 scores on Minecraft Validation\nand Test set for predicting action sequences for LLMs\nusing the entire preceding linguistic and non linguistic\nactions in the game\nstructure that helps conceptualize the nonlinguistic actions; on the other, there is the sequence of actions that also affects continuations of the given conversational context. Using the discourse parser of Thompson et al. (2024), we made a first approximation of these interleaved processes by determining necessary and sufficient situated, conversational conditions for computing instructions. An analysis of the discourse structure in the MSDC shows a large scale pattern of so-called Narrative arcs . These arcs delimit portions of discourse structure linked by Narration relation. Each portion begins with an instruction inby the Architect, terminates with an action sequence am, and involves a negotiation between Architect and Builder about the action sequence to be performed. The negotiation may be extremely short, where the narrative portion then contains just in, am. On the other hand, it might be complex negotiation involving a number of EDUs related by relations like Elaboration. It may also involve questions of clarification or confirmation question by the Builder, in which case the instruction evolves through the portion. A narrative arc may also involve actions by the Builder that the Architect will correct with a linguistic move that will then result in a nonlinguistic action that revises or corrects the prior actions of the Builder. The end of the negotiation is the action sequence that finally carries out the instructions to the satisfaction of the Architect. Figure 3 illustrates a narrative arc starting at Architect turn one with a new instruction that results (in green) in an action sequence in Builder turn two. The Builder then asks a complex, alternative question to confirm that this is the right move. The Architect replies to the question, in effect correcting (in red) the Builder’s previous action, which then results in an action sequence in Builder turn four that corrects the previous builder action. These arcs are relatively self-contained and are recoverable automatically to a relatively high 4\nFigure 3: Excerpt of a narrative arc from the MSDC.\nHere the arc is purple and connects the instruction in\nArchitect turn one to the following instruction in turn\nfive.\ndegree by the parser of Thompson et al. (2024). So, instead of providing the entire conversation history as in Section 3 to the model, we provide the world-state at the beginning of the Narrative arc in terms of net place actions, and the discourse within the Narrative arc up to the present instruction in. We finetune Llama-3-8B on MDC training set using this input. We refer to the resultant model as NeBuLa+N (NeBuLa trained on Narrative arcs). Table 3 gives scores on the validation and test set of MDC for NeBuLa+N(arration). From the table, we can see that the scores are comparable with original NeBuLa. This shows that the Narrative arc is sufficient for action prediction. To study whether the Narrative arc is necessary as well, we evaluated NeBuLa+N on those cases where inanin+1had less content than the Narrative arc. There were 254 such cases in MDC test set. For these samples, we looked at performance of NeBuLa+N when worldstate along with the Narrative arc is given as input. This is denoted as NeBuLa+N/N in Table 3. Similarly, we looked at performance of NeBuLa+N when worldstate along 350Model Validation Test\nNeBuLa+N 0.363 0.NeBuLa+N/N 0.NeBuLa+N/ inanin+1 0.Table 3: Net-Action F1 scores on Minecraft Validation\nand Test set for predicting action sequences for LLMs.\nNeBuLa+N refers to NeBuLa trained on narrative arcs.\nThe next two rows look at those 254 examples in the test\nset where inanin+1has less content than the relevant\nNarrative arc. NeBuLa+N/N gives score of NeBuLa+N\non these samples when world-state and narrative arc is\ngiven as input. Similarly, NeBuLa+N/ inanin+1gives\nscore of NeBuLa+N on the same samples when world-\nstate and inanin+1is given as input.\nwithinanin+1is given as input. This is denoted as NeBuLa+N/ inanin+1in Table 3. As we can see, the score for NeBuLa+N/ inanin+1is significantly lower ( ∼10%) than NeBuLa+N/N. This shows that Narrative arcs are crucial for the task of action prediction. 5 Problems with the Minecraft Corpus In Minecraft, the Architect makes use of several location descriptions. These descriptions are often anaphoric to blocks placed in prior instructions, such as place another block next to that one (one that was placed on previous Builder turn); locations are also sometimes vaguely designated (towards the centre) or underspecified (in a corner, along an edge, n blocks/spaces in from an edge/from the centre). Although the Minecraft environment presents (x, y, z )coordinates, the human participants never used them. This could be because, in the Minecraft environment, players can move their avatars around the board to get different perspectives, which makes it hard to establish an absolute coordinate system. As a result, the net-action F1 metric, which evaluates a model’s action sequence based on whether the block placements match exactly in terms of block color and coordinates with the corresponding gold builder action, is often inappropriate. For instance, if the Builder puts down a block at one corner after receiving the instruction in a corner whereas NeBuLa chooses another corner, the metric would give NeBuLa zero credit whereas intuitively it still did the right thing. To summarize, the evaluation metric treats vague instructions as completely precise ones, and considers one instantiation of an instruction (i.e. the action sequence of Builder in the gold data) to 5\nbe the only ground truth. Another related issue is highlighted in Figure 2 where the action sequence for the Architect’s instruction gets truncated by a question from the Builder “there?\" . In this case, for the aforementioned instruction, only the first three actions ( place yellow -1 1 0 ,pick -1 1 0 ,place yellow -1 4 0 ) constitute the ground truth. To conclude, the underspecified instructions with multiple plausible instantiations, coupled with the strict nature of the metric, puts an upper bound on how much the net-action F1 score can improve on this dataset. More importantly, it doesn’t reveal what a model with a high F1 score actually does learn. We attempt to answer this in the next section. 6 Evaluating NeBuLa on Synthetic Dataset Given the issues associated with Minecraft Corpus and the evaluation metric, we test NeBuLa on simple scenarios using a more just metric. We begin by testing NeBuLa’s ability to construct simple shapes, such as, square, row, rectangle, tower, diagonal, diamond, cube of specific size and understand location (i.e. corner, centre, edge) and orientation descriptions (i.e. horizontal/vertical). We refer to all these shapes as level-1 structures . To do so, we construct a level-1 dataset of 1368 instructions. Some of these instructions simply ask to construct a shape of specific size like “Build a 3×3red square.\", while others are more detailed, for example, “Build a 3×3red horizontal square at the centre.\" For rows/diagonals/towers, we vary size from 3 to9. For squares, the size varies from 3×3to5×5. For cubes, we only use 3×3×3. For rectangles, we use sizes m×n, where m̸=n,m×n <30 and4<=m <= 8. For diamonds, we use two variants to describe size “ mblocks on a side\" and “axes 2m+1long\", where 3<=m <= 6. We use orientation descriptions (i.e. horizontal/vertical) for squares, rectangles, and diamonds. To evaluate NeBuLa on these instructions, we use simple binary functions is_square (C), is_tower (C)etc. for each shape. These functions take as input the predicted construction Cand returns True if C is the desired shape, and False otherwise. For example, is_tower checks whether all the blocks have the same value for X and Z (asYis the vertical dimension) and Y values are distinct and form a sequence 1,2, ..., n where nis the number of predicted blocks. 436For an instruction, we first evaluate if the predicted shape is correct. For correct shapes, we evaluate whether the size/color and location/orientation is correct (for instructions where location/orientation was specified). For an instruction with location description like Build a red tower in a corner , the location is considered correct if the predicted tower is in any of the four corners. Table 4 gives the result of NeBuLa on level-1 dataset. We don’t report color accuracy in the table, as NeBuLa always got the color correct. From the table, we can see that NeBuLa already has a decent command of basic shapes like towers, rows, and diagonals. However, it struggled with shapes like rectangle, square, cube, and diamond. It never correctly constructed diamonds, which might be because there were very few instances of diamonds in Minecraft corpus. For squares and rectangles which were correctly predicted, the model scored very high on orientation accuracy. However, the model has quite low location accuracy across all the correctly predicted shapes. The model rarely achieved an accuracy of above 50%, even with our relaxed evaluation method for locations. As a second step, we look at NeBuLa’s ability to understand location descriptions, in particular ones that are anaphorically specified. To do so, we start with an instantiation (randomly chosen from the set of correct instantiations) for the 1368 instructions in level-1 dataset. So, for a level-1 instruction such as “Build a 3×3red square.\", we have a 3×3 red square already present in the grid. Now given a level-1 structure in the grid, we design level-2 instructions which require placing or removal of a specific color block. For place instructions, we use location descriptions like on top of ,to the side of , touching , and not touching . So an example of level2 place instruction is “place a blue block on top of that.\" where thatrefers to the level-1 structure in the grid. Similarly, for removal instructions, we have the simple instruction “remove a block\" and more complex instructions including location descriptions like you just placed . We also have additional location descriptions for certain level1 structures such as endfor rows, diagonals; top, bottom for towers; corner for cube; centre for cube, odd-size squares and towers. An example of level-2 remove instruction is “remove the top block .\" Similar to level-1, we evaluate NeBuLa on level2 dataset by making use of binary functions like 6\nshape corr# acc-shape acc-size loc-spec acc-loc or-spec acc-or\nTower 504 100% 100% 378 56.0%\nRow 168 100% 100% 126 30.0%\ndiagonal 168 78.6% 95.0% 102 2.0%\nrectangle 140 39.6% 12.0% 44 7.0% 31 100%\nsquare 216 59.3% 96.0% 88 26.0% 75 81.0%\ncube 24 58.3% 85.0% 8 37.0%\ndiamond 144 0% 0%\ntotal 1368 73.0% 83.0% 746 38.0% 106 86%\nTable 4: Evaluation of NeBuLa on shapes and basic locations. Shape accuracy gives how many of the shapes\nNeBuLa made were correct given the instruction. Additionally: acc-size–of those correct shapes how many were of\nthe correct size; loc-spec–of the correct shapes how many had location specified; loc acc–of those correct shapes\nwith specified locations how many were correctly located. We also tested rectangle and square for orientation\n(horizontal or vertical).\nis_ontopof (b, C),is_touching (b, C)where Cis the level-1 structure already present in the grid and bis the predicted block. As an example, for on top of , we check whether there is no block in C which is directly above the block b, and there is a block in C underneath block b. Mathematically, this can be expressed as Coord {y}(b)∩ {y+ 1 : y∈max[Coord {y}(C)]} ̸=∅;Coord {y}(b)∩ {y:y < max [Coord {y}(C)]}=∅, and Coord {x,z}(C)∩Coord {x,z}(b)̸=∅where Coord u(C)denotes the set of coordinates of the construction Cfor dimensions u⊂ {X, Y, Z }. Instruction Accuracy #correct #total\nOverall 80.4% 1100 1Overall place 67.9% 472 Overall removal 93.3% 628 Place on top of 74.2% 132 Place to the side of 98.1% 151 Place touching 99.4% 175 Place not touching 7.5% 14 Removal any 95.3% 223 Removal you just placed 95.3% 206 Removal top 100% 44 44\nRemoval bottom 100% 65 65\nRemoval centre 60.7% 34 56\nRemoval corner 100% 2 2\nRemoval end 96.4% 54 56\nTable 5: Evaluation of NeBuLa place and remove\ninstructions with anaphoric locations.\nTable 5 shows that the model did quite well, with the exception of the instruction involving not touching as location description. Otherwise, the results indicate that NeBuLa has a good knowledge of basic anaphoric location descriptions. We then examined NeBuLa’s errors with on top of. We found that the failure cases mostly were a result of the model placing multiple blocks instead 507of just one on the given level-1 structure. That is, the model does not always understand a block asa single block. In light of these cases, when we check whether all the blocks in predicted bare on top of C, the accuracy improves from 74.2%to97.2%. Thus, some of the difficulties NeBuLa had with instructions come from what might be a limited understanding of the semantics and pragmatics of indefinite and numerical noun phrases. 6.1 Finetuning NeBuLa on Shapes and Locations Our evaluation on level-1 and level-2 data shows that NeBuLa struggles with squares, rectangles, diamonds, and “not touching\" place instructions. To tackle this, we used a subset of the two datasets to augment the training data for NeBuLa. From level-1 data, we took the following subset for training: squares of size 3×3, diamonds of size 3(or axes 5spaces long), and rectangles of sizes 4×3and5×4. From level-2 data, we took those “touching/not touching” instances where the level-1 structure is square or rectangle. Out of total 363 instances for touching/not touching, there were 109such instances. We then finetuned NeBuLa by combining the Minecraft training with this subset of level-1 and level-2 data. The rest of the level-1 and level-2 data was used for testing. Table 6 shows NeBuLa’s performance on level-1 test set after finetuning. As before, we found that NeBuLa always got the color correct. From the table, we can see that the shape accuracy improved significantly for squares, rectangles, and diamonds in comparison with Table 4. Although the location accuracy is still low, it has improved in comparison with original NeBuLa. Interestingly, we also see 7\nshape tot# acc-shape acc-size loc-spec acc-loc or-spec acc-or\nTower 504 99.0% 100% 377 42.0%\nRow 168 99.0% 100% 125 48.0%\ndiagonal 168 74.0% 80.0% 101 39.0%\nrectangle 102 95.0% 49.0% 76 32.0% 65 100%\nsquare 144 89.0% 100% 93 45.0% 86 100%\ncube 24 100% 100% 18 66.0%\ndiamond 108 18.0% 0% 12 100%\ntotal 1218 87.0% 90.0% 715 46.0% 163 100%\nTable 6: Evaluation of NeBuLa after finetuning on shapes and basic locations. Shape accuracy (acc-shape), acc-size,\nlocation-spec, acc-loc, or-spec and acc-or as in Table 4\nInstruction Accuracy #correct #total\nOverall 89.6% 1128 1Overall place 88.7% 520 Overall removal 90.3% 608 Place on top of 79.7% 142 Place to the side of 87.7% 135 Place touching 93.3% 112 Place not touching 97.8% 131 Removal any 94.4% 221 Removal you just placed 84.7% 183 Removal top 100% 44 44\nRemoval bottom 100% 65 65\nRemoval centre 66.1% 37 56\nRemoval corner 100% 2 2\nRemoval end 100% 56 56\nTable 7: Evaluation of NeBuLa with additional\nfinetuning on touching/not touching.\nthat NeBuLa has perfect shape accuracy on cube although cube was not part of the training set. Finally, for correctly predicted shapes, NeBuLa achieved a perfect orientation accuracy. Table 7 shows the results on level-2 test set for NeBuLa after finetuning. Here also, we can see that NeBuLa’s accuracy remains very high on almost all of the simple instructions with the anaphoric location descriptions. Furthermore, its accuracy increased significantly for “not touching\" instructions. This jump in accuracy is significant enough to conclude that NeBuLa has learned the concept of “contact\", at least for our synthetic dataset. On the minecraft test set, we found that NeBuLa’s performance remained high with an average precision of 0.40, recall of 0.414and a net action F1 of 0.391. As we can see, these scores are at-par with the baseline NeBuLa. 7 Conclusions and Future Work We have introduced NeBuLa, an LLM based action prediction model, for the Minecraft Dialogue Corpus. As a baseline, NeBuLa uses the entire 564Minecraft dialogue up to action anto predict an. We showed that this baseline doubles the net action F1 scores of Jayannavar et al. (2020). We then showed that certain discourse structures provided necessary and sufficient information for inferring actions to the level of the baseline. We also analyzed NeBuLa’s errors on Minecraft corpus and provided additional finetuning to improve the model’s ability to interpret underspecified shape descriptions and anaphorically-specified locations using our synthetic dataset. This allowed us to analyze the shortcomings of the net-action F1 metric, and address them using a more realistic evaluation metric. Our evaluation metric captures the notion of relative location, but leaves exact locations typically underspecified, in accordance with our semantic intuitions. For future work, we plan to apply this metric (or a similar relative location metric) on the Minecraft corpus. Given the improvement in performance of NeBuLa after finetuning on our synthetic dataset, we hypothesize that in a more controlled collaborative task, with some pedagogical instructions to the Architect, NeBuLa could contribute as a useful interface for conversational robots that interact with humans. Limitations The MSDC contains a great deal of discourse information, including a full discourse structure analysis. We have only used some of this information. Potentially, we could leverage more information from this dataset to improve NeBuLa’s action prediction performance. We also need to extend our constraints to cover other frequent anaphoric location descriptions in addition to on top of X andto the side of X . Locutions like in front of/ behind, underneath, hanging off, next to (X)all have underspecified parameters of either 8\norientation, distance or direction that allow for several correct placements, once Xhas been identified. For instance, Under (X, Y)holds if Coord {y}(X) ={z−n:y∈Coord {y}(Y)} andCoord {x,z}(X)∩Coord {x,z}(Y)̸=∅. We need to evaluate NeBuLa on these expressions as well. Finally, we need to reevaluate NeBuLa’s predictions as well as builder actions in the MDC with our more appropriate metric, which is suited to the underspecified shape and location descriptions used in the corpus. Ethics Statement Our work here has been to improve the capacities of AI systems in interactive tasks where conversation can be used to optimize performance on collaborative actions. We see no direct ethical concerns that arise from this work. Though conversationally more capable robots, which could be one downstream application of this work, might require additional conversational strategies as constraints to ensure that participating humans retain the final say with regards to the actions in the collaborative tasks. 624",
        "aspect_review": "The paper presents a comprehensive evaluation of the NeBuLa model, demonstrating significant improvements over previous baselines, particularly in net-action F1 scores. NeBuLa doubles the net-action F1 score compared to the baseline, showcasing its effectiveness in the language-to-action task. The analysis of discourse structures and their impact on action prediction is insightful, providing a clear understanding of the model's performance. The use of LLMs and the integration of discourse context in action prediction represent a novel and robust methodological approach. The paper provides a detailed explanation of the challenges in understanding underspecified instructions and the innovative solutions proposed to address them.\n\nThe evaluation metric used may not fully capture the nuances of vague instructions, leading to potential misinterpretations of model performance. While the results are promising, the paper could benefit from additional experiments to further validate the findings. The paper's contributions, while significant, are somewhat limited to the specific task of action prediction in Minecraft. Some aspects of the methodology, such as the specifics of the synthetic dataset used for finetuning, could be described in more detail. Certain sections of the paper could be clearer, particularly in terms of the implementation details and hyperparameter settings.",
        "general_review": "This paper introduces a novel model, NeBuLa, which significantly advances the field of language to action translation by incorporating discourse and nonlinguistic context. The methodology is innovative, leveraging large language models and fine-tuning them on the Minecraft Dialogue Corpus to achieve impressive improvements in net-action F1 scores. The paper demonstrates strong technical depth and rigor, particularly in its discourse-aware approach and the use of synthetic datasets to address limitations in existing metrics. The experimental validation is comprehensive, showcasing NeBuLa's capabilities across various tasks and highlighting its practical impact in improving action prediction accuracy. The paper is well-organized, with a clear presentation of related work and a thoughtful analysis of the challenges posed by underspecified instructions.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in sections explaining the evaluation metrics. 2. There is a lack of detailed hyperparameter settings for the fine-tuning process, which could aid in reproducibility. 3. Some figures could be formatted more clearly to enhance readability. 4. The availability of the code for NeBuLa is mentioned, but additional clarification on how to access it would be helpful."
    },
    "8": {
        "paper": "Extrinsic Evaluation of Cultural Competence in Large Language Models\nAnonymous ACL submission\nAbstract\nProductive interactions between diverse users and language technologies require outputs from the latter to be culturally relevant and sensitive. Prior works have evaluated models’ knowledge of cultural norms, values, and artefacts, without considering how this knowledge manifests in downstream applications. In this work, we focus on extrinsic evaluation of cultural competence in two text generation tasks, open-ended question answering and story generation. We quantitatively and qualitatively evaluate model outputs when an explicit cue of culture, specifically nationality, is perturbed in the prompts. Although we find that model outputs do vary when varying nationalities and feature culturally relevant words, we also find weak correlations between text similarity of outputs for different countries and the cultural values of these countries. Finally, we discuss important considerations in designing comprehensive evaluation of cultural competence in user-facing tasks. 1 Introduction Cultural competence is the ability to effectively and appropriately communicate with socioculturally different audiences (Deardorff, 2009).1People demonstrate cultural competence by tailoring their utterances to the participants in a conversation (Bell, 1984; Hawkins et al., 2021; Wu et al., 2023a). These adaptations range from sociolinguistic variations (e.g., using ‘soccer’ or ‘football’ depending on the context) to appropriately using facts (e.g., in India, the Prime Minister is the head of the government, but in USA, the President is). Hence, for effectively serving diverse users, outputs from large language models (LLMs) need to be culturally relevant (Hovy and Yang, 2021). Cultural competence consists of multiple components, including a person’s knowledge of a cul1interchangeably, the terms intercultural competence and\ncrosscultural competence are also used.ture, which then supplements their skills of effectively communicating with people from that culture (Deardorff, 2006; Fantini and Tirmizi, 2006; Alizadeh and Chavan, 2016). So, cultural competence of LLMs should also be evaluated along both these aspects. Contemporary works have largely targeted the knowledge component of cultural competence by evaluating LLMs’ knowledge of cultural values, norms, and artefacts (§ 2.2). Such evaluation is intrinsic because it is decoupled from the manifestation of this knowledge in downstream applications (Jones and Galliers, 1995). In this work, we focus on extrinsic evaluation of cultural competence. Extrinsic evaluation setups should closely mimic user interactions with a system (Jones and Galliers, 1995). We select the tasks of story generation and open-ended question answering (QA), both of which have high representation in user interactions with LLMs (Zhao et al., 2024). We evaluate the lexical variations in outputs of 6 LLMs for 195 nationalities, and by proxy culture, for these tasks using both qualitative and quantitative analyses. Further, recent intrinsic evaluations have heavily relied on surveys from crosscultural psychology, like Hofstede’s Cultural Dimensions (Hofstede et al., 2010) and World Values Survey (Haerpfer et al., 2022), as a measure of cultural values across countries (Arora et al., 2023; Cao et al., 2023; Durmus et al., 2023; Ramezani and Xu, 2023; AlKhamissi et al., 2024; Masoud et al., 2024). Thus, we evaluate whether the text distributions of outputs correlate with the cultural values of countries, as captured by these surveys. Our three main research questions are: RQ1: Do models vary outputs when explicit cues of culture are present in the input prompt? RQ2: Do model outputs contain culturally relevant vocabulary? RQ3: Are model outputs for countries with similar cultural values, also similar? 1\nBy measuring the variance in the outputs, we find that models make non-trivial adaptations for different nationalities (§ 5.1). Next, inspecting the vocabulary of these outputs, we find that they contain culturally relevant words (§ 5.2). Finally, we find only a weak correlation between the text distributions and cultural values of countries, as measured by crosscultural psychology surveys frequently used in contemporary work (§ 5.3). Our findings show that intrinsic and extrinsic measures of cultural competence do not correlate. This necessitates developing holistic evaluations to analyse cultural competence in tasks representative of user interactions with LLMs. All our code and data will be open-sourced. 2 Related Work 2.1 Cultural Competence Cultural competence is the ability to effectively communicate with a socioculturally different audience (Deardorff, 2009). While multiple definitions exist (Alizadeh and Chavan, 2016), agreed-upon components include (a) the awareness about one’s positionality and attitude, (b) the knowledge about the language, values, beliefs, practices, symbols etc. of a culture, and (c) the skill of appropriately using this knowledge when communicating (HowardHamilton et al., 1998; Deardorff, 2006; Fantini and Tirmizi, 2006; Deardorff, 2009).2Theknowledge component requires understanding differences in values, beliefs, and preferences across societies. Surveys in crosscultural psychology, like Hofstede’s Cultural Dimensions (HCD) (Hofstede, 2001) and World Values Survey (WVS) (Haerpfer et al., 2022) attempt to elicit these differences across cultures, proxied by nationalities, using value-based questions.3Survey responses from a large number of individuals are used to quantify the differences in cultural values across countries. Hofstede’s theory, in particular, has been widely adopted in fields requiring cultural competence such as communication, education, business, and healthcare (Ahern et al., 2012; Burai, 2016; Chang and Wu, 2023; Singh and Kumari, 2023).42For LLMs, we only rely on analogy to ‘knowledge’ and\n‘skills’, and do not invoke analogies to ‘awareness’.\n3For example one of the questions in the Hofstede’s survey\nis \"In choosing an ideal job, how important would it be to you\nto have sufficient time for your personal or home life?\".\n4We note that defining the underpinning concept of culture\nitself remains elusive. Numerous works have attempted to\nsynthesize the definitions of culture across disciplines, high-2.2 Cultural Competence in LLMs There is a growing body of work on ensuring that LLMs align with diverse human values (Hershcovich et al., 2022; Wu et al., 2023b; Kirk et al., 2024; Sorensen et al., 2024) and can serve socioculturally diverse users (Hovy and Yang, 2021; Hershcovich et al., 2022; Adilazuarda et al., 2024). Specifically, prior works have evaluated LLMs for: 1. Reflection of diverse cultural values on crosscultural psychology surveys (like HCD and WVS) using MCQs, Chain of Thought prompting, or personas (Arora et al., 2023; Cao et al., 2023; Durmus et al., 2023; Ramezani and Xu, 2023; AlKhamissi et al., 2024; Masoud et al., 2024). 2. Knowledge about varying norms in social settings like dining, gifting, etc., using yes-no questions (Dwivedi et al., 2023), natural language inference (Huang and Yang, 2023), red-teaming (Chiu et al., 2024), situational questions (Rao et al., 2024; Shi et al., 2024), and graphs (Acharya et al., 2020). 3. Commonsense and figurative language understanding using MCQs (Nguyen et al., 2023; Palta and Rudinger, 2023; Kabra et al., 2023; Kim et al., 2024; Koto et al., 2024; Wang et al., 2024), and pragmatic games (Shaikh et al., 2023). 4. Information about cultural artefacts like food, clothing, etc. (Li et al., 2024b; Seth et al., 2024). These works reveal gaps in LLMs’ knowledge of non-western cultures, complimenting known demographic biases in LLMs (Mishra et al., 2020; Zhou et al., 2022; Basu et al., 2023; Jha et al., 2023; Schwöbel et al., 2023; Naous et al., 2024). These evaluations focus on the knowledge component of cultural competence and are intrinsic because they are decoupled from the manifestation of this knowledge in user-facing tasks. Our work is complementary as we evaluate cultural competence in the extrinsic setup of text generation. 3 Extrinsic Evaluation of Cultural Competence Jones and Galliers (1995) describe extrinsic evaluation criteria as, ‘ those relating to its function, i.e its role in relation to its setup’s purpose ’. So, consider the two broad use cases of LLMs: (a) classification tasks, and (b) generation tasks. While lighting its complex and multi-faceted nature (Kroeber and\nKluckhohn, 1952; Baldwin et al., 2006). Broadly, culture is\na shared collection of knowledge, values, practices, norms,\nand beliefs that manifest in expression as behavioural and\nlinguistic patterns (Kroeber and Kluckhohn, 1952).\n2\nincorporating cultural knowledge has been shown to benefit classification tasks like hate-speech detection and commonsense reasoning (Zhou et al., 2023; Li et al., 2024a; Shi et al., 2024), to the best of our knowledge there is no prior work focusing on open-ended text generation tasks. Specifically, we obtain model outputs when nationalities in prompts are perturbed. We propose quantitative (§ 3.1) and qualitative (§ 3.2) analyses to evaluate these outputs for cultural competence. 3.1 Quantitative Evaluation We evaluate outputs quantitatively in two ways: Lexical Variance In order to quantify how much the generated language varies when nationalities are perturbed, we measure the variance in distance between outputs, where distance is computed according to a specific representation (§ 4.3.1). Correlation with Cultural Values Prior works have relied on cultural values measured by surveys like HCD and WVS for intrinsic evaluation of cultural competence (Arora et al., 2023; Cao et al., 2023; Durmus et al., 2023; Ramezani and Xu, 2023; AlKhamissi et al., 2024; Masoud et al., 2024). So, we evaluate whether the text distributions of outputs correlate with distributions of cultural values. The intuition is to analyse whether countries with similar cultural values have similar text outputs. We use the Kendall’s τcrank correlation for this analysis. For each nationality (called the anchor), we rank all other countries by: (a) the similarity between their output to the output of the anchor, and (b) the difference in their cultural values and that of the anchor to the anchor. We use Kendall’s τcto calculate the rank correlation of these rankings. 3.2 Qualitative Evaluation The qualitative evaluation is intended to assess the characteristics of the outputs when the nationalities are perturbed. For this, we inspect the vocabulary of the LLM outputs by surfacing words that occur more frequently in the outputs of a particular country. We used the TF-IDF statistic to obtain words highly relevant to a particular country. The outputs were first tokenized using NLTK (Bird et al., 2009). Then, we created term frequency vocabulary of all the unigrams occurring in the outputs for each country, considering all outputs of a country as a single ‘document’. We then calculate the TF-IDF score for all these unigrams and manually inspect the top 15 words for a subset of countries. 2154 Experimental Setup 4.1 Tasks and Data We select two tasks, story generation and openended question answering for our experiments. These were selected as they fulfil two main criteria. First, they have a sizeable representation in user interactions with LLMs (Zhao et al., 2024). And second, they represent diverse types of generation tasks with story generation on the creative end of the spectrum, while open-ended question answering being on the factual end of the spectrum. Open-Ended Question Answering (QA) We created a list of 345 topics across 13 categories. We selected the categories (biology, chemistry, economics, environment, humanities, history, law, maths, physics, politics, religion, space, and world affairs) to ensure diversity in topics. Next, we curated topics for each category by referring to textbooks and encyclopedias.5Examples of topics include: ‘elections’ in ‘politics’, ‘inertia’ in ‘physics’, ‘photosynthesis’ in ‘biology’. For this task, use a simple prompt template: ‘Explain {topic} to a/an {nationality} person in English.’ These results in prompts like ‘Explain elections to an Indian Person in English’.6Story Generation We created a list of 35 topics for children’s stories. We used online websites and children’s storybooks to come up with topics. Examples include topics like moral values (‘honesty’, ‘kindness’), characters (‘farm animals’, ‘birds’), and places (‘school’, ‘jungle’). Similar to QA, we use a simple prompt template: ‘Write a children’s story about {topic} for a/an {nationality} kid in English.’ This results in prompts like ‘Write a children’s story about honesty for a Japanese kid in English.’ 4.2 Models We evaluate the following LLMs: (a) GPT 3.5 Turbo ( gpt-3.5-turbo-0125 )7, queried via API between February 23 and March 28 2024. 5The datasheet (Gebru et al., 2021) is in Appendix A and\ncuration logs will be released with the data upon publication.\n6We observed that not including the phrase ‘in English’\nin the prompt resulted in GPT 3.5’s output often being in the\ndominant language of the particular country, for example for\n‘Mexican’ the output is in Spanish. While this is an interesting\nphenomenon, analyzing this is beyond the scope of this paper.\n7https://platform.openai.com/docs/models/gpt-3-5-turbo\n3\n(a) Story Generation\n (b) Question Answering\nFigure 1: Lexical Variance in outputs. The variance of outputs across nationalities is consistently higher than the\nvariance of outputs within nationalities. Story generation has a higher median variance than QA across models.\n(b) Gemma 2B instruct and 7B instruct (Team et al., 2024) (c) Llama 2 7B chat and 13B chat (Touvron et al., 2023) (d) Llama 3 8B instruct (AI@Meta, 2024) . We sample 5 responses per prompt, using a temperature of 0.3. We generate a maximum of 100 tokens for QA and 1000 tokens for stories. 4.3 Metrics 4.3.1 Text Similarity BLEU BLEU (Papineni et al., 2002) calculates the precision of the n-grams present in the modelgenerated candidate text as compared to a gold reference text. We re-purpose this to calculate the similarity between two outputs. Because BLEU is not symmetric, we take the average of the two possible BLEU scores, one with each of the outputs as a candidate and the other as a reference. Word Edit Distance (WED) WED is word-level Levenshtein distance (Levenshtein, 1966), normalized by the length of the longer text. We picked BLEU and WED to focus on capturing the differences in lexical items between two outputs, e.g., the use of ‘soccer’ or ‘football’.88In early expeirments we found that semantic metrics like\nBERTscore (Zhang* et al., 2020) or embedding similarity\nmight not be suitable because: (a) a lot of culturally rele-\nvant words from the outputs were converted to [UNK] tokens,\n(b) we did not see differences in the embeddings for outputs\nthat were qualitatively different, especially in QA; perhaps\npartly because of (a) and and because, intuitively the the dif-\nferent words convey the same meaning.4.3.2 Difference in Cultural Values Following prior work, we rely on data from crosscultural psychology surveys to measure the difference in cultural values among countries. Hofstede’s Cultural Dimensions (HCD) Hofstede’s cultural theory quantifies the culture of a country along 6 dimensions. Using the VSM2013 version of the data available for 94 countries, we represent each country with 6 dimensions.9World Values Survey (WVS) We use data from 64 countries and represent each country with 249 dimensions using the 249 questions from WVS1011We calculate the distance in cultural values between two countries as the magnitude of the vector distance between their HCD or WVS representations. 5 Results 5.1 Variance due to Nationality Perturbation Our first research question was to analyse the extent of variation in outputs when nationalities are perturbed in the prompt. For this, we quantify the lexical variance (§ 3.1) in outputs, as measured by word edit distance in Figure 1. We find that model 9https://geerthofstede.com/research-and-vsm/dimension-\ndata-matrix/\n10There are additional questions that are either non-ordinal\nor descriptive in nature or are experimental, which we ignore.\n11https://www.worldvaluessurvey.org/\nWVSDocumentationWV7.jsp\n4\nNationality Top 15 highest TF-IDF scoring words for GPT 3.5’s outputs of Story Generation\nAfghan amir , ali , afghanistan , ahmad , zahra , amina , rostam , babar , sara , omar ,\ncally , farid , afghan , treehouse , bari\nAmerican tommy , lily , america , jack , jake , buddy , mommy , max , town , daddy ,\nacres , sarah , finley , assignment , surgery\nBritish oliver , england , jack , tommy , lily , willowbrook , thomas , sherwood ,\nlittleton , emily , british , jones , merlin , london , teddy\nCanadian liam , canada , emily , jack , alex , sarah , canadian , maple , tim , lily ,\nbeavers , smith , sammy , moose , robby\nChinese li , mei , china , ming , chen , wu , wei , xiao , wukong , feather ,\nping , lake , bao , snowball , chinese\nGerman hans , germany , lena , anna , fritz , max , gretchen , bauer , lorelei ,\nherr , lila , liesl , rübezahl , emma , karl\nIndian raj , india , rani , arjun , ravi , priya , guru , peacock , krishna ,\nraja , meena , gupta , durga , beggar , temple\nNigerian kola , nigeria , tunde , bola , kemi , ade , oya , adaeze , ayo ,\nzuri , lagos , jide , nigerian , simba , heron\nTable 1: Top 15 highest TF-IDF scoring words for GPT 3.5’s outputs of story generation for selected countries\noutputs do vary with changing nationalities for both tasks across models. Moreover, these variations are non-trivial and task dependent, as described below. Control experiment: variance within nationality We want to ensure that that the variance observed across nationalities are non-trivial, i.e. they do not occur because of the non-deterministic nature of generation in models. For this, we also measure the variance within multiple outputs for a particular nationality. We find that the variance for outputs within nationality is consistently lower than the variance across nationalities. We confirm this with ANOV A having a p-value of <0.05 (Appendix B.2). Effect of task on variance We find that the nature of the task affects the extent of variation. The median variance for story generation is higher than the median variance for QA for every model. This might be expected as story generation, had longer outputs and being a creative task allows for more adaptations. On the other hand, the difference between the upper and lower quartiles of variance for QA is larger than that for stories. This is likely because QA consists of a wider variety of topics ranging from scientific categories, where limited variations might be expected, to topics on politics and history, that allow more variation in answers than others. For example answers while explaining ‘elections’ (politics) might vary more as they are operationalized differently across countries, but explaining ‘inertia’ (physics) might not vary as much. 3305.2 Culturally Relevant Words in Outputs Our second research question was to characterize the content of the outputs and understand whether they contain culturally relevant words. For this, we inspected the vocabulary of the outputs. We extracted words highly correlated to a country using TF-IDF (§ 3.2). The top 15 words from a subset of countries from outputs of GPT 3.5 for story generation and topics in the politics category from QA are presented in Table 1 and 2, respectively. We see that story generation outputs feature different names across countries. For example, ‘amir’ in Afghanistan, ‘raj’ in India, and ‘oliver’ in Britain. Other culturally salient artefacts such as ‘temple’ and ‘peacock’ for Indian, ‘bao’ in Chinese, and ‘london’ for UK, etc. also show up in the list. For the topics in the politics category of the QA task, we see words referring to senate houses and political offices of the countries, for example, ‘lok sabha’ and ‘rajya sabha’ in India, ‘bundestag’ for Germany, and ‘meshrano jirga’ and ‘wolesi jirga’ for Afghanistan. The list also features politically polarised issues such as ‘gun’ in America and ‘brexit’ in UK. Another common feature is the names of political parties, such as ‘bjp’ in India, ‘apc’ and ‘pdp’ in Nigeria, and ‘ndp’ in Canada. Finally, we note that the cultural relevance of all the words on the lists is not obvious (e.g ‘notably’ in German in Table 2). Moreover, not all topics in the QA setting surface such interpretable lists of culturally relevant words. Especially lexicon from scientific topics did not reveal interesting 5\nNationality Top 15 highest TF-IDF scoring words for GPT 3.5’s outputs for ‘Politics’ in QA\nAfghan afghanistan, jirga , ballot , wolesi , meshrano , elders , afghan , tribal ,\npartners , box , strategies , target , stake , exploited , dynamics\nAmerican united , states , basis , four , american , expanded , gun , fundraising , accent ,\ncongress , qualifications , residency , requirements , allowed , register\nBritish uk , british , mps , commons , reach , becomes , five , earlier , lords , scottish ,\nbrexit , kingdom , evolved , socioeconomic , previously\nCanadian provincial , municipal , federal , age , levels , grassroots, shapes,\nriding, sector, aggression, canadian, guaranteed, ndp quebec, ontario\nChinese royalty , enacted , self-interests, solving , achieving, something , channels , box , health ,\ndirecting , self-governing , capable , prosperous , citizenship , accumulation , accomplish\nGerman bundestag , totalitarian , he , argued , precedence , opposed , germany , upholds ,\nnotably , tourism , showcase , transition , mixed , emerged , europe\nIndian india , sabha , lok , rajya , linguistic , lacking , flexibility , chance , violent , anarch ,\nhindu , bharatiya , janata , bjp , indian\nNigerian nigeria, guarantees, figureheads, progressives, apc, pdp, purely, senators, problem, finances,\nidentification, evenly, leave, lawlessness, governors\nTable 2: Top 15 highest TF-IDF scoring words for GPT 3.5’s outputs for ‘politics’ in QA for selected countries\nexamples when inspecting the top-scoring TF-IDF words. This further compliments our earlier finding of output variations being different across tasks. 5.3 Correlation in Outputs & Cultural Values Our third research question is analysing whether the outputs for countries with similar cultural values are similar. We report the Kendall’s τcrank correlation (§ 3.1), averaged across countries, between BLEU text similarity and distance in cultural values measured by HCD and WVS in Figure 2. Effect of measure of cultural value used When HCD is used as the measure of difference in cultural values (Figure 2a), we find that median correlation across the board12is greater than 0. This implies a small but positive correlation between the text distribution and cultural values of countries as measured by HCD. However, when WVS data is used, we find a small and negative correlation between text distribution and cultural values as measured by WVS (Figure 2b). All the rank correlation values were statistically significant within a significance interval of 95% in a two-sided p-test. Correlation for different countries Next, we analyse the Kendall’s τcrank correlation for different countries. Figure 3, shows two example plots for GPT 3.5 for story generation. We find that the correlation for USA, Canada, and India (in HCD) is negative, while that of Russia, China, Japan, and Australia is positive. South American, African, 12except QA for Gemma 2B InstructSoutheast Asian and European countries are split between positive and negative values. This is interesting as prior work has found gaps in models’ knowledge of non-western cultures (for example AlKhamissi et al. (2024); Masoud et al. (2024)), but we do not see a similar trend. Overall, the trend for each country is similar for HCD and WVS. 6 Discussion Correlation between Intrinsic and Extrinsic Metrics of Cultural Competence Together the findings for RQ2 (§ 5.2) and RQ3 (§ 5.3) suggest that intrinsic and extrinsic measures of cultural competence are not correlated. On the one hand, model outputs from our extrinsic setup feature culturally relevant words (§ 5.2). On the other hand, the text distributions are only weakly correlated with measures of cultural values widely used in intrinsic evaluations of cultural competence (§ 5.3). Thus, even if an LLM reflects the values of every country perfectly (as prior work measures by Hofstede’s Cultural Dimensions or World Values Survey), this ability may not be reflective of cultural competence in downstream tasks.13These findings underscore the importance of extrinsic evaluation of cultural competence. We thus 13Complementary facets of intrinsic and extrinsic evalua-\ntion have been observed in multiple settings. For example,\nthere is limited correlation between intrinsic and extrinsic fair-\nness metrics (Gonen and Goldberg, 2019; Goldfarb-Tarrant\net al., 2021; Cao et al., 2022), and in intrinsic metrics of lan-\nguage model quality (like perplexity) and downstream task\nperformance(Faruqui et al., 2016; Dudy and Bedrick, 2020).\n6\n(a) Correlation with Hofstede’s Cultural Dimensions (HCD)\n (b) Correlation with World Values Survey (WVS)\nFigure 2: Kendall’s τcrank correlation between text distribution and cultural closeness of countries. For both plots,\ntext similarity is measured using BLEU . For HCD correlation statistic values are greater than 0, implying a small\nbut positive correlation (2a). However, for WVS, most correlations are less than 0, indicating small and negative\ncorrelation (2b). There are no clear trends among different models or tasks.\n(a) Correlation with Hofstede’s Cultural Dimensions (HCD)\n (b) Correlation with World Values Survey (WVS)\nFigure 3: Kendall’s τcrank correlation between cultural closeness and text outputs of story generation for GPT\n3.5. For both plots, text similarity is measured using BLEU . There is a mix of positive (green) and negative (red)\ncorrelation. Russia, China, and Australia have positive correlations while India, USA, and Canada have negative\ncorrelations. European, South American, and African countries are split between positive and negative correlations.\nbelieve that future work on advancing cultural competence should focus on tasks reflective of user interactions with language technologies. Need for Comprehensive Human Evaluation Our results show that models adapt to explicit cues of culture with culturally relevant words (§ 5.2. But, it is unclear how this will affect user experience. In prior work, Lucy et al. (2023) found mixed reactions from users when an email auto-reply system adapted to cues of their identities. Moreover, we do not consider any implicit cues of culture, like dialect or topical differences in queries (Kirk et al., 2024). Thus, understanding whether model adaptations triggered by implicit and explicit cues of culture are useful or desired by users remains open. 431Further, as the qualitative evaluation shows, the output contains names that are typically associated with the ethnic majorities of the country. This is reflective of biases of the models, which can also lead to potentially offensive, and hurtful generations. While user-facing LLMs might have some, albeit imperfect, safeguards against generating outright toxic content, they might still generate stereotypical text for marginalized groups and cause representational harms (Gadiraju et al., 2023). Thus, the design of extrinsic evaluation of cultural competence should be task-grounded and usercentred. Future work should look into designing human evaluation that considers context (when are adaptations useful?), user agency (do users want adaptations?), and representational harms (who is 7\ndepicted and how?) in a holistic manner. Accounting for the Multi-faceted, Intersectional, and Dynamic Nature of Culture We find that the correlation between text similarity and cultural values is affected by the measure of the cultural values (§ 5.3). One of the reasons for this might be that measures of cultural values like HCD and WVS are imperfect and incomplete. This is because are ample disagreements on the very definition of culture (Baldwin et al., 2006). In fact, Hofstede’s Cultural Dimension Theory has been widely criticized for its static nature and oversimplification of culture (Signorini et al., 2009). Even so, evaluating cultural competence in LLMs heavily relies on these measures of culture, inheriting these flaws. Future work should consider diverse and complimentary measures of culture. Further, like the Hofstede’s theory, most evaluations of cultural competence are also done using static benchmarks. However, the world is an evolving place where cultural norms and values are not static. They change and develop through complex interactions among societies. Future work should focus on incorporating evaluation methods like dynamic benchmarking (Kiela et al., 2021) or dealing with disagreements (Davani et al., 2022), among others to account for the evolving nature of culture. Finally, in our work, we use nationality as a proxy for culture. Our choice was motivated by the availability of data for cultural values for countries and by similar operationalization in prior work. However, culture cannot be anchored by nationalities alone. Moreover, countries are not monoliths and comprise of many and diverse communities. Calls for inclusive evaluations of fairness in language technologies (Bhatt et al., 2022) have led to important recent work on building fairness resources with participatory design (Dev et al., 2023b,a). We believe that methods of evaluation of cultural competence should also similarly embrace participatory and intersectional design. Overall, the holistic evaluation of cultural competence should account for the multi-faceted, intersectional, and dynamic nature of culture. 7 Limitations While our work serves as a starting point and a call to focus on the extrinsic evaluation of cultural competence, it is not free of limitations. First, we perform limited qualitative evaluation, 496and we do not perform any comprehensive human evaluation of the outputs. We describe considerations for comprehensive human evaluation in § 6. Secondly, our work is anchored on nationalities and relies on imperfect measures of cultural values. However, as we describe in detail in § 6, evaluation of cultural competence demands participatory and intersectional approaches, in addition to accounting for imperfect and static measures of cultures. Further, our evaluation of the outputs does not reflect their pragmatic correctness. In other words, have not evaluated whether a model’s adaptations for a particular question (eg. ‘Explain elections...’) correctly reflect how the topic is operationalized in the country. Such evaluation needs either expert knowledge or a comparison with verified sources. Moreover, in measuring the characteristics of the text distributions, we focus only on vocabulary. This provides a starting point for cultural competence. However, culturally sensitive text will need to be evaluated for further characteristics also, for example adhering to the tonality, formality, or other stylistic expectations that might vary culturally. Finally, in our evaluation, we prompt the model with the nationality explicitly and in English. However, there might be other implicit cues of culture that trigger adaptations such as the language and dialect of interaction, and topical differences in queries which we do not account for in this work. We hope that future work can address these limitations to holisitcally evaluate LLMs for cultural competence in user-facing tasks. 8 Conclusion In this work, we evaluated cultural competence in two tasks, story generation and open-ended question answering. Our data contributions include a hand-curated list of 345 diverse questionanswering topics and 35 story generation topics. We also obtain model outputs for 6 models and 195 nationalities which we will make available for further analysis. Our methodological contributions include conceiving two quantitative and one qualitative analyses for evaluation of LLM outputs for cultural competence. Using these methods, we find that models do vary their outputs with varying nationalities (§ 5.1), outputs contain culturally relevant artefacts (§ 5.2), and model outputs weakly correlate with cultural values (§ 5.3). Our findings underscore the importance of comprehensive extrinsic evaluation of cultural competence. 8\nEthical Considerations Broader implications and Social Impact We do not study any sensitive content in this paper, but we note that the outputs of the models could have potentially sensitive and offensive content. Further, the cultural competence of LLMs (or lack of thereof) can lead to varying experiences for users from different demographic backgrounds. We discuss the importance of considering user agency and representational harms in this context in § 6. Author Positionality Statement Anonymized for peer review 558",
        "aspect_review": "The paper introduces a novel approach by focusing on extrinsic evaluation of cultural competence in LLMs through text generation tasks like story generation and open-ended question answering. This is a significant departure from prior works that only considered intrinsic evaluations. The methodology is robust, incorporating both quantitative and qualitative analyses to evaluate cultural competence across 195 nationalities using six different LLMs. The use of lexical variance and TF-IDF scoring to assess cultural relevance in outputs is particularly innovative. The evaluation is comprehensive, using a mix of BLEU scores, word edit distance, and TF-IDF to analyze outputs. The study also contrasts intrinsic measures of cultural values with extrinsic outputs, providing a well-rounded evaluation framework. The analysis is insightful, revealing that intrinsic and extrinsic measures of cultural competence do not correlate well. This highlights the need for more holistic evaluation methods and suggests a new direction for future research.\n\nWhile the methodology is strong, the study does not account for implicit cultural cues such as dialect or topical differences, which could also influence model outputs. The evaluation lacks a comprehensive human assessment to validate whether the adaptations made by models are perceived as culturally competent by users. The results indicate weak correlations between text similarity and cultural values, suggesting that the current measures may not fully capture cultural competence. More dynamic and participatory evaluation methods could be explored.",
        "general_review": "The paper presents a novel approach to evaluating cultural competence in large language models through extrinsic evaluation methods, which is a significant departure from prior intrinsic evaluations. The study is technically rigorous, employing both quantitative and qualitative analyses across multiple models and tasks, which strengthens the validity of the findings. The comprehensive experimental setup, including a wide range of nationalities and tasks like story generation and open-ended question answering, provides a robust validation of the models' cultural adaptability. The practical impact is substantial, as it addresses the critical need for culturally competent AI systems, enhancing user interactions across diverse cultural contexts.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in sections explaining the correlation analyses. 2. Some details regarding the hyperparameters used in model evaluations could be more explicitly stated for reproducibility. 3. The formatting of figures could be slightly improved for better readability. 4. While the code availability is mentioned, a clearer roadmap for accessing and utilizing the data and code upon publication would enhance transparency."
    },
    "9": {
        "paper": "Hateful Word in Context Classification\nAnonymous ACL submission\nAbstract\nHate speech detection is a prevalent research field, yet it remains underexplored at the level of word meaning. This is significant, as terms used to convey hate often involve non-standard or novel usages which might be overlooked by commonly leveraged LMs trained on general language use. In this paper, we introduce the Hateful Word in Context Classification (HateWiC ) task and present a dataset of ∼4000 instances, each labeled by three annotators. Our analyses and computational exploration focus on the interplay between the subjective nature (context-dependent connotations) and the descriptive nature (as described in dictionary definitions) of hateful word senses. HateWiC annotations confirm that hatefulness of a word in context does not always derive from the sense definition alone. We explore the prediction of both majority and individual annotator labels, and we experiment with modeling context- and sense-based inputs. Our findings indicate that including definitions proves effective overall, yet not in cases where hateful connotations vary. Conversely, including annotator demographics becomes more important for mitigating performance drop in subjective hate prediction. 1 Introduction This paper introduces the Hateful Word in Context Classification (HateWiC) task, which aims to determine the hatefulness of a word within a specific context, as illustrated in Figure 1. We argue that hateful word senses are not enough in focus within Hate Speech Detection (HSD) research, and not descriptive only, but highly subjective, asking for another approach than other lexical semantic tasks like Word Sense Disambiguation (WSD). Hateful senses are not enough in focus within HSD research. The current focus of HSD research predominantly revolves around the classification of entire utterances, such as social media posts (Waseem and Hovy, 2016; Davidson Figure 1: Illustration of the HateWiC Classification task\nand a conceptual semantic space that underlies the tar-\ngeted phenomenona of hate-heterogeneous word senses,\nhighlighting the distinction between the descriptive as-\npects (e.g. cookie or person) and hateful connotation.\net al., 2017). Within these utterances, lexical cues frequently play a significant role in the decisionmaking process. Yet, the computational modeling of context-specific hateful word meanings remains largely unexplored, with a few exceptions in this direction (Dinu et al., 2021; Hoeken et al., 2023b). LMs commonly employed in HSD systems demonstrate effective word meaning modeling (Nair et al., 2020), but they tend to lack sensitivity to domain-specific, non-standard or novel word senses (Kumar et al., 2019; Blevins and Zettlemoyer, 2020). This insensitivity becomes particularly critical in detecting hateful word meanings, that are used in unconventional or emerging contexts as the evolution of societal events gives rise to the continuous invention of novel expressions of hate (Qian et al., 2021). Words within the estab1\nlished lexicon, like Oreo , whose primary meaning may not have any negative connotations (a cookie), are repurposed to convey hate towards particular groups or individuals (e.g. based on ethnicity). Hateful senses are not descriptive only. Following theoretic work by Frigerio and Tenchini (2019), hateful terms could be positioned along a meaning continuum from descriptive to expressive, closer to but not atthe expressive outer end. The descriptive component comprises the truth-conditional attributes of a term, often recorded in dictionary definitions. The expressive component, i.e. the connotation of a term, concerns speakers’ attitudes and emotions, making it highly context-specific and subjective. A word’s sense definition could imply a hateful connotation, but this is not always the case, such as when used in a playful or self-identifying way (e.g. the third usage in Figure 1). Thus, a word’s hateful connotation is not exclusively tied to its descriptive definition, a phenomena which we term as hate-heterogeneous senses , but depends on various contextual factors like conveyed content or the reader’s identity. This aspect is often overlooked in HSD systems, typically developed using data reflecting a single (majority) perspective (Zampieri et al., 2019; Mathew et al., 2020). Our contributions. In this study we address the gap in HSD by focusing on subjective hateful word meanings within context. We introduce the HateWiC dataset, a dataset of ∼4000 WiCinstances for which we collected three hatefulness ratings each. We design methods to classify sense representations and evaluate them both against the majority and the individual annotator’s label. In doing so, we experiment with modeling descriptive and subjective aspects of hateful word senses by incorporating sense definitions (as also provided to annotators) and annotator information.12 Related Work In this section, we discuss previous work on the key aspects of this study: HSD at the word level (2.1), incorporating subjectivity in HSD (2.2), and methods for modeling word senses (2.3). 2.1 Hate Speech Detection on Word Level Although the main body of research into HSD has focused on the level of utterances, some studies 1The code used for this study and the directly publicly available part of our data can\nbe found at: https://anonymous.4open.science/r/HateWiC-CC2F . The full HateWiC\ndataset will be open to public upon request and will be licensed under CC BY-NC 4.0.have delved into hate speech on a lexical level. Prior to LLMs, feature-based HSD systems (e.g. Lee et al. (2018)) often incorporated hate speech lexicons. Wiegand et al. (2018) demonstrated the induction of an abusive word lexicon in a noncontextualized setting. A specific subset of hateful terms within context is addressed by Hoeken et al. (2023b), who modelled slur detection employing a dimension-based method similar to the identification of gender bias in word embeddings (Bolukbasi et al., 2016). This approach, that requires a pre-given set of minimal pairs, is much more complex when tackling the broader spectrum of hateful terms, including words with both hateful and non-hateful meanings. Qian et al. (2019) presented a framework aiming to predict the definition of hateful symbols, terms with a non-hateful surface form conveying hate, yet not covering the disambiguation between hate and non-hate. Dinu et al. (2021) introduce the task of disambiguating usages of pejorative words, targeting words with any negative connotations. They curated two small-scale datasets and applied several methods, with an MLP classifying BERT embeddings (Devlin et al., 2019) as most effective. However, their evaluation against the majority label (out of two linguist annotations) largely sidelines the subjectivity inherent to connotation. We extend word sense-level disambiguation of hateful language while incorporating the crucial subjective aspect, on a larger and more comprehensive scale. 2.2 Subjective Hate Speech Detection Most existing datasets and methods in HSD adopt a single, majority perspective, ignoring the inherent subjectivity influenced by diverse social and cultural factors (Zampieri et al., 2019; Founta et al., 2018). This approach has been shown to result in problematic biases, concerning e.g. ethnicity, gender, and political beliefs and highlight the need for new methodologies that account for the varying interpretations of hateful connotations (Davidson et al., 2019; Kumar et al., 2021; Sap et al., 2022). Davani et al. (2022) took steps in this direction by training a model to predict individual annotations as subtasks, still ultimately aiming to predict the majority label. Kanclerz et al. (2022) addressed the task of predicting each individual annotator’s label, by leveraging annotator’s labeling statistics within the dataset. A more comprehensive approach is presented by Fleisig et al. (2023), who included annotators’ demographics, preferences, 2\nand experiences as input, along with text. They utilized RoBERTa (Liu et al., 2019) to embed descriptions of these characteristics. Our research continues this line of work by predicting individual annotator labels and accounting for their demographics in the classification of hateful words. 2.3 Modeling Word Senses Shifting the focus from modeling hateful utterances to the meaning of hateful words within utterances, touches upon various lexical semantic NLP tasks that involve the creation of word sense representations (Vuli ´c et al., 2020a; Schlechtweg et al., 2020; Martelli et al., 2021). Approaches to these tasks often employ contextualized word embeddings extracted from pretrained (often BERT-based) LMs (Loureiro and Jorge, 2019; Martinc et al., 2020; Bommasani et al., 2020). Fine-tuning a model on particular data or tasks, such as WSD or sentiment classification, is performed to potentially inject relevant information into the resulting representations (Giulianelli et al., 2020; Hoeken et al., 2023a). Rachinskiy and Arefyev (2022) leveraged an effective WSD model developed by Blevins and Zettlemoyer (2020), which jointly optimizes two encoders for the context and gloss of a word sense, respectively. For the task of semantic change discovery, they extracted the representations of the context encoder of the WSD-finetuned model. Recently, Giulianelli et al. (2023) introduced an innovative approach to computational sense representations. Their method adopts the definition-assense paradigm, utilizing definitions generated by a Flan-T5 model (Chung et al., 2022) fine-tuned on datasets of definitions with usage examples. Sentence embeddings of these generated contextspecific definitions show promising results on lexical semantic similarity tasks. Despite these advancements focused on descriptive word senses, effective approaches for modeling highly connotative lexical phenomena remain unclear. 3 The HateWiC Dataset We introduce the HateWiC dataset, which includes hate ratings for words within example usages along with their word sense definitions which may be hate-heterogeneous, as illustrated in Figure 1. We describe the dataset construction below. 3.1 Wiktionary Data Data was scraped from the English Wiktionary in November 2023, an online dictionary where any- 204one can contribute to documenting and explaining words in use. Therefore, Wiktionary provides upto-date insights from user perspectives and covers a broader range of terms from diverse domains than traditional dictionaries. Each entry (word or multi-word expression) includes information such as definitions, example uses, and category labels that provide additional context about a word’s use (e.g., ‘British slang’ or ‘Archaic’). Using the Wiktionary API, we extracted all entries that had at least one word sense tagged with the categories Offensive and Derogatory and were also members of the category People, to gather the most relevant terms for hate speech detection purposes. For each of the resulting 1087 terms, we scraped all its sense definitions along with all labeled categories and example sentences (using the WiktionaryParser library). This resulted in 3500 senses and 4671 examples. To suit the dataset for our HateWiC classification task, we manually excluded 642 examples due to historical spelling or other deficiencies, as detailed in Appendix A. After processing, the dataset comprised 4029 instances covering 1888 unique senses, averaging 4.88 examples per sense, and 826 unique terms, averaging 2.29 senses per term. 3.2 Annotation The senses from the Wiktionary data include labels regarding their offensiveness or derogatory nature. However, these classifications do not represent the diverse interpretations of these labels due to their subjective nature. To address this, we collected crowd-sourced annotations using Prolific with a link integration to Argilla. Argilla, an open-source platform launched on HuggingFace Spaces, is used to set up the annotation task on HateWiC data. For each annotation instance, annotators are presented with an example sentence, the target term, and its sense definition. They are then prompted with the question: “ How would you rate the hatefulness of the meaning of the target term within the specific example text? ”. Annotators respond by selecting from the labels: ‘Not hateful’, ‘Weakly hateful’, ‘Strongly hateful’ and ‘Cannot decide’. An example of an annotation instance and the user interface are depicted in a screenshot provided in Appendix B. In the annotation guidelines (accessible on our repository), annotators are instructed to focus their evaluation on the specific usage of the term within the example sentence, rather than the overall connotation of the sentence, or the defini3\ntion, which is only provided to aid in understanding the term’s meaning. Additionally, we emphasize the subjective nature of their judgements. We aimed for three annotations per instance, with each annotator labeling 250 instances.2Using Prolific’s pre-screening filters, we selected annotators who indicated that their primary language is English. To improve the quality of the collected annotations, we excluded and replaced data from annotators who were too fast and/or failed control instances.3Prolific provides demographic information for each annotator, which can be connected to their annotations. The final pool of annotators, after exclusions, consisted of 48 individuals with diverse genders and ethnicities averaging 28 years old (more details in Appendix B). 3.3 Dataset Results After excluding the ‘Cannot decide’ annotations4, the dataset yielded 11902 individual annotations, of which 5708 (48.0%) hateful and 6194 (52.0%) not hateful (after converting to binary by merging ‘Weakly hateful’ and ‘Strongly hateful’). After applying majority voting, out of the 3845 example sentences with a clear majority binary label, 1815 (47.2%) were classified as hateful and 2030 (52.8%) as not hateful, yielding a balanced dataset with respect to hatefulness. Annotators agreed for 60% (i.e. 2414) of the binary classification with a Krippendorff’s alpha of 0.45. For the three-class classification, agreement was 51.3% with a Krippendorff’s alpha of 0.33. In comparison, Mathew et al. (2020) reported an agreement of 0.46 for a similar three-class task, and Vigna et al. (2017) 0.26 for their binary setting. The agreement scores underscore the inherent subjectivity of the task, motivating us to include individual demographics to our modeling. The high degree of context dependency regarding hate becomes even more apparent when we examine the relationship between word senses (the descriptive aspects outlined in their definitions) and the hatefulness ratings assigned to examples of those senses. We identified 319 hateheterogeneous sense definitions, i.e. unique definitions for which example sentences exist in the dataset with both hateful and non-hateful majority 2The average reward per hour was £9.28.\n3More than 2 out of 8 failed control instances and/or less\nthan 45 min. completion time; median time was 90 min.\n4The majority of the 514 ‘Cannot decide’ annotations were\nfound to concern deficient sentences upon closer analysis.annotations. This observation, already implied by the inter-annotator agreement for individual labels, solidifies that the hateful connotation of a word sense is not exclusively determined by its descriptive definition. Two examples from the annotated data illustrate this. Both examples mention the term carrot cruncher with the sense definition “Someone from a rural background; a bumpkin.” where (1) is unanimously annotated as not hateful and (2) is unanimously annotated as strongly hateful. (1) “Me having an up to date style even though I’ve turned into a carrot cruncher .” (2) “At least I come from a part of the world that has got a football team; you’re a friggn’ carrot cruncher and you support the bloody scally’s.” 4 HateWiC Classification Our HateWiC dataset enables the development and evaluation of computational methods for predicting whether the meaning of a target term is hateful within a specific context. We introduce various classification methods that differ with respect to the sense representations (outlined in 4.1) and incorporation of annotator information (4.2) as input to a classification model (4.3), or that levarage an instruction-tuned LLM (4.4). 4.1 Sense Representations For representing the (non-)hateful word sense of a target term, we primarily follow a common procedure in lexical semantic NLP tasks and extract contextualized embeddings from pretrained LMs. To optimize effectiveness on the HateWic task, we experiment with various encoder models and embedding types. Appendix C provides additional details on our employed methods. Encoder models. We experiment with three different encoder models, each trained on different data or tasks. We use the pretrained BERT (base) model (Devlin et al., 2019) and HateBERT (Caselli et al., 2021), a re-trained BERT model on hate speech5. As third, we utilize a trained biencoder model for Word Sense Disambiguation (Blevins and Zettlemoyer, 2020), which we refer to as WSD Biencoder . The model comprises a contextualized word encoder and a gloss encoder initialized with BERT-base encoders. We train it on WordNet data (Miller et al., 1994), following the same procedure as detailed in (Blevins and Zettlemoyer, 2020), for 7 epochs with a batch size of 5https://huggingface.co/GroNLP/hateBERT\n4\n8. Following Rachinskiy and Arefyev (2022), the WSD-optimized contextualized word encoder is then used for obtaining embeddings. Embeddings. The encoders are used to generate different word sense related representations. First, we compute word in context (WiC) embeddings. We feed the example sentence to the encoder model and extract the last hidden layer for the subwordtokenized position(s) that encode the target term (averaging over them in case of multi-subword target terms). Second, we test the incorporation of word sense definitions from Wiktionary. This definition (Def) embedding is obtained by averageing over all token embeddings, using the same procedure as for WiC embeddings but with the definition sentence as input. Third, considering that pregiven definitions may not be available in practical applications, we create T5-generated definition (T5Def) embeddings. We generate definitions using a FLAN-T5 Base (250M parameters) model developed by (Giulianelli et al., 2023)6which was fine-tuned on datasets of English definitions and usage examples. We prompt the model with the same template as it was trained on: “[ SENTENCE ] What is the definition of [ TERM ]?”. Consequently, the generated generated are more context-specific than the Wiktionary definitions. These generated definitions are embedded the same way as the Defembeddings. 4.2 Annotator Information To address the subjective nature of the HateWiC classification task, we incorporate this aspect into our modeling approaches. We experiment with a similar strategy as presented in Fleisig et al. (2023). For each individual annotation of a HateWiC instance, we concatenate an annotator (Ann) embedding to the corresponding sense embedding, that represent a description of annotator’s demograpics. This description is embedded through the same procedure as the definition embeddings and follows this template: “Reader is [ AGE], [ GENDER ] and [ETHNICITY ].” 4.3 Classifying Embeddings We test the effectiveness of (the concatenation of combinations of) the embeddings proposed above on our HateWiC classification task by using them 6https://huggingface.co/ltg/flan-t5-definition-en-baseas input to a classification model. To this end, we train and test a four-layer multi-layer perceptron (MLP) model (a classification algorithm also used in Dinu et al. (2021)) on the HateWiC dataset. 4.4 Classification with LLaMA 2 In addition to the encoder-LM based approaches above, we also experiment with a LLaMA 2 model (Touvron et al., 2023). Due to their instructiontuning training regime, and huge amount of training data, foundation models like LLaMA 2 are proven to be superior to LMs on many zero-shot settings, yet subjective HSD and WSD are by nature very challenging tasks. We aim to see the abilities of an instruction-tuned LLM on this task as a (strong) baseline. We test zero-shot classification with a 7B-sized LLaMA 2 model7. We run the inference of this model using the transformers library. In our prompt, we input the example sentence and the target term and instruct the model to classify the meaning of the term as hateful or not hateful (complete template and configuration parameters are provided in Appendix C). 5 Evaluation Setup We evaluate our proposed methods using various test setups on the HateWiC dataset (5.1). Additionally, we compare our methods with the work of Dinu et al. (2021), as described in 5.2. 5.1 HateWiC Our HateWiC dataset includes three hate ratings for each example sentence, allowing evaluation on two distinct tasks that vary in terms of subjectivity inclusion. For both tasks, we utilize binary labels. 1.Majority label prediction: gold labels represent 4029 majority votes on each example. 2.Subjective label prediction: gold labels consist of all 12442 individual annotations, covering each rating for each example. We conduct evaluations for each task using a ten-fold cross-validation setup. For each fold, we divide the dataset into training, development, and test sets with an 80-10-10 ratio. We experiment with two variants: 1.Random : The data is randomly split based on example sentences, testing performance on sentences not seen during training (similar 7https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n5\nto common practice in WSD-like tasks (Dinu et al., 2021)), which is particularly relevant for individual annotator prediction where multiple instances of the same sentence occur. 2.Out-of-Vocabulary (OoV) : The data is split based on terms, testing performance on unseen terms, i.e. zero-shot capabilities. 5.2 Comparison with Dinu et al. (2021) We also train and test on two small datasets of English tweets developed and used in Dinu et al. (2021). They collected these from existing hate speech datasets, focusing on tweets that mention one of the terms in a curated set of pejorative terms. Each tweet was labeled based on whether the term was used pejoratively. The first dataset, which we will refer to as DINU1 comprised 1004 tweets covering 31 terms. The second, which we name DINU2 , consisted of 301 tweets covering 11 terms. Their reported best method involved MLP classification of BERTweet (Nguyen et al., 2020) and BERT (base) embeddings (extracted as the sum of all model layers for the target word position) on DINU1 and DINU2, respectively. We aimed to use the same evaluation set-up as described in their paper, using five-fold cross-validation and reporting the average over accuracies per term . 6 Results This section presents the results of our proposed methods on the HateWiC classification, evaluated using the above outlined setups. 6.1 Majority HateWiC Classification Table 1 presents the accuracy results on HateWiC classification compared to the majority label. Overall, the performance values demonstrate the effectiveness of all methods, with only minimal differences (max. 2 %-points) between BERT, HateBERT and WSD biencoder models. Training BERT-based models on different types of information regarding hatefulness or word senses does not seem to have a substantial effect. Def-embeddings achieve slightly higher accuracies than WiC-embeddings , and a combination of the two yields the best results. For a test set with OoV terms only, all embedding types show only a slight drop in performance. WiC+Def-embeddings exhibit the smallest decline on the zero-shot setting and achieve 2-5 % higher accuracy than WiC- and Def-embeddings. This indicates that definitions 489Embeddings BERT HateBERT WSD bien.\nRandom OoV Random OoV Random OoV\nWiC 0.75 0.73 0.75 0.71 0.76 0.73\nDef 0.77 0.75 0.78 0.73 0.78 0.73\nT5Def 0.70 0.67 0.70 0.67 0.72 0.69\nWiC+Def 0.78 0.77 0.80 0.77 0.79 0.78\nWiC+T5Def 0.75 0.74 0.76 0.73 0.76 0.73\nTable 1: Accuracy on HateWiC classification compared\nto the majority label, with different input embeddings,\ntested on a random data split (best underlined) and a test\nsplit with OoV terms only (best in bold).\nprovide valuable information, performing better on their own than word information alone, and the combination of both is most effective, especially for OoV-terms. T5-generated definitions demonstrated the lowest accuracy on their own but perform equally or slightly better than WiCembeddings when concatenated. An evaluation of T5-generated definitions compared to Wiktionary definitions showed a SacreBLEU very low score of 3.822 (in range 0 to 100), possibly explaining the differences in performance between them. The distinction between context-independent Def-embeddings and context-specific WiCand T5Def-embeddings becomes more clear upon examining their performance across hatehomogeneous and hate-heterogeneous instances (as defined in Section 3.3), presented in in Table 2. In the case of hate-heterogeneous instances, we observe an accuracy drop of up to 47% when using Def-including embeddings compared to the homogeneous instances. This drop is limited to 24-29% for the other embeddings, showcasing their superior ability in handling less descriptive scenarios. HateBERT\nembeddingsHate-homogeneous\nTrue False\nWiC 0.82 0.55\nDef 0.91 0.44\nT5Def 0.76 0.52\nWiC+Def 0.91 0.49\nWiC+T5Def 0.84 0.55\nTable 2: Accuracy on HateWiC classification compared\nto the majority label w.r.t. hate homogeneity of the\nsense definition (best underlined).\nLLaMA 2 result. The accuracy score on the HateWiC classification using a LLaMa 2 model, following the zero-shot experimental setup detailed in Section 4.4, is 0.68. Unlike the superior performance on many downstream tasks, the LLaMA model falls short compared to the aforementioned 6\nmodels on our HateWiC task. This outcome highlights the subjective nature of the task, indicating that general-purpose models struggle to fully grasp its nuances and perform well on it. 6.2 Subjective HateWiC Classification Performance of our designed methods on predicting individual annotation labels, which showed considerable variation in Section 3.3, are presented in Table 3. Overall, accuracy values are slightly lower (by 2-5 %-points) compared to predicting the majority label, but remain robust. The results exhibit the same patterns in terms of different models, test data setups, and tested embedding types. Adding the Annotator embedding has a minimal effect, generally resulting in equal or slightly improved performance compared to the same type of embedding without concatenated annotator information. Embeddings BERT HateBERT WSD bien.\nRandom OoV Random OoV Random OoV\nWiC 0.71 0.69 0.71 0.69 0.72 0.70\nDef 0.74 0.71 0.75 0.73 0.74 0.71\nT5Def 0.68 0.65 0.68 0.67 0.68 0.67\nWiC+Def 0.75 0.74 0.75 0.73 0.75 0.73\nWiC+T5Def 0.72 0.70 0.72 0.71 0.73 0.69\nWiC+Ann 0.72 0.69 0.72 0.69 0.72 0.70\nDef+Ann 0.74 0.72 0.76 0.72 0.75 0.72\nT5Def+Ann 0.69 0.67 0.69 0.65 0.69 0.68\nWiC+Def+Ann 0.75 0.73 0.75 0.74 0.75 0.74\nWiC+T5Def+Ann 0.72 0.71 0.73 0.71 0.73 0.72\nTable 3: Accuracy on HateWiC classification compared\nto the individual annotator label, with different input\nembeddings, on a random data split (best underlined)\nand a test split with OoV terms only (best in bold).\nTo better understand the impact of subjectivity, we more closely examine instances where subjectivity is most apparent In Table 4 we report performance results not only with respect to the hate homogeneity of word senses, but also to annotator agreement, i.e. whether the annotator agreed with the majority. We present results for HateBERT embeddings in an evaluation setting with random test data split, but similar patterns are observed for BERT and WSD Biencoder embeddings, as well as on on a test data split with OoV terms only. For sentence annotations where the annotator disagreed with the majority label or the sense definition is hate-heterogeneous, the performance of all embeddings drops significantly. This effect is most pronounced for definition-including embeddings (Wiktionary), less so for T5-generated, which aligns with their more context-specific nature. Specifically, there is an accuracy drop of up to 47% in cases of annotator disagreement, and up 556HateBERT\nembeddingsMajority annotation Hate-homogeneous\nTrue False True False\nWiC 0.77 0.40 0.77 0.55\nDef 0.81 0.36 0.83 0.51\nT5Def 0.72 0.42 0.72 0.53\nWiC+Def 0.83 0.36 0.83 0.55\nWiC+T5Def 0.78 0.39 0.78 0.56\nWiC+Ann 0.77 0.49 0.77 0.59\nDef+Ann 0.82 0.44 0.82 0.60\nT5Def+Ann 0.73 0.47 0.72 0.59\nWiC+Def+Ann 0.80 0.44 0.81 0.58\nWiC+T5Def+Ann 0.77 0.48 0.78 0.58\nTable 4: Accuracy on HateWiC classification compared\nto the individual label w.r.t. annotator agreement with\nthe majority label and hate homogeneity of the sense\ndefinition (best underlined).\nto 32% in cases of hate-heterogeneous definitions. However, incorporating annotator information mitigates this effect by up to 11%. Annotator information contributes to the cases where the subjective annotation deviates from the majority label, these cases also align with sense definitions that exhibit both hateful and non-hateful labeled sentences. 6.3 Results on DINU Data The DINU1 and DINU2 evaluation datasets do not provide sense definitions or information on annotators, thereby limiting our testing to our methods that do not require this information. Table 5 presents the results on both DINU1 and DINU2. Model Embedding DINU1 DINU2\nBERT WiC 0.89 0.83\nT5Def 0.81 0.79\nWiC+T5Def 0.90 0.83\nHateBERT WiC 0.87 0.83\nT5Def 0.83 0.80\nWiC+T5Def 0.90 0.84\nWSD Bienc. WiC 0.90 0.82\nT5Def 0.80 0.79\nWiC+T5Def 0.90 0.84\nBest Dinu 0.82 0.83\nTable 5: Accuracy of our methods on the DINU datasets\ncompared the accuracy of the best performing method\nas reported in Dinu et al. (2021) (best underlined).\nOur methods, except for those including T5Defembeddings only, demonstrate improvements over the best-performing methods proposed by Dinu et al. (2021). These improvements are particularly substantial (by 8%) for the larger DINU1 dataset. Consistent with trends observed for the HateWiC dataset, the concatenation of WiC and T5-generated definition embeddings yields the best performance across both DINU sets, underscoring the potential of incorporating automatically gener7\nated definitions in the absence of dictionary definitions for HateWiC classification. 7 Discussion Our study offers valuable insights into the detection of hate speech through the lens of lexical semantics, introducing the HateWiC dataset and presenting classification experiments. The negligible difference observed in our experimental outcomes between HateBERT and general (WSD) models not only questions the efficacy of extensive training on hate speech data for accurately capturing hateful semantics, but also underscores the necessity of a more nuanced approach beyond the existing lexical semantic methods for tasks like HateWiC classification. Our results demonstrate the impact of incorporating sense definitions and annotator characteristics on model performance, particularly in scenarios involving out-of-vocabulary (OoV) terms or high subjectivity. To define or not define? Incorporating sense definitions into our methods to encompass the descriptive component of hateful terms, which, according to lexical semantic theory, primarily contain an expressive component but not exclusively, yielded mixed results. Overall, embedded Wiktionary definitions proved highly effective, outperforming WiC-embeddings alone. T5-generated definitions demonstrated the lowest accuracy on their own but performed equally or slightly better than WiC-embeddings when concatenated with WiC-embeddings. However, in cases with more variation in the subjective ratings, the performance of all embeddings dropped significantly but most pronounced for Wiktionary definition embeddings, though to a lesser extent for T5-generated definitions (with a drop difference of up to 23%). This suggests the potential usefulness of automatically generating context-specific definitions for subjective lexical semantic tasks like HateWiC classification. Future research could consider more advanced definition generation techniques, possibly leveraging larger models or fine-tuning on Wiktionary definitions, while avoiding overreliance on dictionary definitions as the ultimate standard. To individualize anyway? The low interannotator agreement in our dataset underscores the importance of considering individual annotator perspectives in hate speech detection. Our experiments incorporating annotator information in our 628computational methods proved beneficial, particularly in cases of annotator disagreement or hateheterogeneous definitions, where including annotator information mitigated accuracy decline by up to 11%-points. This highlights the value of personalizing models to account for subjectivity in annotations. Future research could explore additional annotator information and conduct ablation experiments to identify the most effective aspects for HateWiC classification. To consider as well? Our study paves the way to obtaining deeper insights into the relationship between hateful and non-hateful word senses. For instance, whether certain semantic relations (e.g. metaphorical, metonymical), categories (e.g. food, animals), or attributes (e.g. color, material) are more likely to distinguish between hateful and nonhateful senses. And even next-level, whether these discriminators are language-specific or show crosslanguage parallels. Identifying such consistencies between (non-)hateful senses could enhance the (automatic) discrimination between them. 8 Conclusion This paper introduces the Hateful Word in Context Classification (HateWiC) task, addressing the underexplored area of subjective hateful word meanings within specific contexts. We present the HateWiC dataset, comprising about 4000 WiCinstances, each annotated with three hateful ratings. Our study focused on the interplay between descriptive and subjective aspects of hateful word senses. We addressed the prediction of both majority and individual annotator labels. We experimented with different types of inputs to our classification system, including sense definitions and annotator demographics. We demonstrated the impact of these factors on model performance, particularly in cases involving out-of-vocabulary terms or high subjectivity. The incorporation of established sense definitions proved highly effective overall but demonstrating diminished performance in less descriptive scenarios. Conversely, including annotator characteristics proved beneficial, particularly in cases of annotator disagreement or hate-heterogeneous definitions. These findings underscore the value of personalizing models to account for subjectivity in annotations. Furthermore, our results suggest the potential usefulness of automatically generating definitions for subjective lexical semantic tasks like HateWiC classification. 8\nLimitations Although the Wiktionary data we utilize offers insights from user perspectives for a wide array of terms, its quality may be lower compared to expert-curated dictionaries. The provided information may contain inaccuracies, as users might not have the necessary expertise, and inconsistency in documentation could exist. However, the collaborative nature of Wiktionary allows for censorship by consensus and adherence to Wiktionary policies, mitigating some of these concerns. A constraint of our evaluation set-up lies in its reliance on binary labels. Hate speech is a multifaceted phenomenon, and a more nuanced class scheme may offer a more comprehensive understanding in future research. Ethics Statement Our study includes demographic data of annotators that concern Prolific prescreening responses which are all with annotator’s consent, self-reported, and are not provided with any direct identifiers like name or address. All prescreening questions, except for age and country of residence, are optional for participants to answer, and most personal questions have a ‘Rather not say’ option. By incorporating demographic information from annotators, we aim to enhance the understanding and prediction of how different groups perceive hate speech. This approach will ultimately lead to more robust and inclusive classification systems. However, the inclusion of demographic data raises privacy concerns, particularly the risk of re-identifying annotators. To address this, we have made our dataset available only upon request, under the CC BY-NC 4.0 license. This measure allows us to better control access to the information, ensuring it is used responsibly, ethically, and exclusively for non-commercial purposes. 716",
        "aspect_review": "The paper introduces a novel task, the Hateful Word in Context Classification (HateWiC), which is a significant advancement in the field of hate speech detection by focusing on word-level context. The methodology is robust, incorporating both sense definitions and annotator demographics to address the subjective nature of hate speech. The paper provides a comprehensive review of related work, clearly situating its contributions within the context of existing research on hate speech detection and word sense disambiguation. It extends previous studies by incorporating subjectivity into hate speech detection. The evaluation is thorough, employing a new dataset of ∼4000 instances with multiple annotations per instance. The experiments are well-designed, comparing various models and embedding types, and the results are insightful, demonstrating the efficacy of the proposed approaches.\n\nWhile the related work is comprehensive, there could be a more detailed comparison with recent advancements in language models that incorporate subjectivity. The motivation for using specific models like LLaMA 2 could be more clearly justified in the context of hate speech detection. Some sections of the paper, such as the detailed description of the dataset construction, could benefit from clearer organization to enhance readability. The reliance on Wiktionary data, which may contain inaccuracies due to its user-generated nature, could be addressed more explicitly in terms of its impact on the findings. While the paper includes various experiments, more ablation studies focusing on the impact of individual components, such as annotator demographics, would strengthen the findings.",
        "general_review": "The paper introduces the novel Hateful Word in Context Classification (HateWiC) task, which addresses the underexplored area of subjective hateful word meanings within specific contexts. This represents a significant advancement in hate speech detection by focusing on lexical semantics rather than entire utterances. The technical depth is evident through the comprehensive experimental validation involving multiple models and embedding strategies, including BERT, HateBERT, and WSD biencoder models. The introduction of the HateWiC dataset, with about 4000 instances annotated for hatefulness, provides a valuable resource for future research and demonstrates significant practical impact. The paper's exploration of the interplay between descriptive and subjective aspects of hateful word senses is insightful and contributes to the understanding of hate speech detection.\n\n1. Minor improvements in writing clarity could enhance the paper's readability. 2. Small details on hyperparameters or implementation specifics, such as the choice of batch size or learning rate, could be more explicitly stated. 3. The formatting of figures could be slightly improved for better visual clarity. 4. There is a slight need for clarification on the code availability, ensuring that all resources are easily accessible to the community."
    },
    "10": {
        "paper": "MIND: Multimodal Shopping Intention Distillation from Large\nVision-language Models for E-commerce Purchase Understanding\nAnonymous ACL submission\nAbstract\nImproving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large VisionLanguage Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MINDand create a multimodal intention knowledge base, which contains 1,264,441 million intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Additional experiments reveal that our obtained intentions significantly enhance large language models in two intention comprehension tasks. 1 Introduction Understanding customers’ intentions behind their purchase behaviors remains crucial in E-commerce as it potentially benefits several downstream tasks, such as product recommendation (Grbovic et al., 2015; Zhao et al., 2014; Li et al., 2020) and search query answering (Zhao et al., 2019; Hirsch et al., 2020). Unlike traditional factual knowledge related to products, intentions are implicit mental states of customers, which typically require commonsense knowledge to understand and reason upon (Bratman, 1984). For example, in Figure 1, the intentions of purchasing a mouse and a keyboard can bethey are very useful to computer users , which Orbit Trackball Mouse\n Wireless Solar KeyboardCustomer C o-buy Record\nLLM\nThey are both \nelectronic devices\nLow typicality\nThey are both of \nwhite color\nHallucination\nVery u seful to \ncomputer users\nFunctionalitie s\nboth offer ergonomic \nand eco -friendly \nImage -grounded\nLVLMFigure 1: Examples showing the process of distilling\npurchase intentions from large language models and\nlarge vision-language models. Without product images,\nlarge language models tend to generate intentions with\nlow typicality and hallucinated facts, while leveraging\nlarge vision-language models resolve such issue.\nis not mentioned either in the customer’s query or products’ metadata. Thus, due to such implicitness, it is infeasible to perform large-scale automatic extraction from text to obtain them. To combat this, Yu et al. (2023) proposed to distill purchase intentions from large language models, such as OPT (Zhang et al., 2022), by prompting them with real purchasing records and relevant product metadata. Human-in-the-loop annotations are also carried out to verify the plausibility and typicality of the generated intentions and train a discriminator for large-scale critic filtering. Yu et al. (2024) further entangled human annotations with instruction tuning to align the distilled intentions with a human-centric perspective. While these works provide a straightforward approach to intention acquisition, several limitations still persist. First, previous works on E-commerce intention knowledge base construction have solely focused on the text modality, thereby sacrificing significant supervision signals from visual modalities, such as 1\nproduct images. This oversight hinders the model from obtaining a more comprehensive understanding of the product, consequently compromising the quality of the generated intentions, as demonstrated in the left lower part of Figure 1. Furthermore, recent work has shown that intentions derived using current distillation methods exhibit bias towards product-centric aspects, excessively emphasizing product properties and metadata (Zhou et al., 2024). Consequently, interactions between the products and customers, including potential use cases and features of interest to customers, are absent from the derived intentions, despite being fundamental in facilitating customers’ shopping experience. Finally, human annotations are heavily deployed in current intention collection methods, which serve as a critical step in controlling the quality of the generated results. This poses a challenge towards constructing scalable yet diverse intention knowledge bases with minimum human supervision cost. To address these issues, we propose MIND, a Multimodal Shopping Intentio NDistillation framework. MIND instructs Large Vision-Language Models (LVLMs) to generate purchase intentions in a three-step manner, based on real user co-buy records and product metadata. Specifically, we select LLaVa (Liu et al., 2023a) as a representative LVLM and incorporate both visual information from the product images and text information from the product name into the generation process. To better align the generated raw intentions with human preferences and alleviate human annotation costs for further quality control, we propose a human-centric role-aware mechanism. This mechanism first instructs LLaVa to discover similar features between the products and then imitates a customer agent to decide whether the products would be bought together under previously generated intentions. By applying MINDto a subset of the Amazon Review Dataset (Ni et al., 2019), we construct a multimodal intention knowledge base. It features 1.26 million of intentions over 126,142 co-buy shopping records across 107,215 products. Human evaluations further confirm: (1) the exceptional quality of our generated intentions, which have higher plausibility and typicality than previous generation methods, and (2) the effectiveness of our proposed human-centric role-aware mechanism. Furthermore, we apply our generated intentions to two downstream tasks in the IntentionQA 113benchmark (Ding et al., 2024), which evaluates a language model’s abilities to discriminate and utilize purchase intentions. Extensive experiments show that distilling our generated intentions into large language models’ provide substantial benefits on both tasks via fine-tuning. Further analyses reveal the strengths and mechanisms behind of our innovative filter module and confirm the superiority of M INDwhen compared to previous intention acquisition methods. 2 Related Works 2.1 Shopping Intention in E-commerce Shopping intention is an implicit mental state that motivates purchase-related behaviors from the customer’s perspective (Koo and Ju, 2010). Various studies have been conducted to examine the impact of consumer shopping intentions on downstream applications (Dai et al., 2006; Zhang et al., 2016; Hao et al., 2022). Recently, Ni et al. (2019) suggested using customer reviews to investigate the underlying purchase intentions in consumer purchase behavior and created a large-scale review dataset based on Amazon. Building upon this, Yu et al. (2023) proposed FolkScope, which aims to guide LLMs in generating user co-buy intentions for different product pairs by grounding them in ConceptNet relations (Speer et al., 2017). While human evaluations confirmed its effectiveness, Zhou et al. (2024) argued that it not only remains expensive to scale up but also fails to align the resulting shopping intentions with human preferences, which encompass a wide range of factors beyond product properties and similarities. To tackle these issues, in our work, we propose MIND, a framework that undermines online co-buy intentions and aligns better with human perceptions. 2.2 Multimodal Knowledge Distillation Since VLMs have yield significant advance recently (Liu et al., 2023d; Li et al., 2023; Zhu et al., 2023), distilling domain-specific knowledge from them has become an effective yet cost-friendly trend in multimodal studies (Liu et al., 2023c; Lu et al., 2024; Jin et al., 2021). Liu et al. (2023c) proposed a framework that applies self-distillation to stimulate the pre-train process of BERT to improve its performance in E-commerce product understanding tasks. Lu et al. (2024) similarly instructed MiniGPT4 (Zhu et al., 2023) to generate user intention form social media posts text and its 2\nFiltered\nIntention s1.26M Intentions\nCo-buy \nRecordsFine -Tuning\nLLAMA\nMistral\nIntention UtilizationIntention Comprehension\nreducing \nclutter on the \ndesk…More \nergonomic to \n…\nProduct Feature Extraction\nWireless Solar \nKeyboard \nOrbit Trackball Mous e\nLLaVA\nMultimodal Intention Generation\nboth offer \nergonomic and \neco-friendly \nsolutions for \ncomputer usersboth offer \nergonomic and \neco-friendly \nsolutions for \ncomputer usersboth offer \nergonomic and \neco-friendly \nsolutions for \ncomputer users\nLLaVA\nHuman -centric Role -aware Filtering\nIf you are a consumer\nwho is eager to \n(intention )\n1. Will you buy these \ntwo products?\n2. Why?\nYes. These products \nmatch my intention !Downstream\nImprovemen tFigure 2: An overview of MIND. We first extract features from products in real-world co-buy records, generate\nintentions multimodally, and apply a human-centric role-aware filter for quality optimization.\nassociated images. Jin et al. (2021) also designed a framework to instruct the student model to imitate teacher model’s behavior, which successfully preserved the teacher model’s capabilities with fewer parameters. In our work, we share the same aspiration and leverage distillation as a tool for data collection that provides downstream benefits in the E-commerce domain. Specifically, we designed a framework to distill E-commerce intentions from LLaVa (Liu et al., 2023a) and construct a comprehensive intention knowledge base based on the resulted generations. 3 The M INDFramework 3.1 Overview of M IND Following Yu et al. (2023), the objective of MIND is formulated as a text generation task. Given a record that shows a customer’s co-buy (purchasing together) of two products, along with the detailed metadata of both products, MINDaims to generate the intentions behind such purchase behaviors that best align with the customer’s mental state during the purchase, which includes their beliefs, desires, and intents (Georgeff et al., 1999). Formally, for a given co-buy record, we define the two products as p1andp2, along with their associated images {p1\ni, p2\ni}, and features and attributes {p1\nf, p2\nf}.MINDaims to leverage a LVLM Fto generate the intentions I(p1, p2)of purchasing both products based on a pre-defined commonsense relation r, denoted as I(p1, p2, r) = F(p1\nf, p2\nf, p1\ni, p2\ni, r). In this paper, we follow Yu et al. (2023, 2024) and use relations from ConceptNet (Speer et al., 2017) to model the intentions. LLaVa-1.5-13b (Liu et al., 2023a) is used as the 196LVLM F. To achieve this objective, we design three sequentially connected steps within MIND, which are shown in Figure 2. These steps are termed as: (1) product feature extraction; (2) co-buy intention generation; (3) human-centric role-aware filtering. Together, they form a collective pipeline for systematic intention acquisition without the need for human supervision and quality filtering. 3.2 Source Data Collection We utilize the Amazon Review Data released by Ni et al. (2019), which contains millions of products from 18 domains. Each product is accompanied by detailed reviews, co-buy records, and metadata, including its product title, features, attributes, and images provided by the retailer. Following Yu et al. (2023), we select products from the Electronics andClothing, Shoes and Jewelry domains as representative products to demonstrate the effectiveness ofMIND. To fit our framework, we filter out products without accessible images that may have been removed from the Amazon platform. 3.3 Product Feature Extraction We begin processing the collected products by first extracting key features with the aid of LVLMs. This is motivated by our observations that product descriptions and attributes, inputed by retailers, tend to be noisy and unorganized, probably for promotion and style organization purposes. Thus, we explicitly instructs LVLMs to augment source product metadata by extracting implicit features from each product’s image and title by leveraging a zero-shot prompt: 3\nPrompt Template for Product Feature Extraction\nVisual Input: pi\nTextual Input: <Instruction> . Given the product\nshown in the image: pf, generate additional features\nby focusing on the product’s attribute, design, and\nquality.\nwhere <Instruction> is a detailed task instruction, and pi, pfare the respective image and details (title, descriptions, etc.) of the product. This enables LVLM to comprehend the product from both visual and textual modalities, thereby providing us with a richer set of features that complements those provided by the retailers. 3.4 Co-buy Intention Generation Then, for each co-buy pair of products (p1, p2), we provide LVLM with the acquired features together with all details of both products, and instructs it again to reason the intentions for purchasing them simultaneously. Specifically, we follow Yu et al. (2023) and leverage 20 commonsense relations from ConceptNet (Speer et al., 2017) as waymarks to lead LVLM in generating purchase intentions with controllable commonsense groundings. Similar to the previous step, a zero-shot prompt is used: Prompt Template for Intention Generation\nVisual Input: p1\ni, p2\ni\nTextual Input: <Instruction> . A customer pur-\nchased a pair of products, as shown in the images.\nThey are: p1\nf, p2\nf. Act as the customer and infer a\npotential intention behind such purchase. Start the\nintention with <Relation> .\nWhere <Instruction> is a detailed task instruction and <Relation> is the corresponding text template of a commonsense relation from ConceptNet. For every relation, we generate only one intention per pair of products due to the large amount of products and co-buy records. However, this is not restricted and can easily scale up. 3.5 Human-centric Role-aware Filtering To effectively manage a large amount of purchase intentions, quality control measures have become imperative. While previous works relied on human annotaions for this purpose, recent works (Zhou et al., 2024) show that co-buy intentions generated by LLMs, despite undergoing human filtering, still fail in capturing the customers’ mental states but 264rather focus on factual similarities of the products, as demonstrated in Figure 1. This phenomenon, refered to as “product-centric,” restricts the potential downstream applications of the generated intentions. To address both issues, inspired by recent works on theory-of-mind (Kosinski, 2023), we propose to incorporate a filtering module, powered by a LVLM, after the generation process. We instruct the LVLM to assume the role of an E-commerce customer and provide it with a generated intention as the objective in the customer’s mental state. Based on this intention, we present the LVLM with a pair of products and ask it to first determine whether the intention successfully motivates the purchase behavior and then generate a rationale to support its decision. This process simulates a real-world scenario where the LVLM functions as a customer, making purchase decisions. By filtering intentions that result in a positive response for purchasing, we obtain intentions that are “humancentric” in the sense that they satisfy the mental state of an agent that is aware of its role as a customer. We term this approach as human-centric role-aware filtering , which serves as an automatic filter to replace manual annotations. We apply this module to all the intentions we collected in previous steps and select the product-intention pairs that are accepted by the module as the final outcomes of our framework. Detailed prompts are provided in Appendix A. 4 Intrinsic Evaluations By applying MINDto products we collected from Amazon Reviews (Ni et al., 2019), we construct a multimodal intention knowledge base, with statistics shown in Table 1. In total, 1.26 million intentions are preserved after applying our proposed filtering module, spanning across 20 relations. Therefore, in this section, we first evaluate M INDintrinsically by examining the quality of the generated intentions and the effectiveness of our proposed filter module through human annotation. 4.1 Annotation Setup We hire human annotators from the Amazon Mechanical Turk platform to evaluate the generated intentions. For strict quality control, we only invite workers satisfying the following requirements: 1) at least 1K HITs approved, and 2) at least 95% approval rate. Then, we conduct two rounds of qualification rounds using a qualification question 4\nset crafted by authors of this paper, which includes both straightforward and tricky questions. Over 600 workers participated and only 90 (15%) of them are deemed qualified by achieving over 87% accuracy. For a generated intention, we task each worker to evaluate four aspects: •Plausibility refers to the degree to which an intention of a co-buy purchase appears correct and reasonable given both products. •Typicality evaluates how well the intention reflects a specific feature that causes the user behavior, which emphasizes on informativeness and causality (Yu et al., 2023). •Human-centric evaluates the extent to which the intention considers and aligns with the mental state and preferences of a human customer. •Filter rationale evaluates the correctness of the reasoning or justification provided by the filtering module for accepting or rejecting a productintention pair. For each aspect, we ask the annotators to rate them as a binary classification task. A random sample of 5,000 generated intentions are annotated, and the final vote is determined by the majority vote from three annotators. 4.2 Annotation Results The results of the annotations are presented in Table 1. The annotators achieved a pairwise agreement of 73.1% and a Fleiss’s κ(Fleiss, 1971) of 0.56, indicating satisfactory internal agreement. The results reveal that MINDeffectively generates purchase intentions that are both highly plausible (94% on average) and typical ( 90% on average) across all relations. This indicates the strong product understanding and intention reasoning capabilities of MIND. Additionally, our proposed humancentric role-aware filter correctly identifies 82% of intentions on average, with 80% of them having appropriate justifications for filtering. These high percentages further validate the effectiveness of our proposed method, which serves as a cost-efficient and highly reliable quality control measure, replacing the need for human annotations. 5 Experiments and Analyses In this section, we first study the downstream benefits brought by intentions generated by MIND. Then, we conduct in-depth analyses of the human-centric role-aware filter in MINDand compare MIND with previous intention acquisition 362Relation #Int. Pla. Typ. Fil. Rat.\nEffect 97,047 0.90 0.83 0.73 0.70\nMannerOf 50,563 0.93 0.89 0.83 0.82\nisA 62,069 0.94 0.88 0.82 0.80\nOther 545 0.94 0.90 0.79 0.75\nMadeOf 40,593 0.95 0.92 0.85 0.82\nSimilarTo 63,558 0.94 0.87 0.83 0.80\nUsedFor 52,383 0.94 0.88 0.81 0.79\nCan 90,392 0.95 0.91 0.82 0.78\nCauseDesire 95,097 0.94 0.90 0.82 0.80\nRelatedTo 64,152 0.93 0.89 0.81 0.79\nPartOf 81,230 0.92 0.87 0.79 0.77\nOpen 122,296 0.93 0.89 0.83 0.82\nCreatedBy 35,723 0.94 0.88 0.78 0.76\nDeriveFrom 60,347 0.95 0.89 0.80 0.77\nDefinedAs 51,680 0.96 0.92 0.84 0.84\nPropertyOf 57,947 0.97 0.90 0.83 0.82\nCapableOf 86,772 0.95 0.90 0.82 0.82\nCause 61,860 0.95 0.92 0.83 0.82\nSymbolOf 64,477 0.95 0.92 0.84 0.82\nDistinctFrom 27,710 0.94 0.89 0.84 0.83\nTotal 1,264,441 0.94 0.90 0.82 0.80\nTable 1: Statistics of the intention knowledge base con-\nstructed via M INDand human annotation results.\nmethod. 5.1 Evaluation Setup We explore the effectiveness of MINDon the IntentionQA benchmark (Ding et al., 2024), a comprehensive multiple-choice question answering dataset comprising two challenging subtasks that require language models to comprehend and utilize intentions in E-commerce scenarios accurately. The first task assesses LLMs’ capability in accurately inferring the intention given a co-buy product pair together with 3 distractors sampled from other product pairs, while the second task evaluates LLMs’ capability in utilizing purchase intention to make reasonable product recommendation by selecting the product that best aligns with the user’s intention from four choices. While existing results show that language models struggle with both tasks, we aim to examine whether MINDcan enhance LLMs’ intention understanding capabilities through finetuning. Specifically, from all intentions generated by MIND, we transform them into instructionfollowing format via natural language templates following Zhou et al. (2023). Then, we fine-tune LLAMA2-7B-chat (Jiang et al., 2023) and Mistral7B-Instruct-v0.2 (Touvron et al., 2023) on the retrieved data as a type of knowledge injection. They are then evaluated in a zero-shot manner by being prompted to select the most plausible choice for every QA pair in IntentionQA. Accuracy is used as the evaluation metric. 5\nMethods BackboneINTENTION UNDERSTANDING INTENTION UTILIZATION\nEasy Medium Hard Avg. Easy Medium Hard Avg.\nRandom - 25.00 25.00 25.00 25.00 25.00 25.00 25.00 25.00\nMajority Vote - 26.37 25.24 26.27 26.15 25.97 28.57 28.57 26.60\nPTLMRoBERTa-Large 214M 41.46 41.98 38.98 41.43 54.95 35.06 30.08 49.84\nDeBERTa-v3-Large 435M 36.40 38.72 37.62 36.90 26.52 29.35 32.33 27.39\nT5-v1.1-xxl 11B 24.84 25.47 25.42 24.99 26.71 26.23 25.56 26.55\nFlan-T5-xxl 11B 75.98 73.58 63.56 74.88 79.26 81.82 81.95 79.89\nT0-pp 11B 71.70 68.87 64.41 70.78 77.11 76.10 78.20 76.99\nCommonsenseHyKAS 435M 71.81 67.17 46.69 69.61 47.02 45.97 48.12 46.90\nCAR 435M 73.69 71.46 54.38 72.20 36.18 43.12 44.36 37.94\nCANDLE 435M 74.34 70.75 52.54 72.52 35.94 43.90 43.61 37.84\nVERA 11B 69.82 70.52 61.02 69.49 59.20 58.18 64.66 59.36\nVERA-CANDLE 11B 70.59 71.33 63.41 70.02 62.18 60.13 66.13 61.81\nOpen LLMLLAMA2-7B-chat 64.98 66.54 53.85 64.61 59.90 54.86 47.37 58.04\nLLAMA2-13B-chat 69.63 63.96 60.78 68.06 45.53 41.95 39.71 44.52\nGemma-7B-instruct 65.55 64.31 52.04 64.61 33.18 36.01 41.51 34.20\nMistral-7B-Instruct-v0.2 76.57 74.53 63.56 75.50 59.78 62.60 65.41 60.64\nMINDDistilledLLAMA2-7B-chat 65.78 64.61 55.75 66.15 59.43 57.13 60.03 59.04\nMistral-7B-Instruct-v0.2 78.57 74.31 80.89 76.97 61.14 65.42 62.16 62.02\nLLM APIChatGPT 75.06 73.76 68.64 74.48 80.74 76.62 68.42 79.23\nChatGPT (CoT) 76.07 74.53 63.56 75.12 78.89 75.32 78.20 78.21\nChatGPT (CoT-SC) 76.51 73.82 63.56 75.32 85.72 77.14 82.71 83.99\nGPT 4 78.12 75.41 66.10 76.97 86.03 82.34 84.96 85.30\nGPT 4 (CoT) 78.12 75.41 66.10 76.97 86.03 82.34 84.96 85.30\nGPT 4 (CoT-SC) 78.80 72.88 65.25 76.97 84.00 80.78 84.96 83.48\nHuman - 89.96 90.00 80.96 89.33 95.50 85.19 100.0 94.00\nTable 2: Evaluation results (Accuracy%) of various language models on both tasks of the IntentionQA benchmark.\n5.2 Baselines For both tasks, we first incorporate random and majority voting to reflect the characteristics of the benchmark. Five Pre-Trained Language Models (PTLMs) are included: RoBERTa (Liu et al., 2019), DeBERTa-v3 (He et al., 2023), T0 (Sanh et al., 2022), T5 (Raffel et al., 2020), and FlanT5 (Chung et al., 2022). Then, performances by five commonsense-injected PTLMs are also reported, including HyKAS (Ma et al., 2021), CAR (Wang et al., 2023a), VERA (Liu et al., 2023b), CANDLE (Wang et al., 2024), and VERACANDLE. We also report the performances of several LLMs, such as LLaMA2 (Touvron et al., 2023), Gemma (Mesnard et al., 2024), Mistral (Jiang et al., 2023), ChatGPT (OpenAI, 2022), and GPT-4 (OpenAI, 2023). For the latter two, we also adopt Chainof-Thought ( COT; Wei et al., 2022) and CoT with Self-Consistency ( COT-SC ; Wang et al., 2023b) prompting. Prompts are shown in Appendix A. 5.3 Results The results are presented in Table 2, demonstrating significant improvements in both tasks when LLMs 415are fine-tuned on intentions generated by MIND. For instance, LLAMA2 achieves accuracy gains of 1.54% and 1.00% for both tasks, respectively. Notably, Mistral yields a remarkable performance gain that even becomes comparable to GPT-4, despite having a significantly lower number of parameters. However, for the intention utilization task, while both fine-tuned LLMs show performance improvements, they still fall behind GPT-4. One potential reason for this gap could be the misalignment between the fine-tuning objective and the evaluated ability of the task, which involves generating intentions for a pair of products and selecting a product based on a given intention. Nevertheless, these results underscore the effectiveness and efficiency ofMIND in enhancing LLMs’ capabilities in Ecommerce intention comprehension and utilization. 5.4 Analyses In this section, we first study the role of our proposed human-centric filtering module by analyzing the error types of the discarded intentions and studying its ablation. Then, we conduct empirical and case comparisons to demonstrate the effective6\n/uni00000028/uni00000044/uni00000056/uni0000005c /uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050 /uni0000002b/uni00000044/uni00000055/uni00000047 /uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000005a/uni00000011/uni00000003/uni00000049/uni0000004c/uni0000004f/uni00000057/uni00000048/uni00000/uni0000005a/uni00000011/uni00000052/uni00000011/uni00000003/uni00000049/uni0000004c/uni0000004f/uni00000057/uni00000048/uni00000055Figure 3: Ablation results on IntentionQA tasks by\nMistral-7B distilled on intentions generated by MIND\nwith/without filtering.\nness of M INDover FolkScope. 5.4.1 Impact of Human-centric Filtering While human annotation results in Section 4.2 show that, after filtering, most of the remaining intentions are highly plausible and typical, we observe that only 46.7% generations passed our proposed filtering module as the last step of MIND. Thus, in this section, we first study the role of such humancentric filtering by looking into the causes of why the intentions get discarded, and further seek insights to resolve such a high filtering loss. To achieve this, we randomly sample 200 intentions that are abandoned by MINDduring the last step and manually annotate the reasons behind based on the rationale provided by the LVLM. Three types of errors are observed and they are categorized as: •81.0% of the filtered intentions, while plausible, do not provide strong enough evidence to motivate a LVLM agent to execute the purchase behavior for two products. For example, the intention “ they both are related to home audio systems” for purchasing a pair of audio adapters lacks customer interaction and solely focuses on the products themselves. A more appropriate intention, for example, “ they both are able to help in connecting audio devices ,” would retain a stronger bond between the products and customers by aligning with their functionalities. •13.0% of the intentions result from misjudgment by the LVLM, where the agent fails to make the correct decision despite the intention being sufficiently plausible and typical. This highlights the need for future improvements, including a more refined filter to enhance our framework. •6.0% of the intentions are discarded due to being implausible or containing factual errors that do not align with the products. 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 4: Relation-wise comparison of typicality scores\nacross all relations between M INDand FolkScope.\nOverall, 87% of intentions are being properly discarded, which is considerably high for an automatic filter without human supervision. 5.4.2 Ablation Study We then study the ablation of MINDby focusing on the role of our proposed human-centric roleaware filter mechanism in its impact toward quality control of the generated intentions. Specifically, we leverage the IntentionQA (Ding et al., 2024) as the evaluation benchmark and separately train two models on (1) intentions that are filtered by our proposed mechanism ( w. filter ) and (2) intentions without filtering ( w.o. filter ). All setups follow the same as described in Section 5.1, and we use Mistral-instruct-7B-v0.2 as the backbone and train it using a unified hyper-parameter setting. The results are plotted in Figure 3. We observe that, without filtering, performances on both tasks across all difficulty levels drop significantly, which is possibly due to the inclusion of more noisy intentions in the training data. This shows that our proposed filtering module is indeed functioning well in controlling high-quality intentions and is beneficial to downstream tasks. 5.4.3 Relation-wise Comparisons Against FolkScope We then compare MIND against FolkScope, the previous state-of-the-art method for large-scale intention acquisition, by analyzing the typicality distribution of intentions across all relations. Specifically, we adopt the same annotation protocols designed by Yu et al. (2023) and transfer our annotation results into a four-point Likert scale (Joshi et al., 2015). Then, for each relation, we compute the average typicality scores among all intentions and plot them for comparison, as shown in Fig7\nItem1 Item2 Intentions\nSamsung SmartCam HD Pro Samsung SmartThings Smart\nHome HubThey are designed to work together in a smart\nhome system\nThey are derived from the same category.\nClarks Women’s Ankle Bootie The Sak Kendra Hobo Shoul-\nder BagThe consumer is looking for a stylish and func-\ntional combination for their daily activities.\nThey both are a manner of ’Women’s Shoes’ and\n’Women’s Handbags’ respectively.\nWestern Party Mustaches Forum Novelties Adult\nCowboy Costume VestThey are both part of a costume or a themed party.\nThey both are a part of the ’Adult Costume’ cate-\ngory.\nColumbia Women’s Loveland\nShorty Omni-Heat Snow BootColumbia Sportswear\nWomen’s Thermarator GloveThey are designed to keep the wearer warm and\ncomfortable during cold weather conditions\nThey both are a part of the Columbia brand.\nBanded Arc Welded Waterproof\nBackpack PolyesterBanded Deluxe UFS Fleece\nFace MaskThey are both used for outdoor activities and pro-\ntection from harsh weather conditions.\nThey are both used for outdoor activities.\nNovelty Video Games Sarcastic\nMens Very Funny T ShirtSuper Mario All Over Charac-\nter 100 Score Print SocksThey both appeal to gamers and video game en-\nthusiasts.\nThey both are of high quality and are really cool.\nWomen’s Exaggerative Vintage\nLeafs Shape Necklace SetsPlated Oil Drip Rhinestone\nFlower Necklace Earring SetsThey both have a vintage-inspired design and fea-\nture colorful flowers and beads\nThey both have a property of ’High Quality’.\nXbox 360 4gb Kinect Bundle Controller Charger - Xbox 360 They both cater to the needs of Xbox 360 gamers.\nThey both are made of plastic.\nTable 3: Case studies of purchase intentions generated by MINDand FolkScope. Intentions generated by MINDare\nhighlighted in blue and those generated by FolkScope are marked in green .\nure 4. From the plot, we observe that intentions generated by MINDexhibit higher typicality scores across nearly all relations compared to those generated by FolkScope, which further demonstrates the superiority of M IND. 5.4.4 Case Studies Against FolkScope Aside from empirical analyses above, we further show the advantages of MIND over FolkScope through additional case studies to highlight key benefits of MIND. To this end, we randomly selected 7 pairs of co-buy products and compared the intentions generated by both frameworks, as shown in Table 3. Our findings from the table indicate thatMIND-generated intentions exhibit a stronger focus on the usage and functionalities that potentially fulfill customers’ needs and intentions when purchasing these products. Conversely, intentions generated by FolkScope tend to be biased towards properties and features that can be easily inferred from the product titles, which are of lesser interest to customers’ shopping intentions. Take the second row in Table 3 as an example. The intentions both are “Women’s Shoes” and “Women’s Handbags” generated by FolkScope merely represent an aggregation of the product categories for the two items. In contrast, MINDproduces intentions 537such as looking for stylish and functional combination for daily activities , which better captures a customer’s intention when shopping for both products. This example further reinforces our previous conclusions that MINDcan generate intentions that are more human-centric and better reflect the customers’ intentions as mental states. 6 Conclusions In this work, we present MIND, a multimodal distillation framework for enhancing E-commerce purchase understanding by automating the pipeline of intention generation and quality filtering via multiple-step instructions over LVLMs. By applying MINDto real-world E-commerce data, we construct the very first multimodal purchase intention knowledge base featuring over 1.2 million intentions. These intentions have been proven to be invaluable in distilling student models that exhibit improved performance in E-commerce intention comprehension and utilization tasks. Further analyses reveal the effectiveness of MINDby validating the proposed filtering mechanism and highlighting the strengths of MINDin comparison to FolkScope. Our work sheds light on improving large-scale Ecommerce intention acquisition and application. 8\nLimitations First, MINDgenerates intention by leveraging several zero-shot prompts without additional exemplars. This decision is made as we observe that few-shot prompts may “guide” LVLM to generate intentions that tend to be similar to the provided exemplars, which harms diversity. However, it remains an open question whether more advanced prompting methods (Song et al., 2023; Parnami and Lee, 2022) would help in the generation process. It’s also worth noting that the LVLM used in our work may be outdated as new products show up on E-commerce platforms. However, switching LLaVa to more up-to-date LVLMs, preferably pre-trained on E-commerce data, can address this concern. Finally, MINDutilizes an automatically functioning filter as quality control. While we have shown its effectiveness, it remains challenging to effectively regulate the filter mechanism to be either lenient or strict. Further investigation is required to provide insights into the alignment between the values of VLMs and the real world, enhancing our understanding of them. Ethics Statement To avoid generating harmful intentions and toxic filter rationales in MIND, we recruit 4 expert annotators who are graduate students specializing in multilmodality and natural language processing to evaluate the generated intentions and rationales. We ask all experts to go through 200 sampled data and no harmful contents are reported. The crowdsourced annotators are paid a wage that complies with the local law. The expert annotators involved in this research are knowledgeable about the annotation protocol and the intended utilization of their annotations. They are willingly to contribute without expecting any compensation. The training and evaluation datasets utilized in this study are publicly available, anonymized, and shared under open-access licenses for research purposes, adhering to their intended usage. Thus, we believe this paper does not yield any ethical issue. 604",
        "aspect_review": "The paper is well-organized and clearly articulates the motivation behind the MIND framework, its methodology, and the results. The use of visual aids like figures and tables effectively complements the textual descriptions, making complex concepts more accessible. The structure follows a logical flow from introduction to conclusion, ensuring readers can easily follow the progression of ideas.\n\nThe evaluation section, while comprehensive, could benefit from a broader set of benchmarks to further validate the efficacy of the MIND framework across different scenarios. While the results are promising, the paper could provide more detailed statistical analysis to strengthen the claims of improvement over existing methods. The analysis could delve deeper into the limitations of the proposed filtering mechanism, exploring potential biases and failure cases in more detail. The reliance on the Amazon Review dataset may limit the generalizability of the findings. Exploring additional datasets could enhance the robustness of the conclusions. Some sections of the paper are dense with technical jargon, which might be challenging for readers who are not deeply familiar with the field. The paper could include a more detailed comparison with a wider range of existing methods to highlight the unique advantages of MIND. The methodology, while innovative, could benefit from clearer explanations of certain components, such as the specific prompts used in the LVLMs.",
        "general_review": "The paper introduces a novel multimodal framework, MIND, that effectively addresses the limitations of existing methods in understanding purchase intentions in E-commerce. It leverages Large Vision-Language Models to incorporate both visual and textual data, enhancing the quality and human-centric nature of inferred intentions. The framework demonstrates strong technical depth through its innovative human-centric role-aware filtering mechanism, which reduces reliance on costly human annotations. Comprehensive experimental validation is provided through extensive evaluations and comparisons with previous methods, showcasing significant improvements in intention comprehension tasks. The construction of a large-scale multimodal intention knowledge base further highlights its practical impact, offering valuable resources for downstream applications in E-commerce.\n\n1. The paper lacks clarity in some sections, particularly in the explanation of the filtering mechanism, which could benefit from more detailed descriptions or examples. 2. There are minor details missing regarding the hyperparameters and implementation specifics, which may hinder reproducibility. 3. Some figures could be better formatted for clarity, as they contain dense information that might be difficult for readers to quickly interpret. 4. The availability of code is not clearly stated, which could be a minor hindrance for researchers looking to build upon this work."
    },
    "11": {
        "paper": "Foundational Autoraters:\nTaming Large Language Models for Better Automatic Evaluation\nAnonymous ACL submission\nAbstract AsLLM Sadvance, evaluating generated text reliably becomes more challenging due to the high costs of human evaluation. To make progress toward better LLM autoraters, we introduce FLAME , a family of Foundational Large Autorater ModEls.FLAME is trained on our large and diverse collection of nearly 100 quality assessment tasks comprising 5M+ human judgments, curated and standardized usingpublicly released human evaluations from previous research. FLAME signiﬁcantly improves generalization to a wide variety of heldout tasks, outperforming proprietary LLM S like GPT-4 and CLAUDE on many tasks. Additionally, we show that our FLAME multitask mixture can be further optimized for speciﬁc downstream applications, e.g., reward modeling evaluation, through a novel tail-patch ﬁnetuning technique. Notably, on REWARD BENCH , our model (86.7) is the top-performing generative model trained solely on permissively licensed data, outperforming both GPT-4-0125 (85.9) and GPT-4 O(84.7). Our analysis reveals that FLAME is signiﬁcantly less biased than popular LLM- AS-A-JUDGE models on the COBBLE Rcognitive bias benchmark, while effectively identifying high-quality responses for code generation. We release our FLAME data collection at this http URL. 1 Introduction The increasing power and versatility of large language models ( LLM S) bring with them a growing challenge: How can we reliably assess their long-form outputs? Recent research suggests a promising solution: these models themselves, after undergoing large-scale multitask instruction tuning, can generalize to follow new human instructions (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Chung et al., 2024), making them suitable for use as autoraters of model outputs. This is particularly appealing because human evaluation, 042though crucial for assessing model performance, is limited by subjectivity (Krishna et al., 2023a), variability among raters (Karpinska et al., 2021), and the high costs of extensive evaluations (Min et al., 2023; Vu et al., 2023; Wei et al., 2024). To align LLM autoraters with human preferences, training on human judgments is crucial (Ouyang et al., 2022). However, obtaining these judgments is costly and time-consuming. Collecting existing human evaluations from previous research seems promising but faces challenges like lack of standardization, diverse evaluation criteria, inadequate documentation, and data privacy or proprietary concerns. Using model outputs for autorater training offers consistency (Jiang et al., 2023; Kim et al., 2024) but comes with risks, such as reinforcing biases and hallucinations (Gudibande et al., 2023; Muennighoff et al., 2024). Additionally, it may violate terms of use for proprietary LLM services, which prohibit using their models’ outputs to develop competing models.1To address these limitations, we curated and standardized human evaluations from prior research to create FLAME , a collection of approximately 100 quality assessment tasks comprising 5M+ total human judgments (§3). FLAME spans a wide variety of task types, from assessing machine translation quality to evaluating how well AIassistants follow user instructions. We hypothesized that training on this large and diverse data collection would enable LLM autoraters to learn robust, generalized patterns of human judgment, minimizing the impact of noisy or low-quality human judgments. To ensure transparency and reproducibility, we use only publicly available human evaluation data with permissive licenses from previous studies (§3.1). To overcome challenges in collecting such data, which rarely adhere to a particular standard 1https://openai.com/policies/terms-of-use,\nhttps://policies.google.com/terms/generative-ai\n1\nand often lack documentation, we thoroughly examined the associated research (§3.3) and consulted with the original authors to address ambiguities or inconsistencies (spending 3+ hours per dataset). We train LLM autoraters using supervised, multitask ﬁne-tuning on our data collection. All tasks are formulated into a uniﬁed text-to-text format with manually crafted task deﬁnitions and evaluation instructions. We format examples as inputtarget pairs, where the input includes task-speciﬁc context and the target contains human evaluations (Figure 1). This approach facilitates effective transfer learning across tasks, allowing our models to interpret and respond uniformly. Additionally, our task format is simple, intuitive, and easily accommodates new tasks. We demonstrate that training an instructiontuned LLM, i.e., PALM-2 24B (Anil et al., 2023), on our FLAME collection signiﬁcantly improves its performance on various quality assessment tasks, outperforming models such as GPT-4 ,CLAUDE , and LLAMA -3on many held-out tasks. Additionally, we show that our FLAME multitask mixture can be further optimized for speciﬁc downstream applications, using reward modeling evaluation as a case study. Speciﬁcally, we employ a novel tail-patch ﬁne-tuning technique to analyze how each dataset impacts performance on targeted distributions, i.e., REWARD BENCH (Lambert et al., 2024), allowing us to determine the optimal proportions of individual datasets in our multitask training mixture. Notably, our targeted variant FLAME -RMachieves an average accuracy of 86.7 on REWARD BENCH , surpassing both GPT-4-0125 (85.9) and GPT-4 O(84.7), and achieving the highest performance among generative models trained on permissively licensed datasets. Overall, our models outperform popular LLM- AS-A-JUDGE models on 6 out of 12 autorater evaluation benchmarks, covering 53 tasks (§4.3). Motivated by these results, we further explore whether biases exist in our autoraters, a common criticism of LLM- AS-A-JUDGE autoraters (§5.1), and their potential utility for AIdevelopment, particularly in identifying high-quality model responses (§5.2). Our analysis reveals that our models are signiﬁcantly less biased than popular LLM- AS-A-JUDGE models on the COBBLE Rcognitive bias benchmark (Koo et al., 2023), while effectively identifying high-quality responses for code generation. In summary, our main contributions are: (1) A curated collection of approximately 100 diverse quality assessment tasks with 5M+ human judg- 132ments, available at this http URL; (2) Our LLM autoraters, which outperform all proprietary LLMAS-A-JUDGE models like GPT-4 and CLAUDE on 6 out of 12 benchmarks, including REWARD BENCH and LLM-A GGRE FACT; and (3) A novel tail-patch ﬁne-tuning strategy for optimizing task mixtures to speciﬁc objectives. Our work demonstrates the potential of accessible AIsolutions, which we hope will spur more fundamental research into reusable human evaluations and the development of effective and efﬁcient LLM autoraters. 2 Related work Below, we discuss existing literature in the space of autoraters, drawing connections to FLAME . Task-speciﬁc autoraters: In the pre- LLM era, several works relied on token embedding similarities (Zhang et al., 2020) or log probabilities (Yuan et al., 2021) from pretrained models like BERT (Devlin et al., 2019) for automatic text evaluation. Other work ﬁne-tuned models on human ratings to create autoraters for speciﬁc tasks, including machine translation (Sellam et al., 2020; Thompson and Post, 2020; Rei et al., 2020; Fernandes et al., 2023; Qin et al., 2023), text summarization (Gao et al., 2019; Durmus et al., 2020; Deutsch et al., 2021), and question answering (Chen et al., 2020; Lin et al., 2022). FLAME , unlike these task-speciﬁc autoraters, is trained on various quality assessment tasks and can be prompted at inference time to perform new tasks. LLM- AS-A-JUDGE autoraters: With the advent of instruction tuned LLM Slike GPT-4 , recent work has used these models as judges (Fu et al., 2023; Gong and Mao, 2023; Bai et al., 2023) to evaluate LLM capabilities on various benchmarks, including ALPACA EVAL(Li et al., 2023c; Dubois et al., 2024), MT-B ENCH (Zheng et al., 2023a), and WILDBENCH (Lin et al., 2024). However, LLM- ASA-JUDGE autoraters tend to favor their own generated responses (Panickssery et al., 2024; Liu et al., 2023a; Bai et al., 2023), exhibiting “cognitive” biases toward aspects like length, order, and entity preference (Koo et al., 2023). In contrast, our models are trained on a large, diverse collection of human evaluations, allowing them to learn unbiased, generalized patterns of human judgment (§5.1). Unlike LLM- AS-A-JUDGE autoraters, our models are not tasked with evaluating their own responses, preventing self-preference bias. 2\nINS TRUCTI ONS: \n\"\"\"Task definition and evaluation \ninstructions.\"\"\" \nC ONTEXT: \n\"\"\"Input fields for context, each \nstarting with a label indicating its \ntype or purpose and is separated by \na newline. E.g., \n'article': <article> \n'summary': <summary> \n\"\"\" \nEV AL UA TI ON: \n\"\"\"Target fields. Each field starts with \na label indicating its type or purpose \nand is separated by a newline. E.g., \n'choice': <choice> \n'explanation': <explanation> \n\"\"\"\nINSTRUCTIONS: \ntitle: Is all of the information in the summary fully attributable to the source article? \ndescription:  In this task, you will be shown a summary and a source news article on which \nthe summary is based. Your task is to evaluate whether the summary is attributable to the \nsource article. Answer 'Yes' if all the information in the summary is fully supported by the \nsource article, or 'No' if any information in the summary is not supported by the source \narticle. Provide an explanation for your answer. \noutput_fields: answer, explanation \nCONTEXT: \narticle:  Tower Hamlets Council said it would sell Draped Seated Woman after \n\"unprecedented\" budget cuts. The work has not yet been valued but a Moore sold for £17m \nearlier this year. The council said the rising threat of metal theft and vandalism made \nit too expensive to insure if it was on show. The sculpture was bought by the former \nLondon County Council for £6,000 in 1960. The bronze sculpture, nicknamed Old Flo, was \ninstalled on the Stifford council estate in 1962 but was vandalised and moved to the \nYorkshire Sculpture Park in 1997. A council spokesperson said: \"With unprecedented cuts \nto council budgets, the council finds itself in a difficult situation and being forced to \nmake hard decisions.\" \nsummary:  A Moore sculpture of a woman sitting on a concrete plinth is to be sold. \n----- \nEVALUATION: \nanswer:  No\nexplanation: The detail that the woman is \"sitting on a concrete plinth\" is not in the \narticle. \nFigure 1: All of our quality assessment tasks are formulated into a uniﬁed text-to-text format with manually crafted\ntask deﬁnitions and evaluation instructions. We format examples as input-target pairs, where the input includes\ntask-speciﬁc context and the target contains human evaluations.\nGeneral-purpose LLM autoraters: Recent work has explored training general-purpose LLM autoraters. Jiang et al. (2023) introduced TIGERSCORE , aLLAMA -2model trained on GPT-4 generated error analysis data across various tasks, including summarization and long-form QA. Similar approaches include PROMETHEUS (Kim et al., 2023), INSTRUCT SCORE (Xu et al., 2023b), and PROMETHEUS 2(Kim et al., 2024). Unlike these efforts, our approach relies solely on open-source human evaluations instead of model outputs. We show that FLAME signiﬁcantly outperforms PROMETHEUS -2on REWARD BENCH (Table 2). Reward models: Our work relates to reward models ( RM S) used for aligning LLM Sto human preferences via reinforcement learning with human feedback ( RLHF , Ouyang et al., 2022; Korbak et al., 2023). In RLHF , human preference data is either used to train stand-alone discriminative RM S, or directly fed into LLM Svia algorithms like DPO (Rafailov et al., 2023) or SLIC-HF (Zhao et al., 2023). While we evaluate our models as RM Sin our REWARD BENCH experiments (§4), there are key distinctions: (1) RM Sprimarily train on pairwise preference data, whereas our models utilize diverse task types in a uniﬁed format; (2) RM Soptimize for overall preference, while our models can be prompted to judge speciﬁc aspects (e.g., safety). 3 The FLAME collection At a high level, we ﬁne-tune instruction-tuned LLM Son our multitask mixture of standardized human evaluations. This large and diverse data collection is carefully selected to cover a wide range of Classify\n37.1%\nFree-form\n13.9%Pointwise\n10.9%\nPairwise\n38.1%Figure 2: A breakdown of our FLAME collection by\ntask type, with each slice representing the % of data-\npoints (out of 5M+) for that speciﬁc task type.\nLLM capabilities (§3.1-3.2). We manually crafted task deﬁnitions and evaluation instructions, reformulating all tasks into a uniﬁed text-to-text format (§3.3). We train two LLM autorater variants: one with example-proportional mixture weights (FLAME ), and the other with reward modeling optimized mixture weights ( FLAME -RM), determined using a tail-patch ﬁne-tuning strategy (§3.4). 3.1 Principles for training data selection We adhere to the following principles while choosing our datasets: Public, open-source datasets: To ensure reproducibility, we use only permissively licensed datasets available on HUGGING FACE(Lhoest et al., 2021) or the original authors’ GITHUBrepositories. Human-labeled annotations: We exclusively use datasets with human-labeled annotations, avoiding those generated by models like GPT-4 due to potential inaccuracies and legal concerns raised in recent research (Gudibande et al., 2023; Muennighoff et al., 2024). 3\nVarious task types: We gather datasets across various task types to train FLAME to generalize to new quality assessment tasks. These include pointwise evaluations (e.g., “Rate coherence on a Likert scale of 1-5. ” ), pairwise evaluations (e.g., “Which response is better, A or B?” ), classiﬁcation tasks (e.g., “Is the claim supported by the document?” ), and free-form explanation tasks (e.g., “Which response is better? Explain your judgment. ” ). See Figure 2 for a breakdown. Various LLM capabilities: We choose datasets from literature that assess diverse capabilities in modern LLM S, such as factuality, instruction following, long-form generation quality, math, coding, safety, etc. See §3.2. 3.2 Capabilities covered by FLAME mixture Following the principles outlined in §3.1, we curated a large collection of 5M+ datapoints, composed of 972training tasks (see our list of datasets in Appendix A.1). Our data collection assesses key modern LLM capabilities, detailed below (see breakdown in Figure 3): General response quality: To evaluate LLM response quality, we use a variety of datasets that measure helpfulness, coherence, creativity, and ﬂuency. These include pairwise comparison datasets like STANFORD SHP (Ethayarajh et al., 2022) and LMSYS (Zheng et al., 2023b), and pointwise rating datasets such as SUMMA EVAL(Fabbri et al., 2021). Additionally, to measure LLM instructionfollowing capabilities, we include datasets like GENIE(Khashabi et al., 2021), INSTRU SUM(Liu et al., 2023b), and RISUM(Skopek et al., 2023). Attribution / Factuality: To address the increasing importance of measuring hallucinations in generated LLM responses, we incorporate several datasets that assess attribution or grounding, measuring whether claims or responses are supported by source documents. These include summarization evaluation (Pagnoni et al., 2021), LLM response hallucination (Li et al., 2023a), fact veriﬁcation (Schuster et al., 2021), dialog faithfulness (Dziri et al., 2022a), and natural language inference ( NLI) (Williams et al., 2018).3Mathematical reasoning: We construct datasets to help FLAME differentiate between correct and incorrect solutions to mathematical problems. We leverage PRM800K (Lightman et al., 2024) and 2An additional 53 tasks were kept for evaluation, see §4.1.\n3We include NLIsince its setup naturally ﬁts attribution.\nQuality\n36.8%\nSafety\n11.0%Math\n3.4%\nCoding\n10.4%\nIF\n7.1%\nFactuality\n31.3%Figure 3: A breakdown of our FLAME collection by\ncapability, with each slice representing the % of data-\npoints (out of 5M+) for that speciﬁc capability.\nextract human vs incorrect LLM-generated solutions, as well as pairs of (correct, incorrect) LLMgenerated solutions. Coding: In addition to natural language evaluation, we also train FLAME to perform code evaluation. We utilize COMMIT PACK(Muennighoff et al., 2024), CODE CONTESTS (Li et al., 2022a), and COFFEE(Moon et al., 2023) to construct pairs of (correct, buggy) programs in popular programming languages in response to a coding prompt or GITHUB issue. The model is trained to select the correct program in each pair. Safety: Developing safe and harmless AIassistants for broad public use is increasingly important. To facilitate safety evaluation, we train FLAME to identify unsafe LLM responses. Our training data includes both pairwise and classiﬁcation tasks from sources like BEAVER TAILS(Ji et al., 2023) and HELPFUL HARMLESS RLHF (Bai et al., 2022). 3.3 Uniﬁed FLAME prompt format Having carefully selected our training datasets (§3.1-3.2), we then convert them into a uniﬁed textto-text format. This involves preprocessing each dataset, which usually requires about 3-4 hours of manual work per dataset. First, we gather all relevant data ﬁles from the associated HUGGING FACE orGITHUBrepository. Then, we pinpoint and extract the speciﬁc data columns containing the quality assessments conducted by human annotators. Next, we meticulously craft detailed task deﬁnitions and evaluation instructions for each quality assessment task, ensuring consistency and standardization. We leverage any available instructions provided to the original human annotators to maintain alignment with their evaluation criteria. These instructions guide the model in identifying the input and output format, as well as understanding the speciﬁc aspects it should assess. Finally, all 4\n20000 40000 60000 80Training steps (FLAME)65.067.570.072.575.077.580.082.585.0RewardBench Safety performance\nFLAME\nFLAME-RM400 800 1200 1600 2000 2400 2800Training steps (FLAME-RM)Figure 4: FLAME -RMsigniﬁcantly outperforms FLAME\ninREWARD BENCH safety performance, using 20x less\ncompute, with improved stability, and a 2.5% perfor-\nmance gain.\ntasks are formulated as text-to-text tasks (Figure 1). Task deﬁnitions and evaluation instructions and the list of desired output ﬁelds are placed under the INSTRUCTIONS block, while the input ﬁeld values are placed under CONTEXT . This ﬂexible text-to-text format can be easily adapted to various quality assessment tasks. 3.4 Optimizing FLAME for reward modeling evaluation ( FLAME -RM) While our vanilla FLAME mixture is effective across many tasks (§4.3), it struggles with specialized tasks like reward modeling evaluation, showing suboptimal and unstable performance across checkpoints. We attribute this instability to suboptimal mixture weights that undersample useful tasks. To address this, we introduce a novel tail-patch ablation strategy, enabling us to efﬁciently optimize nearly 100 hyperparameters. Using REWARD BENCH as a case study, our reward-modeling optimized mixture ( FLAME -RM) achieves a 2.1% performance increase with 20\u0002less compute and signiﬁcantly improved stability across checkpoints (Figure 4). Vanilla mixture weights (“ FLAME ”): Our vanilla FLAME mixture assigns weights based on the number of examples per task, capped at a maximum of 216to avoid oversampling large tasks. However, as shown in Figure 4, this approach results in unstable performance for REWARD BENCH . Tail-patch ablations to determine task usefulness: Setting the right proportion of each individual task in our mixture is non-trivial due to the nearly 100 hyperparameters. Instead, we examine the usefulness of each individual training task, and use this information for weight assignment. First, 356we select a checkpoint partially4trained on our vanilla mixture which has fair (but not optimal) performance across REWARD BENCH categories. Next, we ﬁne-tune it exclusively on an individual task for only 3K steps (“tail patch”). We posit that training on a useful task would bridge the gap between fair and optimal performance. A re-weighted mixture based on tail-patch ablations (“ FLAME -RM”): After training a tail-patch on each task, we rate how helpful each training task is to each category of REWARD BENCH using one of four ratings: Helpful (+2, performance significantly improves and remains stable), Somewhat helpful (+1, performance slightly improves), No clear effect (0, performance is nearly unchanged), Harmful (-1, performance is signiﬁcantly worse). We then organize tasks into seven bundles: Generally helpful (tasks with total highest ratings >= 5), Category-speciﬁc , one for each of the ﬁve REWARD BENCH categories (most beneﬁcial tasks for a speciﬁc category where performance crosses a threshold\u001c),5Others with a ﬁxed mixing weights for each bundles: wgeneral = 100K,wspecific = 30K,wothers = 3K, respectively.6The ﬁnal weight of each task equals the total of mixing weights from the groups it belongs to. For instance, if a task is generally helpful and is helpfulforCHATHARDand SAFETY , then it contributes wt=wgeneral +2\u0003wspecific to our mixture. Since SAFETY is the most unstable category (Figure 4), we setwt= 250Kfor the top 3 SAFETY tasks. FLAME RMis built on top of the initial instruction-tuned checkpoint and ﬁne-tuned with the re-weighted mixture for only 3K steps. 3.5 Training Details We initialize our model with the PALM-2 24B model (Anil et al., 2023), instruction tuned on the FLANcollection (Chung et al., 2024; Longpre et al., 2023). We optimize our FLAME for a total of 60K steps, while our FLAME -RMrequires just 3K steps to achieve strong performance. All our models are trained using the T5Xlibrary (Roberts et al., 2023), with a learning rate of 0.0001 using the Adam optimizer (Kingma and Ba, 2015), batch size of 8, and 4We hypothesize that starting from a partially trained\ncheckpoint rather than the initial checkpoint is better for tail-\npatch ablations, since the model has already seen some multi-\ntask data and is familiar with its general distribution.\n5\u001c= 95%;66%;84%;99:8%;85% for CHAT,CHATHARD,\nCODE,MATH, and SAFETY , respectively.\n6We note that we did not tune these weight values, all\nnumbers were set once based primarily on our intuition.\n5\ndropout rate on 0.05. We use an input length of 2048 tokens, and target length of 1024 tokens. All models are trained on 128 CLOUD TPU V5Echips.74 Experimental Results Having discussed the procedure used to build FLAME and FLAME -RMin §3, we now present our main experiments. We compare FLAME to several popular baseline LLM-as-judge models (§4.2) on an evaluation suite composed of 12 benchmarks and 53 tasks (§4.1). Overall, we ﬁnd that FLAME outperforms proprietary LLM Slike GPT-4 ,CLAUDE on several tasks (§4.3), despite being trained only on permissively licensed publicly available data. 4.1 Evaluation Datasets We evaluate FLAME on a total of 12 benchmarks (composed of 53 tasks) to measure its performance as a pairwise and pointwise autorater: RewardBench (Lambert et al., 2024) is a popular leaderboard for evaluating reward models used for RLHF. REWARD BENCH contains a suite of pairwise preference tasks, where reward models need to choose the better response among two responses to a prompt. REWARD BENCH is composed of four categories spanning many desired capabilities in LLM S (Chat, Chat-Hard, Reasoning - Math + Coding, Safety), and is built using 23individual datasets.8LLM-AggreFact (Tang et al., 2024) is a benchmark measure the attribution / grounding capabilities of autoraters. Given a reference document and a claim, the AutoRater must determine whether the claim is fully supported in the reference document. Tang et al. (2024) combine tenattribution datasets from recent works in LLM factuality, building a holistic benchmark for attribution evaluation. Other pairwise evaluation tasks : Besides RewardBench, we use several pairwise preference tasks to evaluate FLAME .None of these datasets were used while training FLAME , so these tasks represent a true held-out setting. Our preference tasks consist of: (1) AlpacaFarm (Dubois et al., 2023); (2) RankGen (Krishna et al., 2022); (3) Contrastive Search (Su and Xu, 2022); (4) Machine Translation in literary settings, or LitMT (Karpinska and Iyyer, 2023); (5) Helpful, Honest and Harmless Alignment, or HHH-Align (Askell et al., 7cloud.google.com/tpu/docs/v5e-training\n8We exclude the “Prior sets” of REWARD BENCH in our\nevaluation, since we used 3 of the 4 datasets in training FLAME .2021); (6) CoPoem (Chakrabarty et al., 2022); (7) Expert-LFQA (Xu et al., 2023a). Other pointwise evaluation tasks : Additionally, we evaluate FLAME on several tasks needing Likertscale evaluations. These include pointwise human ratings from: (1) HelpSteer (Wang et al., 2023);9(2) Dipper paraphrase pointwise quality evaluation (Krishna et al., 2023b); (3) Pointwise Summarization Feedback (Stiennon et al., 2020).104.2 Evaluated Models As baselines we evaluate several popular LLMas-a-judge models from prior work, including LLAMA-3-70B-I NSTRUCT (Meta, 2024), MIXTRAL 8X7B(Jiang et al., 2024), CLAUDE 3 O PUS(Anthropic, 2024), GPT-3.5- TURBO -0125 (OpenAI, 2024a), GPT4-0125 , and OpenAI’s current ﬂagship model GPT4O(OpenAI, 2024b).11We also compare against a few additional models reported on the REWARD BENCH leaderboard (Lambert et al., 2024), notably GEMINI -1.5(Reid et al., 2024), and PROMETHEUS -28X7B(Kim et al., 2024). Among our models , we evaluate PALM-2 24B models ﬁnetuned on FLAME and FLAME -RM(§3.4-3.5). To disentangle the effect of pretraining and FLAME training, we evaluate our initialization checkpoint ( PALM-2-24B ) from §3.5 which has not seen FLAME data. 4.3 Main Results We present results across all tasks in Table 1, and REWARD BENCH in Table 2. Overall, we ﬁnd that: FLAME variants outperform all baseline on 6 out of 12 benchmarks . In Table 1 we ﬁnd that despite being trained only on public data, FLAME shows strong performance in a wide variety of pairwise and pointwise tasks. Notably, it outperforms all proprietary state-of-the-art LLM Son six out of twelve tasks. This includes the LLM-Aggfact benchmark (81.2 vs next best 80.6, GPT-4-0125 ), conﬁrming its utility as a cheap and effective attribution evaluator. However, FLAME notably lags behind GPT-4-0125 on ExpertQA (73.4 vs 77.0). We hypothesize this is due to the lack of expert technical knowledge in the much smaller FLAME ’s pa9We leverage the ﬁve tasks in the validation split during\nthis evaluation, and use the train split in our FLAME mixture.\n10We leveraged only the pairwise ratings from this dataset\nduring training, and left pointwise for evaluation.\n11For comparable experiments with FLAME , we use the\nsame uniﬁed prompt instructions (§3.3) while evaluating each\nLLM-as-a-judge baseline model. We use the default decoding\nhyperparameters for each API suggested by the API provider.\n6\nModel LLM RewardPointwise Tasks Pairwise Tasks\nAggfact Bench H-Steer Dipper SumFB Alpaca RankG ContS LitMT HHH Copoem EQA\nLLAMA-3-70b-Inst 76.1 76.0 39.7 42.8 50.8 53.9 65.6 53.1 60.5 91.9 53.6 71.1\nMixtral-8x7b 73.8 77.8 34.0 42.2 43.8 55.1 63.3 56.6 61.7 90.0 52.9 71.5\nGPT-3.5-turbo 70.0 64.5 32.0 45.0 15.6 55.5 58.2 57.5 54.3 85.5 49.0 69.9\nClaude Opus 79.2 80.7 41.3 50.6 31.6 49.6 55.1 45.1 71.1 94.6 49.0 71.1\nGPT-4-0125 80.6 85.9 40.8 45.0 46.5 49.6 62.5 55.8 67.6 94.6 56.9 77.0\nGPT-4o 80.2 84.7 40.1 45.6 30.9 50.4 66.3 57.5 72.7 92.3 55.6 75.0\n(our 24b models)\nPaLM-2-24b 54.8 62.9 20.0 48.3 13.3 52.3 58.2 46.0 62.5 85.5 54.2 70.3\nFLAME -24b 80.4 84.6 52.2 42.8 42.2 56.3 65.6 58.4 64.1 88.2 54.2 68.4\nFLAME -RM-24b 81.2 86.7 24.2 50.6 50.4 54.7 61.7 48.7 69.9 90.0 53.6 73.4\nTable 1: Performance of FLAME compared to LLM- AS-A-JUDGE baselines on a wide variety of quality assess-\nment tasks. Overall, we ﬁnd that FLAME outperforms all proprietary LLM- AS-A-JUDGE baselines in 6 out of 12\nbenchmarks, including LLM-A GGRE FACTand REWARD BENCH . See §4.1 for the source of each evaluation dataset.\nModel Avg Chat Hard Safe Reason\n(generative baselines on RewardBench leaderboard)\nGPT-3.5-turbo 64.5 92.2 44.5 62.3 59.1\nPrometheus-2 75.3 93.0 47.1 83.5 77.4\nLlama3-70B 76.0 97.6 58.9 69.2 78.5\nMixtral-8x7b 77.8 95.0 64.0 73.4 78.7\nClaude-Opus 80.7 94.7 60.3 89.1 78.7\nGem1.5-Flash 82.1 92.2 63.5 87.7 85.1\nGPT-4o 84.7 96.6 70.4 86.7 84.9\nGPT-4-0125 85.9 95.3 74.3 87.2 86.9\nGem1.5-Pro 88.1 92.3 80.6 87.5 92.0\n(our 24b models)\nPALM-2-24B 62.9 89.9 61.2 55.3 45.2\nFLAME 84.6 94.4 69.1 80.7 94.1\nFLAME -RM 86.7 94.7 71.7 85.7 94.8\nTable 2: A comparison of FLAME with other genera-\ntive reward models (“LLM-as-judges”) on the REWARD -\nBENCH benchmark. FLAME outperforms all generative\nmodels on REWARD BENCH except proprietary Gemini\n1.5 Pro, despite being trained only on public data.\nrameters, which is necessary to evaluate ExpertQA answers. Surprisingly, we also ﬁnd GPT-4-0125 generally outperforms GPT-4 Oon quality assessment tasks.12Finally, we note that FLAME variants outperform our initialization checkpoint ( PALM-2 24B ) on both tasks, showcasing the utility of FLAME ﬁne-tuning. Our FLAME -RMvariant outperforms GPT-4 on RewardBench . In Table 2, we ﬁnd that on average FLAME -RMoutperforms several proprietary LLMas-judge generative baselines on REWARD BENCH , including GPT-4-0125 (86.7 vs 85.9). This is due to a notable performance increase in the “Reasoning” split of the REWARD BENCH benchmark, with 12This comes as a surprise as GPT-4 Ois ranked higher than\nGPT-4-0125 on the LMSys leaderboard (Chiang et al., 2024).\nOur results corroborate to the REWARD BENCH leaderboard,\nwhere GPT-4 Ois ranked behind GPT-4-0125.competitive performance in other splits. Moreover, FLAME -RMoutperforms the much larger and opensource LLAMA-3-70B on every split of REWARD BENCH (86.7 vs 76.0) on average. Even our vanilla FLAME variant, without tail-patch optimization (§3.4), shows strong REWARD BENCH performance (84.6), outperforming models like Claude-Opus (80.7) and Gemini 1.5 Flash (82.1).135 Further analysis of FLAME In this section, we provide an analysis to elucidate some interesting aspects of our models. We depart from the traditional focus on analyzing the effect of factors like model size, data size, and data quality within multitask learning, which have been extensively studied in recent work on multitask/instruction learning (Raffel et al., 2020; Longpre et al., 2023). Instead, we explore the biases inherent in these autoraters, and demonstrate their potential utility for AIdevelopment, such as sampling highquality responses. 5.1 Autorater Bias Analysis A common criticism of LLM- AS-A-JUDGE autoraters is bias towards certain judgments (Liu et al., 2023a; Panickssery et al., 2024). In this section, we evaluateFLAME on the CoBBLEr benchmark (Koo et al., 2023), and ﬁnd that FLAME is signiﬁcantly less biased than alternatives. This benchmark measures six kinds of biases in autorater models: (1) ORDER: does the autorater have a preference towards the response position? (2) COMPASSION : does the autorater’s judgment change if the response13We present some additional analysis of length bias and\nhill-climbing issues in REWARD BENCH in Appendix B. We\nencourage readers and future work to not over-index on RE-\nWARD BENCH performance, and instead consider holistic im-\nprovements across a variety of evaluation tasks (§4.1).\n7\nAutorater Avg ( #) Order ( #) Compass. ( #) Length ( #) Egocentric ( #) Bandwagon ( #) Attention ( #)\nRandom 0.30 0.50 0.50 0.00 0.25 0.25 0.25\n(baselines as reported in Koo et al., 2023)\nFalcon 0.31 0.77 0.27 0.09 0.05 0.28 0.40\nCohere 0.41 0.50 0.65 0.10 0.27 0.82 0.14\nLLAMA2-70B 0.19 0.61 0.26 0.12 0.06 0.04 0.03\nInstructGPT 0.45 0.38 0.48 0.16 0.28 0.85 0.54\nChatGPT 0.45 0.41 0.66 0.13 0.58 0.86 0.06\nGPT-4 0.31 0.23 0.79 0.06 0.78 0.00 0.00\n(our models)\nFLAME -RM 0.15 0.14 0.20 0.03 0.35 0.20 0.00\nFLAME 0.11 0.08 0.12 0.00 0.35 0.10 0.00\nTable 3: Autorater bias analysis on the CoBBLEr benchmark from Koo et al. (2023). For all columns, lower\nis better / less biased . Overall, we ﬁnd that FLAME is signiﬁcantly less biased than popular LLM-as-a-judge\nmodels like GPT-4 and ChatGPT. Compared to Table 2 in Koo et al. (2023), we combine ﬁrst/last numbers for\nOrder/Compassion, report jbias\u00000:5jfor Length, and only report the order variant in Egocentric.\n.\nRanker CodeGen16B davinci002 InCoder6B\n(10 samples reranked in round-robin fashion)\nNone 21.2 17.6 14.6\nFLAME 31.7 22.0 18.9\nFLAME -RM 32.9 25.0 18.9\nOracle 46.9 63.4 29.3\nTable 4: Pass@1 performance on the HumanEval\ncoding benchmark. Across models, ranking 10 sam-\nples with FLAME improves pass@1 performance, with\nFLAME -RMoutperforming FLAME .\ngenerating LLM’s name is used instead of aliases? (3)LENGTH : does the autorater have a preference for longer or shorter outputs? (4) EGOCENTRIC : does the autorater have a preference for outputs generated by itself? (5) BANDWAGON : does the autorater get swayed by sentences like “90% people prefer response A” ? (6) ATTENTION : does the autorater get distracted by irrelevant sentences about responses, such as “Response A is about cats. ” ? We leverage the original prompt/response pairs from Koo et al. (2023), adapting them to use the uniﬁed FLAME format (Figure 1). We compare FLAME ’s bias to other LLM-as-judges reported in Koo et al. (2023), including GPT-4 . In Table 3, we ﬁnd that FLAME is signiﬁcantly less biased than GPT-4 and other autoraters reported in Koo et al. (2023), with an average bias of just 0.12 compared to 0.31 in GPT-4 (lower is better). FLAME outperforms GPT-4 in 5 out of 6 bias categories, further supporting its utility as a robust and unbiased autorater. 5.2 Using FLAME to re-rank decoded outputs A possible application of autoraters is selecting the best output among a pool of responses (Nakano et al., 2021; Krishna et al., 2022), a technique pop- 557ularly known as “Best-of-N” sampling. In this section, we show that ranking LLM-generated code samples with FLAME leads to performance improvements. We utilize the popular HumanEval Python programming benchmark (Chen et al., 2021) for our experiments. We re-rank 10 samples generated by OpenAI davinci-002, InCoder6B (Fried et al., 2023), and CodeGen-16B (Nijkamp et al., 2023) using a round-robin competition, and measure the performance of the topranked sample.14In Table 4, we ﬁnd that, we can signiﬁcantly improve pass@1 accuracy by ranking 10 output samples for all three codegeneration models. On CodeGen16B, FLAME RMimproves pass@1 from 21.2 to 32.9, bridging nearly half the gap to the Oracle ranker (46.9). 6 Conclusion We introduce FLAME , a family of foundational autorater models that can perform various quality assessment tasks. FLAME is trained on a large and diverse collection of curated and standardized human evaluations derived exclusively from permissively licensed datasets. We demonstrate FLAME ’s strong zero-shot generalization abilities, outperforming proprietary models like GPT-4 ,CLAUDE on many held-out tasks. Additionally, we present a novel mixture weight tuning approach that dramatically improves effectiveness and efﬁciency on reward modeling. FLAME is the highest performing generative reward model trained only on permissively licensed data, and exhibits signiﬁcantly less bias than popular LLM- AS-A-JUDGE models. 14We use relatively weak LLM Sfrom Chen et al. (2023)\nsince: (1) we want to study whether weaker LLM Scan beneﬁt\nfrom FLAME re-ranking; (2) HumanEval benchmark has been\nextensively hill-climbed on to develop newer 2024 LLM S.\n8\nLimitations and Future work Our data collection faces challenges due to evolving model evaluation standards and the need for new evaluation types for emerging applications. Expanding our collection with open-source contributions could address this issue. Our models, trained primarily on English data with a context length of 2048 tokens, might not perform well on multilingual or long-context quality assessment tasks, such as book-length summarization evaluation. In future releases, we plan to include training on more multilingual datasets with longer context lengths. Finally, in this work, we train our models using a supervised multitask fashion. Exploring alternative training approaches like RLHF and DPO is a promising direction for future work. Ethical Considerations and Risks All considerations and risks outlined by prior work for pretrained and instruction-tuned LLM S(Chowdhery et al., 2022; Anil et al., 2023) apply to LLM autoraters. We recommend following standard practice for responsible development of these models (Achiam et al., 2023; Gemini-Team et al., 2023; Reid et al., 2024). Additionally, LLM autoraters raise new risks due to increased quality assessment capabilities. First, our models can inherit and amplify biases from human evaluations, leading to unfair or discriminatory outcomes. For instance, the model may replicate biases related to race, gender, or other sensitive attributes from the training data, potentially harming certain groups. Second, overreliance on LLM autoraters risks automating decisions that need human understanding and empathy. To mitigate these risks, transparency in model development and use, along with robust measures like bias audits, data anonymization, and incorporating diverse perspectives, is essential for promoting fairness, accountability, and trustworthiness. 627",
        "aspect_review": "The paper provides a comprehensive review of existing autorater models, highlighting the limitations of task-specific and LLM-as-a-judge autoraters, and positioning FLAME as a novel solution with broader applicability. The evaluation is thorough, covering a wide range of benchmarks including REWARD BENCH and LLM-AggreFact, where FLAME outperforms proprietary models like GPT-4, demonstrating its effectiveness. FLAME is trained on a large, diverse collection of 5M+ human judgments across approximately 100 quality assessment tasks, ensuring robust generalization across various LLM capabilities. FLAME introduces a novel tail-patch fine-tuning strategy for optimizing task mixtures, significantly improving performance on reward modeling evaluation tasks. The methodology is well-structured, with a unified text-to-text format for tasks and a detailed description of the multitask fine-tuning process, ensuring transparency and reproducibility.\n\nWhile the experiments are comprehensive, there is a lack of detailed analysis on the impact of individual hyperparameters, which could provide deeper insights into the model's performance. The evaluation could benefit from additional analysis of FLAME's performance on multilingual or long-context tasks, which are currently not addressed. The paper acknowledges the limitation of primarily English data, which may affect performance on multilingual tasks, but does not provide immediate solutions. Although FLAME outperforms many proprietary models, there is limited discussion on how it compares to other open-source models in terms of computational efficiency and scalability. The paper could improve clarity by providing more detailed implementation specifics, such as exact hyperparameter settings and training schedules.",
        "general_review": "The paper introduces FLAME, a novel family of foundational autorater models that significantly outperform proprietary LLMs like GPT-4 and CLAUDE on various tasks. The methodology is robust, utilizing a large, diverse collection of over 5 million human judgments across approximately 100 quality assessment tasks. The paper demonstrates strong technical depth and rigor, particularly with the introduction of the tail-patch fine-tuning technique, which optimizes task mixtures for specific objectives like reward modeling. Comprehensive experimental validation is provided, showing FLAME's superior performance on multiple benchmarks and its reduced bias compared to other LLM-as-a-judge models. The practical impact is significant, as FLAME offers an accessible and effective solution for automatic evaluation, trained solely on permissively licensed data, promoting broader research and application in AI development.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the tail-patch fine-tuning technique. 2. Some small details on hyperparameters and implementation specifics are not thoroughly discussed, which might be useful for replication purposes. 3. There are very minor formatting issues in some figures that could be improved for better readability. 4. Clarification on the availability of the code and datasets would enhance the paper's transparency and usability."
    },
    "12": {
        "paper": "Towards Probing Speech-Specific Risks in Large Multimodal Models:\nA Taxonomy, Benchmark, and Insights\nAnonymous ACL submission\nAbstract\nLarge Multimodal Models (LMMs) have achieved great success recently, demonstrating a strong capability to understand multimodal information and to interact with human users. Despite the progress made, the challenge of detecting high-risk interactions in multimodal settings, and in particular in speech modality, remains largely unexplored. Conventional research on risk for speech modality primarily emphasises the content (e.g., what is captured as transcription). However, in speechbased interactions, paralinguistic cues in audio can significantly alter the intended meaning behind utterances. In this work, we propose a speech-specific risk taxonomy, covering 8 risk categories under hostility (malicious sarcasm and threats), malicious imitation (age, gender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on the taxonomy, we create a small-scale dataset for evaluating current LMMs capability in detecting these categories of risk. We observe even the latest models remain ineffective to detect various paralinguistic-specific risks in speech (e.g., Gemini 1.5 Pro is performing only slightly above random baseline).1Warning: this paper contains biased and offensive examples. 1 Introduction Large language models (LLMs) (Touvron et al., 2023a; Chiang et al., 2023; Anil et al., 2023) have showcased superior ability to in-context learning and robust zero-shot performance across various downstream natural language tasks (Xie et al., 2021; Brown et al., 2020; Wei et al., 2022). Building on the foundation established by LLMs, Large Multimodal Models (LMMs) (Chu et al., 2023a; Reid et al., 2024; Tang et al., 2024; Hu et al., 1The code for all experiments will be available with publi-\ncation. The data access will be granted via submitting a form\nindicating the researchers’ affiliation and the intention of use.\nSpeech-SpecificRisk TaxonomyStereotype BiasesSarcasmHostilityMalicious Imitation\nEthnicityAgeGenderThreatsEthnicityAgeGenderGenderToneEmotionparalinguistic sub-categoriesAgeEthnicityGenderAgeEthnicitycuescategories Figure 1: Our taxonomy of risk categories for speech.\n2024) equipped with multimodal encoders extend the scope beyond mere text, and facilitate interactions centred on visual and auditory inputs. This evolution marks a significant leap towards more comprehensive and versatile AI systems. Although LMMs show the capability to process and interact in a wide-range of multimodal forms, they still embody several challenges associated with safety and risks. Investigating these potential issues in LMMs requires both a modalityspecific definition of risk, and suitable benchmarks. While there is a dedicated body of work in the text domain to probe various aspects of LLMs beyond downstream performance, such categorical investigations are missing for other modalities such as speech. For instance, existing risk detection protocols for speech modality (Yousefi and Emmanouilidou, 2021; Rana and Jha, 2022; Nada et al., 2023; Reid et al., 2022; Ghosh et al., 2021) only focus on the content aspect (i.e., what could be captured by speech transcription), and neglect risks induced by paralinguistic cues, the unique feature of speech. To highlight this further, consider how 1\nvarious interpretations of the transcript “I feel so good\" arises depending on the utterance form (e.g., varying tones, and emotions such as angry, sad, depressed, or imitation of a specific gender, age or ethnicity) in audio speech. In this work, we move towards addressing this gap for speech modality by introducing a protocol to evaluate the capability of LMMs in detecting the risks induced specifically by paralinguistic cues. To our knowledge, our work is the first to explore the risk awareness at the paralinguistic level. We propose a speech taxonomy, covering 3 main categories: hostility, malicious imitation, and stereotypical biases, and further expand them into 8 corresponding sub-categories, which emphasise the implicit and subtle risks induced by paralinguistic cues in speech. Figure 1 provides a high-level overview of risk categories considered in this work (§3). We then manually create a high-quality set of seed transcriptions for 4 of the sub-categories (hostile-sarcasm, and gender, age, ethnicity stereotypical biases; 10-15 examples per each sub-category). The seed set has been controlled to not leak the category of risk through the transcript alone. The seed sets are then expanded further by leveraging GPT-4. All samples (262 samples) were further filtered by 3 human annotators to maintain quality, resulting in 180 final transcriptions. To convert these transcripts into audio, we used advanced text-to-speech (TTS) systems, Audiobox (Vyas et al., 2023) and Google TTS2, to generate various synthetic speeches with paralinguistic cues, resulting in 1,800 speech instances. In experiments, we evaluate 5 most recent speech-supported LMMs, Qwen-Audio-Chat (Chu et al., 2023a), SALMONN-7B/13B (Tang et al., 2024), WavLLM (Hu et al., 2024), and Gemini-1.5Pro (Reid et al., 2024), under various prompting strategies. Notably, Gemini 1.5 Pro performs very similar to random baseline ( 50%), while WavLLM performs worse that random guessing. Among the other two models, Qwen-Audio-Chat has a more stable success pattern under various prompting strategies, while SALMONN-7/13B do the best under certain prompting configurations. We attribute these differences in performance to different selection and adaptation of audio encoders. Among the risk categories, the one that seems the most difficult is Age Stereotypical Bias where even the best 2Audiobox: audiobox.metademolab.com ; and Google\nTTS: cloud.google.com/text-to-speech .configuration’s result is only slightly above random baseline ( 54%). For Gender andEthnicity Stereotypical Biases the best result gets above 60%, and forMalicious Sarcasm it goes further into ( 70%). To the best of our knowledge our paper presents the first speech-specific risk taxonomy, focused exclusively on risks associated with paralinguistic aspects of audio. We hope our taxonomy, benchmark, and evaluation protocol to encourage further investigation of risk in speech modality, and guide LMM developers towards more holistic evaluation and safeguarding across modalities. 2 Related Work The research on LLMs has shown increased focus on safety and responsibility, leading to significant advancements in benchmarking these models’ ability to handle and respond to harmful content in text modality. Notable contributions in this area include the three-level hierarchical risk taxonomy introduced by Do-Not-Answer (Wang et al., 2023), which created a dataset containing 939 prompts that model should not respond to. SafetyBench (Zhang et al., 2023b) explored 7 distinct safety categories across the multiple choice questions, while CValues (Xu et al., 2023) established the first Chinese safety benchmark for evaluating the capability of LLMs. Goat-bench (Khanna et al., 2024) evaluated LMMs in detecting implicit social abuse in memes. Although many research efforts focus on mitigating the generation of harmful content, OR-Bench (Cui et al., 2024) presented 10 common rejection categories including 8kseemingly toxic prompts to benchmark the over-refusal of LLMs. On conventional toxic speech detection task, the research has mostly focused on the content aspect. DeToxy-B (Ghosh et al., 2021) is proposed as a large-scale dataset for speech toxicity classification. Rana and Jha (2022) combined emotion by using multimodal learning to detect hate speech, and Reid et al. (2022) presented sensing toxicity from in-game communications. While contentfocused line of research was relevant for a while, the transcription generated by the recent highly capable Automatic Speech Recognition (ASR) systems such as Whisper (Radford et al., 2023) could merge this line of research into text-based safety research (e.g., through a cascaded design of ASR and LLM). However, this type of cascaded approach also excludes the paralinguistic cues in audio as the focus remains on the transcription of ASR. 2\nWhile early works in Speech-based LLMs shown minimal real progress in speech understanding (Su et al., 2023; Zhang et al., 2023a; Zhao et al., 2023), recent works through alignment of representation spaces between speech encoder’s output and text-based LLM’s input (either with full end-to-end training, or partial training of adaptors) have shown promising progress (Chu et al., 2023a; Reid et al., 2024; Tang et al., 2024; Hu et al., 2024). These models, now matured enough, exhibit high competence in understanding speech (Lin et al., 2024a,b; Ma et al., 2023; Xue et al., 2023). Building on this context, our research aims to evaluate the capability of LMMs to detect risks initiated by paralinguistic cues, addressing a critical gap in the current understanding of speech-specific risks. 3 Our Speech-Specific Risk Taxonomy Our speech taxonomy is as shown in Figure 1. To delineate the risks associated with paralinguistic cues, we establish 3 primary categories of risk speech. In contrast to conventional risk concerns centred on the speech content , we emphasise the significance of paralinguistic cues, including tone, emotion, and speaker information. Subsequently, we identify 8 corresponding subcategories in which ostensibly low-risk speech content may be transformed into delivery, manifested in an implicit and subtle manner, due to the influence of corresponding paralinguistic cues. 3.1 Hostility This category includes risks covering malicious sarcasm andthreats . Hostility in communication typically conveys aggression, disparagement, and the intent to harm, significantly increasing psychological pressure and violating principles of respect and politeness. Emotion and tone serve as paralinguistic cues that induce hostility, transforming ostensibly low-risk content into risky speech, altering the perceived intent of the words spoken. Malicious Sarcasm. We distinguish risky sarcasm and jokes based on the scenarios and the deliveries. Our considered sarcasm often arises in workplace and teamwork, where speakers express strong anger and mockery. In these scenarios, sarcasm is perceived as particularly aggressive and can have detrimental effects on mental health, leading to stress and anxiety among colleagues (Colston, 1997; Toplak and Katz, 2000; Katz et al., 2004; Zhu and Wang, 2020). 208Threats. They represent a severe form of aggressive communication. In our definition, it is implicitly delivered by the speaker’s emotion and tone, which creates a fear atmosphere and conveys implication to harm. The presence of threats within communication significantly harms the psychological health of others, and often escalate conflicts, leading to toxic environment. 3.2 Malicious Imitation This category encompasses risky communication that involve the deliberate mimicry of voice characteristics associated with gender, age, and ethnicity. Such imitations, in the form of ridiculing and offending, aim to propagate and reinforce stereotypes, discrimination, or bias, leading to undermining the dignity of individuals and psychological trauma. The paralinguistic cues here are the comparison between the speaker’s original voice and the exaggerated change of voice characteristics. Gender. Gender-based imitation possibly involves exaggerating the feminine voice coupled with implicit stereotypes, aiming to demean and undermine the female group. Age. Age-based imitation often targets the elderly. The imitative voice coupled with specific content depict them as a weak and old-fashioned group who is out of touch, which can reinforce stereotypes and exacerbate ageist. Ethnicity. Ethnicity-based imitation targets accents of groups with different cultural background. This form of imitation often perpetuates racial and ethnic stereotypes, deepening cultural divides and exacerbating tensions in multicultural settings. 3.3 Stereotypical Biases This category focuses on the risks associated with conversations that exhibits implicit stereotypes based on gender, age, and ethnicity. Stereotypical biases in communication often implicitly manifests through responses that may appear neutral but are loaded with underlying discriminatory attitudes. We characterise the paralinguistic cues harbouring risks in this category to include the gender, age, and ethnicity of the first and second speakers. Gender. In cases of gender-based stereotypical bias, responses may implicitly convey stereotypical beliefs about abilities, roles, or behaviours associated with the female group. The content may be neutral, but the paralinguistics cues may harbour risks offensive to others. We consider risky interactions that contain a female and a male speaker. 3\nRisk Sub-category Risk Low-risk Total\nMalicious Sarcasm 375 375 Age Stereotypical Bias 250 250 Gender Stereotypical Bias 155 155 Ethnicity Stereotypical Bias 120 120 Total 900 900 1Table 1: Our speech dataset for various risk types.\nAge. Stereotypical Bias against the elderly is exhibited in conversations that reflect age-related stereotypes. Responses to the elderly individuals may assume incompetence, resistance to change, or being out of touch. We consider risky interactions that contain an elderly and a young speaker. Ethnicity. In the case of ethnicity stereotypical bias, responses may reflect stereotypes to a group, biases to their ability, or discrimination to cultural practices. It reinforces ethnic stereotypes and can hinder the equal treatment of individuals from diverse cultural backgrounds. We consider risky interactions in this category that contain an accented speaker and a native speaker. 4 Data Collection and Curation We curate our speech dataset for evaluation by (i) manually creating samples as seeds for each speech sub-category based on the corresponding risk description, (ii) leveraging seed instances to prompt GPT-4 to expand the sample set, and (iii) using advanced TTS systems, Audiobox and Google TTS, to generate synthetic speech for 4 risk subcategories according to their specific paralinguistic descriptions (see Figure 2). Due to the safeguards and limitation of existing TTS system, we generate synthetic speech for these risk sub-categories: malicious sarcasm, age, gender, and ethnicity stereotypical biases. Table 1 provides our dataset statistics. More specifically, each sample in our dataset is a quadraple (x, z, s, y )where (i) xis the textual content (created by human or GPT4), (ii) zis the description of paralingustic cues covering emotion, tone, gender, age, and ethnicity, (iii) sis the automatically generated speech s=TTS (x, z)based on Audiobox (Vyas et al., 2023) or Google TTS3, and (iv) yis the label in { low-risk ,malicious sarcasm ,age,gender ,ethnicity stereotypical biases }. Creating a speech dataset entirely through human effort presents significant challenges, primarily due to its high costs, extensive time require3Audiobox: audiobox.metademolab.com ; and Google\nTTS: cloud.google.com/text-to-speech .\nFigure 2: Our data curation pipeline.\nments, and the difficulty of finding individuals capable of accurately acting specific speech descriptions. These challenges often make the process inefficient and impractical, which lead us to leverage GPT-4 and advanced TTS systems for speech rendering, allowing to create diverse and scalable datasets at a fraction of the cost and time. However, we still need to bypass the safeguard restricting us to obtain safety-related data. The rest of this section outlines how to address these challenges. 4.1 Text Samples Seeds. We first manually create 20 sample pairs of(x, z)for each risk sub-category label y. These samples are quality controlled and filtered by 3 expert annotators based on these criteria: (i) the content xis ostensibly low-risk, and (ii) when combined with paralinguistic z, it is mapped to the risk label y(including the 4 risk labels plus the lowrisklabel). A sample is removed if at least two annotators find it low quality. GPT-4 Generation. Manually creating samples is a time-consuming and costly process. Capitalising on the wide knowledge of GPT-4, we leverage the human-curated samples as seed templates, and prompt GPT-4 to generate more samples. Normally, we may describe a risk sub-category and include human-curated samples, and request GPT-4 to generalise them to more scenarios. However, GPT-4 tends to refuse responding to such requests due to its safeguards. We thus employ a strategy analogous to (Wang et al., 2023) to overcome this issue, as explained below. Specifically, we feed fabricated conversation histories into GPT-4, where we first define a risk subcategory and request GPT-4 to produce samples according to this description. We then utilise human curated samples as pseudo-responses from GPT-4. Finally, we request GPT-4 to generate 30 samples. These samples are annotated and filtered by human annotators, serving as seeds for iterative generation. We mix human-generated and GPT4-generated samples as the text sample set where each sample has a risk version and a low-risk version by keeping the same xand modifying z. 4\n4.2 Synthesising Speech Sarcasm & Age Stereotypical Bias. For each (x, z)in these categories, we generate 5 high-risk speech and 5 low-risk speech using Audiobox.4We provide detailed speech descriptions for generation in Table 8 of Appendix C. The low-risk versions are generated from the modified paralinguistic description z′, as described in the following. •Formalicious sarcasm , We describe zas\"speaking with angry emotion, and a mocking tone\" , and z′as\"speaking with happy and excited emotions\". •Forage stereotypical bias , we distinguish between risk speech and low-risk speech based on the age of the first speaker. We describe zas\"the first speaker is an elderly person, the second person is a young person\", and the corresponding z′is\"the first speaker is a young person, the second person is also a young person\" . We first generate 5 speech of the second-speaker for each sample, and then generate 10 speech of the first-speaker, including 5 risk version and 5 low-risk version, based on zand z′. We finally manually cut the long silence and noise in collected speech, and concatenate speech waves of the first and the second speakers with 0.8 seconds silence in between. Gender, Ethnicity Stereotypical Biases. We utilise Google TTS5service to generate synthetic speech for risk categories: gender stereotypical bias andethnicity stereotypical bias . To distinguish the risk and low-risk speech, we control the gender and ethnicity of the first speaker. •Forgender stereotypical bias , We describe zas \"the first speaker is a woman, the second person is a man\", and the corresponding z′is\"the first speaker is man, the second person is also a man\" . we randomly select 5 female and 5 male voices from the en-US language list to serve as the first speaker, and an additional 5 male voices as the second speaker. We then create conversations by pairing each of the 5 female first-speakers with the 5 male second-speakers to constitute the risk speech samples. Similarly, pairing each of the 5 male first-speakers with the 5 male second-speakers generates the low-risk speech samples. All speech waves are concatenated with 0.8 seconds of silence in between. •Forethnicity stereotypical bias , a similar strategy 4Google TTS does not provide the age of speakers to gen-\nerate the elderly voice needed for our dataset.\n5Audiobox provides a random voice for each generation,\nsuggesting it’s not able to provide consistent speakers across\nsamples in the same sub-category.is employed. We describe zas\"the first speaker is a person with accent and diverse ethnicity backgrounds, the second speaker is a person with American native accent\", and the corresponding z′is \"the first speaker is a person with American native accent, the second speaker is also a person with American native accent\" . However, due to the limitation of Google TTS providing only Indianaccented voices, we are restricted to using Indian voices as the first speaker. Specifically, we select 5 voices each from the en-IN and en-US language lists to serve as the first speaker, with an additional set of 5 voices chosen from the en-US list as the second speaker. These selections are then systematically paired and concatenated into conversations following the same protocol used for the genderbased pairings. 5 Experiments We evaluate the capabilities of LMMs in detecting the risk induced by paralinguistic cues under 4 risk sub-categories: malicious sarcasm , and stereotypical biases for gender, age , and ethnicity . We first describe our evaluation prompts and model selection (§5.1). Next, we report the results on 5 LMMs via a diverse set of evaluation prompts (§5.2). Lastly, we analyse the capabilities of LMMs and patterns observed (§5.3). 5.1 Experimental Settings Prompts. Our basic prompts for evaluation are Yes/No (Y/N) questions (e.g., Is there any indication of sarcasm in the speaker’s delivery in the audio? ), and Multi-choice (MC) questions (e.g., What is the speaker implying? Choose the most appropriate response.; A. Compliments, support and gratitude; B. Sarcasm ). We also try Chainof-thought (CoT) style which allows us to investigate whether step-by-step reasoning could improve LMMs’ detection capability by appending Let’s think step-by-step (Kojima et al., 2022) to the start of both Y/N and MC prompts. This is denoted as CoT + Y/N, or CoT + MC. Additionally, to increase LMM’s chance of success, we also try appending more revealing (Pre-task) questions in the Y/N and MC prompts by asking the LMM to first predict a relevant paralinguistc cue in the audio before attempting to answer the Y/N or MC questions (e.g., Please recognize the speaker’s sentiment, and ... ). This is denoted as Pre-task + Y/N, or Pre-task + MC. We provide detailed prompts for each risk 5\nSarcasm Gender Age Ethnicity WeightAvg.\nPrompt Acc Acc Acc Acc Acc\nQwen-Audio-Chat-7B\nY/N 66.00 55.81 48.40 49.58 57.17\nCoT + Y/N 62.27 50.00 54.60 48.75 56.22\nPre-task + Y/N 50.00 50.00 50.00 50.00 50.00\nMC 61.47 45.48 51.60 61.67 56.00\nCoT + MC 61.47 48.39 53.20 56.25 56.22\nPre-task + MC 76.67 50.97 50.00 50.42 61.34\nAvg. 62.98 50.11 51.30 52.78\nSALMONN-7B\nY/N 50.00 50.00 50.00 50.00 50.00\nCoT + Y/N 50.00 50.00 50.00 50.00 50.00\nPre-task + Y/N 52.00 55.81 48.60 50.83 51.56\nMC 59.20 49.68 49.60 60.83 55.11\nCoT + MC 58.93 48.06 53.00 63.33 56.00\nPre-task + MC 64.00 52.58 55.20 50.00 57.72\nAvg. 55.69 51.02 51.07 54.16\nSALMONN-13B\nY/N 64.80 50.00 50.00 50.00 56.17\nCoT + Y/N 50.80 50.32 48.40 50.00 49.94\nPre-task + Y/N 50.40 62.58 45.80 45.42 50.56\nMC 61.60 34.84 42.40 63.33 51.89\nCoT + MC 60.00 37.74 41.20 52.50 49.94\nPre-task + MC 64.27 46.45 45.40 52.92 54.45\nAvg. 58.65 46.99 45.53 52.36\nWavLLM-7B\nY/N 50.00 49.68 35.20 46.67 45.39\nCoT + Y/N 50.00 49.03 36.20 46.67 45.56\nPre-task + Y/N 49.33 48.39 49.80 31.67 46.94\nMC 50.00 49.68 50.00 49.58 49.89\nCoT + MC 50.00 50.00 49.40 49.58 49.78\nPre-task + MC 50.00 50.32 49.20 50.00 49.83\nAvg. 49.89 49.52 44.97 45.70\nGemini-1.5-Pro\nY/N 52.50 55.48 51.80 49.17 52.37\nCoT + Y/N 59.00 56.13 49.80 45.83 54.19\nPre-task + Y/N 52.00 57.42 50.00 55.83 52.89\nMC 50.50 50.00 51.60 52.08 50.93\nCoT + MC 51.75 50.97 51.20 55.83 52.01\nPre-task + MC 56.00 55.81 51.60 47.08 53.56\nAvg. 53.63 54.30 51.00 50.97\nTable 2: Evaluation of models on various prompts\nacross 4 risk sub-categories. The results are pre-\nsented using the accuracy. Under each risk sub-\ncategory: yellow indicates the best average perfor-\nmance, red indicates the best individual performance,\nandgreen indicates the best for weighted average.\nsub-categories in Table 10 of Appendix E. Models. We evaluate 5 recent LMMs with instruction-following and speech understanding capabilities. Qwen-Audio-Chat (Chu et al., 2023a) is an instruction following version of QwenAudio (Chu et al., 2023b) with a Whisper audio encoder and QwenLM (Bai et al., 2023). 445SALMONN-7/13B (Tang et al., 2024) is a Whisper and BEATs (Chen et al., 2023) dual audio encoders and VicunaLLM (Chiang et al., 2023). We evaluate both 7B and 13B variants.WavLLM (Hu et al., 2024), is the latest LMM achieving stateof-the-art on universal speech benchmarks and is equipped with Whisper and WavLM (Chen et al., 2022) dual encoders and LLaMA-2 (Touvron et al., 2023b).Gemini-1.5-Pro (Reid et al., 2024) is a widely used recent proprietary LMM with native multi-modal capabilities. We used the API access for Gemini-1.5-pro. In all evaluations, we set the temperature as 0 and switched off sampling for reproducibility of experimental results. Accuracy and macro-averaged F1 score are used as metrics. 5.2 Main Results We report evaluation results in Table 2 (F1 exhibits similar pattern - see Table 6 of §A). We show the average performance among LMMs for each task, and the weighted average performance by the number of task samples for each combination between LMM and prompt across 4 risk sub-categories. Our findings are summarised along various axes. Prompting Styles. Do Y/N and MC exhibit a systematic difference in performance? Do CoT and Pre-task query improve the results? Do models show high degree of sensitivity to prompting style? Is there a preferred mode of prompting? We observe that, on most of sub-categories, MC is a more effective prompting strategy. Especially, SALMONN reacts with severe misalignment and biases on Y/N, but it achieves the best performance when it is switched to MC. CoT, as a common strategy to promote logical thinking of LLMs, does not show its impact on LMM for combining multimodal cues. In contrast, the adoption of Pre-task activates most of models to achieve a better result on various sub-categories. It suggests the implicit signal from paralinguistic cues help models integrating multimodal cues. These observations leads to Pre-task + MC as the best prompting strategy. Models. Is there a model outperforming the rest on all risk sub-categories? Is there a specific pretraining protocol or choice of encoder-LLM that has a clear advantage? Are there models that perform near random baseline? We don’t conclude there is a model outperforming the rest on all sub-categories, however, results exhibit two patterns that models follow. QwenAudio-Chat achieves the best overall performance 6\nSR SC GR AGR AR Avg.\nModel Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1\nQwen-Audio-Chat-7B 56.00 45.44 50.00 33.33 32.58 37.55 50.00 33.33 50.00 33.33 47.72 36.60\nSALMONN-7B 59.20 53.33 50.10 33.54 61.61 55.97 61.20 60.84 49.58 33.15 56.34 47.37\nSALMONN-13B 55.20 44.92 50.00 33.33 78.39 82.81 44.80 35.40 50.00 33.33 55.68 45.96\nWavLLM-7B 50.00 33.33 76.19 76.10 50.97 49.86 50.00 35.03 50.00 33.33 55.43 45.53\nGemini-1.5-Pro 50.13 42.71 93.52 93.52 - - - - - - - -\nTable 3: Paralinguistic Tasks: Sentiment Recognition(SR), Speaker counting(SC), Gender Recognition(GR), Age\nGroup Recognition(AGR), Accent Recognition(AR).\nacross 4 sub-categories and also achieves competitive performance on each sub-category. Its average performance across 6 prompting strategies outperform other models on 2 sub-categories, demonstrating its stabilility and robustness to prompts. Gemini-1.5-Pro follows the similar pattern, which suggests a overall stable and robust performance across different prompting stragegies and achieve the best average F1 score on 3 sub-categories. However, SALMONN-7B/13B demonstrate an opposite pattern where they show outstanding risk detection ability on 3 sub-categories of stereotypical biases and achieve the best performance, respectively. But they exhibit vulnerable to prompts, especially, SALMONN-7B could not make a reaction under Y/N even though effective Pre-task strategy slightly mitigates this, and SALMONN-13B are not able to maintains the consistent performance across different prompts under the same sub-category (e.g., 62.58 vs. 34.84 under gender stereotypical bias). Meanwhile, WavLLM fails to detect any risk, and show severe misalignment and biases across all subcategories. By observing these two patterns and the pre-training protocol of LLMs, we attribute them to the different states of audio encoders. Specificlly, audio encoders in Qwen-Audio-Chat and Gemini-1.5-Pro are fine-tuned in pre-training stage leading them to effectively extract features from inputs and generate more stable and consistent embeddings, exhibiting robustness to prompts. However, frozen audio encoders coupled with adapter in SALMONN and WavLLM are more likely to be vulnerable to the change of inputs and prompts, and the dual encoders settings mixed with irrelevant non-speech feature limit its ability to generate more stable and consistent embeddings. Difficulty of Sub-categories. Are there risk subcategories that are much harder for models to detect and why? Most of models perform near or over 60% of accuracy on detection of malicious sarcasm where its paralinguistic cue is sentiment displayed as emo- 537Gender Age Ethnicity\nPrompt Acc F1 Acc F1 Acc F1\nQwen-Audio-Chat-7B\nLevel-1 51.94 39.64 51.00 43.81 49.44 33.79\nLevel-2 54.41 46.35 50.80 44.42 50.14 34.12\nSALMONN-7B\nLevel-1 51.94 39.56 49.53 33.35 50.28 34.17\nLevel-2 54.73 42.80 49.40 33.07 50.00 33.33\nSALMONN-13B\nLevel-1 54.30 42.43 48.07 33.93 48.47 37.32\nLevel-2 51.84 39.62 47.47 34.84 46.81 33.41\nWavLLM-7B\nLevel-1 49.03 34.88 40.40 31.49 41.67 31.67\nLevel-2 51.83 40.72 41.87 33.78 46.81 36.53\nGemini-1.5-Pro\nLevel-1 56.34 53.82 50.53 49.17 50.27 49.69\nLevel-2 54.84 47.59 49.60 41.28 52.22 47.55\nGPT4\nText + Y/N 93.55 93.52 98.00 97.99 91.67 91.65\nTable 4: Results of Level-2 difficulty analysis with im-\nproved prompts across 3 conversational sub-categories\n(Gender, Age, and Ethnicity Stereotypical Biases). The\nresults are the average accuracy and macro-averaged F1\nover 3 types of Y/N prompts (except GPT4). Bold is\nthe performance which benefits from Level-2 prompts.\ntion and speaking tone in utterances. Emotion recognition as a basic speech task is included in the pre-training stage of most models, resulting in models’ ability to recognise and reason with it. However, detection in stereotypical biases produce 2 more complex difficulties for models to overcome: (i) recognise the number of speakers, and (ii) recognise the voice features of the first speaker. Most of models lack of training to solve these issues, leading to a overall performance below 60% of accuracy. We analyse these difficulties, and include GPT-4 evaluation as performance ceiling assuming these difficulties are overcome. 5.3 Analysis and Discussion Level-2 Evaluation. In conversational risk subcategories, we avoid mentioning the number of speakers in vanilla Y/N prompts (Level-1), leading 7\nModel Sentiment Gender Age Ethnicity\nQwen-Audio-Chat-7B 53.34 11.62 9.20 23.34\nSALMONN-7B 28.00 11.62 10.40 26.66\nSALMONN-13B 29.60 30.32 17.60 26.66\nWavLLM-7B 1.34 3.22 29.60 36.66\nGemini-1.5-Pro 18.00 14.84 3.60 11.66\nTable 5: SAR (%) results of Speaker Awareness.\nto difficulties for models to be aware of the number of speakers and recognise the voice features of the speakers. In Level-2 prompts, we add \"the second speaker\" into vanilla Y/N prompts implying the number of speakers and reduce the difficulty. For comparison, we add GPT-4 evaluation as performance ceiling where we explicitly declare the gender, age, or ethnicity of speakers coupled with transcripts and Level-1 prompts. According to results presented in Table 4, perfor-\nmance of most models on gender prejudice get im-\nproved as the gender recognition is a relatively sim-\nple speech task, and the difficulty lying in speaker\ncounting is reduced in Level-2 prompts, leading to\nhigher performance. For age and ethnicity preju-\ndice, we only observe a slight improvement among\nmodels, demonstrating the performance is still lim-\nited by the capabilities of recognising the corre-\nsponding paralinguistic cues. By the evaluation on\nGPT-4, we imitate the situation where all paralin-\nguistic cues are recognised, and the performance\nguarantees the quality of our samples.\nSpeaker Awareness. Under the same risk sub-\ncategory, the content of risk speech and low-risk\nspeech are consistent. To investigate the changes of\nresults brought about by different speakers, we in-\ntroduce a metrics Speaker Awareness Rate (SAR),\nwhich is used to measure the awareness of the cor-\nresponding paralinguistic cues,\nSAR =TPrate −FPrate\nHigher SAR means models can be effectively aware of the change of speakers’ paralinguistic cues, leading to the change of prediction results. We present our results in Table 5. Qwen-AudioChat and SALMONN-13B achieve the best performance on sentiment and gender awareness, respectively. And these 2 models also achieve the second and the best performance on the subsequent corresponding paralinguistic tasks in Table 3. However, WavLLM that outperforms other models on age and ethnicity awareness fails on almost all risk detecting and paralinguistic tasks. It can be effectively aware of the change of speaker, but exhibits 576a deficiency in alignment and bias. We speculate an improved instruction-tuning may activate the capability of WavLLM. Paralinguistic Tasks. The premise of risk detection is to recognise the paralinguistic cues well, therefore, we provide several paralinguistic tasks to analyse models’ abilities. •Sentiment Recognition (SR) We use speech from sarcasm as test set, where the sentiment of risk speech is labelled as \"negative\", and low-risk speech is labelled as \"neutral or positive\". QwenAudio-Chat and SALMONN-7B/13B achieve similar performance on SR, consistent with results in sarcasm detection. Similarly, failure of WavLLM and Gemini-1.5-Pro leads to a deficiency on sarcasm detection. •Speaker Counting (SC) We use conversational speech as test set and label them as \"Two\", and the speech that only contains the first speaker’s utterances is labelled as \"One\". Gemini-1.5-Pro and WavLLM outperform other models on SC, however, WavLLM fails in the subsequent tasks and Gemini-1.5-Pro even can not provide an answer, which prevents them from being successful in related risk detection. •Gender, Age Group, and Accent Recognition (GR, AGR, and AR) We label risk speech from the corresponding risk type as \"woman\", \"elderly person\" and \"Indian accent\"; for low-risk speech, we label them as \"man\", \"young person\", and \"American accent\". Qwen-Audio-Chat exhibits the lack of alignment, but also demonstrates the awareness of the change of speaker. SALMONN 7B/13B achieve the best performances on AGR and GR, respectively, explaining the outstanding capabilities in the corresponding risk detection tasks. Accent recognition is a shortage among models, however, they still show the risk awareness in the risk detection evaluation. 6 Conclusion We presented a speech-specific risk taxonomy where paralinguistic cues in speech can transform low-risk textual content into high-risk speech. We created a high quality synthetic speech dataset under human annotation and filtering. We observed that even the most recent large multimodal models (such as Gemini 1.5 pro) perform near random baseline, with some of the recent speechLLMs scoring even worse than random guesses. 8\n7 Limitations We expect to extend our evaluation experiments to all risk types in our taxonomy, however, the existing safeguards of TTS system prevents the generation of such synthetic data. Our ongoing plan to hire human speakers for collecting real data is currently undergoing ethics committee review atredacted for anonymity . Additionally, all LMMs are evaluated on our synthetic dataset, and human-generated speech could potentially introduce other artefacts, making this task even more challenging. We provided certain conjectures to explain evaluation results and the capabilities of LMMs, but this initial attempt requires further analyse in separate works. 8 Ethics Statement This research aims to open an avenue for systematically evaluating the capabilities of Large Multimodal Models in detecting risk associated with speech modality. The nature of this data is inherently sensitive. To ensure our data (and its future extensions) access facilitates progress towards safeguarding and does not contribute to harmful designs, we will place the data access behind a request form, demanding researchers to provide detailed affiliation and intention of use, under a strict term of use. Additionally, we have adhered to the usage policy of Audiobox and Google TTS, and did not generate speech containing any explicit toxic content . 655",
        "aspect_review": "The paper introduces a novel speech-specific risk taxonomy focusing on paralinguistic cues, which is a significant and unexplored area in the domain of Large Multimodal Models (LMMs). This contributes to the broader understanding and development of safer AI systems. The study provides comprehensive experimental validation by evaluating several state-of-the-art LMMs using a newly created dataset. The evaluation spans multiple risk sub-categories, offering insights into the models' capabilities and limitations.\n\nThe experiments are limited to synthetic datasets due to the constraints of current TTS systems, which might not fully capture the complexities of human speech. While the taxonomy and dataset are novel, the immediate practical impact might be limited until further real-world data is incorporated. The paper could benefit from clearer exposition in some sections, particularly in explaining the experimental setup and results interpretation. Additionally, the discussion on the limitations of current models could be expanded.",
        "general_review": "The paper introduces a novel and comprehensive taxonomy for speech-specific risks in large multimodal models, focusing on paralinguistic cues, which is groundbreaking in the field. The methodology is robust, with a clear and insightful empirical analysis of the capabilities of current LMMs in detecting these risks. The creation of a high-quality dataset and the thorough experimental validation across multiple models and prompting strategies demonstrate strong technical depth and rigor. This research has significant practical impact as it addresses a critical gap in understanding speech-specific risks, potentially guiding future development of safer AI systems.\n\n1. The paper could benefit from minor writing improvements for clarity, particularly in the explanation of experimental results. 2. Additional details on hyperparameters and implementation choices would enhance reproducibility. 3. Some figures could be formatted more clearly to improve readability. 4. The code availability section could be slightly clarified to ensure ease of access for future research."
    },
    "13": {
        "paper": "Revisiting the Impact of Pursuing Modularity for Code Generation\nAnonymous ACL submission\nAbstract\nModular programming, which aims to construct the final program by integrating smaller, independent building blocks, has been regarded as a desirable practice in software development. However, with the rise of recent code generation agents built upon large language models (LLMs), a question emerges: is this traditional practice equally effective for these new tools? In this work, we assess the impact of modularity in code generation by introducing a novel metric for its quantitative measurement. Surprisingly, unlike conventional wisdom on the topic, we find that modularity is not a core factor for improving the performance of code generation models. We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code. 1 Introduction With recent advances in the capabilities of large language models (LLMs; OpenAI, 2024; Gemini Team, 2024; inter alia ), their application areas have expanded beyond simple text-based tasks. Among these, coding assistants are becoming practically essential for programmers, enhancing their efficiency through tasks such as natural language to code (NL2Code) generation. Similar to other use cases of LLMs, coding assistants are typically utilized in zero- or few-shot manners, without task-specific fine-tuning. The problem is that as the length of code is usually much longer than that of a sentence, the number of code examples available for each run is strictly limited. Furthermore, the same functionality can be represented with different forms of code, making it challenging for users to select a proper example for a target task. It is thus important to understand what characteristics of the code provided to the agents contribute to the final performance of such models. Among the many possible properties that influence the characteristics of code snippets, this work in- 040vestigates the impact of code modularity on the performance of LLMs for NL2Code generation. Modular programming, the practice of building software with independent components, has long been considered a cornerstone of good software development. While this paradigm facilitates desirable properties of code for human programmers , such as reusability, readability, and maintainability, it remains an open question whether it offers the same level of effectiveness for LLMs . Notably, Jain et al. (2024) argued that leveraging a set of modular functions can improve code generation accuracy for both in-context learning (ICL) and fine-tuning. As it is not trivial to guarantee the modularity of each code snippet, the authors asked GPT-3.5-Turbo1to convert an existing code snippet into a more modular one, while ensuring its functional correctness. However, we claim that their report warrants revisiting for two reasons. First, since LLMs are known for their verbosity, it is unclear whether the conversion process aimed solely for modularity or inadvertently introduced unexpected side effects. Second, the lack of a formally defined quantitative method for estimating modularity hinders more extensive analyses related to the problem. In this paper, we (re-)investigate the effectiveness of pursuing modularity in NL2Code generation. We aim to push the boundaries of previous work by (1) introducing a novel metric that quantifies the modularity of a code snippet using numeric values. Based on the metric, we (2) classify code snippets as modular or non-modular without relying on LLMs, and evaluate how each category contributes to performance.2Moreover, beyond previous work, we (3) conduct experiments on models with parameters exceeding 7B (i.e., 33B and 34B) 1https://platform.openai.com/docs/models/gpt-3-5-turbo\n2Note that this was infeasible in the previous study (Jain\net al., 2024) as there was no clear standard for determining\nwhether each code snippet is modular or not.\n1\nFigure 1: Illustration of Code Complexity (CC) and\nModularity Score ( MOS) computation. We first build\ncontrol flow graphs from the given code to derive CC\nvalues. The CC values are then used to calculate inter-\nmediate values, e.g., m, which are ultimately used to\nderive MoS based on its definition involving nandm∗.\nto investigate the impact of model scale. In experiments, we discover that contrary to conventional wisdom in the literature, the modularity of a code example may not be the crucial factor for performance . We also explore potential explanations for why LLMs do not exhibit a preference for modular code compared to non-modular code. 2 Quantitative Definition of Modularity To assess the impact of code modularity, the first essential step is to develop a method that provides a measurable score for code modularity. While the previous study (Jain et al., 2024) bypassed this vital step,3we present a reasonable metric for estimating code modularity, which is challenging due to the inherent subjectivity of the concept itself. Inspired by the software engineering literature, we employ Cyclomatic Complexity (CC) (McCabe, 1976) to determine the ideal number of modules, m∗, for a given code snippet. CC is calculated as E-N+ 2, where EandNcorrespond to the number of edges and nodes, in a control-flow graph representation of the target code. CC can be computed at either the whole code level (total CC; CCtotal) or the function level (meaning the average CC across all functions in the code; CC avg). A high CC value generally indicates a complex code structure. It functions as a guideline for code decomposition, suggesting that a function whose CC is exceeding a certain threshold value τ, e.g., 5 (McCabe, 1976) or 10 (McConnell, 2004), might benefit from being broken down into smaller sub3The authors instead utilized LLMs to transform all code\nsnippets into supposedly modular ones.functions. Based on the concept, we assume that the average CC of an ideal modular code example, denoted by CCavg∗, should be equal to the threshold τ.4In other words, ideally, every function within a modular code snippet is expected to have a CC value of τ. Following the intuition, we define m∗, the number of ideal modules, as follows: m∗=⌊CCtotal\nCCavg∗⌋=⌊CCtotal\nτ⌋, Finally, we define the modularity score, dubbed MOS, as follows: MOS=\n\nmin\u0000\n1,n\nm∗\u0001\nifm∗>0\n0 ifm∗=n= 0\n1 ifm∗= 0, n > 0, where nis equal to the actual number of modules in the target code. That is, the closer n(actual number of modules) is to m∗(ideal number of modules), the higher the modularity is considered to be.53 Four Code Categories by Modularity This allows us to create four distinct clusters of code separated by their modularity levels. Figure 2 in Appendix illustrates example code from each category for the same problem. Modular Code (MC) is a collection of code snippets with the highest MOSamong solutions for each problem in a dataset. Singular Code (SC) represents another set of solution code examples for the same problems corresponding to MC, with M OS being 0. Transformed Modular Code (TMC) can be ob-\ntained by utilizing GPT-3.5-Turbo ( f) to transform\nSCinto code with high MOS. The conversion pro-\ncess can be represented by the following:\nTMC =f(I, Q, SC),\nwhere Irepresents a transformation instruction and Qis the problem description of SC.64Given two choices for τ, i.e., 5 or 10, we set τto 5 to\nencourage a sparser distribution of modularity scores ( MOS).\n5In extreme cases where m= 0 (no modularization re-\nquired), the modularity score is set to 0 if no actual modules\nare used ( n= 0) and 1 otherwise ( n > 0).\n6See Figure 3 for prompt details on the conversion process.\n2\nModel SizeCode\nTypeIntroductory Interview Competition Average\npass@1 pass@5 pass@1 pass@5 pass@1 pass@5 pass@1 pass@5\nCode Llama 7BTMC 14.67 19.63 2.28 3.98 0.21 0.59 4.45 6.66\nTSC 13.84 17.15 2.16 3.61 0.07 0.24 4.20 6.07\nDeepSeekCoder 6.7BTMC 34.26 40.74 9.60 13.41 0.76 1.93 13.49 17.63\nTSC 33.24 39.73 8.55 12.40 0.55 1.21 12.55 16.64\nTable 1: Results on APPS measured by pass@ k. We use n=10 for pass@1 and pass@5. The best results are in bold\nfor each section. Two-shot prompting is applied to generate code. Code Type refers to two distinct groups of code\nused for demonstrations. We find that TMC slightly outperforms TSC but the performance gaps are insignificant.\nModel SizeCode\nTypeCodeContests\npass@1 pass@10\nCode Llama7BMC 1.98 8.02\nSC 2.58 8.81\nTMC 2.57 10.18\nTSC 4.35 10.67\n34BMC 4.11 12.78\nSC 5.83 14.1\nTMC 3.39 13.55\nTSC 5.61 15.32\nDeepSeekCoder6.7BMC 5.3 12.78\nSC 7.15 16.27\nTMC 8.02 17.88\nTSC 8.19 17.79\n33BMC 6.79 16.14\nSC 8.87 20.5\nTMC 9.38 22.74\nTSC 8.78 22.09\nTable 2: Results on CodeContests measured by pass@ k.\nWe use n=10 for pass@1 and n=50 for pass@5, re-\nspectively. The best results are in bold for each section.\nTwo-shot prompting is applied for generating code given\nnatural language queries. Code Type refers to four dis-\ntinct groups of code used for demonstrations. We reveal\nno significant impact of code type on performance.\nTransformed Singular Code (TSC) is a variation from TMC , whose modularity is manually removed by human programmers. By minimizing the influence of factors other than modularity through the comparison of TSC andTMC , it encourages a rigorous evaluation of the impact of modularity. 4 Experimental Setups We explore the impact of modularity by comparing how the four code collections, categorized by their modularity levels, affect performance. To mimic real-world usage, we focus on the case of utilizing code LLMs with few-shot in-context learning. We leverage two-shot demonstrations (providing two 152code examples) unless otherwise specified.7We will open-source our code after the review process. Models. We use two LLMs for code generation— Code Llama (7B, 34B; Rozière et al., 2024) and DeepSeekCoder (6.7B, 33B; Guo et al., 2024). Datasets. We employ two NL2Code generation datasets—APPS (Hendrycks et al., 2021) and CodeContests (Li et al., 2022).8They are based on competitive programming contests and provide a set of different solutions for each programming problem. For each dataset, the groups of MC andSCdemonstrations are chosen from solutions for randomly selected problems. SCexamples are then converted intoTMC , and finally, TSC is manually obtained. In this study, we focus our evaluation on Python. Evaluation Metrics. We apply an unbiased version of pass@ k(Chen et al., 2021), which measures the functional correctness of generated programs by running them against test cases. For each problem, LLMs are prompted to generate nprograms, and we determine c, the number of programs that pass the test cases. In addition, k(k≤n) specifies the granularity of evaluation such that the metric indicates the probability of finding at least one correct solution when sampling kprograms out of the ngenerated ones. The metric is then averaged over all problems. As a result, pass@ kis computed as: pass@k=Eproblems\"\n1−\u0000n−c\nk\u0001\n\u0000n\nk\u0001#\n. 5 Main Results Table 1 and Table 2 present results on APPS and CodeContests, categorized by the modularity of the 7Refer to Figure 4 and Figure 5 for prompt details.\n8Note that representative code generation benchmarks, e.g.,\nHumanEval (Chen et al., 2021), typically provide code snip-\npets whose length restricts the possibility of modularization.\n3\nModel Size Pearson Spearman\nCode Llama 7B -0.34 (0) -0.31 (0)\nDeepSeekCoder 6.7B -0.21 (0.04) -0.25 (0.01)\nTable 3: Correlations between modularity ( MOS) and\nperformance (pass@1), evaluated on CodeContests.\nThey consistently show weak negative relationships.\nNumbers in parentheses represent p-values.\ncode demonstrations. All results are the average of five independent runs with different random seeds. In Table 1, we observe, as previously reported, that the performance of TMC is slightly better than TSC .9However, their marginal performance gaps raise questions about the impact of modularity. In Table 2, the relationship between modularity and performance becomes less clear. When comparing MCtoSC, we observe that MCconsistently underperforms SC, which contradicts previous findings. Furthermore, the comparison between TMC andTSC —a more controlled setting for evaluating modularity—shows no clear correlation between code modularity and performance. This is despite the fact that the transformation process by GPT3.5-Turbo ( MC→TMC ,SC→TSC ) seems to contribute to non-trivial increases in performance. Therefore, we argue that the previously reported effectiveness of modularity on performance was likely due to unforeseen consequences of the transformation process, rather than the modularity itself. 6 Analysis 6.1 Correlation Study We conduct an extra experiment to dive deeper into the modularity-performance relationship. Specifically, given 100 code samples used as demonstrations,10we compute the Pearson and Spearman correlations between their modularity ( MOS) and resulting performance (pass@1). For simplicity, we perform one-shot ICL on CodeContests. Experimental results are presented in Table 3 and Figure 6 in Appendix. Surprisingly, the results reveal weak negative correlations between modularity and performance, suggesting that modularity may not offer benefits, or even hinder performance in some cases. 9For APPS, we conducted experiments only with TMC\nandTSC due to computational constraints.\n10For balanced sampling, we create bins along the MOS\ndimension and sample an equal number of data from each bin.\nAll the examples are either MC orSCtype.Model Size P(CSC|D)⇑P(CMC|D)⇑\nCode Llama7B 59.4 40.6\n34B 52.5 47.5\nDeepSeekCoder6.7B 54.8 45.2\n33B 60.3 39.7\nTable 4: Win rates (%) for SCvs.MC generation. We\nfind a bias in LLMs towards generating SCover MC.\n6.2 Do LLMs Prefer Modular Code? The minimal performance gap between (T)MC and\n(T)SC suggests that LLMs may not have a strong\npreference for generating modular code. To ver-\nify this hypothesis, we experiment to compare the\nlikelihood of LLMs generating modular versus non-\nmodular solutions for the same problem. Formally,\nthe normalized probability of generating a code\nsnippet Cgiven a problem description Dis:\nP(C|D) =1\nnn−1Y\nt=0P(xt+1|D, x≤t),\nwhere C, consisting of tokens x1, . . . , x n, belongs to either MC(CMC) orSC(CSC). We sample nearly 9,000 problems from CodeContests containing bothCMCandCSC. We then compare P(CMC|D) andP(CSC|D)to identify which kind of code is preferred more frequently by LLMs. Table 4 supports our theory, highlighting a preference for SCby LLMs over MC. Our findings align with Le et al. (2024) and Li et al. (2024), who observed a performance drop when using (naïvely defined) MC demonstrations for encouraging modular code generation. The analysis implies limitations in LLMs’ ability to consistently generate well-modularized code, although they might have been exposed to such code during training. 7 Conclusion In this work, we propose a metric, called MOS, for quantifying the modularity of code snippets and evaluate its impact on performance. Our evaluation reveals no significant correlation, or even a possible weak negative correlation, between modularity and performance. This suggests that factors influencing the usefulness of code examples may differ between human and LLM perspectives. Exploring the influence of other code properties beyond modularity is a promising direction for future work. 4\nLimitations Due to limited computational resources, we focused on designing focused yet generalizable experimental settings. This limited the scope of our investigation, but considering more extensive configurations, such as fine-tuning, employing much larger models, and evaluating other programming languages, in future work will help validate and potentially broaden the applicability of our findings. Despite these limitations, we believe our findings offer valuable insights due to our comprehensive exploration of the feasible configurations within the available resources. Furthermore, identifying a core factor besides modularity that directly affects performance holds significant promise for improving code generation. 261",
        "aspect_review": "The paper introduces a novel metric for quantifying code modularity, challenging traditional assumptions about the role of modularity in code generation by LLMs. The study conducts comprehensive experiments with large models (33B and 34B parameters) and diverse datasets, providing robust insights into the impact of modularity. Contrary to conventional wisdom, the results reveal that modularity is not a significant factor in enhancing LLM performance, offering a fresh perspective on code generation. The paper provides a thorough analysis of the modularity-performance relationship, including correlation studies and preference evaluations, which enriches the understanding of LLM behavior. The methodology is rigorous, employing advanced techniques like control flow graphs and Cyclomatic Complexity to derive the modularity score, ensuring a sound experimental setup.\n\n1. Minor details on hyperparameters used in experiments could be clarified to enhance reproducibility. 2. The paper could benefit from additional analysis on the impact of modularity in different programming languages. 3. Some minor writing improvements could enhance clarity, particularly in sections explaining the transformation process. 4. Slight clarification on the availability of the code and data used for experiments would be beneficial.",
        "general_review": "1. Notable Novelty: The paper introduces a novel metric for quantifying code modularity, challenging conventional wisdom about its impact on code generation performance. 2. Strong Technical Depth and Rigor: The study conducts extensive experiments with large models, providing a thorough investigation of the relationship between modularity and performance. 3. Comprehensive Experimental Validation: The experiments cover multiple datasets and model sizes, offering a robust evaluation of the proposed metric and its implications. 4. Significant Practical Impact: The findings have potential implications for the development of coding assistants, questioning the traditional emphasis on modularity in code generation.\n\n1. Minor Writing or Clarity Improvements: Some sections could be clearer in explaining the transformation process of code snippets. 2. Small Details on Hyperparameters or Implementation: The paper could benefit from additional details on hyperparameter settings used in experiments. 3. Very Minor Formatting on Figures: Some figures could be formatted more clearly for better readability. 4. Slight Clarification on Code Availability: While the authors mention open-sourcing the code, clearer details on the timeline and platform for release would be helpful."
    },
    "14": {
        "paper": "Explaining Mixtures of Sources in News Articles\nAnonymous ACL submission\nAbstract\nHuman writers plan, then write (Yao et al., 2019). For large language models (LLMs) to play a role in longer-form article generation, we must understand the planning steps humans make before writing. We explore one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. We ask: why do specific stories call for specific kinds of sources? We imagine a process where sources are selected to fall into different categories. Learning the article’s plan means predicting the categorization scheme chosen by the journalist. Inspired by latent-variable modeling, we first develop metrics to select the most likely plan underlying a story. Then, working with professional journalists, we adapt five existing approaches to planning and introduce three new ones. We find that two approaches, or schemas: stance (Hardalov et al., 2021) and social affiliation best explain source plans in most documents. However, other schemas like textual entailment explain source plans in factually rich topics like “Science”. Finally, we find we can predict the most suitable schema given just the article’s headline with reasonable accuracy. We see this as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans, like discourse or plot-oriented plans. We release a corpora, NewsSources , with schema annotations for 4M articles, for further study. 1 Introduction As language models (LMs) become more proficient at long-form text generation and incorporate resources (Lewis et al., 2020) and tools (Schick et al., 2023) to support their writing, recent work has shown that planning before writing is essential (LeCun, 2022; Spangher et al., 2023a; Park et al., 2023). However, supervised datasets to support learning and studying plans are few: they are difficult or expensive to collect, synthetic, or narrowly tailored to specific domains (Zhou et al., 2023). 043Headline: NJ Schools Teach Climate\nChange at all Grade Levels\nMichelle Liwacz asked her first graders:\nwhat can penguins do to adapt to a warming\nEarth? ←potential labels: Academic, Neutral\nGabi , 7, said a few could live inside her\nfridge. ←potential labels: Unaffiliated, Neutral\nTammy Murphy , wife Governor Murphy,\nsaid climate change education was vital to help\nstudents. ←poten. labels: Government, Agree\nCritics said young kids shouldn’t learn dis-\nputed science. ←labels: Unaffiliated, Refute\nApoll found that 70 percent of state resi-\ndents supported climate change being taught\nat schools. ←potential labels: Media,Agree\nTable 1: Informational sources synthesized in a single\nnews article. How would we choose sources to tell\nthis story? We show two different explanations, given\nby two competing schema: affiliation and stance. Our\ncentral questions: (1) Which schema best explains the\nsources used in this story? (2)Can we predict, given a\ntopic sentence, which schema to use?\nOne approach to collecting large amounts of diverse planning data is to observe natural scenarios in which planning has already occurred. In this work, we consider one such real-world scenario: source selection by human journalists. Consider the article shown in Table 1. The author shares her plan1: NJ schools are teaching climate change in elementary school. We wanted to understand: how are teachers educating children? How do parents andkids feel? Is there pushback ? As can be seen, the journalist planned, before writing, the different kinds of sources (e.g. teachers, 1Plan: https://nyti.ms/3Tay92f [paraphrased].\nFinal article: https://nyti.ms/486I11u , see Table 1.\n1\nFigure 1: We seek to infer unobserved plans in natural data, focusing on one scenario: source-selection made\nby human journalists during news writing. Although the reasons why sources are chosen are unobservable, we\nshow that one explanation (in the diagram, represented by squares :{\n ,\n ,\n}), is preferred over another\n(represented by circles :{\n ,\n ,\n}) if it better predicts the observed text ( conditional perplexity ) and the\nexplanation is more internally consistent ( posterior predictive ).\nkids) she wished to use. Why did she choose these groups? Was it: A. to include varied social groups? B. to capture different sides of an issue? Answering this question, we argue, allows us to infer why she chose each source. If the answer is A, we can infer, then, that the writer probably chose her sources because each fell into a different social group. If the answer is B, the sources were more likely chosen because each agreed or disagreed with the main event. Table 1 shows this duality. Establishing P(A)> P(B)means we can better infer why each source was used, allowing us to collect plans from natural text data. Now, the core problem in this endeavor emerges: a document’s plan is not typically observable. We directly address this and show that we can differentiate between plans in naturally observed text . Inspired by latent variable modeling approaches (Airoldi and Bischof, 2016), we uncover a document’s most likely plan on the following basis: a proposed plan betterdescribes adocument’s actual plan ifitgives more information about the completed document . We introduce simple metrics for this goal: conditional perplexity and posterior predictive likelihood, in Figure 1 (Section 2.2). Next, to create a straightforward setting to demonstrate the power of these metrics, wework with professional journalistsfrom multiplemajor news organizations to identify planning approaches they regularly take. We operationalize these approaches as schema, or explanatory frameworks under which each source in the news article is assigned to a different discrete category (e.g. in theaffiliation schema, for example, the sourcecategories would be Government ,Media ...). We 092adapt five schema from parallel tasks and introduce three novel schemas to better describe sourcing criterion. We implement our schemas by annotating over 600 news articles with 4,922 sources and training supervised classifiers. We validate our approach with these journalists: they deem the plans we infer as correct with > .74F1 score. Finally, the choice of schema, we find, can be predicted with moderate accuracy using only the headline of the article (ROC=.67), opening the door to new computational journalism tooling. In sum, our contributions are threefold: •We frame source-type planning as a lens through which to study planning in writing. •We collect 8 different plan descriptions, or schemas (5 existing and 3 we develop with professional journalists ). We build a pipeline to extract sources from 4 million news articles and categorize them, building a large public dataset called NewsSources . •We introduce two novel metrics: conditional perplexity andposterior predictive to compare plans. We find that different plans are optimal for different topics. Further, we show that the right plan can be predicted with .67 ROC given just the headline. With this work, we hope to inspire further unsupervised inferences in document generation. Studying journalistic decision-making is important for understanding our information ecosystem (Winter and Kr ¨amer, 2014; Manninen, 2017; DeButts and Pan, 2024), can lead to important computational journalism tools (Quinonez and Meij, 2024) and presents a real-world case-study in planning. 2\n2 Source Categorization 2.1 Problem Statement Ourcentralquestionis:why didthewriter select sources s1, s2, s3...fordocument d? Intuitively, let’s say we read an article on a controversial topic. Let’s suppose we observe that it contains many opposing viewpoints: some sources in the article “agree” with the main topic and others “disagree”. We can conclude that the writer probably chose sources on the basis of their stance (Hardalov et al., 2021) (or their opinion-based support) rather than another explanation, like their discourse role (which describes their narrative function). More abstractly, we describe source-selection as a generative process: first, journalists plan how they will choose sources (i.e. the setofkcategories sources will fall into), then they choose sources, each falling into 1-of- kcategories. Different plans, or categorizations, are possible (e.g. see Figure 1): the“right” plan istheonethatbest predicts the finaldocument. Each plan, or categorizations, is specified by a schema . For the 8 schemas used in this work, see Figure 2. To apply a schema to a document, we frame an approach consisting of two components: (1) an attribution function, a: a(s) =q∈Qdfors∈d (1) introduced in Spangher et al. (2023b), which maps each sentence sin document dto a source Qd= {q(d)\n1, ...q(d)\nk}2and (2) a classifier, c: cZ(s(q)\n1, ...s(q)\nn) =z∈Z (2) which takes as input a sequence of sentences attributed to source q(d)and assigns a type z∈Zfor schema Z. This supervised framing is not typical in latentvariable settings; the choice of zand the meaning ofZare typically jointly learned without supervision. However, learned latent spaces often do not correspond well to theoretical schemas (Chang et al., 2009), and supervision has been shown to be helpful with planning (Wei et al., 2022). On the other hand, supervised models trained on different schema are challenging to compare, especially when different architectures are optimal for each schema. A latent-variable framework here is ideal: 2These sources are referenced in d. There is no considera-\ntion of document-independent sources.comparing different graphical models (Bamman et al., 2013; Bamman and Smith, 2014) necessitates comparing different schemas, as each run of a latent variable model produces a different schema. 2.2 Comparing Plans , or Schemas We can compare plans in two ways: (1) how well do they explain each observed document? and (2) how structurally consistent are they? Explainability A primary criterion for a plan is for it to explain the observed data well. To measure this, we use conditional perplexity3p(x|z) (3) which measures the uncertainty of observed data, x, given a latent structure, z. Measuring p(x|z) for different z(fixing x) allows us to compare z. Conditional perplexity is a novel metric we introduce, inspired by metrics to evaluate latent unsupervised models, like the “left-to-right” algorithm introduced by (Airoldi and Bischof, 2016).4Structural Likelihood: A second basic criterion for a latent structure to be useful is for it be consistent, which is a predicate for learnability. We assess the consistency of a set of assignments, z, by calculating the posterior predictive : p(z|z−, x) (4) Deng et al. (2022) exploring using full joint distribution, p(z),latent perplexity , to evaluate the structure text xproduced by generative language models (“ model criticism ”). We simplify using the full distribution and instead evaluate the conditional predictive to study document structure. This, we find in early experiments, is easier to learn and thus helps us differentiate different Zbetter (“ schema criticism ”).5Now, we describe our schemas. For an illustration of each metric, please refer to Figure 1. The overall goal of the metrics is to determine which schema, or labeling of sources, best explains the observed news article . As the 3We abuse notation here, using pas both probability and\nperplexity: p(x) = exp {−Elogp(xi|x<i)}.\n4We note that the term, conditional perplexity , was origi-\nnally introduced by Zhou and Lua (1998) to compare machine-\ntranslation pairs. In their case, both xandzare observable; as\nsuch, they do not evaluate latent structures, and their usage is\nnot comparable to ours.\n5Our work is inspired by Spangher et al. (2023b)’s work,\nwhere zwas the choice of specific action, rather than a general\naction-type.\n3\nFigure 2: Label-sets for source-planning schemas. ExtrinsicSource Schemas Affiliation, Role and Retrieval-\nmethod (Spangher et al., 2023b) capture characteristics of sources extrinsic to their usage in the document. Func -\ntional Source Schemas: Argumentation (Al Khatib et al., 2016), Discourse (Choubey et al., 2020) and Identity\ncapture functional narrative role of sources. Debate-Oriented Schemas : Natural Language Inference (NLI) (Dagan\net al., 2005) and Stance (Hardalov et al., 2021) capture the role of sources in encompassing multiple sides. The\nthree novel schemas we introduce are shown with borders: Affil., Identity and Role. For definitions, see App. D.\nfigure shows, if schema A describes an article better than schema B, then labels assigned to each source under schema A (e.g. in Figure 1: squares,\n ,\n ,\n ) will outperform labels assigned under Schema B (e.g. circles,\n ,\n ,\n ). 2.3 Source Schemas Our schemas, or descriptions of plans, are shown in Figure 2. In this work, we collect 8 schemas including three we introduce: Identity,Affiliation andRole. Each schema provides a set of categories describing the sources used in a news article. See Appendex D for more details and definitions for each schema. We note that none of these schemas are complete and that real-world plans likely have elements outside of any one schema (or are combinations of multiple schema). However, this demonstration is important, we argue, to prove that we candifferentiate between purely latent plans in long-form text. We now introduce each schema: Debate-Oriented Schemas Both the Stance and NLI schemas are debate-orienced schemas. They each capture the relation between the information a source provides and the main idea of the article. NLI(Dagan et al., 2005) captures factual relations between text, while Stance (Hardalov et al., 2021) captures opinion-based relations . A text pair may be factually consistent and thus be classified as “Entailment” under a NLIschema, but express different opinions and be classified as “Refute” under Stance . These schemas say a writer uses sources for the purpose of expanding or rebutting information in the narrative. 242Schema Macro-F1 Schema Macro-F1\nArgumentation 68.3 Retrieval 61.3\nNLI 55.2 Identity 67.2\nStance 57.1 Affiliation 53.3\nDiscourse 56.1 Role 58.1\nTable 2: Classification f1 score, macro-averaged, for the\n8 schemas. We achieve moderate classification scores\nfor each of schema. In Section 2, when we compare\nschemas, we account for classification acc. differences\nby introducing noise to higher-performing classifiers.\nFunctional Source Schemas The following schemas: Argumentation ,Discourse andIdentity all capture the role a source plays in the overall narrative construction of the article. For instance, a source might provide a “Statistic” for a wellformed argument ( Argumentation (Al Khatib et al., 2016)), or “Background” for a reader to help contextualize ( Discourse (Choubey et al., 2020)). Identity, a novel schema, captures how the reader identifies the source. For example, a “Named Individual” is identifiable to a reader, whereas an “Unnamed Individual” is not. As identified in Sullivan (2016) and our journalist collaborators, this can be a strategic planning choice: some articles are about sensitive topics and need unnamed sources. Extrinsic Source Schemas Affiliation ,Role and Retrieval schemas serve to characterize attributes of sources external to the news article. Stories often implicate social groups (McLean et al., 2019), such as “academia” or “government.” Those group identities are extrinsic to the story’s architecture but important for the selection of sources. Sources may be selected because they represent a group 4\n(i.e. Affiliation ) or because their group position is important within the story’s narrative (e.g. “participants” in the events, i.e. Role).Retrieval , introduced by Spangher et al. (2023b), captures the channel through which the information was found. Although these schema are news-focused, we challenge the reader to imagine ones that might exist in other fields. For instance, a machine learning article might compare models selected via, say, a Community schema: each from open-source ,academic andindustry research communities. 3 Building a Silver-Standard Dataset of Different Possible Plans The schemas described in the previous section give us theoretical frameworks for identifying writers’ plans. To compare plans and selecttheplan that best describes adocument , we must first create a dataset where informational sources are labeled according to each schema . 3.1 Dataset Construction and Annotation We obtain the NewsEdits dataset (Spangher et al., 2022), which consists of 4 million news articles, and extract sources using a methodology developed by Spangher et al. (2023b), which authors established was state-of-the-art for this task. This dataset spans 12 different news sources (e.g. BBC, NYTimes, etc.) over a period of 15 years (2006-2021). For our experiments, we sample 90,000news articles that are long and contain more than 3sources (on average, the articles contain ∼7.5sources). We annotate sources under each of our new schemas. We recruited two annotators, one an undergraduate and the other a former journalist. The former journalist trained the undergraduate for 1 month to identify and label sources, then, they independently labeled 425 sources in 50 articles with each schema to calculate agreement, scoring κ=.63, .76, .84onAffiliation ,Role andIdentity labels. They then labeled 4,922 sources in 600 articles with each schema, labeling roughly equal amounts. Finally, they jointly labeled 100sources in25documents with the other schemas for evaluation data over 1 month, with κ≥.54,all in the range of moderate to substantial agreement (Landis and Koch, 1977). 3.2 Training Classifiers to Label Sources We train classifiers to label sources under each schema. Unless specified, we use a sequence clas- 313sifier using RoBERTa-base with self-attention pooling, as in Spangher et al. (2021a). We deliberately chose smaller models to scale to large amounts of articles. We will open-source all of the classifiers trained in this paper. Affiliation ,Role,Identity We use our annotations to train classifiers which take as input all sentences attributable to source qand output a category in each schema, or p(t|s(q)\n1⊕...⊕s(q)\nn). Argumentation ,Retrieval ,Discourse We use datasets, without modification, that were directly released by the authors. Each is labeled on a sentencelevel, on news and opinion datasets. We train classifiers to label each sentence of the news article, s. Then, for each source q, we assign a single label, y, with the most mutual information6across sentences attributed to that source, s(q)\n1, ...s(q)\nn. NLI, Stance We use an NLI classifier trained by Williams et al. (2022) to label each sentence attributed to source qas a separate hypothesis, and the article’s headline as the premise. We use mutual information to assign a single label. We create a stance training dataset by aggregating several news-focused stance datasets7. We then fine-tune GPT3.5-turbo8to label news data and label 60,000 news articles. We distill a T5model with this data (Table 2 shows T5’s performance). 3.3 Classification Results As shown in Table 2, we model schemas within a range of f1-scores ∈(53.3,67.2), showing moderate success in learning each schema9. These scores are middle-range and likely not useful on their own; we would certainly have achieved higher scores with more state-of-the-art methods. However, we note these classifiers arebeingused forcomparative, explana torypurposes, sotheir efficacy liesin how well they help uscompare plans , as we will explore in the next section. 6arg max yp(y|q)/p(y))\n7FNC-1 (Pomerleau and Rao, 2017), Perspectrum (Chen\net al., 2019), ARC (Habernal et al., 2017), Emergent (Ferreira\nand Vlachos, 2016) and NewsClaims (Reddy et al., 2021). We\nfilter these sets to include premises and hypothesis ≥10 words\nand≤2 sentences.\n8We use OpenAI’s GPT3.5-turbo fine-tuning endpoint, as\nof November 16, 2023.\n9When using these classifier outputs for evaluating plans,\nin the next section, we introduce noise (i.e. random label-\nswapping), so that all have the same accuracy.\n5\nConditional Perplexity p(x|z) Posterior Predictive p(ˆz|z−, x)\nSchema n PPL ∆base-k ( ↓)∆base-r ( ↓) F1 ÷base-k ( ↑)÷base-r ( ↑)\nNLI 3 22.8 0.62 -0.08 58.0 1.02** 1.01 **\nStance 4 21.5 -1.71 -3.21** 39.1 0.88** 0.83 **\nRole 4 22.3 -0.06 -0.33** 38.7 1.11** 1.10 **\nIdentity 6 21.8 -0.42 -0.94 25.0 1.00 1.15 **\nArgumentation 6 21.7 -0.52 -1.04 30.7 1.10 ** 1.12 **\nDiscourse 8 22.3 0.54 -0.75 19.2 1.06 ** 1.08 **\nRetrieval 10 23.7 1.47 0.36 15.8 1.10 ** 1.12 **\nAffiliation 14 20.5 -2.11** -3.04** 10.5 1.26 ** 1.16 **\nTable 3: Comparing our schemas against each other. In the first set of experiments, we show conditional perplexity\nresults, which tell us how well each schema explains the document text. Shown is PPL (the mean perplexity per\nschema), ∆kmeans (PPL - avg. perplexity of kmeans) and ∆random (PPL - avg. perplexity of the random trial).\nStatistical significance ( p < .05) via a t-test calculated over perplexity values is shown via **.In the second set of\nexperiments, we show posterior predictive results, measured via micro F1-score. We show F1 (f1-score per schema),\n÷kmeans (F1 / f1-score of kmeans), ÷random (F1 / f1-score of random trial). Statistical significance ( p < .05) via\nat-test calculated over 500-sample bootstrapped f1-scores is shown via **.\n4 Comparing Schemas We are now ready to explore how well these schemas explain source selection in documents. We start by describing our experiments, then baselines, and finally results. All experiments in this section are based on the 90,000news articles filtered from NewsEdits, labeled as described in the previous section. We split 80,000/10,000train/eval. 4.1 Implementing Planning Metrics We now describe how we implement the metrics introduced in Section 2.2: (1) conditional perplexity and (2) posterior predictive . Conditional Perplexity To measure conditional perplexity ,p(x|z), we fine-tune GPT2-base models (Radford et al., 2019) to take in it’s prompt a sequence of latent variables, each for a different source, and then assess likelihood of the observed article text .10This is similar to measuring vanilla perplexity on observed text, except: (1) we provide latent variables as conditioning (2) by fixing the model used and varying the labels, wearemeasuringthesignalgiven byeach setofdifferentlabels. Our template for GPT2 is: ⟨h⟩h⟨l⟩(1) l1(2) l2...⟨t⟩ (1) s(q1)\n1...s(q1)\nn(2)... Red is the prompt, or conditioning, and green is the text over which we calculate perplexity. <tokens> (e.g. “(1)”, “ ⟨text⟩”) are structural 10We note that this formulation has overlaps with recent\nwork seeking to learn latent plans (Deng et al., 2022; Wang\net al., 2023; Wei et al., 2022).markers while variables l, h, s are article-specific. his the headline, liis the label for source iand s(q1)\n1...s(q1)\nnare the sentences attributable to source i.WedonotuseGPT2 forgeneration, butforcomparativepurposes, tocompare thelikelihood of observed articletextundereach schema. We note that this implements Eq. 3 only if we assuming green preserves the meaning of x, the article text. Our data processing (Section 3.1), based on highaccuracy source-extraction models (Spangher et al., 2023b), gives us confidence in this.11Posterior Predictive To learn the posterior predictive (Equation 4), we train a BERT-based classification model (Devlin et al., 2018) to take the article’s headline and a sequence of source-types with a one randomly held out . We then seek to predictthatsource-type, and evaluate using F1-score. Additionally, we follow Spangher et al. (2023b)’s observation that some sources are more important (i.e. have more information attributed). We model the posterior predictive among the 4 sources per article with the most sentences attributed to them. 4.2 Baselines Vanilla perplexity does not always provide accurate model comparisons (Meister and Cotterell, 2021; Oh et al., 2022) because it can be affected by irrele11Initial experiments show that text markers are essential\nfor the model to learn structural cues. However, they also\nprovide their own signal (e.g. on the number of sources). To\nreduce the effects of these artifacts, we use a technique called\nnegative prompting (Sanchez et al., 2023). Specifically, we\ncalculate perplexity on the altered logits, Pγ=γlogp(x|z)−\n(1−γ) logp(x|ˆz), where ˆzis a shuffled version of the latent\nvariables. Since textual markers remain the same in the prompt\nforzandˆz, this removes markers’ predictive power.\n6\nvant factors, like tokenization scheme. We hypothesized that the dimensionality of each schema’s latent space might also have an effect (Lu et al., 2017); larger latent spaces tend to assign lower probabilities to each point. Thus, we benchmark each schema against baselines with similar latent dimensions. Base-r, or Random baseline . We generate k unique identifiers12, and randomly assign one to each source in each document. kis set to match the number of labels in the schema being compared to. Base-k, or Kmeans baseline . We first embed sources as paragraph-embeddings using Sentence BERT (Reimers and Gurevych, 2019)13Then, we cluster all sources across documents into kclusters using the kmeans algorithm (Likas et al., 2003), where kis set to match the number of labels in the schema being compared to. We assign each source it’s cluster number. 4.3 Results and Discussion As shown in Table 3, the supervised schemas mostly have have lower conditional perplexity than their random and unsupervised kmeans baselines. However, only the Stance ,Affiliation and Role schemas improve significantly (at p < . 001), and theRole schema’s performance increase is minor. Retrieval has a statistically significant less explainability relative to it’s baselines. There is a simple reason for why some schemas have either the same or more conditional perplexity compared to their baselines: they lack explainability over the text of the document, but are not random and thus might lead to overfitting. We examine examples and find that Retrieval does not impact wording as expected: writers make efforts to convey information similarly whether it was obtained via a quote, document or a statement. We face a dilemma: in generating these schemas, we chose Retrieval because we assumed it was an important planning criterion. However, our results indicate that it holds little explanatory power. Is it possible that some plans do not get reflected in the text of the document? To address this question, we assign ˆZ= arg min Zp(x|z), the schema for each datapoint with the lowest perplexity, using scores calculated 12Using MD5 hashes, from python’s uuid library.\n13Specifically, microsoft/mpnet-base ’s model\nhttps://www.sbert.net/docs/pretrained_mo\ndels.html .in the prior section14, we calculate the lowestperplexity schema. Table 5 shows the distribution of such articles. We then task 2 expert journalists with assigning their ownguess about which schema best describes the planning for the particular article, for 120 articles. We observe an F1-score of 74, indicating a high degree of agreement. Interestingly, we also observe statistically significant improvements of kmeans over random baselines in all cases (except k= 3). In general, our baselines have lower variance in perplexity values than experimental schemas. This is not unexpected: as we will explore in the next section, we expect that some schemas will best explain some articles, resulting in a greater range in performance. For more detailed comparisons, see Appendix B. Posterior predictive results generally show improvement across trials, with the Affiliation trial showing the highest improvement over both baselines. This indicates that most tagsets are, to some degree, internally consistent and predictable. Stance is the only exception, showing significantly lower f1 than even random baselines. This indicates that, although Stance is able to explain observed documents well (as observed by it’s impact on conditional perplexity), it’s not always predictable how it will applied. Perhaps this is indicative that writers do not know a-priori what sources will agree or disagree on any given topic before talking to them, and writers do not always actively seek out opposing sides. Finally, as another baseline, we implemented latent variable model. In initial experiments, it does not perform well. We show in Appendex G that the latent space learned by the model is sensible. Bayesian models are attractive for their ability to encode prior belief, and ideally they would make good baselines for a task like this, which interrogates latent structure. However, more work is needed to better align them to modern deeplearning baselines. 5 Predicting Schemas Taken together, our observations from (1) Section 3.3) indicate that schemas are largely unrelated and (2) Section 4.3 indicate that Stance andAffiliationboth have similar explanatory power (although Stance is less predictable). We next ask: which kinds of articles are better explained by one schema, and which are better explained by the other? 14across the dataset used for validation, or 5,000 articles\n7\nStance Affiliation\nBush, George W Freedom of Speech\nSwift, Taylor 2020 Pres. Election\nData-Mining Jazz\nArtificial Intelligence Ships and Shipping\nRumors/Misinfo. United States Military\nIllegal Immigration Culture (Arts)\nSocial Media Mississippi\nTable 4: Top keywords associated with articles favored\nby stance or affiliation. Keywords are manually assigned\nby news editors\nIn Table 4, we show topics that have low perplexity under the Stance schema, compared with the Affiliation schema (we calculate these by aggregating document-level perplexity across keywords assigned to each document in our dataset). As we can see, topics requiring greater degrees of debate, like “Artificial Intelligence”, and “Taylor Swift” are favored under the Stance Topic, while broader topics requiring many different social perspectives, like “Culture” and “Freedom of Speech” are favored underAffiliation . We set up an experiment where we try to predict ˆZ= arg min Zp(x|z), the schema for each datapoint with the lowest perplexity. We downsample until assigned schemas, per articles, are balanced and train a simple linear classifier15to predict ˆZ. We get .67 ROC-AUC (or .23 f1-score). These results are tantalizing and offer the prospect of being able to better plan source retrieval in computational journalism tools, by helping decide an axis on which to seek different sources. More work is needed to validate these results. 6 Related Work Latent Variable Persona Modeling Our work is inspired by earlier work in persona-type latent variable modeling (Bamman et al., 2013; Card et al., 2016; Spangher et al., 2021b). Authors model characters in text as mixtures of topics. We both seek to learn and reason about about latent charactertypes, but their line of work takes an unsupervised approach. We show that supervised schemas outperform unsupervised. Multi-Document Retrieval In multiple settings – e.g. multi-document QA (Pereira et al., 2023), multi-document summarization (Shapira et al., 2021), retrieval-augmented generation (Lewis et al., 2020) – information from a single source is assumed to be insufficient to meet a user’s needs. In 15Bag-of-words with logistic regressionAffiliation 41.7% Argument. 1.2%\nIdentity 22.7% Discourse 1.1%\nStance 17.7% NLI 1.1%\nRole 13.4% Retrieval 1.1%\nTable 5: Proportion of our validation dataset favored by\none schema, i.e. ˆZ= arg max Zp(x|z)\ntypical information retrieval settings, the goal is to retrieve a single document closest to the query (Page et al., 1998). In settings where multiple sources areneeded , on the other hand, retrieval goals are not clearly understood16. Our work attempts to clarify this, and can be seen as a step towards better retrieval planning. Planning in Language Models Along the line of the previous point, chain-of-thought reasoning (Wei et al., 2022) and few-shot prompting, summarized in (Sanchez et al., 2023), can be seen as latent-variable processes. Indeed, work in this vein is exploring latent-variable modeling for shot selection (). Our work, in particular the conditional perplexity formulation and it’s implementation, can be seen as a way of comparing different chain-ofthought plans as they relate to document planning. Computational Journalism seeks to apply computational techniques to assist journalists in reporting. Researchers have sought to improve detection of incongruent information (Chesney et al., 2017), detect misinformation (Pisarevskaya, 2017) and false claims made in news articles (Adair et al., 2017). 7 Conclusions In conclusion, we explore ways of thinking about sourcing in human writing. We compare 8 schemas of source categorization, and adapt novel ways of comparing them. We find, overall, that affiliation andstance schemas help explain sourcing the best, and we can predict which is most useful with moderate accuracy. Our work lays the ground work for a larger discussion of discovering plans made by humans in naturally generated documents. It also takes us steps towards tools that might be useful to journalists. Naturally, our work is a simplification of the real human processes guiding source selection; these categories are non-exclusive and inexhaustive. We hope by framing these problems we can spur further research in this area. 16As Pereira et al. (2023) states, “retrievers are the main\nbottleneck” for well-performing multi-document systems.\n8\n8 Limitations A central limitation to our work is that the datasets we used to train our models are all in English. As mentioned previously, we used English language sources from Spangher et al. (2022)’s NewsEditsdataset, which consists of sources such as nytimes.com, bbc.com, washingtonpost.com, etc. Thus, we must view our work with the important caveat that non-Western news outlets may not follow the same source-usage patterns and discourse structures in writing their news articles as outlets from other regions. We might face extraction and labeling biases if we were to attempt to do such work in other languages. Another limitation of our work is that we only considered 8 supervised schemas. While we worked closely with journalists to develop these schemas and attempted to make them as comprehensive and useful as possible, it’s entirely possible, in fact probable, that these 8 schemas do not describe sources that well. As mentioned in the main body, we fully anticipate that more work needs to be done to determine further, more optimal schemas. And it’s likely, ultimately, that unsupervised approaches to developing more nuanced plans are desirable. Furthermore, the metrics we evaluated are approximate and depend on schemas learned by ML models. Both of these facts could incentivize biased models. However, we attempted to mitigate this by conducting an experiment afterwards with journalists to blindly label articles. Our annotation approach was done only two annotators, in a master-apprentice style and hence might be skewed in distribution. However, because the master was an experienced journalist with many years of newsroom experience at a major newsroom, we took their tagging to be gold-standard. 9 Ethics Statement 9.1 Risks Since we constructed our datasets on well-trusted news outlets, we assumed that every informational sentence was factual, to the best of the journalist’s ability, and honestly constructed. We have no guarantees that our classification systems would work in a setting where a journalist was acting adversarially. There is a risk that, if planning works and natural language generation works advance, it could fuel 625actors that wish to use it to plan misinformation and propaganda. Any step towards making generated news article more human-like risks us being less able to detect and stop them. Misinformation is not new to our media ecosystem, (Boyd et al., 2018; Spangher et al., 2020). We have not experimented how our classifiers would function in such a domain. There is work using discourse-structure to identify misinformation (Abbas, 2022; ?), and this could be useful in a source-attribution pipeline to mitigate such risks. We used OpenAI Finetuning to train the GPT3 variants. We recognize that OpenAI is not transparent about its training process, and this might reduce the reproducibility of our process. We also recognize that OpenAI owns the models we fine-tuned, and thus we cannot release them publicly. Both of these thrusts are anti-science and anti-openness and we disagree with them on principle. We tried where possible to train open-sourced versions, as mentioned in the text. 9.2 Licensing The dataset we used, NewsEdits (Spangher et al., 2022), is released academically. Authors claim that they received permission from the publishers to release their dataset, and it was published as a dataset resource in NAACL 2023. We have had lawyers at a major media company ascertain that this dataset was low risk for copyright infringement. 9.3 Computational Resources The experiments in our paper required computational resources. We used 64 12GB NVIDIA 2080 GPUs. We designed all our models to run on 1 GPU, so they did not need to utilize model or dataparallelism. However, we still need to recognize that not all researchers have access to this type of equipment. We used Huggingface models for our predictive tasks, and will release the code of all the custom architectures that we constructed. Our models do not exceed 300 million parameters. 9.4 Annotators We recruited annotators from our educational institutions. They consented to the experiment in exchange for mentoring and acknowledgement in the final paper. One is an undergraduate student, and the other is a former journalist. Both annotators are male. Both identify as cis-gender. The annotation conducted for this work was deemed 9\nexempt from review by our Institutional Review Board. 676",
        "aspect_review": "The paper conducts comprehensive experimental validation with professional journalists, adapting five existing approaches to planning and introducing three new ones. The validation shows that two schemas, stance and social affiliation, best explain source plans in most documents, demonstrating strong technical depth and rigor. The paper introduces a significant practical impact by releasing a large public dataset, NewsSources, with schema annotations for 4 million articles. This dataset provides a valuable resource for further study in computational journalism and long-form text generation.\n\nThe paper could benefit from minor writing or clarity improvements, particularly in the explanation of complex methodologies and schema comparisons. There are small details on hyperparameters or implementation that are not fully detailed, which could aid in reproducibility and understanding of the experiments. While the dataset is extensive, the paper only considers 8 supervised schemas, which might not fully encapsulate the diversity of source selection strategies in journalism. The interpretation of results, particularly in the comparison of schemas, could be expanded to provide deeper insights into the implications of the findings.",
        "general_review": "This paper presents a novel approach to understanding source-selection planning in news article generation, which is crucial for enhancing the capabilities of large language models in long-form text generation. The authors introduce innovative metrics such as conditional perplexity and posterior predictive likelihood to evaluate latent plans, showcasing strong technical depth and rigor. The comprehensive experimental validation, including collaboration with professional journalists and the creation of a large annotated dataset, further strengthens the study. Additionally, the paper has significant practical impact by providing a framework for computational journalism tools and releasing a valuable dataset for future research.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in explaining some technical concepts to ensure broader accessibility. 2. There are small details regarding hyperparameters and implementation that could be elaborated for reproducibility. 3. Some figures might require very minor formatting adjustments to enhance readability. 4. Slight clarification on the availability and accessibility of the code and trained models would be beneficial."
    },
    "15": {
        "paper": "Automatically Generated Definitions\nand their utility for Modeling Word Meaning\nAnonymous ACL submission\nAbstract\nModeling lexical semantics is a challenging task, often suffering from interpretability pitfalls. In this paper, we delve into the generation of dictionary-like sense definitions and explore their utility for modeling word meaning. We fine-tuned two Llama models and include an existing T5-based model in our evaluation. Firstly, we evaluate the quality of the generated definitions on existing benchmarks, setting new state-of-the-art results for the Definition Generation task. Next, we explore the use of definitions generated by our models as intermediate representations subsequently encoded as sentence embeddings. We evaluate this approach on lexical semantics tasks such as the Wordin-Context, Word Sense Induction, and Lexical Semantic Change, setting new state-of-the-art results in all three tasks. 1 Introduction Modeling lexical semantics using unstructured text has a longstanding history in Natural Language Processing due to its crucial role in both Natural Language Understanding and Natural Language Generation (Karanikolas et al., 2024; Pustejovsky and Boguraev, 1993). Over the past decades, there have been many relevant technological developments: from count-based (Naseem et al., 2021) to static (Mikolov et al., 2013) and contextualized (Peters et al., 2018) language models, and most recently, generative models (Hadi et al., 2023). Each of these advancements has contributed significantly to the goal of modeling the meaning of words . Modern language models are based on the Transformer (Vaswani et al., 2017) architecture. Given a word, these models generate semantic representations for each occurrence of the word based on its surrounding context (Apidianaki, 2023). Ideally, these representations should be similar for semantically related word usages and different for semantically distinct ones. Typically, contextualized 040vectors (i.e., embeddings, Pilehvar and CamachoCollados, 2021) or lexical substitutes (i.e., bag-ofwords, Arefyev and Zhikov, 2020) are employed to represent word usages. However, recent advancements in text generation are shifting the attention towards representing word usages through generated sense definitions (Giulianelli et al., 2023). Automatically generated sense definitions provide a dual advantage. Firstly, they distill the information stored in a sentence by abstracting away from the context. Their use potentially condenses various word usage representations pertaining to the same underlying meaning. Secondly, generated definitions provide a means to directly interpret word meaning from unstructured text, thereby enabling language models to serve as surrogate for dictionaries when encountering unfamiliar words (Malkin et al., 2021), or known words in unfamiliar settings (Weiland et al., 2023). In this work, we automatically generate definitions for words in-context by relying on two fine-tuned variants of the Llama chat models (Touvron et al., 2023) refined through instruction tuning (Zhang et al., 2024) on lexicographic resources. We call the models LlamaDictionary and assess their performance in Definition Generation, achieving new state-of-the-art results on multiple datasets. We further extend our evaluation by usingLlamaDictionary and the existing Flan-T5 model fine-tuned by Giulianelli et al. (2023) for large scale modeling of word meaning. Specifically, we employ the generated sense definitions as intermediate sense representations. These representations are encoded using a pretrained sequence embedding model rather than using standard token embeddings. We evaluate our approach on three popular Natural Language Processing tasks, namely Word-in-Context, Word Sense Induction, and Lexical Semantic Change, achieving new stateof-the-art results on all three tasks. 1\nOur original contribution: •We introduce LlamaDictionary , a novel finetuned large language model designed to generate sense definitions for words in-context . •We evaluate the use of LlamaDictionary and existing Flan-T5 with thirteen SBERT models, achieving new state-of-the-art results in the Definition Generation task. •We demonstrate the effectiveness of LlamaDictionary andFlan-T5 as a preprocessing tool for large-scale word meaning analysis and achieve state-of-the-art results in the Word-in-Context, Word Sense Induction, and Lexical Semantic Change task. 2 Background and related work 2.1 Word usage representations With the advent of Transformers, we have witnessed the emergence of large language models capable of contextualizing words within diverse contexts. Unlike static models (Pennington et al., 2014), we now rely on a multitude of contextualized embeddings per word. On one hand, this capability represents an invaluable tool for modeling lexical semantics (Petersen and Potts, 2023), as distances between embeddings have proven to be excellent discriminators of word meaning. On the other hand, it poses interpretability challenges, as embeddings tend to represent contextual variance rather than lexicographic senses (Kutuzov et al., 2022). Further challenges arise from the broad and heterogeneous distribution of semantic structure across embedding dimensions (Senel et al., 2018). Lexical substitutes are often employed as alternative representations to raw embeddings (Alagic et al., 2018). These representations consist of sets of automatically generated replacements for specific occurrences of words in-context. Unlike embeddings, lexical substitutes can be directly inspected to infer word meaning. However, the interpretation process requires more time and effort compared to the conventional practice of consulting a dictionary for satisfying meaning definitions. Additionally, interpreting the meaning of a word remains challenging, as lexical substitutes can include stopwords and partial word pieces (Card, 2023), equally plausible alternatives with different meanings (Chiang and Lee, 2023), and even contradictory replacements (Justeson and Katz, 1991). 128With the recent advancements in text generation, automatically generated sense definitions become a viable approach for word usage representation, as these definitions offer descriptive interpretations of words in-context , providing a valuable tool with a level of interpretability comparable to manually curated vocabularies (Gardner et al., 2022). 2.2 Generating word sense definitions Generating word sense definitions has initially gained attention to enhance the interpretability of static embeddings (Mickus et al., 2022; Gadetsky et al., 2018). Originally, the task involved generating a natural language definition given a single embedding of a target word (Noraset et al., 2017). However, since words can carry multiple meanings, advancements in contextualized modeling have shifted the focus to the generation of appropriate sense definitions for words in context (Zhang et al., 2022; Huang et al., 2021; Mickus et al., 2019; Ishiwatari et al., 2019). Generated definitions are useful in a multitude of applications such as the generation of lexicographic resources for low-resource languages (Bear and Cook, 2021), explaining register- or domainspecific vocabulary (Ni and Wang, 2017; August et al., 2022), or language learning scenarios (Zhang et al., 2023; Kong et al., 2022; Yuan et al., 2022). While early works use sequence-to-sequence models for definition modeling (Ni and Wang, 2017; Gadetsky et al., 2018; Mickus et al., 2019), later works utilize pretrained language models such as BART (Bevilacqua et al., 2020; Segonne and Mickus, 2023; Lewis et al., 2020) and T5 (Huang et al., 2021; Tseng et al., 2023; Raffel et al., 2020). More recently, Giulianelli et al. (2023) has proposed using generated definitions as interpretable word usage representation for the analysis of lexical semantic change and provided a new model called Flan-T5 . Inspired by this work, we follow the idea that definitions can be used as interpretable representations and also position our work with a focus on modeling word meaning and meaning change. Inspired by Bevilacqua et al. (2020), we encode definitions as sentence embeddings. However, we model the meaning of words in-context with a single sense definition rather than a set. 3 Automatic definition generation In this work, we fine-tuned two popular opensource generative models through instruction tun2\nWord usage\ndatasetsInstruction\nfine-tuning\nTARGET: target word\nEXAMPLE: example sentence\nDEFINITION : sense glossSYSTEM: You are a lexicographer\nfamiliar with providing concise\ndefinitions of word meanings.\nUSER: Please provide a concise de-\nfinition for the meaning of the\nword \"{TARGET}\"  in the following\nsen-tence: {EXAMPLE}\nASSISTANT : {DEFINITION}WordNet -  Oxford -  Wikitionarymeta-llama/Llama-2-7b-chat-hf\nmeta-llama/Meta-Llama-3-8B-Instruct\nLlamaDictionarydefinition \nSBERT word usage embedding Figure 1: LlamaDictionary is a Llama chat model fine-tuned with lexicographic resources to generate a sense\ndefinition from an input word usage.\ning, namely Llama2chat1and Llama3instruct2. We specifically chose to fine-tune chat models because they were already optimized to generate responses adhering to specific instruction prompts. We call the models resulting from finetuning LlamaDictionary . In the following, we refer to Llama2Dictionary andLlama3Dictionary for the fine-tuned versions of Llama2chat and Llama3instruct, respectively. Using Llama2Dictionary and Llama3Dictionary , we complement the existing Flan-T5 3B model by Giulianelli et al. (2023) with two larger Llama 7B and 8B, chat-based versions. 3.1 Data We fine-tune Llama2chat and Llama3instruct on the same English data used by Giulianelli et al. (2023). The data consists of word usages ⟨w, e, d ⟩, where wrepresents a target word, edenotes an example context where woccurs, and dis a humancurated definition for the lexicographic sense of the word win the example e. The considered word usages span three benchmarks previously extracted from the Oxford English Dictionary (Gadetsky et al., 2018), WordNet (Ishiwatari et al., 2019), andWikitionary (Mickus et al., 2022), respectively. However, while Giulianelli et al. (2023) use all the Train-Dev-Test partitions during fine-tuning, we use only Train and Dev and reserve Test for evaluation purposes. Table 1 reports the main statistics of these benchmarks. 1meta-llama/Llama-2-7b-chat-hf\n2meta-llama/Meta-Llama-3-8B-InstructOxford WordNet Wikitionary Tot.\nTrain # words\n# definitions\n# def. per word33,97,2.957,13,1.7518,31,1.7345,142,3.17\nDev # words\n# definitions\n# def. per word8,12,1.381,1.752,4,1.7711,18,1.59\nTest # words\n# definitions\n# def. per word8,12,1.381,1,1.772,4,1.6911,18,1.57\nTable 1: Train-Dev-Test partitions of the considered\nbenchmarks. For each partition, we report the number\nof unique words, the number of unique definitions, and\nthe average number of definitions per target word.\n3.2 Fine-tuning Llama2chat and Llama3instruct with 7 and 8 billion parameters, respectively, are large, decoder-only architectures trained on publicly available online data, followed by supervised fine-tuning through instruction tuning (Zhang et al., 2024) and iterative refinement using reinforcement learning from human feedback (Kaufmann et al., 2024). We further fine-tuned these models through instruction tuning for sense definition generations. Given the high costs associated with fine-tuning large language models, we employed a parameterefficient fine-tuning (Han et al., 2024) that enables efficient adaptation by only fine-tuning a small number of additional model parameters instead of the entire model. This approach significantly reduces computational and storage costs. Specifically, we fine-tuned using Low-rank Adapter (LoRA, Hu et al., 2021).3Experimented hyper3We have also experimented with Quantization combined\nwith LoRA (QLORA, Dettmers et al., 2023) obtaining very\nsimilar evaluation results (see Figure 4). These are omitted\ndue to space restriction but will be available in our Github\nrepository where we will publish all our code, data, and results.\n3\nparameters are reported in Table 10 and 11. For fine-tuning, we used cross-entropy loss calculated on all tokens over 4 epochs, with a batch size of 32, a maximum sequence length of 512, andpacking to train efficiently on multiple samples simultaneously (Kosec et al., 2021). In line with Huerta-Enochian (2024), who demonstrated that prompt loss can be safely ignored for many datasets, we observed lower preliminary results in the evaluation tasks for models chosen based on validation performance. Therefore, we selected the final model based on the checkpoint at the last training epoch. 3.3 Instruction-tuning We fine-tuned Llama2chat and Llama3instruct using the prompt shown in Figure 1. For each word usage ⟨w, e, d ⟩, we substituted TARGET with the actual target w, and EXAMPLE andDEFINITION with the example eand the definition d, respectively. For our prompt, we drew inspiration from prompts used in previous work, specifically, we employed a prompt similar to those used by Giulianelli et al. (2023). In line with Li et al. (2023), we incorporated an emotional stimulus (in Figure 1, Please ) to enhance the performance. Additionally, similarly to Koco ´n et al. (2023); Laskar et al. (2023); Periti et al. (2024b), we structured our prompt in a format that facilitates parsing and comprehension. 4 Evaluation setup Our evaluation is structured into two parts. First, we assess the quality of definitions generated by LlamaDictionary andFlan-T5 through the Definition Generation (DG) task. For this evaluation, we directly utilize the generated sense definitions. Next, we explore their utility in three popular Natural Language Processing tasks, namely Word-in-Context (WiC), Lexical Semantic Change (LSC), and Word Sense Induction (WSI). Specifically, instead of using standard token embeddings, we view sense definitions as intermediate sense representations and encode these as embeddings through a pretrained sequence embedding model. Formally, this means that: given an occurrence of a word w, we employ a generative model g(i.e., LlamaDictionary orFlan-T5 ) to generate a definition d, which we subsequently encode as a vectorvusing a sentence embedding model m, i.e., Code and data are submitted as supplementary material.v=m(d) =m(g(w)). Following Giulianelli et al. (2023), we used the all-distilroberta-v1 sentence SBERT model (Reimers and Gurevych, 2019) to encode definitions as contextualized sentence embeddings. To validate our results, we also evaluate twelve other SBERT models which show comparable results. Furthermore, we extend our evaluation by also considering generated definitions by the Flan-T5 model recently fine-tuned by Giulianelli et al. (2023)4as this model has not been evaluated on the WiC, WSI, and LSC tasks previously. 4.1 Definition generation (DG) Given a target word wand an example usage\ne, the task is to generate a natural language\ndefinition dthat is grammatical, fluent, and\nfaithful to the meaning of the target word w\nas used in the example usage e(Giulianelli\net al., 2020).\nWe assess the models in generating sense definitions for both familiar ( Seen during training) and unfamiliar ( Unseen ) domains and styles. ForSeen evaluation, we use the WordNet ,Oxford, and Wiktionary Test sets (see Table 1). ForUnseen evaluation, we consider the Test sets of two additional benchmarks comprising word usages from The Urban Dictionary (the largest online slang dictionary) (Ni and Wang, 2017) and Wikipedia (with rare words and phrases) (Ishiwatari et al., 2019). The Train set of these benchmarks were not considered during training. The decision to exclude Urban andWikipedia from training was threefold. Firstly, their exclusion broadens the scope of our evaluation by considering familiar and unfamiliar usages. Secondly, it enabled a direct comparison with Flan-T5 , a T5based (Raffel et al., 2020) model. Finally, we refrained from fine-tuning the model with bad, slang, or offensive words, and with numerous erroneous entries (e.g., definitions comprising single Arabic numerals or part-of-speech tags) in Urban (Huang et al., 2021). Table 3 reports the main statistics of these benchmarks. For comparison with previous work, we evaluated LlamaDictionary andFlan-T5 by considering standard Natural Language Generation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), SacreBLEU (Post, 2018), ROUGE4ltg/flan-t5-definition-en-xl\n4\nTarget wExample e Definition d LlamaDictionary\nrevitalize This food revitalized the patient Restore strength Give new life or energy to\ngloveMaxwell gloved his hand so that he\nwould n’t leave fingerprints ,\nthen pulled the triggerTo put a glove or gloves on .Wear a glove to protect the hand\nwhen performing an activity\nTable 2: Examples of pertinent definitions generated by LlamaDictionary for two word usages. The generated\ndefinitions are unfairly penalized by standard evaluation metrics.\nUrban Wikipedia\nTest # words\n# definitions\n# def. per word25,34,1.3556,8,6.84\nTable 3: Test partitions of Unseen DG benchmarks.\nL (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and EXACT MATCH. Since some pertinent definitions may be unfairly penalized due to missing lexical overlap (see Table 2), we follow Giulianelli et al. (2023) and consider BERT-F1 Score (Zhang et al., 2020), which represents a semantic and thus valuable metric for this task. 4.2 Word-in-Context (WiC) Given a target word wand two contexts c1\nandc2where woccurs, the task is to identify\nwhether the occurrences of winc1andc2\ncorrespond to the same meaning or not (Pile-\nhvar and Camacho-Collados, 2019).\nWe evaluate the utility of sense definitions using sequence embeddings v=m(g(w))on the original WiC benchmark (Pilehvar and CamachoCollados, 2019). We refrain from using the Train set and instead generate two embeddings, v, for each context pair (one for c1and one for c2) within the Dev and Test partitions (see Table 4). To address the WiC task, we then train a threshold-based classifier, for each tested model, using the cosine distance between the two embeddings of each pair in the Dev set. The training process involves selecting the threshold that maximizes the performance on the Dev set. Finally, we apply this classifier to conduct our evaluation over the Test set. We utilize accuracy as the assessment metric for comparison with previous work (Pilehvar and CamachoCollados, 2019). WiC\nPartition\n# pairs\n# wordsDev\n599Test\n1,1,Table 4: Test-Dev partitions for Word-in-Context.4.3 Lexical Semantic Change (LSC) Given a set of target words wand two cor-\nporaC1andC2of different time periods,\nthe task is to rank the targets according to\ntheir degree of lexical semantic changeabe-\ntween C1andC2(Schlechtweg et al., 2020).\na“Innovations which change the lexical mean-\ning rather than the grammatical function of a\nform” (Bloomfield, 1933)\nWe evaluate our approach on the original SemEval-English LSC benchmark (Schlechtweg et al., 2020). The dataset consists of two corpora and a test set of 46 target words (see Table 5). Train and Dev sets are not available as the task is set in an unsupervised scenario. To address the LSC task, we leverage popular methods generally applied using word embeddings rather than sentence embeddings (Periti and Tahmasebi, 2024). In particular, we evaluate two different approaches: Average Pairwise Distance (APD) is defined asform-based method, meaning that it quantifies change without modeling the underlying meanings of the words. Given a word w, APD computes the degree of change as the average pairwise distance between the embeddings of wgenerated for C1and C2(Giulianelli et al., 2020). Average Pairwise Distance Between Sense Prototypes (APDP) is defined as sense-based method, meaning that it quantifies change after modeling the underlying meanings of the words via clustering. Following previous work (Rother et al., 2020) and the recent BERTopic pipeline (Grootendorst, 2022), we consider the HDBSCAN algorithm (McInnes et al., 2017). Given a word w, APDP computes the degree of change as the average pairwise distances between the sense prototypes of win the time periods C1andC2, where sense prototypes are the set of embeddings obtained by averaging the embeddings of C1andC2in each cluster, respectively (Kashleva et al., 2022). For comparison with previous work, we utilize the Spearman rank correlation between gold scores 5\nand predictions as the assessment metric. Test LSC - WSI\n# words\n# clusters per word\nmax # of clusters\nmin # of clusters46\n9.4\n55\n1\nTable 5: Test set for Lexical Semantic Change and Word\nSense Induction, EN portion of SemEval-2020 Task 1.\n4.4 Word Sense Induction (WSI) Given a set of occurrences for a target word\nw, the task is to automatically determine\nthe different senses of wwithout relying\non predefined sense inventories (Agirre and\nSoroa, 2007).\nFor simplicity, we follow the recent comparison by Periti and Tahmasebi (2024) and perform a WSI evaluation on the same benchmark used for the LSC evaluation, as it also includes gold scores for WSI. Thus, we evaluate the clustering result obtained by using HDBSCAN against labels provided for clusters in the LSC data. As assessment metrics, we utilize Rand Index (RI) (Rand, 1971) and its Adjusted version (ARI) (Hubert and Arabie, 1985) as well as Purity (Manning, 2009). RI/ARI evaluate the similarity among two clustering results. ARI can yield low scores when a clustering result contains numerous small, yet coherent clusters. This does not necessarily indicate poor clustering quality, especially when the clusters are semantically meaningful. PUR assigns each cluster to the class that is most frequent in the cluster, measuring the accuracy of this assignment by counting the relative number of correctly assigned elements. 5 Evaluation results In our evaluation, we used Llama2Dictionary and Llama3Dictionary with the parameters reported in Table 11 and Flan-T5 . See Table 14 for specific parameters for each task. 5.1 Definition Generation (DG) For the Seen benchmark evaluation, we consider the average performance over WordNet andOxford (see Table 6). Note that, for Wikitionary , we do not compare with Flan-T5 as the entire benchmark (i.e., Train-Dev-Test) has been used for training. Further details and comparisons with state- 413of-the-art methods across multiple benchmarks are reported in Table 15. ForFlan-T5 , we report the original score presented by Giulianelli et al. (2023) ( reported ) and the score we obtain in our evaluation ( observed ). We believe that slight differences, where the observed results consistently under-perform compared to the reported results, are likely due to different parameter setting (e.g., temperature or greedy decoding). Nonetheless, the results are very similar. Compared to Flan-T5 observed ,LlamaDictionaryobtains higher results in all considered metrics. In addition, for reported , we achieve higher results for all metrics except BERT-F1, where our result is comparable (0.889 compared to 0.909). This is a interesting result considering that Flan-T5 has been fine-tuned on more data than LlamaDictionary , i.e., all Train-Dev-Test sets of Wikitionary . For the Unseen benchmarks, previous works have typically also used the data during training and are thus not fairly comparable. We report these results in Table 11. Thus we can evaluate only Llama2Dictionary andLlama3Dictionary and find that the latter consistently outperforms the former, unlike for the Seen benchmarks where the models were more even. This can be attributed to the fact that the Llama3-based model is larger than Llama2 in terms of parameters and training data. For the Unseen benchmarks, the BERT-F1 scores, that rely on semantic similarity, are comparable to the Seen benchmarks. For the remaining scores, that rely on lexical overlap, the results for theUnseen benchmark is consistently, and significantly lower. We believe that this drop stems both from the issues discussed in Table 2 as well as the fact that the base Llama chat models, which have undergone safety tuning , are likely restricted from generating foul language, malicious, and toxic content that can be found in the Urban dictionary. Compared to the Seen benchmarks, the Unseen benchmarks also contain multi-word phrases for which the models have not been trained. 5.2 Word-in-Context (WiC) Our results are reported in Table 7. Result using different SBERT models are summarized in Figure 2. Notably, we achieve a new state-of-the-art performance of .731 for the WiC task leveraging the definitions generated by Flan-T5 + SBERT. The result by Bevilacqua et al. (2020) is particularly interesting for comparison, as it has also been obtained by relying on generated definitions. 6\nWordNet -Oxford Seen Urban -Wikipedia Unseen\nLlama2Dict.\nLlama3Dict.Flan-T5 rep.\nFlan-T5 obs.Llama2Dict.\nLlama3Dict.-\nFlan-T5 obs.\nROUGE-L..400..364..184-\n.BLEU..283..266..100-\n.BERT-F1..889..885..849-\n.NIST..956-\n.828..405-\n.SACREBLEU22.21.975-\n18.8514.5.484-\n5.METEOR..426-\n.333..184-\n.EX. MATCH50.50.093-\n.110..000-\n.Table 6: Average results for the Definition Generation\ntask. The best results are highlighted in bold .\nFigure 2: Left: Accuracy distribution on the base WiC\ntask, using thirteen SBERT models. Right : ARI, PUR,\nand RI distribution on the WSI task, by considering our\nsettings for the LSC task.\nHowever, unlike our approach, they use multiple definitions per word usage. In contrast, we use a single definition per word usage, achieving higher results by employing both LlamaDictionary and Flan-T5 . As the WiC task requires distinguishing underlying meaning of word occurrences, the high performance of both Flan-T5 andLlamaDictionary indicates that the use of definitions is a reasonable approach to capturing the intended sense while offering interpretability. WiC Accuracy\nLevine et al. (2020) .Bevilacqua et al. (2020) .Peters et al. (2019) .Chang and Chen (2019) .Flan-T5 + SBERT .Llama2Dictionary + SBERT .Llama3Dictionary + SBERT .Table 7: Evaluation results for the Word-in-Context\ntask. The best result is highlighted in bold .\nFigure 3: Avg. Spearman correlation by addressing LSC\non different settings: different sentence length ( left) and\nshort word removal ( rigth ).\n5.3 Lexical Semantic Change (LSC) During our evaluation, we noticed that some of the annotated sentences present in the LSC benchmark were too long to be processed by our generative models (e.g., long word usages containing multiple sentences). This prompted us to evaluate the results by considering different sentence lengths, specifically 50, 100, 150 and 200 characters as well as the full sentences length. Our results are reported in Figure 3 and are consistently statistically significant. However, since we needed to discard up to 30% of sentences for LlamaDictionary , we proceeded with our experiments using up to 200 characters from each sentence. Recent findings show that form-based approaches typically outperform sense-based approaches for the LSC task (Periti et al., 2024a) and that training models on WiC tasks enhances the modeling of lexical semantics (Arefyev et al., 2021). Similarly, we obtain higher performance for the form-based approach (APD, i.e., .662 – .682) than the sense-based one (APDP, i.e., .575 – .667), see Table 8. Although our results are lower than the established WiC-trained baselines, they are, on average, higher than those obtained using pretrained models (see Periti and Montanelli (2024) for an extensive overview). Additionally, we also note that processing the generated definitions by removing short words with fewer than 2, 3 or 4 characters, in addition to punctuation, consistently boosts the performance of Flan-T5 , reaching correlations of .755, .762 and .827, respectively (see Figure 3). However, we did not observe the same boost for definitions generated byLlamaDictionary . After reviewing a small set of generated definitions, we hypothesize that this is due to the length of definitions generated by the models, with LlamaDictionary trained to provide concise definitions (See Figure 1). When compared to state-of-the-art form-based approaches, our approach achieves medium-strong correlation results but does not outperform the con7\nsidered baselines. When we consider APDP, the Llama2Dictionary model obtains the highest result, achieving a new state-of-the-art of .667 for interpretable LSC. This aligns with Giulianelli et al. (2023), who observe that the clusters of definitions have a lower intra-cluster dispersion compared to clusters using token and sentence embeddings. LSC method Spearman\nWiC-trained Aida and Bollegala (2024) form-based .WiC-trained Periti and Tahmasebi (2024) form-based .Keidar et al. (2022) form-based .Giulianelli et al. (2022) form-based .Flan-T5 + SBERT form-based .Llama2Dictionary + SBERT form-based .Llama3Dictionary + SBERT form-based .WiC-trained Periti and Tahmasebi (2024) sense-based .Rother et al. (2020) sense-based .Montariol et al. (2021) sense-based .Flan-T5 + SBERT sense-based .Llama2Dictionary + SBERT sense-based .Llama3Dictionary + SBERT sense-based .Table 8: Evaluation results for the Lexical Semantic\nChange task. The best result is highlighted in bold .\nResults are reported using both form-based and sense-\nbased methods.\n5.4 Word Sense Induction (WSI) Our WSI evaluation relies on a recently developed benchmark originally designed for LSC. This benchmark contains cluster labels derived from manually annotated judgments of words in-context . These can therefore be considered as silver label data, rather than gold label data, as the clusters themselves have not been manually labeled. Our results are reported in Table 9. We observe the highest results for the WiC-trained XLLEXEME model (Cassotti et al., 2023), and GPT-4, were the training data is unknown and thus could include both WiC data and the WSI data used in this evaluation (Balloccu et al., 2024). When compared to standard pretrained models (i.e., BERT, mBERT, XLM-R), our results are consistently higher. In line with Periti and Tahmasebi (2024), we observe low results in terms of ARI. We believe this stems from the quality of the original clusters to which we are comparing. The more flexible RI metric in Table 9 shows results comparable to the PUR scores. In terms of the resulting clusters, we obtain an average number of clusters of 3.91 compared to the 9.61 of the original benchmark. This is in line with our intuition that definitions can be considered as prototypes of multiple word usages. 551model ARI PUR RI\nResults from\nPeriti and Tahmasebi (2024)BERT\nmBERT\nXLM-R\nXL-LEXEME\nGPT-4.....340.....877.....FlanT5\nLlama2Dictionary\nLlama3Dictionary...073...832...Table 9: Evaluation results for the Word Sense Induc-\ntion task. The best result is highlighted in bold .\n6 Conclusion Inspired by recent advancements in text generation, in this work, we investigated the potential of fine-tuned large language models to generate sense definitions for words in-context . Specifically, we fine-tuned two new Llama chat based models, called LlamaDictionary , and assessed their performance along with an existing Flan-T5 model on the Definition Generation task. Next, we explored their utility for modeling word meaning by addressing lexical semantic tasks such as WordIn-Context, Word Sense Induction, and Lexical Semantic Change. In our experiments, we considered the generated definitions as intermediate representations, passed through a sentence embedding model. Our results consistently show that we can use generated definitions to explicitly model the meaning of word usages through interpretable definitions. In all tasks, the use of sentence embeddings for generated definitions outperformed the use of standard token embeddings for word occurrences, setting new state-of-the-art results. Across tasks, we find that the use of the larger 7B and 8B LlamaDictionary models compared to the smaller 3B T5-based model obtain slightly higher results in the Definition Generation task, while being equally strong on the lexical semantics tasks. An extension of the LlamaDictionary models is to fine-tune them on all the benchmarks that have been used for theFlan-T5 model, as well to fine-tune the models further on generated usage sentences (Malkin et al., 2021; Ma et al., 2024). Our evaluation using automatically generated sense definitions in this paper paves the way for future advancements in modeling lexical semantics. For example, by offering an automatic labeling of senses, we can support the creation of lexicographic resources for all languages, including lowresource languages (Kong et al., 2022), providing a way to better know what change our words have experienced over time. 8\nLimitations In our work, we consider only English data as there are few available benchmarks, neither for training nor comparison on other languages. Given the necessary resources, we believe our approach to be language-agnostic and readily applicable to other languages. We limited our experiments to LlamaDictionary and Flan-T5 due to the cost and required computational resources for fine-tuning other large language models. We indeed exceeded the allocated resources on our National Super-computing during our experiments. Such large-scale models and experimental data must be approached cautiously as they will otherwise generate enormous computational costs (both in terms of monetary and environmental costs). A further limitation of our models arises from the fact that existing Definition Generation benchmarks occasionally include multiple definitions for the same word meanings (e.g., Table 13). While this may serve as a form of regularization for training models, we believe that it may have influenced the uniformity in style and wording of our models. Unfortunately, statistics for these issues are non-existent. We thus advocate for further refinement to ensure consistency and coherence across definitions. We believe that, ideally, maximizing uniformity in definitions is desirable to develop models that offer consistent responses for similar word usages. This will be beneficial for any largescale follow-up analysis relying on our evaluated approach. In this paper, we integrated generated definitions with sentence embeddings. However, generated definitions often display higher lexical similarity to one another compared to word usages. Given the anisotropic nature of embedding spaces in large language models (Ethayarajh, 2019), the use of sentence embeddings might complicate discerning differences in definition of different complexity for language learners (Yuan et al., 2022). We thus believe future research should also explore the utilization of definition generation models alongside more conventional text-mining methods, such as count-based models. Count-based models may offer a more straightforward approach to processing interpretable, lexical similar definitions. 642References Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007) , pages 7–12, Prague, Czech Republic. Association for Computational Linguistics. Taichi Aida and Danushka Bollegala. 2024. A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection. Preprint , arXiv:2403.00226. Domagoj Alagic, Jan Snajder, and Sebastian Pado. 2018. Leveraging Lexical Substitutes for Unsupervised Word Sense Induction. Proceedings of the AAAI Conference on Artificial Intelligence , 32(1). Marianna Apidianaki. 2023. From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation. Computational Linguistics , 49(2):465–523. Nikolay Arefyev, Maksim Fedoseev, Vitaly Protastov, Daniil Homiskiy, Adis Davletov, and Alexander Panchenko. 2021. DeepMistake: Which Senses are Hard to Distinguish for a Word-in-Context Model. InProceedings of the Conference on Computational Linguistics and Intellectual Technologies (Dialogue) , (online). RSUH. Nikolay Arefyev and Vasily Zhikov. 2020. BOS at SemEval-2020 Task 1: Word Sense Induction via Lexical Substitution for Lexical Semantic Change Detection. In Proceedings of the Fourteenth Workshop on Semantic Evaluation , pages 171– 179, Barcelona (online). International Committee for Computational Linguistics. Tal August, Katharina Reinecke, and Noah A. Smith. 2022. Generating Scientific Definitions with Controllable Complexity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8298–8317, Dublin, Ireland. Association for Computational Linguistics. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej Dusek. 2024. Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 67–93, St. Julian’s, Malta. Association for Computational Linguistics. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization , pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics. 9\nDiego Bear and Paul Cook. 2021. Cross-Lingual Wolastoqey-English Definition Modelling. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021) , pages 138–146, Held Online. INCOMA Ltd. Michele Bevilacqua, Marco Maru, and Roberto Navigli. 2020. Generationary or “How We Went beyond Word Sense Inventories and Learned to Gloss”. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7207–7221, Online. Association for Computational Linguistics. Leonard Bloomfield. 1933. Language . Holt, Rinehart and Winston, New York. Dallas Card. 2023. Substitution-based Semantic Change Detection using Contextual Embeddings. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 590–602, Toronto, Canada. Association for Computational Linguistics. Pierluigi Cassotti, Lucia Siciliani, Marco DeGemmis, Giovanni Semeraro, and Pierpaolo Basile. 2023. XLLEXEME: WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1577–1585, Toronto, Canada. Association for Computational Linguistics. Ting-Yun Chang and Yun-Nung Chen. 2019. What does this word mean? explaining contextualized embeddings with natural language definition. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 6064–6070, Hong Kong, China. Association for Computational Linguistics. Cheng-Han Chiang and Hung-yi Lee. 2023. Are Synonym Substitution Attacks Really Synonym Substitution Attacks? In Findings of the Association for Computational Linguistics: ACL 2023 , pages 1853–1878, Toronto, Canada. Association for Computational Linguistics. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. Preprint , arXiv:2305.14314. George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using n-gram Cooccurrence Statistics. In Proceedings of the Second International Conference on Human Language Technology Research , HLT ’02, page 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Kawin Ethayarajh. 2019. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. InProceedings of the 2019 Conference on Empirical 755Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 55–65, Hong Kong, China. Association for Computational Linguistics. Artyom Gadetsky, Ilya Yakubovskiy, and Dmitry Vetrov. 2018. Conditional Generators of Words Definitions. InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 266–271, Melbourne, Australia. Association for Computational Linguistics. Noah Gardner, Hafiz Khan, and Chih-Cheng Hung. 2022. Definition Modeling: Literature Review and Dataset Analysis. Applied Computing and Intelligence , 2(1):83–98. Mario Giulianelli, Marco Del Tredici, and Raquel Fernández. 2020. Analysing Lexical Semantic Change with Contextualised Word Representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 3960– 3973, Online. Association for Computational Linguistics. Mario Giulianelli, Andrey Kutuzov, and Lidia Pivovarova. 2022. Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change. In Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change , pages 54–67, Dublin, Ireland. Association for Computational Linguistics. Mario Giulianelli, Iris Luden, Raquel Fernandez, and Andrey Kutuzov. 2023. Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3130–3148, Toronto, Canada. Association for Computational Linguistics. Maarten Grootendorst. 2022. BERTopic: Neural Topic Modeling with a Class-based TF-IDF Procedure. Preprint , arXiv:2203.05794. Muhammad Usman Hadi, Qasem al Tashi, Rizwan Qureshi, Abbas Shah, Amgad Muneer, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, and Mubarak Shah. 2023. Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects. Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. 2024. Parameter-efficient finetuning for large models: A comprehensive survey. Preprint , arXiv:2403.14608. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. Preprint , arXiv:2106.09685. 10\nHan Huang, Tomoyuki Kajiwara, and Yuki Arase. 2021. Definition Modelling for Appropriate Specificity. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2499–2509, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Lawrence Hubert and Phipps Arabie. 1985. Comparing Partitions. Journal of classification , 2:193–218. Mathew Huerta-Enochian. 2024. Instruction finetuning: Does prompt loss matter? Preprint , arXiv:2401.13586. Shonosuke Ishiwatari, Hiroaki Hayashi, Naoki Yoshinaga, Graham Neubig, Shoetsu Sato, Masashi Toyoda, and Masaru Kitsuregawa. 2019. Learning to Describe Unknown Phrases with Local and Global Contexts. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 3467–3476, Minneapolis, Minnesota. Association for Computational Linguistics. John S. Justeson and Slava M. Katz. 1991. Cooccurrences of Antonymous Adjectives and Their Contexts. Computational Linguistics , 17(1):1–20. Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, and Michael Vassilakopoulos. 2024. Large Language Models versus Natural Language Understanding and Generation. In Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics , PCI ’23, page 278–290, , Lamia, Greece,. Association for Computing Machinery. Kseniia Kashleva, Alexander Shein, Elizaveta Tukhtina, and Svetlana Vydrina. 2022. HSE at LSCDiscovery in Spanish: Clustering and Profiling for Lexical Semantic Change Discovery. In Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change , pages 193–197, Dublin, Ireland. Association for Computational Linguistics. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2024. A survey of reinforcement learning from human feedback. Preprint , arXiv:2312.14925. Daphna Keidar, Andreas Opedal, Zhijing Jin, and Mrinmaya Sachan. 2022. Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1422–1442, Dublin, Ireland. Association for Computational Linguistics. Jan Koco ´n, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Koco ´n, Bartłomiej Koptyra, Wiktoria Mieleszczenko-Kowszewicz, Piotr Miłkowski, Marcin Oleksy, Maciej Piasecki, Łukasz Radli ´nski, 867Konrad Wojtasik, Stanisław Wo´ zniak, and Przemysław Kazienko. 2023. ChatGPT: Jack of All Trades, Master of None. Information Fusion , 99:101861. Cunliang Kong, Yun Chen, Hengyuan Zhang, Liner Yang, and Erhong Yang. 2022. Multitasking Framework for Unsupervised Simple Definition Generation. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5934–5943, Dublin, Ireland. Association for Computational Linguistics. Matej Kosec, Sheng Fu, and Mario Michael Krell. 2021. Packing: Towards 2x NLP BERT Acceleration. Andrey Kutuzov, Erik Velldal, and Lilja Øvrelid. 2022. Contextualized Embeddings for Semantic Change Detection: Lessons Learned. In Northern European Journal of Language Technology, Volume 8 , Copenhagen, Denmark. Northern European Association of Language Technology. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. In Findings of the Association for Computational Linguistics: ACL 2023 , pages 431– 469, Toronto, Canada. Association for Computational Linguistics. Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2020. SenseBERT: Driving some sense into BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4656–4667, Online. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023. Large Language Models Understand and Can be Enhanced by Emotional Stimuli. Preprint , arXiv:2307.11760. Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out , pages 74–81, Barcelona, Spain. Association for Computational Linguistics. Xianghe Ma, Michael Strube, and Wei Zhao. 2024. Graph-based Clustering for Detecting Semantic Change Across Time and Languages. In Proceedings of the 18th Conference of the European Chapter of 11\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 1542–1561, St. Julian’s, Malta. Association for Computational Linguistics. Nikolay Malkin, Sameera Lanka, Pranav Goel, Sudha Rao, and Nebojsa Jojic. 2021. GPT Perdetry Test: Generating new meanings for new words. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5542–5553. Christopher D Manning. 2009. An Introduction to Information Retrieval . Cambridge university press. Leland McInnes, John Healy, and Steve Astels. 2017. HDBSCAN: Hierarchical density based clustering. Journal of Open Source Software , 2(11):205. Timothee Mickus, Denis Paperno, and Matthieu Constant. 2019. Mark my Word: A Sequence-toSequence Approach to Definition Modeling. In Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing , pages 1– 11, Turku, Finland. Linköping University Electronic Press. Timothee Mickus, Kees Van Deemter, Mathieu Constant, and Denis Paperno. 2022. Semeval-2022 task 1: CODWOE – comparing dictionaries and word embeddings. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022) , pages 1–14, Seattle, United States. Association for Computational Linguistics. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. Preprint , arXiv:1301.3781. Syrielle Montariol, Matej Martinc, and Lidia Pivovarova. 2021. Scalable and Interpretable Semantic Change Detection. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4642–4652, Online. Association for Computational Linguistics. Juan Pablo Munoz, Jinjie Yuan, Yi Zheng, and Nilesh Jain. 2024. LoNAS: Elastic Low-Rank Adapters for Efficient Large Language Models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages 10760–10776, Torino, Italia. ELRA and ICCL. Usman Naseem, Imran Razzak, Shah Khalid Khan, and Mukesh Prasad. 2021. A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models. ACM Trans. Asian Low-Resour. Lang. Inf. Process. , 20(5). Ke Ni and William Yang Wang. 2017. Learning to explain non-standard English words and phrases. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: 979Short Papers) , pages 413–417, Taipei, Taiwan. Asian Federation of Natural Language Processing. Thanapon Noraset, Chen Liang, Larry Birnbaum, and Doug Downey. 2017. Definition Modeling: Learning to Define Word Embeddings in Natural Language. Proceedings of the AAAI Conference on Artificial Intelligence , 31(1). Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research , 12:2825–2830. 1Jeffrey Pennington, Richard Socher, and Christopher 1Manning. 2014. GloVe: Global Vectors for Word 1Representation. In Proceedings of the 2014 Confer- 1ence on Empirical Methods in Natural Language Pro- 1cessing (EMNLP) , pages 1532–1543, Doha, Qatar. 1Association for Computational Linguistics. 1Francesco Periti, Pierluigi Cassotti, Haim Dubossarsky, 1and Nina Tahmasebi. 2024a. Analyzing Semantic 1Change through Lexical Replacements. Preprint , 1arXiv:2404.18570. 1Francesco Periti, Haim Dubossarsky, and Nina Tah- 1masebi. 2024b. (Chat)GPT v BERT Dawn of Justice 1for Semantic Change Detection. In Findings of the 1Association for Computational Linguistics: EACL 12024 , pages 420–436, St. Julian’s, Malta. Associa- 1tion for Computational Linguistics. 1Francesco Periti and Stefano Montanelli. 2024. Lexical 1Semantic Change through Large Language Models: 1a Survey. ACM Comput. Surv. Just Accepted. 1Francesco Periti, Sergio Picascia, Stefano Montanelli, 1Alfio Ferrara, and Nina Tahmasebi. 2023. Study- 1ing Word Meaning Evolution through Incremental 1Semantic Shift Detection: A Case Study of Italian 1Parliamentary Speeches. 1Francesco Periti and Nina Tahmasebi. 2024. A Sys- 1tematic Comparison of Contextualized Word Em- 1beddings for Lexical Semantic Change. Preprint , 1arXiv:2402.12011. 1Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt 1Gardner, Christopher Clark, Kenton Lee, and Luke 1Zettlemoyer. 2018. Deep Contextualized Word Rep- 1resentations. In Proceedings of the 2018 Conference 1of the North American Chapter of the Association for 1Computational Linguistics: Human Language Tech- 1nologies, Volume 1 (Long Papers) , pages 2227–2237, 112\nNew Orleans, Louisiana. Association for Computa- 1tional Linguistics. 1Matthew E. Peters, Mark Neumann, Robert Logan, Roy 1Schwartz, Vidur Joshi, Sameer Singh, and Noah A. 1Smith. 2019. Knowledge Enhanced Contextual Word 1Representations. In Proceedings of the 2019 Confer- 1ence on Empirical Methods in Natural Language Pro- 1cessing and the 9th International Joint Conference 1on Natural Language Processing (EMNLP-IJCNLP) , 1pages 43–54, Hong Kong, China. Association for 1Computational Linguistics. 1Erika Petersen and Christopher Potts. 2023. Lexical 1Semantics with Large Language Models: A Case 1Study of English “break”. In Findings of the Asso- 1ciation for Computational Linguistics: EACL 2023 , 1pages 490–511, Dubrovnik, Croatia. Association for 1Computational Linguistics. 1Mohammad Taher Pilehvar and Jose Camacho-Collados. 12019. WiC: the Word-in-Context Dataset for Eval- 1uating Context-Sensitive Meaning Representations. 1InProceedings of the 2019 Conference of the North 1American Chapter of the Association for Computa- 1tional Linguistics: Human Language Technologies, 1Volume 1 (Long and Short Papers) , pages 1267–1273, 1Minneapolis, Minnesota. Association for Computa- 1tional Linguistics. 1Mohammad Taher Pilehvar and Jose Camacho-Collados. 12021. Contextualized Embeddings , pages 69–96. 1Springer International Publishing, Cham. 1Matt Post. 2018. A Call for Clarity in Reporting BLEU 1Scores. In Proceedings of the Third Conference on 1Machine Translation: Research Papers , pages 186– 1191, Brussels, Belgium. Association for Computa- 1tional Linguistics. 1James Pustejovsky and Branimir Boguraev. 1993. Lexi- 1cal Knowledge Representation and Natural Language 1Processing. Artificial Intelligence , 63(1):193–223. 1Colin Raffel, Noam Shazeer, Adam Roberts, Kather- 1ine Lee, Sharan Narang, Michael Matena, Yanqi 1Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the 1limits of transfer learning with a unified text-to-text 1transformer. Journal of Machine Learning Research , 121(140):1–67. 1William M. Rand. 1971. Objective Criteria for the Eval- 1uation of Clustering Methods. Journal of the Ameri- 1can Statistical Association , 66(336):846–850. 1Nils Reimers and Iryna Gurevych. 2019. Sentence- 1BERT: Sentence Embeddings using Siamese BERT- 1Networks. In Proceedings of the 2019 Conference on 1Empirical Methods in Natural Language Processing 1and the 9th International Joint Conference on Natu- 1ral Language Processing (EMNLP-IJCNLP) , pages 13982–3992, Hong Kong, China. Association for Com- 1putational Linguistics. 1089David Rother, Thomas Haider, and Steffen Eger. 2020. 1CMCE at SemEval-2020 Task 1: Clustering on Man- 1ifolds of Contextualized Embeddings to Detect His- 1torical Meaning Shifts. In Proceedings of the Four- 1teenth Workshop on Semantic Evaluation , pages 187– 1193, Barcelona (online). International Committee for 1Computational Linguistics. 1Dominik Schlechtweg, Barbara McGillivray, Simon 1Hengchen, Haim Dubossarsky, and Nina Tahmasebi. 12020. SemEval-2020 Task 1: Unsupervised Lexical 1Semantic Change Detection. In Proceedings of the 1Fourteenth Workshop on Semantic Evaluation , pages 11–23, Barcelona (online). International Committee 1for Computational Linguistics. 1Vincent Segonne and Timothee Mickus. 2023. Defi- 1nition Modeling : To model definitions. Generating 1Definitions With Little to No Semantics. In Proceed- 1ings of the 15th International Conference on Compu- 1tational Semantics , pages 258–266, Nancy, France. 1Association for Computational Linguistics. 1Lutfi Kerem Senel, Ihsan Utlu, Veysel Yucesoy, 1Aykut Koc, and Tolga Cukur. 2018. Semantic 1Structure and Interpretability of Word Embeddings. 1IEEE/ACM Trans. Audio, Speech and Lang. Proc. , 126(10):1769–1779. 1Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- 1bert, Amjad Almahairi, Yasmine Babaei, Nikolay 1Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti 1Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton 1Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, 1Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, 1Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- 1thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan 1Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, 1Isabel Kloumann, Artem Korenev, Punit Singh Koura, 1Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- 1ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- 1tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- 1bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- 1stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, 1Ruan Silva, Eric Michael Smith, Ranjan Subrama- 1nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- 1lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, 1Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, 1Melanie Kambadur, Sharan Narang, Aurelien Ro- 1driguez, Robert Stojnic, Sergey Edunov, and Thomas 1Scialom. 2023. Llama 2: Open Foundation and Fine- 1Tuned Chat Models. Preprint , arXiv:2307.09288. 1Yu-Hsiang Tseng, Mao-Chang Ku, Wei-Ling Chen, Yu- 1Lin Chang, and Shu-Kai Hsieh. 2023. Vec2Gloss: 1definition modeling leveraging contextualized vec- 1tors with Wordnet gloss. In Proceedings of the 37th 1Pacific Asia Conference on Language, Information 1and Computation , pages 679–690. 1Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob 1Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz 1Kaiser, and Illia Polosukhin. 2017. Attention is all 1you need. In Proceedings of the 31st International 113\nConference on Neural Information Processing Sys- 1tems, NIPS’17, page 6000–6010, Red Hook, NY , 1USA. Curran Associates Inc. 1Hendryk Weiland, Maike Behrendt, and Stefan Harmel- 1ing. 2023. Automatic Dictionary Generation: Could 1Brothers Grimm Create a Dictionary with BERT? In 1Proceedings of the 19th Conference on Natural Lan- 1guage Processing (KONVENS 2023) , pages 102–120, 1Ingolstadt, Germany. Association for Computational 1Lingustics. 1Thomas Wolf, Lysandre Debut, Victor Sanh, Julien 1Chaumond, Clement Delangue, Anthony Moi, Pier- 1ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, 1Joe Davison, Sam Shleifer, Patrick von Platen, Clara 1Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven 1Le Scao, Sylvain Gugger, Mariama Drame, Quentin 1Lhoest, and Alexander Rush. 2020. Transformers: 1State-of-the-Art Natural Language Processing. In 1Proceedings of the 2020 Conference on Empirical 1Methods in Natural Language Processing: System 1Demonstrations , pages 38–45, Online. Association 1for Computational Linguistics. 1Jiaxin Yuan, Cunliang Kong, Chenhui Xie, Liner Yang, 1and Erhong Yang. 2022. COMPILING: A Bench- 1mark Dataset for Chinese Complexity Controllable 1Definition Generation. In Proceedings of the 21st 1Chinese National Conference on Computational Lin- 1guistics , pages 921–931, Nanchang, China. Chinese 1Information Processing Society of China. 1Hengyuan Zhang, Dawei Li, Yanran Li, Chenming 1Shang, Chufan Shi, and Yong Jiang. 2023. Assisting 1Language Learners: Automated Trans-Lingual Defi- 1nition Generation via Contrastive Prompt Learning. 1InProceedings of the 18th Workshop on Innovative 1Use of NLP for Building Educational Applications 1(BEA 2023) , pages 260–274, Toronto, Canada. Asso- 1ciation for Computational Linguistics. 1Hengyuan Zhang, Dawei Li, Shiping Yang, and Yan- 1ran Li. 2022. Fine-grained Contrastive Learning for 1Definition Generation. In Proceedings of the 2nd 1Conference of the Asia-Pacific Chapter of the Asso- 1ciation for Computational Linguistics and the 12th 1International Joint Conference on Natural Language 1Processing (Volume 1: Long Papers) , pages 1001– 11012, Online only. Association for Computational 1Linguistics. 1Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, 1Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian- 1wei Zhang, Fei Wu, and Guoyin Wang. 2024. Instruc- 1tion Tuning for Large Language Models: A Survey. 1Preprint , arXiv:2308.10792. 1Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. 1Weinberger, and Yoav Artzi. 2020. Bertscore: 1Evaluating text generation with bert. Preprint , 1arXiv:1904.09675. 114\nA Fine-tuning 1In our experiments, we conducted multiple rounds 1of fine-tuning, systematically testing various pa- 1rameters. Specifically, we detail these configura- 1tions in Table 10. In line with Huerta-Enochian 1(2024), who recently demonstrated that prompt 1loss can be safely ignored for many datasets, we 1observed lower preliminary results in the evalua- 1tion tasks for models chosen based on validation 1performance. Therefore, we selected the final mod- 1els (see Table 11) based on the checkpoint from the 1last training epoch that had the best performance 1on the Definition Generation task. 1Parameter Experimented values\nModelmeta-llama/Meta-Llama-3-8B-Instruct ,\nmeta-llama/Llama-2-7b-chat-hf\nGPU A100:fat (80 GB)\nHours 7-8\nPEFT LoRA, QLoRA\nDropout 0.05, 0.1,0.2\nWeight decay 0.001, 0.0Learning rate 1e-4, 1e-5\nLora ranks 8, 32, 64, 128, 256, 512, 1Lora alpha 16, 64, 256, 512, 1024, 2Warmup ratio 0.03, 0.05\nEval steps Train epochs 4, 5, 10\nMax seq. length Batch size 32\nOptimizer Adam\nLoRA target modulesq_proj, k_proj, v_proj,\no_proj, gate_proj, up_proj,\ndown_proj, lm_head\nTable 10: Settings and parameters used during training.\nParameters shown in small font represent preliminary\nexperiments that were not further evaluated.\nFinal setting Llama2Dictionary Llama3Dictionary\nGPU A100:fat (80 GB) A100:fat (80 GB)\nHours 7-8 8-9\nPEFT LoRA LoRA\nDropout 0.1 0.05\nWeight decay 0.001 0.Learning rate 1e-4 1e-4\nLora ranks 1024 Lora alpha 2048 1Warmup ratio 0.05 0.05\nEval steps epochs epochs\nTrain epochs 4 4\nMax seq. length 512 Batch size 32 32\nOptimizer Adam Adam\nLoRA target modulesq_proj, k_proj, v_proj,\no_proj, gate_proj, up_proj,\ndown_proj, lm_headq_proj, k_proj, v_proj,\no_proj, gate_proj, up_proj,\ndown_proj, lm_head\nTable 11: Parameters of our final models. Our code will\nbe publicly available for further details. For finetuning,\nwe rely on the transformers library (Wolf et al., 2020).\nA.1 Lora rank-alpha 1We conduct fine-tuning using LoRA, (Hu et al., 12021) and QLORA, (Dettmers et al., 2023) obtain- 1ing very similar evaluation results. Drawing from 1219insights from prior research (Munoz et al., 2024) 1as well recent online discussions, we adopted a 1strategy where the LoRA alpha αwas set to dou- 1ble the LoRA rank r. In our experiments for the 1Definition Generation task, larger ranks resulted 1in higher performance on WordNet and slightly 1higher performance on Oxford benchmarks. How- 1ever, no improvement was noted for Wiktionary 1(see Figure 4). 1B SBERT models 1In our experiments, we made an effort to evalu- 1ate all the Bi-Encoder SBERT models available at 1https://sbert.net/ (see Table 12). This thor- 1ough assessment ensures that our findings are ro- 1bust and accurate. While we acknowledge that 1other models may exist, the evaluation results we 1present remain valuable and consistent across the 1models tested, contributing to the broader perspec- 1tive presented in the paper. 1Further parameters are related to our procedure 1for addressing the Word-in-Context, Word Sense 1Induction, and Lexical Semantic Change tasks. We 1report these parameters in Table 14. 1SBERT models\nall-mpnet-base-v2\nmulti-qa-mpnet-base-dot-v1\nall-distilroberta-v1\nall-MiniLM-L12-v2\nmulti-qa-distilbert-cos-v1\nall-MiniLM-L6-v2\nmulti-qa-MiniLM-L6-cos-v1\nparaphrase-multilingual-mpnet-base-v2\nparaphrase-albert-small-v2\nparaphrase-multilingual-MiniLM-L12-v2\nparaphrase-MiniLM-L3-v2\ndistiluse-base-multilingual-cased-v1\ndistiluse-base-multilingual-cased-v2\nTable 12: Experimented SBERT models. We report in\nbold the model used for the results obtained in the main\npaper. We use this model as it was used in previous\nexperiments by Giulianelli et al. (2023).\nC Definition Generation 1In our work, we extensively evaluated our 1LlamaDictionary models along with the Flan-T5 1models by Giulianelli et al. (2023), setting new 1state-of-the-art results on the Definition Genera- 1tion tasks across multiple benchmarks. In Table 15, 1we provide a full comparison, including individ- 1ual scores for each benchmark and the measures 1considered. 115\nBenchmark Target w Example e Definition e\nWordNet accuracy He was beginning to doubt the accuracy of his compass The quality of being near\nto the true value\nOxford accuracy However, these studies have not generally had enough\nparticipants to provide precise estimates of accuracy .The quality or state of be-\ning correct or precise\nWiktionary accuracy The efficiency of the instrument will also depend upon the\naccuracy with which the piston fits the bottom and sides\nof the barrel. When the piston is depressed to the bottom,\nit is considered in theory to be in absolute contact, so as to\nexclude every particle of air from the space between it and\nthe bottom.The state of being accu-\nrate; being free from mis-\ntakes, this exemption aris-\ning from carefulness; ex-\nactness; correctness\nOxford yesterday Yesterday the weather was beautiful On the day preceding to-\nday\nOxford yesterday It was in yesterday ’s newspapers The day immediately be-\nfore today\nOxford yesterday I am doing a research paper on women ’s voting rights ;\nyesterday and todayOn the day before today\nOxford yesterday On a day like today after yesterday , i tend to reflect ,\ninternalize , and re-address the balanceThe day before today\nTable 13: Example of correct but inconsistent definitions from the considered benchmarks. It is unnecessary to\ntrain the model to provide different answers. Ideally, a single definition should be used for different examples of the\nconsidered target.\nEvaluation tasks\nDG WiC WSI LSC\ngen. modelLlamaDictionary ,\nFlan-T5LlamaDictionary ,\nFlan-T5LlamaDictionary ,\nFlan-T5LlamaDictionary ,\nFlan-T5\ntemperature 0.0 0.0 0.0 0.0\nenc. model roberta-large all-distilroberta-v1 all-distilroberta-v1 all-distilroberta-v1\nmetric BERTScore cosine cosinecosine (APD)\ncanberra (APDP) following\nPeriti et al.; Periti and Tahmasebi\nclustering - - HDBSCAN HDBSCAN\nHDBSCAN-allow_single_cluster - - True True\nHDBSCAN-min_cluster_size - - 2 2\nHDBSCAN-cluster_selection_method - - leaf leaf\nTable 14: Models and parameters used for addressing the DG, WIC, WSI, and LSC tasks. We rely on the HDBSCAN\nimplementation of the scikit-learn library (Pedregosa et al., 2011).\n16\nFigure 4: Average performance of trained models using LoRA (Hu et al., 2021) and QLoRA (Dettmers et al.,\n2023) with parameters from Table 10. We conducted experiments with LoRA alpha αset to double the rankrand\nobserved that larger ranks resulted in higher performance on WordNet and slightly higher performance on Oxford\nbenchmarks. However, no improvement was noted for Wiktionary . We report BERT-F1 and BLEU as examples.\nSimilar trends were observed for other performance metrics.\n17\nROUGE-L BLEU BERT-F1 NIST SACREBLEU METEOR EXACT MATCH\nWordNet - seen\nNoraset et al. (2017) - .236∗- .497∗- - -\nNi and Wang (2017) - .248∗- .403∗- - -\nGadetsky et al. (2018) - .237∗- .443∗- - -\nIshiwatari et al. (2019) - .248 - .435∗- - -\nHuang et al. (2021) - .327 - .646 - - -\nZhang et al. (2022) - .320 - .747 - - -\nGiulianelli et al. (2023) Reported .522 .328 .921 - - - -\nGiulianelli et al. (2023) Observed .405 .320 .893 .907 23.302 .374 .Llama2chat .564 .513 .920 1.391 41.096 .536 .Llama3Instruct .435 .339 .893 1.012 27.400 .480 .Oxford - seen\nNoraset et al. (2017) - .149∗- .327∗- - -\nNi and Wang (2017) - .176∗- .313∗- - -\nGadetsky et al. (2018) - .120 - .358∗- - -\nIshiwatari et al. (2019) - .185 - .382∗- - -\nHuang et al. (2021) - .265 - .742 - - -\nBevilacqua et al. (2020) .294 .088 .768 - - .135 -\nZhang et al. (2022) - .271 - .794 - - -\nGiulianelli et al. (2023) Reported .387 .186 .897 - - - -\nGiulianelli et al. (2023) Observed .324 .213 .878 .749 14.400 .292 .Llama2chat .398 .291 .840 .969 21.410 .367 .Llama3Instruct .365 .228 .885 .900 16.550 .373 .Wikitionary - seen\nLlama2chat .222 .131 .666 .408 6.963 .183 .Llama3Instruct .267 .156 .863 .517 8.100 .232 .Urban - unseen\nNoraset et al. (2017) - seen - .515∗- .104∗- - -\nNi and Wang (2017) - seen - .899∗- .174∗- - -\nGadetsky et al. (2018) - seen - .088∗- .194∗- - -\nIshiwatari et al. (2019) - seen - .105 - .192∗- - -\nHuang et al. (2021) - seen - .177 - .355 - - -\nZhang et al. (2022) - seen - .194 - .410 - - -\nGiulianelli et al. (2023) - unseen Observed .106 .053 .835 .167 2.160 .068 .Llama2chat -unseen .110 .055 .812 .170 2.247 .071 .Llama3instruct -unseen .115 .057 .836 .197 2.331 .079 .Wikipedia - unseen\nNoraset et al. (2017) - seen - .446∗- .334∗- - -\nNi and Wang (2017) - seen - .527∗- .552∗- - -\nGadetsky et al. (2018)- seen - .450∗- .331∗- - -\nIshiwatari et al. (2019)- seen - .538 - .567∗- - -\nHuang et al. (2021)- seen - .556 - .640 - - -\nGiulianelli et al. (2023) - unseen Observed .240 .138 .863 .511 8.212 .263 .Llama2chat -unseen .213 .123 .716 .523 7.399 .232 .Llama3Instruct -unseen .253 .144 .863 .614 8.638 .290 .Table 15: Evaluation results for the Definition Generation task. The best result is highlighted in bold. Our model is\ntrained exclusively on the training set of the WordNet, Oxford, and Wiktionary datasets. Results marked with∗are\nreported from experiments in Huang et al. (2021).\n18\n",
        "aspect_review": "The paper presents a novel approach to modeling word meaning by generating dictionary-like definitions, which improves interpretability and sets new state-of-the-art results in multiple lexical semantics tasks. Comprehensive experimental validation is conducted across various benchmarks, demonstrating the effectiveness of the proposed models in achieving state-of-the-art results. The paper is well-organized, with clear descriptions of the methodology and detailed experimental results, making it easy for readers to follow and understand. The methodology is robust, involving fine-tuning of large language models with instruction tuning and parameter-efficient techniques, showcasing technical depth and rigor. The paper provides thorough explanations and discussions of the models, tasks, and results, offering valuable insights into the use of generated definitions for lexical semantics.\n\nWhile the paper achieves state-of-the-art results, there is room for improvement in certain metrics, particularly for the unseen benchmarks. The evaluation is limited to English data, which may restrict the generalizability of the findings to other languages. The paper could benefit from more comparisons with additional baseline models to further highlight the strengths of the proposed approach. The paper's contributions are primarily in the area of definition generation, and broader implications for other NLP tasks could be explored. The fine-tuning process is resource-intensive, which may limit the accessibility and reproducibility of the approach for researchers with limited computational resources.",
        "general_review": "The paper presents a novel approach to modeling lexical semantics by generating dictionary-like sense definitions, which is a significant contribution to the field. The methodology is technically rigorous, involving the fine-tuning of Llama models and the use of a T5-based model, which sets new state-of-the-art results in multiple tasks. The paper provides comprehensive empirical validation across several benchmarks, demonstrating the effectiveness of the approach in Definition Generation, Word-in-Context, Word Sense Induction, and Lexical Semantic Change tasks. The practical impact is significant as it offers a new way to interpret word meanings, potentially serving as a surrogate for dictionaries and aiding in language understanding tasks.\n\n1. The paper could benefit from minor improvements in writing clarity to ensure that the methodology and results are easily comprehensible to a broader audience. 2. Some small details on hyperparameters and implementation specifics could be elaborated for better reproducibility. 3. Figures in the paper might require minor formatting adjustments for enhanced readability. 4. There is a slight need for clarification on the availability of the code and data used in the experiments to facilitate further research."
    },
    "16": {
        "paper": "SMILE : Single-turn to Multi-turn Inclusive Language Expansion via\nChatGPT for Mental Health Support\nImportant: Our objective is to explore the potential of large language models to serve as virtual counselors for mental\nhealth support, and we do NOT advocate for their use as a substitute in therapeutic treatment without professional\nsupervision.\nAnonymous ACL submission\nAbstract\nDeveloping specialized dialogue systems for mental health support requires multi-turn conversation data, which has recently garnered increasing attention. However, gathering and releasing large-scale, real-life multi-turn conversations to facilitate advancements in mental health presents challenges due to data privacy protection, as well as the time and cost involved. To address the challenges related to data scarcity, we introduce SMILE , asingleturn to multi-turn inclusive language expansion technique with a solid theoretical foundation that prompts ChatGPT to transform public single-turn dialogues into multi-turn ones. Our study first focuses on basic characteristics, dialogue diversity, and quality among four large language models, verifying that our proposed method is superior to other baseline methods and that GPT-4o is the optimal option. Thus, we employ our method to generate a large-scale, diverse, and high-quality dialogue dataset named SMILE CHAT, consisting of 13k dialogues. Finally, we utilize SMILE CHAT to fine-tune six large language models, giving birth to mental health chatbots, MECHAT. Empirical evaluations demonstrate that MeChat excels in generating empathic, professional, helpful, and safe responses in mental health support, showing its high quality and practicality. 1 Introduction We all know the importance of mental health, and mental health issues (Kessler et al., 2005) have been a persistent concern for human beings. Recently, advancements in natural language processing (NLP) technology (Vaswani et al., 2017; Ouyang et al., 2022; Ni et al., 2022) have led to the emergence of neural-based conversational AI applied in various domains, including mental health (Liu et al., 2022; Tu et al., 2022). Virtual counselors powered by AI, as an innovative solution for mental health, can effectively address accessibility barriers such as the high cost of treatment and the SMILE Prompt\nSingle-turn Dialogue+\nLLMMulti-turn DialoguesMental Health Chatbot（MeChat）Figure 1: The SMILE method used to generate dialogues\nfor mental health support.\nshortage of trained professionals to meet the demand. Furthermore, such dialogue systems provide mental health support as an effective and practical online counseling approach for those in need, safeguarding user privacy and mitigating the stigma that often accompanies help-seeking. However, the lack of publicly available, large-scale, diverse, and high-quality multi-turn chat datasets in the mental health support domain hinders the development of specialized dialogue systems. Motivation. Indeed, building a practical, safe, and effective conversational agent for mental health is a goal that many researchers have been pursuing. However, the first step in creating such a system is to have training data. Conversations related to mental health support often contain sensitive information and must be kept confidential (Lu et al., 2021) to safeguard the privacy of individuals seeking help. Making these conversations publicly available may discourage individuals from seeking support or negatively impact their personal and professional lives once known to people they are acquainted with. To facilitate progress in the NLP community, some researchers have attempted to collect various dialogue corpora (Liu et al., 2021; Sun et al., 2021; Zheng et al., 2022) through crowd-sourcing, data crawling, or data augmentation to build a dialogue agent capable of providing emotional and mental health support. How to construct a large-scale, di1\nverse and high-quality multi-turn chat dataset for mental health motivates us to carry out the work as presented in this paper. Challenges. To be more specific, crowd-sourcing conversations (Liu et al., 2021) for emotional support has limitations due to the high cost and time required to train and manage annotators, as well as the difficulty in mimicking real-life interactions, that is, interlocutors may lack an understanding of the dilemma of living with mental disorders. An alternative is crawling QA (Sun et al., 2021) on a public mental health forum for training psychological support models. However, single-turn conversations may not be sufficient for resolving mental health issues, as multiple interaction exchanges are often needed. Multi-turn conversations, which can better simulate real-world conversations, are therefore more practical for training psychological support models. While the post-triggered machineaugmented method (Zheng et al., 2022) can address the limitations of scale and topic diversity, it does not take into account the responses of experienced supporters. Our Approach. To tackle the challenges mentioned above, we introduce SMILE , single-turn to multi-turn inclusive language expansion via ChatGPT, as shown in Figure 1. Specifically, we instruct ChatGPT to transform publicly available question-answer pairs (public QAs), which can also be viewed as single-turn dialogues, into multi-turn conversations. With the proposed method, we build alarge-scale, diverse, and high-quality multi-turn conversation dataset for mental health support. Our paper is organized as follows: •We first present our method ( §3), including data preprocessing and text generation, which mainly elaborates on the SMILE method and baseline methods. •We set up experiments ( §4) to demonstrate the superiority and effectiveness of the SMILE method. •We demonstrate the superiority and effectiveness of the SMILE method through basic characteristics, dialogue diversity, and quality among four large language models (§5). Following the validation of its superiority and effectiveness and expert evaluation, we leverage the SMILE method to generate a large-scale and diverse multi-turn chat dataset, SMILE CHAT, for mental health support. •Finally, we propose training a dialogue sys- 122tem to explore the quality of conversation ( §6) and collecting a 1073 real-life counseling dialogue sessions to construct a test set for model evaluation. Our Contributions We make our data, code, and model publicly available. We believe our work offers a new perspective on constructing a largescale, diverse, and high-quality multi-turn dialogue dataset for mental health within the research community. Our contributions can be summarized as follows: •We introduce SMILE , which is based on a solid theoretical foundation (Hill, 2020) and provides a novel perspective for alleviating the scarcity of multi-turn conversations in mental health. •Through the analysis of basic characteristics, dialogue diversity and quality, we verify the feasibility and effectiveness of our proposed method. This method can construct multi-turn dialogues based on medical, financial, and legal QAs, thereby alleviating the dialogue scarcity in other application domains. •To better assess the quality of SMILE CHAT, we collect a real-life counseling dataset with 47 counseling dialogues to build an real-life test set, PsyTest, which contains 1073 test samples. •We release SMILE CHAT, which comprises 13k Chinese multi-turn dialogues with an average of 15.7 turns. Additionally, we make our dialogue model, MECHAT, and real-life test set, PsyTest, publicly available. 2 Related Work 2.1 Applications of ChatGPT ChatGPT has proven to be a powerful AI tool for various NLP tasks since its release. Currently, it is being utilized in several domains, such as conversational AI (Alessa and Al-Khalifa, 2023; Köpf et al., 2023; Chen et al., 2023), education (Küchemann et al., 2023; Eshghie and Eshghie, 2023), code programming (Dong et al., 2023; Yeti¸ stiren et al., 2023) and healthcare (Zhao et al., 2023; Yang et al., 2023b). Furthermore, ChatGPT’s efficiency and costeffectiveness have been well-documented, making it competitive to human annotators (Gilardi et al., 2023; Zhu et al., 2023) even in zero-shot accuracy tasks. Xu et al. (2023) have proposed the use of self-chatting, where ChatGPT engages in a con2\nversation with itself, resulting in 111.5k dialogues collected from Quora and Stack Overflow sources and 47k conversations from the medical domain. Auto-GPT1, an AI agent, is capable of breaking down a natural language goal into sub-tasks and using various tools and the internet in an automated loop to achieve the objective. Shen et al. (2023) have suggested using ChatGPT for task planning when receiving user inquiries, selecting appropriate models based on function descriptions from Hugging Face, executing each subtask using the chosen AI model, and summarizing the response based on the execution’s outcomes. In summary, ChatGPT has already demonstrated its enormous potential as an intelligent pipeline tool that can significantly advance NLP development, despite having only a restricted API available for researchers. 2.2 Datasets for Mental Health Support Research on mental health support has significantly depended on the availability of publicly available datasets (Sun et al., 2021; Liu et al., 2021; Zheng et al., 2022) in recent years. The large-scale conversational datasets have enabled researchers to investigate various aspects of mental health, including identifying mental health conditions (Liu et al., 2023; Srivastava et al., 2022), understanding clients’ reactions (Li et al., 2023), predicting support strategies (Sun et al., 2021; Li et al., 2023), deciding personalized interventions (Golden et al., 2023) and understanding response safety within a dialogue history (Qiu et al., 2023). Liu et al. (2021) first define the emotional support conversation task and then, via crowdsourcing, construct ESConv, an emotional support conversation dataset containing 1053 dialogues with rich support strategies. However, the collection of ESConv requires high cost and time yet leads to a small-scale dialogue dataset. To this end, Zheng et al. (2022) present an approach for augmenting data scale with informative dialogue posts and then constructing AugESC, a modelsynthesized dataset with 102k dialogues. The previous two datasets are limited to English. To facilitate the research in Chinese, hence Sun et al. (2021) crawl QA posts in a public mental health support platform compiling PsyQA. 1https://github.com/Significant-Gravitas/Auto-GPT3 Method A QA can be considered a single-turn dialogue. PsyQA2, a high-quality Chinese dialogue dataset focused on mental health support, features one question mapped to multiple answers. The dataset was anonymized prior to release. Our dataset creation pipeline, based on PsyQA, includes two main stages: (1) QA preprocessing and (2) text generation. 3.1 QA Preprocessing In short, this process involves QA filtering and wording cleaning. QA Filtering Transforming single-turn dialogues into multi-turn ones requires rich contextual information. Therefore, we measure the Chinese character length of both client questions and counselor answers, retaining only instances where both exceed 300 characters. This process resulted in 13,709 QA pairs. Wording Cleaning We aim to construct a largescale, diverse, and high-quality multi-turn conversation corpus using the proposed SMILE method based on PsyQA. While QA can be considered a single-turn conversation between a real client and a counselor, there are differences in wording compared to actual multi-turn conversations. For instance, the term \" 楼主\" (literally meaning \"thread starter\") frequently appears in QA but is rarely used in conversation. Therefore, we propose a two-stage process to clean the wording in PsyQA, mitigating linguistic discrepancies before rewriting QA into multi-turn conversations. This process includes both automatic and manual cleaning procedures. For a detailed process, please refer to Appendix A. 3.2 Text Generation First, let us denote the input xas a sequence {x1, x2, ..., x n}, and the output yas a sequence {y1, y2, ..., y m}. The generation process of the language model can be expressed as the conditional probability distribution p(y|x), which represents the probability of generating output ygiven the input x. Therefore, text generation via the large language model can be formulated as follows: p(y|x) =mY\nt=1p(yt|y<t, x) (1) 2https://www.xinli001.com/qa\n3\nwhere ytrepresents the t-th token generated by the model. However, xis our main focus in this paper, next we will demonstrate the details of prompt design. 3.2.1 Prompt Design In this section, we mainly focus on describing prompt design. To provide a clearer understanding of our method in a more controllable setting and elucidate the superiority of introducing singleturn dialogues as a reference, we first design two baseline prompts for comparison. Standard Prompt The standard prompt does not contain any single-turn dialogues or specific dialogue topic and instead uses only the simplest prompt to generate multi-turn dialogues. The standard prompt is illustrated in Figure 6 in Appendix B. The input in Equation 1 is x=I, where Irepresents the standard prompt. We simplify the method name as standard and consider this method as our baseline. Standard Prompt with a Specific Dialogue Topic Intuitively, feeding a single, fixed prompt into a large language model often results in the generation of low diversity. Therefore, we provide a specific dialogue topic for the standard prompt. The input in Equation 1 is x= (I, T), where Trepresents the dialogue topic chosen in uniform sampling in the topic set. We simplify the method name to standardT and adopt this method as our baseline, as illustrated in Figure 7 in Appendix B. SMILE Prompt Our paper aims to highlight the superiority of the introduction of single-turn dialogues during generating dialogues. Our proposed method, referred to as the SMILE method, instructs the ChatGPT to rewrite single-turn dialogues into multi-turn ones. Figure 8 depicts the concrete prompt details. The input in Equation 1 isx= (I, T, D ), where TandDrepresent the dialogue topic hidden in the QA and single-turn dialogue, respectively. 4 Experiments 4.1 Large Language Models for Generation In this paper, we propose to use GLM-43, DeepSeek-V2-Chat4(also known as DeepSeek), 3https://open.bigmodel.cn/\n4https://www.deepseek.com/GPT-3.5-Turbo5, and GPT-4o6to generate dialogue given a specific prompt. Based on official recommendations, we set the temperature and top-p for GLM-4 to 0.7 and 0.9, respectively. For DeepSeek, GPT-3.5-Turbo, and GPT-4o, we set both the temperature and top-p to 1.0. For all four models, we set the maximum number of tokens for generation to 4000. 4.2 Dialogue Topic Collection To address the issue of monotonous generation, we collaborate with three professional counselors, refer to existing literature (Rickwood et al., 2007; Pedrelli et al., 2015), and ultimately compile a comprehensive set of dialogue topics. This set comprises 60 distinct types, each accompanied by a corresponding explanation. For more details, please refer to Appendix G. 4.3 QA Sampling Importantly, we have obtained 13,709 QA pairs. However, to emphasize the superiority of the SMILE method, we propose to sample 500 QAs to conduct preliminary experiments among four large language models. To ensure a fair comparison and prevent repeated occurrences of the same question, we first randomly select 500 non-duplicate questions. We then randomly choose one answer to serve as the corresponding response. The data samples obtained are employed as seed dialogues for the SMILE method, which are subsequently restructured into multi-turn conversations via large language models. 4.4 Text Representation We utilize three prompts to instruct four large language models, and each generate 500 dialogues. Therefore, we will analyze from the perspectives of methods and models. A multi-turn dialogue between a client and a counselor is represented as d={(u1, r1), ...,(ui, ri), ...,(un, rn)}(2) where uiandrirepresents the utterances of the i-th turn spoken by the client and counselor, respectively. A string of a dialogue without any speaker role tokens can be denoted as ds= 5The model we use is gpt-3.5-turbo-0125, with training\ndata up to Sep 2021.\n6The model we use is gpt-4o-2024-05-13, with training\ndata up to Oct 2023.\n4\nModelsAvg. Turns Avg. Characters\nstandard standardT SMILE standard standardT SMILE\nGLM-4 9.3 9.5 9.1 23.1 23.6 26.3\nDeepSeek 10.9 11.2 11.5 19.3 20.4 29.3\nGPT-3.5-Turbo 4.3 4.2 4.2 33.8 31.7 60.5\nGPT-4o 12.2 11.9 14.7 19.2 18.4 18.1\nTable 1: Analysis of basic characteristics. The best\nresults are highlighted in bold, the worst results are\nhighlighted in red.\n[u1;r1;u2;r2;...;un;rn], where [; ]denotes the operation of textual concatenation. Text representation is used for analyzing semantic diversity. To obtain the text embedding of a dialogue, we use the BAAI/bge-m3 model7, which accepts a maximum of 8192 tokens. Each dialogue is first preprocessed into a single string without any speaker tokens and then mapped to a 1024dimensional vector. For example, to compute the cosine similarity between two different dialogues, we can obtain cos(di, dj) =ei·ej\n∥ei∥∥ej∥(3) where eiandejdenote the text embeddings from two distinct dialogues. 5 Results 5.1 Basic Characteristics We present the basic analysis of average turns and average characters in Table 1. More dialogue turns indicate better performance. The model GPT4o achieves the best performance, while GPT-3.5Turbo achieves the worst results in terms of dialogue turns. Furthermore, GPT-3.5-Turbo obtains unsatisfactory results in terms of average Chinese characters. 5.2 Dialogue Diversity To demonstrate the effectiveness of the SMILE method, we mainly focus on three aspects of diversity: lexical features, semantic features, and dialogue topics. 5.2.1 Lexical Features For lexical analysis, we utilize the popular used Chinese tokenizer Jieba8to tokenize the dialogue. To measure the lexical features, we adopt distinct- n (n= 1,2,3) metrics (Li et al., 2016), which are widely used for measuring the diversity of dialogue datasets. Each dialogue is first preprocessed into a 7https://huggingface.co/BAAI/bge-m3\n8https://github.com/fxsjy/jiebasingle string without any speaker tokens. We provide statistics for 500 dialogues per prompt method, as presented in Table 2. Our proposed SMILE method results in rich vocabularies, with significantly higher numbers of unique unigrams, bigrams, and trigrams compared to the baseline methods. Specifically, a simple and fixed prompt tends to produce monotonous output, whereas incorporating dialogue topics into a single, fixed prompt leads to substantial diversification in the output. However, the SMILE method outperforms two baseline methods in terms of Distinct-1, Distinct-2, and Distinct-3 across four large language models. 5.2.2 Semantic Features To measure the semantic diversity of a dialogue dataset, we suggest calculating the cosine similarity between every pair of different dialogues. This involves computing the pairwise cosine similarity for each pair of distinct dialogues, resulting in 406\u00002\u0001\npairs and their corresponding cosine values, as described in Equation 3. We present the results in Figure 2, which demonstrates that the median of the SMILE method is significantly lower than those of the baseline methods. The SMILE method exhibits the most extensive semantic diversity. Further, GPT-4o obtains the best performance. 5.2.3 Dialogue Topics To measure the diversity of dialogue topics in a dialogue dataset, we utilize information entropy to measure the diversity of topic distribution. The higher the information entropy, the more uniform the distribution, indicating greater diversity. The formula for calculating information entropy (Rényi, 1961; Lin, 1991) is as follows: H(X) =−nX\ni=1p(xi) log2p(xi) (4) where H(X)is the information entropy. p(xi)is the probability of the occurrence of topic xi. To obtain dialogue topics for each dialogue in each prompt method, we design a prompt provided with 60 distinct dialogue topics, as illustrated in Appendix G and Figure 9, to automatically label dialogue topics for each dialogue with Qwen1.5110B-Chat. We present the information entropy for each prompt method in Table 3, demonstrating that the dialogues generated using the SMILE method 5\nModel MethodTotal\nUnigramsUnique\nUnigramsDistinct-1 ( ⇑)Total\nBigramsUnique\nBigramsDistinct-2 ( ⇑)Total\nTrigramsUnique\nTrigramsDistinct-3 ( ⇑)\nstandard 140095 2573 0.018 139595 19328 0.138 139095 44760 0.standardT 145194 3630 0.025 144694 27305 0.189 144194 60642 0.421 GLM-4\nSMILE 158811 5758 0.036 158311 40559 0.256 157811 85254 0.standard 137997 2126 0.015 137497 15113 0.110 136997 33944 0.standardT 149544 3197 0.021 149044 24156 0.162 148544 53403 0.360 DeepSeek\nSMILE 221763 7156 0.032 221263 50561 0.229 220763 106986 0.standard 91686 2238 0.024 91186 16108 0.177 90686 36664 0.standardT 84324 3059 0.036 83824 20708 0.247 83324 43909 0.527 GPT-3.5-Turbo\nSMILE 163760 6613 0.040 163260 46538 0.285 162760 98236 0.standard 153760 3562 0.023 153260 29902 0.195 152760 71735 0.standardT 144117 4530 0.031 143617 35918 0.250 143117 79236 0.554 GPT-4o\nSMILE 178595 7833 0.044 178095 55716 0.313 177595 113213 0.Table 2: Statistics of 500 conversations in each prompt method.\nMethod GLM-4 DeepSeek GPT-3.5-Turbo GPT-4o Avg.\nstandard 3.43 3.13 3.08 3.44 3.27\nstandardT 4.51 4.50 4.85 4.76 4.66\nSMILE 4.37 4.55 4.32 4.59 4.46 (∆ =−0.2)\nTable 3: Information entropy of dialogue topics.\nstandard-GLM-4standardT-GLM-4SMILE-GLM-4standard-DeepSeekstandardT-DeepSeekSMILE-DeepSeekstandard-GPT-3.5-TurbostandardT-GPT-3.5-TurboSMILE-GPT-3.5-Turbostandard-GPT-4ostandardT-GPT-4oSMILE-GPT-4o0.20.40.60.81.0Pairwise Cosine Similarity\nFigure 2: Pairwise dialogue cosine similarity among\nfour settings: our proposed three methods and a refer-\nence point using sampled data from PsyQA.\nare substantially more diverse than those generated using the standard method and are compatible with the standardT method, which uniformly samples dialogue topics. 5.3 Dialogue Quality To comprehensively evaluate the quality of generated dialogues, we propose an assessment framework that includes five perspectives: professionalism, informativeness, helpfulness, empathy, and safety. In collaboration with experts9in psychological counseling, we propose a set of assessment 9One is a Ph.D. in psychology and holds State-Certificated\nClass 3 Psycho-counselor with 4 years of experience in coun-\nseling. Another two individuals are State-Certificated Class 3\nPsycho-counselors with a master’s degree. The last one is a\ndoctoral student majoring in computer science and is the first\nauthor of this paper.criteria, as shown in Figure 11. We randomly sample 20 dialogues for each method from four large language models. Expert evaluations demonstrate that GPT-4o with the SMILE obtain the best performance. 5.4 S MILE CHAT Dataset Through the analysis of basic characteristics, dialogue diversity and expert evaluation, we conclude that the proposed method can generate a diverse andhigh-quality chat dataset. Therefore, we utilize the SMILE method to guide GPT-4o in generating all multi-turn conversations based on 13k QAs one round, leading to a large-scale dialogue dataset. Through the SMILE method with GPT-4o, we compile a collection of 13709 conversations, SMILE CHAT. Table 5 presents the statistics of the collected corpus. We present a dialogue generated by GPT-4o in Figure 10. 6 Dialogue System We aim to build a high-quality multi-turn chat dataset for mental health. Therefore, we also analyze the dialogue quality based on the performance of the dialogue system trained with S MILE CHAT. 6.1 Task Formulation To train a dialogue system for mental health, we need to split each dialogue into several training sessions. Specifically, a sampled t-turn dialogue session can be represented as follows: dt={(u1, r1),(u2, r2), ...,(ut, rt)} ∼D (5) We build a dialogue model that can predict the counselor’s utterance rtbased on the dialogue historyht={u1, r1, u2, r2, ..., u t}. Our objective is to maximize the likelihood probability as follows: L=−LX\nt=1logp(rt|u1, r1, ..., u t) (6) 6\nModelProfessionalism Informativeness Helpfulness Empathy Safety\nstandard standardT SMILE standard standardT SMILE standard standardT SMILE standard standardT SMILE standard standardT SMILE\nGLM-4 8.02 9.36 11.01 10.21 10.47 14.21 11.31 11.36 12.00 5.00 5.00 6.00 3.00 3.00 3.00\nDeepSeek 8.21 9.47 11.43 12.38 13.79 14.57 11.42 11.50 12.00 5.00 5.00 6.00 3.00 3.00 3.00\nGPT-3.5-Turbo 5.67 5.63 5.72 9.65 9.87 10.24 6.07 6.21 6.32 5.00 5.00 6.00 3.00 3.00 3.00\nGPT-4o 8.01 9.56 11.82 12.74 13.89 14.95 11.43 11.54 12.00 5.00 5.00 6.00 3.00 3.00 3.00\nTable 4: Results of human evaluation. The best results are highlighted in bold.\nCategory Total Client Counselor\n# Dialogues 13709 - -\n# Utterances 430400 215200 215# Avg. turns per dialogue 15.7 - -\nAvg. utterances per dialogue 31.4 15.7 15.7\nAvg. length per utterance 18.8 18.7 19.0\nTable 5: Statistics of the dialogue dataset, SMILE CHAT.\nwhere Lis the sequence length of rt. 6.2 Experimental Setup Baseline Model To validate the dialogue quality of our collected dataset, we conduct fine-tuning experiments on six popular large language models, including Qwen1.5-4B-Chat, Qwen1.5-7B-Chat (Bai et al., 2023), Baichuan2-7B-Chat (Yang et al., 2023a), deepseek-llm-7b-chat (DeepSeek-AI et al., 2024), internlm2-chat-7b (Cai et al., 2024), and Yi-1.5-6B-Chat (AI et al., 2024). Training Data To meet the data format requirements for instruction-based fine-tuning, we split the dialogue into multiple sessions, with the counselor’s last utterance concluding each session. Additionally, we incorporate the system prompt (detailed in Appendix E) as a prefix to dialogue messages, following OpenAI’s data format. Parameter-efficient Fine-tuning To preserve the original capabilities of the model while adapting to downstream dialogue tasks and reducing computational costs, we employ Low-Rank Adaptation (LoRA, (Hu et al., 2021)) on all linear layers in the model for efficient fine-tuning. Hyperparameters In this paper, all experiments are conducted on the NVIDIA A100 80G GPU for model training. During training, we set the training batch size to 4 on per device, and set the step of gradient accumulation to 2, meaning that gradient from every 2 steps would be accumulated and then used for parameter update. The learning rate is 1e-4. We adopt the cosine-type learning rate scheduler to adjust the learning rate throughout the training process. The entire training will span across 4 epochs. To accelerate the training and balance model performance, we also enable the use of 16-bit half-precision floating point numbers. In 515this paper, we implement the fine-tuning based on LLaMA Factory (Zheng et al., 2024), an efficient model tuning framework. 6.3 Evaluation 6.3.1 Automatic Evaluation Metrics To conduct automatic evaluation, the evaluation metrics we use consist of BLEU-1/2/3/4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), Rouge-1/2/L (Lin, 2004), Distinct1/2/3 (D-1/2/3) (Li et al., 2016), BERTScore (Zhang et al., 2019), and character difference between average predicted characters and average golden characters. Test Set To better understand and assess the dialogue quality of SMILE CHAT dataset, we propose to utilize real-life multi-turn counseling conversations. We develop an online mental health support platform that enables professional counselors to offer each client a free text-based counseling service, lasting approximately standard 50 minutes each time. We collect 47 real counseling dialogues. To protect user privacy, we ask experts to conduct a data anonymization process, removing information related to user identification (e.g., names, address). Then we split each long dialogue into multiple small sessions with the last utterance spoken by the counselor. We name this test set PsyTest, which contains 1073 test samples with a maximum turn of 10, and the average characters of counselor responses is 24.45. Results The results of the automatic evaluation, including 10 metrics, are presented in Table 6. Notably, the evaluated dialogues are based on realworld counseling data rather than generated dialogues, which excludes the influence stemming from synthetic data. In psychological counseling, automatic evaluation metrics can not measure the performance of models fine-tuned with domainspecific data. It is worthy noting that, the character difference of Qwen1.5-4B-Chat is 28.02 and it obtain better results on 9 metrics. However, average predicted characters are significantly larger than that in golden responses. 7\nMethod Model BLEU-1 BLEU-2 BLEU_3 BLEU-4 METEOR Rouge-1 Rouge-2 Rouge-L BERTSCORE Character Difference ( ∆)\nBaselineQwen1.5-4B-Chat13.65 5.29 2.93 1.88 14.33 18.42 3.08 15.56 0.6079 28.02\nFine-tuned 13.05 4.47 2.31 1.58 11.2 17.55 2.13 14.58 0.5938 2.42\nBaselineQwen1.5-7B-Chat8.68 2.34 1.02 0.65 11.79 13.01 1.1 9.69 0.6021 82.68\nFine-tuned 13.81 5.13 2.74 1.8 12.3 18.51 2.74 15.43 0.5947 1.23\nBaselineBaichuan2-7B-Chat11.15 4.24 2.21 1.41 12.85 16.55 2.7 13.38 0.6084 45.91\nFine-tuned 13.25 4.7 2.49 1.7 11.61 18.11 2.42 14.97 0.5918 1.05\nBaselinedeepseek-llm-7b-chat12.2 4.9 2.53 1.54 14.71 17.83 3.24 14.15 0.6074 51.0\nFine-tuned 12.38 4.48 2.36 1.59 10.73 17.82 2.42 14.73 0.5991 0.13\nBaselineinternlm2-chat-7b5.8 1.95 0.85 0.48 11.5 10.97 1.34 7.64 0.5859 151.67\nFine-tuned 13.92 4.98 2.64 1.74 12.26 18.65 2.59 15.05 0.5947 1.93\nBaselineYi-1.5-6B-Chat11.39 3.84 1.93 1.25 12.23 16.51 2.02 13.0 0.5928 42.13\nFine-tuned 12.74 4.22 2.16 1.51 10.76 17.66 1.97 14.21 0.5874 2.26\nTable 6: Results of automatic evaluation in PsyTest dataset. Character Difference( ∆) = Average Predicted Characters\n- Average Golden Characters. The best results are highlighted in bold after fine-tuning.\n0%25%50%75%100%Ground TruthBaselineFine-tuned Wins24%Fine-tuned Loses76%42%58%\nFigure 3: Human evaluation results, comparing the fine-\ntuned model to the baseline and golden. We present\nthe win and lose rates of each compared pair in randomly sampled sessions. Fleiss’ kappa (Fleiss et al.,\n1981) is used to measure the inter-rater agreement, and\nall values fall within moderate agreement with 0.5≤\nκ≤0.6.\n6.3.2 Human Evaluation Metrics We conduct a human evaluation to study the model performance trained with our proposed dialogue corpus. We select Qwen1.5-7B-Chat-finetuned as our mental health chatbot ( MECHAT). First, we randomly sample 100 sessions from PsyTest. Subsequently, we obtain 100 generated responses from the MECHAT model. Three professional counselors are then presented with a dialogue history and three randomly shuffled responses (baseline, fine-tuned, ground truth). They are tasked with selecting the optimal response for the dialogue history, considering aspects illustrated in Section 5.3. The evaluation is conducted based on the criterion in Figure 11. Results We employ majority voting to reach final decisions among three professional counselors. As depicted in Figure 3, the model trained with SMILE CHAT demonstrates a significant performance improvement compared to the baseline model. Moreover, the responses generated by MECHAT surpass the golden response to some extent in 42% of all dialogue sessions. Case Study We present a case study, as shown in Figure 4. First, during counseling conversations, Client: I just went through a breakup, and it hurts so much.Counselor:Response 1 [Fine-tuned]: Breakups are very painful experiences, and I can understand your feelings. (Affirmation and Reassurance) Can you tell me more about the situation surrounding your breakup? (Inquiring Subjective Information)Response 2 [Baseline]: I'm really sorry to hear that. Breakups can be incredibly painful. (Affirmation and Reassurance) I understand that it can bring about feelings of disappointment, sadness, loneliness, and loss. (Reflection of Feelings) The end of a relationship may make us feel like we've invested a lot, only to get an ending in return, which can leave us feeling even more lost and helpless. (Interpretation)If you're willing, we can discuss the impact of the end of this relationship on you and explore your current emotions. We can also talk about how to cope with these emotions and find some constructive solutions. (Invite to Explore New Actions)✓Figure 4: Case study. Counseling strategies used in the\ntwo responses are presented in parentheses. Strategies in\ngreen are supportive, while those in red are challenging\nand should not be used in the early stages of counseling.\nthe text should not be too long for the client to understand. Second, at the beginning of a conversation, a counselor generally will not challenge the client; instead, they will provide support. Third, response 1 is more professional, informative, helpful, empathic, and safe. Therefore, fine-tuning LLMs is indispensable in mental health support. 7 Conclusion This paper introduces SMILE , an effective solution for the scarcity of multi-turn conversations in mental health. By analyzing basic characteristics, dialogue diversity, and quality, we confirm the superiority and effectiveness of our method. Our approach enables the automatic creation of SMILE CHAT, a large-scale, diverse, and highquality dialogue corpus with 13k dialogues averaging 15.7 turns each. Both automatic and human evaluations using the PsyTest dataset, comprising 1,073 test samples, demonstrate that SMILE CHAT significantly enhances dialogue system performance in mental health. With the release of SMILE CHAT, our dialogue models ( MECHAT), and the authentic test set (PsyTest), we provide valuable resources to the research community. 8\nLimitations We release a large-scale, diverse, and high-quality multi-turn conversational dataset for mental health support, generated by rewriting single-turn conversations into multi-turn conversations using LLMs. Consequently, the dataset unavoidably incorporates LLMs’ model knowledge. Furthermore, we discuss how LLMs do not fully utilize the rich vocabulary and content of single-turn conversations, as reflected in the distinct- nmetric. Furthermore, there is currently no comprehensive dataset available for evaluating the effectiveness of mental health support models automatically. Therefore, we have identified this as a limitation of our paper and intend to address it in future work. Ethical Considerations Our research is reviewed and approved by the xxxx University Institutional Ethics Committee (xxxxxx). PsyQA is a single-turn dialogue dataset collected from an online mental health support platform. Specifically, help-seekers submit a post containing their mental health states and issues, and many professional counselors write down their responses to user questions to assist help-seekers. Therefore, PsyQA is a high-quality Chinese dialogue related to mental health support in the form of one question mapping to multiple answers. Following the data copyright guidelines formulated by PsyQA (Sun et al., 2021), we release the multi-turn dialogue corpus publicly available for research community. If researchers wish to reproduce the multi-turn dialogues using PsyQA, they must sign an agreement with the original data owner. Accordingly, we release our datasets and models for research purposes, thus facilitating further advancement in the academic community. Data Sharing Considering the nature of psychological counseling data, we must cautiously share this dataset. Regarding the rules for releasing data, third-party researchers who require access to the PsyTest dataset must provide us with their valid ID, proof of work, the reason they are requesting the data (e.g., the research questions), etc. They are required to be affiliated with a non-profit academic or research institution. This includes obtaining the approval of an Institutional Review Board (IRB), having principal investigators working full-time, as well as obtaining written approval from the in- 656stitution’s Office of Research or equivalent office. Additionally, they must sign the Data Nondisclosure Agreement and promise not to share the data with anyone. Expert Salary Each expert was paid 300 RMB for their work per day, which is higher than the average wage (250 RMB/day) in our city. Overall, we have guaranteed that our salary level is competitive in our city. 665",
        "aspect_review": "The paper introduces a novel and theoretically grounded methodology, SMILE, which effectively transforms single-turn dialogues into multi-turn conversations using ChatGPT, addressing the scarcity of multi-turn datasets in the mental health domain. The paper provides comprehensive experimental validation, demonstrating the superiority of the SMILE method over baseline methods in generating diverse and high-quality dialogues. Expert evaluations further confirm the effectiveness of the generated dialogues. The motivation to alleviate data scarcity in mental health dialogue systems is well-justified. The use of ChatGPT to expand single-turn dialogues into multi-turn conversations is a creative solution that maintains data privacy while providing high-quality training data. The resulting SMILE CHAT dataset is large-scale, diverse, and high-quality, significantly enhancing the performance of mental health chatbots. The release of this dataset, along with the MECHAT models and PsyTest, provides valuable resources for the research community.\n\nThe reliance on LLM-generated dialogues means the dataset may incorporate biases or limitations inherent in the models used, which could affect the authenticity of the dialogues. While the methodology is robust, there is a lack of detailed discussion on the potential biases introduced by using LLMs for dialogue generation. The paper could benefit from additional evaluation metrics that specifically assess the effectiveness of mental health support, beyond general dialogue quality metrics. The impact of the generated dataset on real-world applications is not fully explored, and there is a need for further validation of the dataset's effectiveness in practical settings.",
        "general_review": "The paper introduces a novel method, SMILE, for generating multi-turn dialogues from single-turn dialogues using ChatGPT, which addresses the data scarcity in mental health support dialogue systems. This approach is grounded in a solid theoretical foundation and demonstrates strong technical depth and rigor. The experimental validation is comprehensive, showing that the generated dialogues are diverse and of high quality, as evidenced by various metrics and expert evaluations. The practical impact is significant, as the resulting SMILE CHAT dataset and MECHAT dialogue models offer valuable resources for the research community, potentially advancing the development of mental health support systems.\n\n1. The paper could benefit from minor writing improvements for better clarity in some sections. 2. Details on hyperparameters and implementation specifics could be expanded slightly for reproducibility. 3. Some figures could be formatted more clearly to enhance readability. 4. While the code and data are available, slight clarification on the access process could improve transparency."
    },
    "17": {
        "paper": "StablePrompt : Automatic Prompt Tuning using Reinforcement Learning\nfor Large Language Model\nAnonymous ACL submission\nAbstract\nFinding appropriate prompts for the specific task has become an important issue as the usage of Large Language Models (LLM) has expanded. Generating prompts that appropriate the characteristics of the target LLM and tasks is manual and time-consuming work. Reinforcement Learning (RL) is promising for prompt tuning due to its ability to incrementally produce better results through interaction with the environment. However, its inherent training instability and environmental dependency make it difficult to use in practice. In this paper, we propose StablePrompt, which strikes a balance between training stability and search space, mitigating the instability of RL and producing high-performance prompts. We formulate prompt tuning as RL problem between the agent and target LLM, and introduce Adaptive Proximal Policy Optimization (APPO), a modified version of PPO for prompt tuning. APPO incorporates an anchor model that is adaptively updated based on the training trajectory. Using the anchor model for the KL-divergence term in PPO keeps the search space flexible and ensures training stability. StablePrompt achieves State-of-The-Art performance on various tasks, including text classification, question answering, and text generation. Furthermore, our methods are robust to the type and size of the model. 1 Introduction From Semantics (Bréal, 1900) to GPT-4 (Achiam et al., 2023), language models have undergone a huge evolution. Recently, large language models (LLM) have been applied beyond traditional natural language processing tasks to more general and diverse fields. While appropriate prompts enable LLMs to perform on par with or surpass human performance in many areas, finding these prompts is a manual and time-consuming process. 040Various automatic prompt tuning methods are developed to address this problem, such as directly optimizing soft prompts (Bailey et al., 2023), leveraging the generative abilities of LLMs (Zhou et al., 2022; Pryzant et al., 2023; Wang et al., 2023), and editing manual prompts (Zhang et al., 2022b; Dong et al., 2023; Hou et al., 2023). However, they struggle to find prompts that satisfy different formats of inputs and outputs for each task. Finding prompts that make the target LLM respond in the correct format is also difficult due to slight differences in LLM type and size. Reinforcement Learning (RL) is a promising method for addressing this problem as it allows the prompt to be updated using the responses of target LLM as rewards. However, its inherent training instability and environmental dependency raise practical challenges. In this paper, we propose StablePrompt, a noble RL-based method that keeps training stability while ensuring search space flexibility. We define automatic prompt tuning as an RL problem between an agent and a target LLM, and introduce Adaptive Proximal Policy Optimization(APPO) as an optimizer designed for prompt tuning. APPO combines the advantages of the original PPO (Schulman et al., 2017), which is well used in RL tasks, and the RLHF-style PPO (Ouyang et al., 2022), which is used for LLM training. Original PPO can explore a relatively large search space but suffer from learning instability. RLHF-style PPO leverages the language generation ability of LLM by using a fixed initial model but often finds suboptimal prompts due to the narrow search space. To take advantage of both methods, APPO employs an anchor model. The anchor model, a snapshot of the agent model in the training trajectory, is updated only when a significant increase in performance is observed We use this anchor model to modify the KL-divergence term of the PPO. Additionally, we extend StablePrompt for input1\n“Classify this sentence is positive or negative”“Read the choices and problem carefully and choose correct one”“Rewrite given sentence to passive form”Text Classification:SST-2Question Answering:MMLUText Generation:PassivizationAgent\nMistralGemmaLlamaTargetMistralGemmaLlama\n⋮this is a good script, good dialogue, funny even for adults.positivenegativeI think this movie …Generated PromptsData+\nRL-RewardWhich of the following …The professor mentioned …++AnswerFigure 1: Overview of StablePrompt. We construct an RL-framework using LLM as an agent and target, and train\nagent LLM to generate appropriate prompts for a given training dataset.\ndependent prompt generating, which is named Test-Time Editing version StablePrompt (TTEStablePrompt). TTE-StablePrompt is designed to handle tasks that are difficult to solve with a single prompt such as question answering. It can be easily extended by modifying the reward function while maintaining the framework of StablePrompt. We validate StablePrompt on various tasks, including few-shot text classification, question answering, and text generation. To the best of our knowledge, our method is the first RL-based approach that works on agents LLM larger than 7B. We conduct experiments on diverse agents and target LLMs, including Mistral (Jiang et al., 2023), Gemma (Team et al., 2024), Llama (Touvron et al., 2023), and Falcon (Almazrouei et al., 2023). In our experiments, we show that StablePrompt generates appropriate prompts for a wide range of LLM sizes, types, and tasks. StablePrompt achieves State-ofThe-Art (SoTA) performance across various tasks. Our contributions are summarized as follows: •We propose StablePrompt, which is RL-based prompt tuning method using APPO. APPO introduces an anchor model and modifies the KL-divergence term to keep training stable while ensuring the search space is flexible. •StablePrompt achieves SoTA performance on various tasks, including text classification, question answering, and text generation. It also can be used with various types and sizes of agents and target LLMs. •We extend StablePrompt to create an inputdependent prompt. It achieves high performance on tasks that are hard to solve with a single prompt. 1162 Relatd Work Automatic prompt tuning aims to find the appropriate prompts for a given dataset and target model. Soft prompt tuning (Bailey et al., 2023; Lester et al., 2021) uses direct gradient descent to search prompts. While it can find the optimal prompt, the generated prompt is often not readable and requires a substantial amount of data to converge. By contrast, discrete prompt tuning aims to find prompts in the form of natural language. This approach often operates like black-box optimization, making it suitable for API-based LLMs. It can be broadly categorized into generation-based methods and RL-based methods. Generation-based methods rely on the text generation abilities of LLMs to find prompts. Examples include APE (Zhou et al., 2022), which generates prompts by using example input-output pairs, ProTeGi (Pryzant et al., 2023), which improves prompts through iterative conversation, and PromptAgent (Wang et al., 2023), which edits prompts based on a Monte Carlo tree search. Since these methods rely on the performance of a pre-trained LLM without additional tuning, they struggle with tasks that are outside the scope of pre-training. RL-based methods use reinforcement learning to find prompts. Such as GrIPS (Prasad et al., 2022), BoostPrompt (Hou et al., 2023) and PACE (Dong et al., 2023), which edit the initial manual prompt using RL with an agent model. While these methods are relatively stable in training, they heavily depend on the quality of the manual prompt and the predefined action space for editing. RLprompt (Deng et al., 2022) is a pioneering work that proposed a method for training agent LLMs using RL. RLprompt adds an MLP layer to the agent LLM for training stability and trains only 2\non this layer. However, as the hidden size of the agent LLM increases, the size of the MLP layer increases, making it difficult to use. Also, these MLP layers are trained using only RL, which loses the initial model’s language generation ability. TEMPERA (Zhang et al., 2022b) is another pioneering study that used RL to explore inputdependent prompting. It adopts an agent model that shares a stem of the target LLM to generate input-dependent prompts. However, like GrIPS, it limited the action space to predefined prompts, suffering from a similar problem. It also struggles with scalability when the hidden size of the agent LLM becomes large. In this study, we extend the pioneering frameworks, to develop a stable and scalable RL-based method. 3 Method 3.1 RL Formulation Problem Definition Our method follows RLprompt (Deng et al., 2022) in terms of RL formulation. We formulate the discrete prompt tuning as the problem of finding the optimal discrete prompt z⋆for a given target model MTand a dataset D.z is defined in the target model’s vocabulary VLand satisfy the following equation. maxz∈VLR(MT(z, x), y) (1) where Ris pre-defined reward function, Lis length of prompt, and x, y∈D. We introduce an agent model Maas LLM that generates prompts autoregressively from random input-output pair (xr, yr)∈Dand task-agnostic meta prompt. We define this set of inputs as state s. Detailed meta-prompt can be found in Figure 6. Agent model generates prompts up to the length l according to the Ma(zl|s,z<l). After zis created, it receives a reward from the R(MT(z, x), y). The full training objective function is below: max MaR(MT(z, x), y),z∼ΠL\nl=1Ma(zl|s,z<l)\n(2) Original PPO As a method for training LLM agents with RL, we adopt Proximal Policy Optimization (PPO). We add a value head to the last layer of the LLM agent, which is trained using MSE loss to predict reward values for inputs. Lv= (vpreds−reward )2(3) 197The value expected from value head is used with reward to compute advantage A, which uses Generalized Advantage Estimation (GAE) and clipped. A=GAE (vpreds, reward ) (4) ratio =θt(z|s)\nθt−1(z|s)(5) Aclipped =clip(ratio, 1−ϵ,1 +ϵ)∗A (6) where θis parameter of agent model and tis timestep. Then calculate the penalty Pwhich is the KLdivergence between the previous version of the agent model and the current version. The full agent loss is the following : P=KL(θt(z|s)||θt−1(z|s)) (7) Lagent =Aclipped +P (8) The final PPO objective is the following : LPPO =Lv+Lagent (9) In practice, we perform parameter-efficient training using LoRA (Hu et al., 2021) and update only value head and LoRA adaptor. 3.2 StablePrompt Anchor Model We introduce an anchor model, which is a copy of the agent model with validated performance improvements in the training trajectory. The anchor model starts as a copy of the initial agent and is carefully updated at a predefined update period ut. If the performance of the current agent model is higher than an update threshold compared to the anchor model, the anchor model is updated to the copy of the current agent model. Conversely, if the agent model underperforms the anchor model by less than a rollback threshold, the agent model is rolled back to the anchor model. This allows the anchor model to adaptively update based on the characteristics of the task. If the reward signal is stable or requires many updates to find the optimal prompt, the anchor model is updated accordingly. On the other hand, if the reward signal is unstable or does not require many updates to find the optimal prompt, the anchor model is updated in a few steps or not. The suggested anchor model reduces the environmental dependence of RL by ensuring a performance-validated model for the KL-divergence penalty. 3\n+PromptAdaptiveProximal Policy Optimization(APPO)KLdivergenceRewardWeight UpdateInput DataMeta Prompt…Input 1Output 1Input kOutput kAnchor ModelTarget Model\nAgent Model\nUpdate ModelCommutativelyFigure 2: Training framework of StablePrompt. Generate prompts using the Task agnostic meta-prompt, and\ncalculate the reward of the generated prompts with training data.\n𝜃!𝜃\"#$%&'Optimal Prompt\nOriginal PPOAPPO (Ours)\nFigure 3: Illustration comparing APPO to the origi-\nnal PPO. The circle represents the constraint of KL-\ndivergence, and each dot represents the parameter of\nthe agent model at each time step. APPO is robust to\nincorrect rewards because it maintains an anchor model,\nwhile PPO deviates from the optimal prompt as incor-\nrect rewards accumulate.\nAdaptive PPO The KL-divergence penalty term (7) uses the parameters of the previous model to prevent the current model from changing too much. But as the steps get longer, the model can gradually diverge from the initial. When unstable reward signals accumulate, this can lead the model into a local minima. In RLHF-style PPO (Ouyang et al., 2022), the penalty term (7) is modified by KL(θt(z|s)||θ0(z|s))to prevent the agent model from deviating too far from the initial version. This is appropriate for a task like RLHF that needs to answer a wide variety of questions while not losing the initial language generation capability. However, in prompt tuning, RLHF-style PPO limits the agent’s search space to the initial agent, which leads to suboptimal prompts. We propose Adaptive PPO (APPO), which combines the advantages of RLHF-style and original PPO, achieving both training stability and an extensive search space. We leverage the anchor model to modify Equation (7) as follows : PAPPO =KL(θt(z|s)||θanchor (z|s)) (10) 266This term restricts the agent model from diverging too far from an anchor model. This approach allows for more conservative agent updates compared to the original PPO while ensuring a larger search space compared to RLHF-style PPO. The full objective of APPO is below: LAPPO\nagent =Aclipped +PAPPO (11) LAPPO =Lv+LAPPO\nagent (12) Reward Function We design reward functions for RL. For text classification, we use accuracy and softmax difference. While accuracy is a good reward function, it has discrete values, which can lead to many prompts having the same accuracy. This problem is often encountered in scenarios with limited training data, such as few-shot text classification. To mitigate this, we introduce the softmax difference, which subtracts the highest value among the incorrect options from the value of the correct answer from the softmax output. R(z, x, y) =caAcc(z, x, y) +csD(z, x, y)(13) D=MT(z, x)i=y−max[MT(z, x)i̸=y](14) This metric is used to rank prompts when they have the same accuracy. The softmax difference is also widely used in other RL frameworks for classification (Han et al., 2023). For text generation, we use the F1 score directly as the reward function. 3.3 Test Time Editing StablePrompt For tasks that are difficult to solve with a single prompt, we expand StablePrompt to generate prompts that depend on the input query. We call this extended version as Test-Time Editing StablePrompts (TTE-StablePrompt). In TTE-StablePrompt, input state sis defined by the set of meta-prompt, example input-output pairs and current input. The agent generates a prompt 4\nfor the current input. The generated prompt and current input are fed into the target model to calculate rewards. Detailed meta prompt can be found in Figure 6. We keep the same settings for the other parts of the method. This approach is different from StablePrompt, which uses the average value of the training batch as its reward. The reward of TTE-StablePrompt is calculated using only the current input. This instance reward signals in TTE-StablePrompt train the agent model to generate prompts that are effective for specific inputs, rather than prompts that are effective across the entire dataset. 4 Experiment 4.1 Few Shot Text Classification Datasets Few-shot text classification is a task that has been used in many previous prompt tuning studies, including (Deng et al., 2022; Zhang et al., 2022b). We use datasets that are subsets of GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), including sentiment analysis datasets (SST-2) and natural language inference datasets (MRPC, MNLI, QNLI, SNLI, and RTE). For inference, similar to previous studies, we used a verbalizer with predefined class label tokens. When determining the model’s predictions, we selected candidates only from the set of verbalizers. Detailed dataset statistics and verbalizer settings can be found in the Table 7. Baselines As a baseline, we compare the various methods. Our baselines include supervised fine-tuning methods such as LoRA fine-tuning and soft prompt tuning (Bailey et al., 2023). We also use fixed prompts including hand-crafted manual prompts, few-shot prompts, and zero-shot chain of thought (Zero-Shot CoT) (Wei et al., 2022). For direct comparison with StablePrompt, we use various discrete prompt tuning methods. These include generation-based methods such as APE (Zhou et al., 2022) and ProTeGi (Pryzant et al., 2023), and RL-based method such as GrIPS (Prasad et al., 2022), PromptBoosting (Hou et al., 2023) and RLprompt (Deng et al., 2022), which is directly comparable to ours. In the case of TEMPERA (Zhang et al., 2022b), the agent model grows proportionally to the size of the target model, making it difficult to experiment with 7B models. Therefore, we conducted experiments on the 330M RoBERTa-large (Liu et al., 2019) model and included the results in the Table 8. 354Implementation Detail We perform two experiments on Few-Shot Text Classification. One is an experiment with both the target and the agent model fixed to gemma-1.1-7B-it (Gemma-7B) (Team et al., 2024) for comparison with the baselines. For RLprompt, due to its MLP layer size overhead, we use GPT2-XL (Radford et al., 2019) as the agent. The other experiment runs StablePrompt on five target models: gemma-1.1-2B-it (Gemma-2B), Gemma-7B, Mistral-7B-it-v2.0 (Mistral-7B) (Jiang et al., 2023), llama3-8B-it (Llama3-8B) (Touvron et al., 2023), and falcon-11B (Falcon-11B) (Almazrouei et al., 2023), and four agent models: Gemma-2B, Gemma-7B, Mistral-7B, Llama3-8B. We report the average accuracy of 6 datasets. All experiments were performed with three distinct random seeds. For the generated prompts, we used the template \"[prompt] Input : [input] Output :\"for prediction. We use only 16 samples per label for training. The generated prompts of each step are queued in pairs with rewards. At the test time, the top 5 prompts in order of reward are selected for testing and report the highest performance. This is the same method as RLprompt. Detailed numbers are shown in the Table 6. Result Table 1 shows the performance of various baselines and StablePrompt. StablePrompt achieves State-of-The-Art (SoTA) performance on all tasks except QNLI. In QNLI, StablePrompt also achieves the best performance among the discrete prompt tuning methods. The average score also outperforms APE and achieves SoTA. We present the full version of the generated prompt in the Appendix C.1. Figure 4 illustrates the performance of StablePrompt across various Agent-Target pairs. The values in the heatmap are the averages of six datasets. StablePrompt outperforms manual prompts across all pairs except (Gemma-2B, Llama3-8B) pair. These results demonstrate that our method is robust to model sizes, such as a small agent model of 2B and a large target model of 11B. Specifically, when comparing Mistral-7B and Falcon-11B, the manual prompt performance is higher with falcon-11B, but with appropriate prompting from StablePrompt, Mistral-7B outperforms falcon-11B. This shows that an appropriate prompt can enable a small model to easily understand a given task and achieve performance comparable to that of a large model. 5\nMethod SST-2 MRPC RTE QNLI MNLI SNLI Average\nFine-TuningFine-Tuning 71.9 59.6 55.7 63.1 41.1 64.8 59.3\nSoft prompt tuning 78.3 57.1 51.6 89.0 34.9 55.8 61.1\nFixed promptManual prompt 89.1 51.0 64.0 73.0 67.0 47.0 65.2\nZero-shot CoT 57.9 38.4 81.6 75.2 71.1 66.3 65.1\nFew-shot prompt 55.0 49.0 76.0 82.0 58.0 52.2 62.0\nDiscret prompt tuningGrIPS 84.7( ±4.6) 55.6( ±2.6) 60.9( ±3.5) 28.9( ±1.2) 44.4( ±1.1) 63.5( ±2.3) 59.4\nPromptBoosting 65.4( ±1.0) 52.7( ±1.1) 71.6( ±0.9) 71.6( ±1.1) 35.5( ±1.4) 52.6( ±1.8) 58.2\nAPE 83.2( ±7.7) 55.3( ±4.9) 78.6(±1.3) 75.0( ±2.2) 54.6( ±7.9) 72.3 (±4.8) 70.1\nProTeGi 69.2( ±8.4) 48.8( ±1.3) 73.2( ±6.3) 74.2( ±7.7) 56.6(±10.9) 61.3( ±12.3) 64.0\nRLprompt 70.8( ±6.5) 56.0 (±1.5) 67.3( ±2.5) 62.6( ±1.3) 54.6( ±1.9) 56.6( ±1.3) 61.3\nStablePrompt\n(Ours)92.5(±1.3) 71.3(±3.4) 81.5(±2.8) 75.9 ( ±1.4) 63.3(±1.2) 74.1(±1.4) 76.4\nTable 1: Result of 6 few-shot text classification datasets. Generated prompt can found in Appendix C.1\nMP G2 G7 M7 L8\nG2G7M7L8F11\n51 62.563.162.561.771.273.676 75.474.962.577.578.979.379.855.454.959.562.764.970.974.878.878.778.2Agent ModelTarget Model\nFigure 4: Heatmap of few-shot text classification tasks\non diverse target-agent pairs. Reported numbers are\nan average of 6 datasets. MP : Manual prompt, G2:\nGemma-2B, G7: Gemma-7B, M7: Mistral-7B, L8:\nLlama-3-8B, F11: Falcon-11B.\n4.2 Induction Task Datasets We experimented with an induction task in which the agent has to provide a rule for an input-output pair as a prompt. We used the Instruction Induction dataset (II) (Mishra et al., 2022) and BigBench-Instruction Induction dataset (BBII) (Zhou et al., 2022), a subset of BiGBench (Ghazal et al., 2013). These include tasks such as editing the input sentence or finding answers according to rules. Each task requires prompts in the form of instructions designed to help the target model induce the correct answer. The tasks consist of text classification and text generation, requiring an understanding of various fields such as spelling, morphosyntax, and phonetics. We conducted experiments on BBII, which has 20 subsets, and Instruction Induction, which has 23 subsets. The dataset details can be found in Appendix A.3. 423BBII II\nText\nClassificationText\nGenerationInstruction\nInduction\nManual Prompt 51.57 37.61 33.70\nPromptAgent 28.50 - -\nAPE 56.46 49.59 51.94\nProTeGi 56.58 55.61 51.60\nStablePrompt\n(Ours)57.75 61.36 65.80\nTable 2: Result of BigBench-Hard Instruction Induction\n(BBII) and Instruction Induction (II) datasets. For BBII,\nwe divided it into two parts based on the type of task.\nFull results can be found in Table 10 and Table 11.\nImplementation Detail We performed experiments with two different target models. One used the Gemma-7B and the other performed experiments on InstructGPT3.5. For the first experiments, due to the large number of datasets, we used APE and ProTeGi as baselines, and we included PromptAgent (Wang et al., 2023) which is a Monte Carlo tree search-based generation method designed for BigBench text classification tasks. In experiments using InstructGPT3.5, due to cost issues, we used APE, which requires fewer steps. For APE, we used various agent models such as (Zhang et al., 2022a; Zeng et al., 2022). We use PACE (Dong et al., 2023), an RL-based editing method designed for induction tasks, and a human prompt from the same paper as the baseline. For text classification, we used the same reward function as Section 4.1. For text generation, we use f1 score as reward function. We use the same template as Section 4.1 for both BBII and II. Results Experiments on Gemma-7B target model are presented in Table 2. Our method achieves SoTA on both BBII and II. In particular, it outper6\nMethod Human promptHuman prompt\n+ PACEAPE StablePrompt\nAgetn Model - - GLM OPT InstructGPT3.5 Mistral\nParameters - - 130B 175B unknown 7B\nAntonyms 85.0 87.0 78.0(±0.5) 82.7( ±0.7) 81.0( ±0.7) 83.7( ±0.9)\nCause selection 84.0 85.0 53.3( ±0.1) 65.3( ±1.0) 72.0( ±1.0) 88.7(±1.0)\nPassivization 100.0 100.0 7.3( ±0.0) 100.0( ±0.0) 100.0( ±0.0) 100.0( ±0.0)\nSecond Letter 99.0 100.0 3.3( ±0.9) 100.0( ±0.0) 100.0( ±0.0) 100.0( ±0.0)\nSentiment 91.0 92.0 87.7(±0.8) 82.7( ±0.9) 88.3( ±0.8) 90.7( ±0.9)\nTranslation en-fr 89.0 88.0 79.7( ±0.8) 85.3( ±0.8) 84.3( ±0.8) 90.3(±1.0)\nAverage on 6 tasks 91.3 92.0 51.8 68.6 89.3 92.8\nAverage on 24 tasks 79.8 80.3 - - 77.5 81.5\nTable 3: Result of 6 selected tasks and an average of all 24 tasks in the Instruction induction dataset with\nInstructGPT3.5 as the target model. Full results can be found in Table 12.\nSTEMSocial\nSciencesHumanities Other Average\nmanual prompt\n+ fewshot47.1 61.6 55.4 54.5 53.9\nZero-Shot CoT 49.2 59.6 54.5 56.0 54.2\nAPE 45.0 59.3 56.4 51.1 52.1\nProTeGi 45.7 59.7 56.0 55.3 53.3\nRLprompt 46.5 55.1 56.6 55.7 52.8\nStablePrompt\n(Ours)47.8 63.6 58.6 59.0 56.3\nTTE-StablePrompt\n(Ours)49.6 65.7 59.6 58.8 57.5\nTable 4: Result of MMLU QA datasets with Gemma-7B\nas target model. Full results can be found in Table 13\nforms on the text generation tasks II and BBII. This shows the effectiveness of the RL framework on the text generation tasks, where the format of the output is important. Table 3 shows the experiments conducted using InstructGPT3.5. StablePrompt shows strong performance even with the large black-box model InstructGPT3.5 as a target model. This highlights the benefits of RL-based method, which works well when the target model is not publicly accessible. Note that our method outperforms APE, which uses models larger than 100B as the agent. In particular, the 7B model trained by StablePrompt produces better prompts than the commercial black box model InstructGPT3.5. This shows that our method does not rely on the ability of the agent model and is cost-efficient by using a small model. 4.3 Question Answering Datasets We conducted an experiment on a Question Answering (QA) task. In this paper, we use the MMLU (Hendrycks et al., 2020) dataset, which requires users to answer questions from various fields. We report the performance of 57 question 469topics from MMLU, categorized into STEM, Humanity, Social Science, and Others. The verbalizer is used in the same way as for text classification. We present 4 options (A,B,C,D) in a question and use the alphabet corresponding to each option as a verbalizer. The reward function is the same as Section 4.1. Detailed numbers of datasets can be found in Appendix A.3. Implementation Detail The target and agent model are both fixed with Gemma-7B. For the prompt, we used the template \"[Prompt] Question : [Question] Choice : [Choice] Output :\" . We trained the model using 20 question-answer pairs from the validation dataset for each topic. Result Table 4 shows the performance of various baselines. StablePrompt achieves the highest performance among the baselines. In particular, StablePrompt outperformed in all fields except STEM. Note that TTE-StablePrompt outperforms StablePrompt. There are many different questions on the same topic that are difficult to solve with a single prompt. TTE-StablePrompt, which gives different instructions depending on the input within the same subject, is more effective than StablePrompt, which only uses a single prompt. TTEStablePrompt also performs better than Zero-Shot CoT, which uses the same multi-step reasoning and is known to perform well on maths/science tasks. Figure 5 shows question-choice pairs from the machine learning dataset in MMLU, along with the prompts generated by APE, StablePrompt, and TTE-StablePrompt. APE and StablePrompt generate almost semantically similar prompts which can be generally used for all questions in subject. However, TTE-StablePrompt generates prompts appropriate to the given question (emphasized with 7\nQuestionIn building a linear regression model for a particular data set, you observe the coefficient of one of the features having a relatively high negative value. This suggests that:ChoiceA : This feature has a strong effect on the modelB : This feature does not have a strong effect on the modelC : It is not possible to comment on the importance of this feature without additional informationD : Nothing can be determined.StablePrompt**Instructions:** Look at the input and try to understand what information is being presented. Consider the …TTE-StablePromptRemember the bias-variance trade-off in model building. Consider whether a high negative coefficient …APECarefully read the statement and consider the available choices. Based on you understanding of the information …Figure 5: Generated prompts and input in machine learning subset of MMLU dataset. We truncated the latter part of\nthe generated prompt for readability. Full prompt can find in Appendix C.5\nOriginal\nPPORLHF-style\nPPOAPPO\n(Ours)\nSST2 91.5(±0.7) 91.1( ±1.0) 92.5(±1.3)\nMRPC 65.9( ±9.0) 70.6(±2.2) 71.3(±3.4)\nRTE 80.2( ±2.1) 80.3(±2.0) 81.5(±2.8)\nQNLI 70.2( ±2.1) 76.7(±1.6) 75.9(±1.4)\nMNLI 66.2(±2.5) 61.0( ±1.2) 63.3(±1.2)\nSNLI 69.5( ±1.9) 70.4(±3.3) 74.1(±1.4)\nAverage 73.3 74.2 76.4\nTable 5: Result of ablation study on few-shot text clas-\nsification tasks. We report the average and standard\ndeviation of experiments from 5 distinct random seeds.\nunderlining). This shows that a simple TTE extension effectively creates an input-dependent prompt. 4.4 Ablation Study Experiment Settings We conduct an ablation study for APPO. We use same settings as few-shot text classification. We fix the agent and the target model to Gemma-7B. Results Table 5 shows the performance of PPO variants. APPO performs well on average across all tasks by leveraging the strengths of both the original PPO and RLHF-style PPO through adaptive anchor model updates. APPO can either behave like the RLHF-style PPO, with no updates, or like the original PPO, with updates in each every update period. In particular, when the performance gap between the original PPO and RLHF-style PPO is significant, APPO adapts to the better performing model. This pattern is observed in tasks like 523MRPC, QNLI, and MNLI. Additionally, in tasks such as SNLI, APPO can identify more appropriate prompts than either the original or RLHF-style PPO alone. 5 Conclusion In this paper, we propose a novel RL-based prompt tuning method, StablePrompt. We define prompt tuning as an RL problem and introduce APPO, which is an improved version of PPO. APPO has the advantages of both RLHF-style and the original PPO. It introduces an anchor model that is updated adaptively to reduce task dependency and increase training stability. We demonstrate its performance through experiments on a variety of tasks, model sizes, and model types. To the best of our knowledge, this is the first RL-based tuning approach that works effectively on agent models larger than 7B. 6 Limitation The limitations of this study can be summarized as follows: (1) This paper does not cover experiments that are significantly beyond the scope of prior learning, such as medical and legal domains; however, since it is a training-based method, it is expected to be scalable in future work. (2) This paper can be used to abuse LLM for specific purposes. This is a particular threat to commercial LLMs in the API format because they are based on black-box optimization. 8",
        "aspect_review": "The paper introduces StablePrompt, a novel RL-based prompt tuning method using Adaptive Proximal Policy Optimization (APPO) which combines the strengths of existing PPO methods to enhance training stability and search space flexibility. StablePrompt is the first RL-based prompt tuning approach that effectively works on agent models larger than 7B, showcasing significant innovation in the field. The method achieves State-of-The-Art performance across various tasks, demonstrating its robustness and potential impact on the field of prompt tuning for large language models.\n\nWhile the experiments demonstrate the effectiveness of StablePrompt, more detailed exploration of hyperparameters and implementation details could enhance reproducibility. The paper could benefit from minor improvements in clarity, particularly in the explanation of the APPO algorithm and its integration with the anchor model.",
        "general_review": "The paper presents a novel and highly impactful method, StablePrompt, which addresses the critical challenge of prompt tuning for large language models using reinforcement learning. The introduction of Adaptive Proximal Policy Optimization (APPO) is a standout feature, offering a balance between training stability and search space flexibility, which is a significant advancement over existing methods. The comprehensive experimental validation across various tasks, including text classification, question answering, and text generation, demonstrates the robustness and state-of-the-art performance of the proposed method. Additionally, the method's applicability to different types and sizes of models highlights its practical impact and potential for wide adoption.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the APPO mechanism. 2. Additional details on hyperparameters and implementation specifics would enhance reproducibility. 3. Some figures could be formatted more clearly to improve readability. 4. A slight clarification on the availability of the code would be beneficial for the community."
    },
    "18": {
        "paper": "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to\nIdentify Communities on Social Media\nAnonymous ACL submission\nAbstract\nThe large scale usage of social media, combined with its significant impact, has made it increasingly important to understand it. In particular, identifying user communities, can be helpful for many downstream tasks. However, particularly when models are trained on past data and tested on future, doing this is difficult. In this paper, we hypothesize to take advantage of Large Language Models (LLMs), to better identify user communities. Due to the fact that many LLMs, such as ChatGPT, are fixed and must be treated as black-boxes, we propose an approach to better prompt them, by training a smaller LLM to do this. We devise strategies to train this smaller model, showing how it can improve the larger LLMs ability to detect communities. Experimental results show improvements on Reddit and Twitter data, on the tasks of community detection, bot detection, and news media profiling. 1 Introduction The rise of social media platforms over the last decade has had a tremendous impact on people’s lives, affecting their perspectives on key events such as political elections (Mitchell et al., 2016; Shu et al., 2019) and led to the creation of segregated information communities, also known as “echo chambers” (Gentzkow and Shapiro, 2011; Quattrociocchi et al., 2016; Dubois and Blank, 2018; Garimella et al., 2018). Following the the principal of social homophily (McPherson et al., 2001; Bessi et al., 2016), these tightly-knit communities consist of like-minded users, which have similar viewpoints and content preferences. Identifying these information communities can lead to better performance in a number of important social media related downstream tasks, such as news media profiling (fake news and political bias detection), user content recommendation, trend prediction, crisis monitoring, sentiment analysis, and Which users have similar perspectives and thus should be the same community?User 1: User 1 Text DescriptionUser 2: User 2 Text Description …User 6: User 3 Text DescriptionOutput: User 1, User 3.Figure 1: An example of the LLM Community Detection\nTask: Given a set of users and their textual descriptions, deter-\nmine which users are similar and have similar perspective.\nmore (Bedi and Sharma, 2016). For example, for media profiling, groups of users sharing left-biased news in the past, are likely to do so in the future. The community identification task is typically formulated as a form of graph analysis, either predicting missing edges (i.e., friendship relationships), graph clustering (i.e., community detection), or more recently with deep learning, such as using graph neural networks (GNN) (Liu et al., 2020). However, due to the diversity of content found on social media, understanding users’ perspectives using a fixed training set is highly challenging. For example, in the settings of emerging news events , the system is evaluated on its ability to adapt to new events, consisting of previously unseen users and topics. This temporal and topic shift at test time, hurts the performance of many models, and they must be retrained (Zhang et al., 2023). Since these settings are highly realistic (new topics and events emerge on social media everyday), we focus this paper on them and we evaluate these settings across a range of social media-related tasks. In this paper, we explore a new direction for tackling such social inference tasks, inspired by the recently popular Large Language Models (LLMs), such as ChatGPT (OpenAI, 2022), which perform well on many NLP tasks. Specifically, given their ability to assess textual similarity well (OpenAI, 2023; Li and Li, 2023), we ask – can the strong textual similarity performance extend to the task of community detection? Given a set of users and text describing their viewpoints, we explore whether 1\nLLMs can identify if any of the users are similar. This way, social inference is reduced to a simpler text similarity problem (comparing user’s text descriptions), and LLMs can help us form information communities. Fig. 1 shows an example of this community detection LLM task. Intuitively, given their massive training datasets, LLMs have the potential to generalize across time periods and events, identify users with similar viewpoints, and thus perform well in the important emerging news events settings. However, we find that this task is still difficult for LLMs. We noticed that LLMs often focus on the high-level aspects of users to determine if they belong in the same community, favoring similarity of interest topics rather than nuanced opinions about them. As a result, LLMs often do not form meaningful communities. For example, two users discussing a popular entity like “Donald Trump”, could be considered similar by a LLM, when in reality it’s the context and attitudes expressed towards “Donald Trump” that makes them similar or not. If instead the LLM focused on how the users discuss Donald Trump (for example, their opinions on Trump’s perspective on issues like gun control) then the LLM could correctly separate users into meaningful communities. Our key technical contribution follows this intuition. We hypothesize that focusing the LLM on the relevant aspects of users would result in better information communities . We propose several models for automatically adding to the LLM prompt the exact topics and entities it should focus on to separate users into an information community. With the help of this additional information, the LLM can compare the user descriptions, focus on the divisive issues, and form the correct community. We call this additional prompt sentence a focus area . For example, in the running Donald Trump example, the focus area could be: Focus on how the users discuss Donald Trump’s views on gun control . Tab. 8 show more ex. of good focus areas, and Fig. 2 shows how they can be useful. Since many of the best performing LLMs are only accessible through an API, or are too large for task-specific training, we treat these models as black-boxes, and train a smaller LM to generate the focus area. This approach offers several advantages, such as being directly usable on top of any LLM, without changing the LLMs performance. We compare several variants of our approach, using the LLM directly (without focus areas), using the LLM to generate the focus areas and finally, (a) No focus area, incorrect(b) Generated focus area, correct!\nPrompt: What users are in the same community?\nFocus Area: How users discuss Trump’s views on gun control.\nPrompt: What users are in the same community?Figure 2: An example of how Focus Areas can help. Without\nthem (a), the LLM incorrectly forms the community (red\nusers), but with them (b), the LLM focuses on the divisive\nissues and correctly forms the community (green).\ntraining the smaller LM to generate the focus areas and augment the LLM prompts. We train the smaller LM using Reinforcement Learning (RL). The reward signal used by the RL algorithm is obtained by combining several rewards, such as the performance of the LLM when using the generated focus areas and “unsupervised” metrics capturing focus area topic relevance, informativeness, impact, length, and more (see Sec. 3.4.1 for details). We evaluate our approach in two settings. First, we define an intrinsic evaluation over Reddit and Twitter data, where users are sampled from known communities. Our goal is to recover the groundtruth community memberships via the focus-area augmented LLM prompts. Second, we look at the contribution of a focus-area augmented LLM based approach for downstream tasks that require social information – identifying false information and political bias in news media. Here, the gold community membership is unknown and can only be gauged by its contribution the downstream task. In both settings we model the out-of-domain emerging news event settings, by training the focus-area generator on a single community, and using it to generate focus areas for new, unseen communities. In short, we make the following contributions: (1)We propose to use large, frozen LLMs to detect information communities on social media. (2) We train a smaller LM to generate a focus area, an additional prompt sentence to feed into the bigger LLM, to better detect information communities. To train the LM, we devise a novel Supervised and RL training procedure, specific to the social media setting. (3)We show how better community detection can improve the performance of downstream social media tasks in the challenging settings of emerging news events, specifically community detection, bot detection, and news source profiling (factuality/bias detection). We use Reddit and Twitter data. Sec. 3 describes our framework, Sec. 4 our re2\nsults, Sec. 5 analyzes, and Sec. 6 concludes. 2 Related Work Over the last few years, there has been a number of works analyzing social media, whether it is news media profiling (Baly et al., 2018, 2020), fake news detection (Mehta et al., 2022; Yang et al., 2023), Reddit analysis (Arazzi et al., 2023), Bot Detection (Tan et al., 2023), or topic analysis (Roy and Goldwasser, 2023). These works utilize a variety of ML frameworks, such as LLMs (Su et al., 2023) and graphs (Phan et al., 2023; Ali et al., 2023), and evaluate a variety of settings such as crossdomain (Shu et al., 2022) and low-resource (Lin et al., 2022) ones. A more realistic and more challenging setting to analyze, which we also do, is one in which test samples mention different topics and feature different users than seen at training time. Due to their importance, these settings have also recently received more attention (Zhang et al., 2023; Mehta and Goldwasser, 2023a,b). An important part of social media analysis is analyzing the users on social media. Specifically, prior work (Bessi et al., 2016; Ali et al., 2023) shows how grouping the users into information communities can provide insight for downstream tasks, such as fake news detection (Mehta et al., 2022), content recommendation (Singh et al., 2022), or even general analysis (Aguilar-Gallegos et al., 2022) such as how users view major events(Hao et al., 2024). In general, understanding user perspectives and forming these communities, is important, see: App. A. Large Language Models (LLMs) have been applied to a large amount of social media related tasks, like fake news detection (Su et al., 2023), as they can capture a large amount of knowledge learned from their extensive pre-training. While they can succeed at many NLP tasks like summarization (Pu et al., 2023), they still struggle on reasoning tasks like needed for social media analysis. However, as we later show, when appropriately prompted, their performance on these tasks improves. Prompting LLMs has been studied in a variety of ways, whether it be chain-of-thought reasoning (Wei et al., 2022), chain-of-hindsight (Liu et al., 2023), self-refinement (Madaan et al., 2023) or RLHF (Sun, 2023). Similar to Akyurek et al., we aim to train a smaller language model to prompt bigger, frozen LLMs. Similarly, improving LLMs using feedback has also received increasing research attention, across a variety of tasks, such 214as summarization (Ma et al., 2023; Yao et al., 2023; Hu et al., 2023). However, compared to tasks like summarization and action planning, social media analysis requires a more nuanced analysis, which affects the way we train our models (i.e. reward functions), and the feedback we provide. 3 Model Our goal in this paper is to improve big, frozen LLMs performance on social media related tasks, specifically detecting user communities, as described in Sec. 3.1. To do this, we train a smaller LLM to add additional text, which we call a “focus area” (Sec 3.2), to the prompt of the bigger one. We train the smaller model first using Supervised Learning (Sec. 3.3), and then Reinforcement Learning (Sec. 3.4). Similar to Akyurek et al., we refer to the bigger, frozen LLM as LLM task, and the smaller one as LLM prompt . 3.1 User Community Detection As mentioned in Sec. 1, detecting user communities has many advantages, such as understanding social media, content recommendation, etc. Moreover, using frozen LLMs to do this can bring further benefits, such as generalizing to new domains, avoiding fine-tuning big models, etc. Thus, in this section, we describe how we formulate the community detection task for frozen LLMs. As the big, frozen, LLM taskmodel can’t be trained, it must be prompted. However, LLMs have limited context size, so we cannot prompt them with all the users on social media. Thus, we instead define the following, more simplified community detection task , which can be extended: Given a set of six users U=u1, ...u 6, each with a textual description describing them, determine which, if any, users are similar to each other and should be in the same community c1=u1, ...u c. LLM taskresponds in natural language, listing the users that are in the same community, and the ones that aren’t. Fig. 1 shows a shortened example of this task, including our prompt to LLM task, and App. B provides details (including generalization). The textual description of each user in the prompt to LLM taskis formed based on their social media posts, and provides information to LLM task to help it determine the user similarity. To form it, we prompt Chat-GPT to create a summary of the user given their posts (Twitter tweets, Reddit posts, etc.). We form this summary as it simplifies 3\nthe community detection process, capturing the key details of the users viewpoints, while also being simpler to analyze than the individual posts. To ensure a relevant summary, we sample posts from users so that all six users Udiscuss at least one entity in common. An ex. of the LLM taskprompt we use is shown below in Tab. 1, Fig. 4, and App. B. We note that in this setup, we ask LLM taskto detect a max of one community, placing all other users after, or not in a community. This setup can handle real-world settings, where there may be multiple, one, or no user communities in the users presented to the LLM. If there are multiple, LLM task should form the most tightly-knit community. Format Language\nChat-GPT QuestionWhat is the user discussing\n... what is their perspective?\nInput Reddit Comment: ...\nOutput The user mentions ...\nTable 1: The question, text, and output format we use to\ncreate user summaries using LLM task(shown for Reddit).\n3.2 LLM prompt Definition: Focus Areas In order to improve LLM task’s ability to detect communities, we provide it an additional sentence as part of the prompt, which we call a focus area . The focus area tells the LLM exactly what to focus on when reading the user summaries, in order to properly separate the users into communities. We define this focus area to be a short sentence that details the divisive issues and topics that the current set of users are discussing. The focus area significantly simplifies LLM task’s job, as it now just has to compare the user summaries based on the issues provided, to determine the community. Moreover, it makes sure LLM taskdoes not focus on high-level topics when determining user similarity, but rather on divisive issues. For ex., a focus area could be: Focus on tax increase in California (more: Tab. 8). 3.3 LLM prompt : Supervised Training To generate the focus areas, we train a smaller LLM, LLM prompt , similarly to Akyurek et al.. We initialize it as an encoder-decoder model and finetune it to generate focus areas, given user summaries. We use T5-Base (Raffel et al., 2020), with 223M params, and then train on gold focus areas. We approximate the gold focus areas using the gold communities and LLM task, prompting it to generate the focus area based on the user summaries. Specifically, since we know the gold com- 305munities from the training data, we ask LLM task: What topics separate the gold communities? Since LLM taskis told what the gold communities are, it is able to consider what separates the users to form the gold communities, and generate an initial focus area. We show an example in Tab. 2, a detailed example in Fig. 5, and provide details in App. F. Format Language\nLLM taskQuestionWhat topics should we focus on\nto determine first 3 users are in a\ncommunity, while others are not?\nInput User 1Summary, ... UsernSummary\nOutput Focus on ...\nTable 2: The question, input text, and output format we to\ncreate gold focus areas.\n3.4 LLM prompt : Reinforcement Learning The supervised training phase above initializes the model to generate focus areas, but unfortunately, due to the gold data, many are still too high-level, and thus can be improved, for better community detection. Further, the gold data used to train LLM prompt comes from LLM task, and our goal is to improve LLM task’s performance. Thus, we must train LLM prompt directly on community detection, which we do using LLM task’s predicted community outputs, when the output focus area from LLM prompt is used. However, as LLM taskis not trainable, we use Reinforcement Learning (RL), with several novel reward functions (RF), which we design specifically for community detection and describe in Sec 3.4.1. We then describe our curriculum learning RL training procedure in Sec. 3.4.2. 3.4.1 Reward Functions We use 4 novel reward functions to train LLM prompt to generate better focus areas. To optimize them, we use the same training dataset as Sec. 3.3. RF1: Coverage, Community Detection Performance: Our first reward, Coverage , described in detail in Sec. 4.3.1, optimizes community detection directly, thus learning focus areas that help improve community detection performance. Specifically, given two gold communities c1, c2, and two predicted communities p1, p2, the reward is: How many users from each predicted community are part of the same gold community? To compute this reward while ignoring the order of predicted communities, we first find the largest overlapping gold community for each predicted one, and then compute the overlap accuracy score for each. We note that while LLM taskis prompted to predict one 4\nGiven this sentence, write another similar sentence that mentions 3 more entities and at least 10 more words.Sentence: Focus on …Output: Focus on …Figure 3: How we prompt ChatGPT to generate more in-\nformative focus areas (positive class), given ones from the\ntraining set (negative class). We then train a binary LR model\non this data.\ncommunity (for simplicity), it still places the rest of the input users together in another community, and we have gold data for two communities, which is why this reward function evaluates both. RF2: Entity Frequency: Our second reward, entity frequency, improves focus areas by getting them to mention entities that may be useful to separate users into communities. To do this, we find entities that are more frequently mentioned by one gold community compared to the others, and provide a reward based on how many of those entities the focus area mentions. Specifically, we first extract entities (Spacy NER-tagger (Honnibal et al., 2020)) from each user summary, keeping ones that are mentioned more than once across a gold community. Then, we find the entities that are mentioned more often by one of the gold communities. We provide a reward based on how many of these entities are mentioned in the generated focus area scaled to a max of 3 (i.e. 3+ entities = 1.0 reward). RF3: Focus Area Informativeness: This reward function scores focus areas, aiming to make them more informative, so they capture more details about communities. This is essential, as our motivation for providing focus areas to LLM task is to make it not rely on general topics, but rather details, to determine communities. To score focus areas, we train a Logistic Regression model on data generated using ChatGPT. We use gold focus areas as negative examples, and for positive examples, we prompt ChatGPT to generate more informative versions of the gold focus areas (as seen in Fig. 3). RF4: Focus Area Length: Our final reward function optimizes focus areas to be longer in length, so they can capture more details. We determine the number of words in the predicted focus area, provide 0.5 reward if it is less than 10, 1.0 if it’s more than 35, and otherwise a value that scales linearly between 0.5 and 1.0 (up to 35 words). 3.4.2 Curriculum Learning We finetune LLM prompt using Proximal Policy Optimization (Schulman et al., 2017) and the re- 389Dataset Train Val Test\nReddit Politics 2,789 100 Reddit Economic - BotPercent - Twitter - Table 3: Dataset size statistics. Each sample has 6 users, and\nall test users are unique across samples.\nward functions above, using the implementation by (Akyurek et al., 2023; Ramamurthy et al., 2022).To stabilize the learning of the reward functions from above (Sec. 3.4.1), we use curriculum learning. Alg. 1 provides pseudo-code for our overall training process, App. D details of RL + Reward Functions, and App. E details of curriculum learning. Our rewards balance each other, i.e. generating useful, entity relevant, informative, and longer focus areas. Algorithm 1 Algorithm to Train LLM prompt to Gen-\nerate Focus Areas\n1:Input: LLM prompt ,LLM task(Initialized Prompt Model,\nFrozen Task Model)\n2:Input: DatasetPn\ni=1D= (u1...u 6, c1, c2, f)(Users\nu1, ...u 6to separate into communities c1, c2and Gold\nFocus Area fto train LLM prompt\n3:Output: LLM prompt (Trained Focus Area Generation\nModel)\n4:Supervised Training: Maximize f :\nE[logpθ(f|u1, . . . , u n)](Train LLM prompt to gen-\nerate focus areas)\n5:while not converged do\n6: Sample mini-batch:Pn\ni=1D= (u1...u 6, c1, c2)\n7: Generate focus area: ˆf∼LLM prompt(u1...u 6)\n8: Use Focus area to get community prediction: ˆc1∼\nLLM task(u1...u 6,ˆf)\n9: Get Reward Based on Community Prediction: R=\nReward (c1)\n10: Update LLM prompt based on reward R\n11: end while\n12:return LLM prompt (Trained Focus Area Generation\nModel)\n4 Experiments 4.1 Datasets Our goal in this paper is to improve big, frozen LLMs ( LLM task) ability to detect communities. Specifically, given a set of six users with their profile/post summaries, LLM taskshould be able to detect which (if any) users belong to the same community. We now describe our evaluation datasets, including on downstream tasks (4.1.3). Tab. 3 shows the number of samples in our different datasets. 4.1.1 Reddit Our first dataset, collected by us, directly evaluates how well LLM taskcan detect communities. To get the gold data, we use the social media site Reddit. 5\nReddit is made up of communities called subreddits, each of which consists of posts relating to a central topic, such as “Politics”. Reddit users make these posts, and other users interact with the posts by commenting or voting on them (up-vote or down-vote). Each subreddit additionally has designated moderators, users who monitor the subreddit, performing actions such as deleting posts that are not relevant to the subreddit. Further, users often down-vote posts that disagree with the ideas of the subreddit. Thus, subreddits and their up-voted content are very similar to real life communities, as they contain similar minded users that discuss topics relevant to the central theme of the subreddit. Building on this, we hypothesize that users in the same subreddit, who have a positive up-vote score across all their posts in the subreddit, are members of the subreddit’s community. Thus, a set of users from one subreddit form one community, and a set of users from another from a different community, and LLMs should be able to tell the difference. We build two datasets to evaluate this, sampling data from two polarizing subreddits, or communities. The first (Political) dataset is from the “Democrats” subreddit and the “Conservative” subreddit, while the second (Economic) is from “Capitalism” and “Socialism”. Each dataset sample has six users across two communities (three from the first subreddit/community, and three from the second), which must be separated. To construct each sample, we find two posts, one from each subreddit, that discuss the same topics (made within three weeks of each other and their titles’ having at least one entity in common (Akbik et al., 2019)). For each post, we sample three users that belong to the subreddit and comment on the post. As long as their comments have a positive up-vote score, we know that these three users and post is representative of that subreddit’s community. After doing this for both subreddits, we obtain a total of six users, three from one subreddit community and three from another, which forms a sample for our dataset. After creating summaries for each user based on their post comments (as discussed in Sec. 3.1), we can ask the LLM to detect the communities. 4.1.2 TwiBot Our second dataset also evaluates how well LLM taskcan detect which users in a given set of six users are in the same community. However, this dataset is from Feng et al., and evaluates whether Twitter users are bots or not. The dataset, named 463TwiBot-20, consists of Twitter users, their metadata (tweets, profile information), and a label signifying whether they are bots or not. The dataset additionally groups users into four broad categories: Politics, Business, Entertainment, and Sports. We construct test samples, each with six users from two communities, using this dataset, where each sample has users belonging to the same category, and the two communities are bot and not bot. While other works (Feng et al., 2022; Tan et al., 2023) also used this dataset, we do not compare to them directly, as our setup is unique to our task (other works use graphs, etc. which we evaluate in Sec. 4.1.3). 4.1.3 News Source Profiling Our final evaluation is on downstream tasks, showing how detecting communities can improve news source profiling (factuality/bias detection). We use the dataset originally proposed by Baly et al. (2020, 2018) and also evaluated by Mehta et al. (2022). The dataset consists of sources scraped from Media Bias/Fact Check1, each labeled on a 3-point scale for factuality (high, low, mixed) and bias (left, center, right). Following prior work (Baly et al., 2020), we aim to predict the factuality/bias of the news sources using Twitter data, which provides social context. It consists of sources (the classification targets), the articles they publish, and users who interact with the sources or articles (propagate the articles, follow users/sources). Following Mehta et al., we build an information graph using this data. We follow the challenging fully inductive evaluation protocol proposed by Mehta and Goldwasser (2023a), where the test set graph is not connected to the training set graph in any way (no users, sources, articles or edges in common). Similar to Mehta et al., we hypothesize that detecting user communities can increase profiling performance. This is because, similar users are likely to have similar views and thus spread similar content, which has similar factuality/bias. This has also been shown in social homophily theory (Bessi et al., 2016). Thus, we randomly sample groups of users, ask LLM taskto form communities., and connect users in the same communities in the graph. 4.2 Training/Test Procedure We train only on our first Reddit dataset, which consists of politics subreddits: ‘Democratic’ and ‘Conservative’, collected between the start of 2013 and end of 2016. Thus, we don’t train/finetune on 1https://mediabiasfactcheck.com\n6\nDataset: Model Coverage # Test Samples\nReddit Political: No Focus Areas 42.01 Reddit Political: Gold (ChatGPT) Focus Areas 44.66 Reddit Political: LLM prompt Focus Areas: Supervised Learning 45.48 Reddit Political: LLM prompt Focus Areas: RL Curriculum Learning 47.85 Reddit Economic: No Focus Areas 42.25 Reddit Economic: Gold (ChatGPT) Focus Areas 44.60 Reddit Economic: LLM prompt Focus Areas: Supervised Learning 44.00 Reddit Economic: LLM prompt Focus Areas: RL Curriculum Learning 45.58 TwiBot: No Focus Areas 21.63 TwiBot: Gold (ChatGPT) Focus Areas 19.19 TwiBot: LLM prompt Focus Areas: Supervised Learning 22.55 TwiBot: LLM prompt Focus Areas: RL Curriculum Learning 22.72 Table 4: Results on Reddit Political, Reddit Economic, and TwiBot (Bot detection (Feng et al., 2021)) community detection\ndatasets when using ChatGPT for LLM taskand T5-Base for LLM prompt . All of this test data is in the unseen emerging news\nevents settings, and features new topics published after the time period the training set was collected from. Using focus areas\nimproves performance on all three datasets, and training LLM prompt using RL leads to the best performance on each dataset. This\nshows the benefit of our framework to learn useful focus areas, and those focus areas to improve community detection\nperformance, even on domains and time periods not seen at training time.\nany of the other test datasets . We provide details in App. E.1, and release our code and data.2As discussed in Sec. 1, all of our test data is in the challenging emerging news events setting , which consists of topics and time periods not seen at training time. We first test on the two Reddit datasets, which feature posts made between 2018 and the end of 2023, and then TwiBot-20 (Feng et al., 2021). Finally, we evaluate news media profiling, which features posts from after 2019. Importantly, this evaluation is also in the fully inductive setting, so the test set graph does not have any users or nodes in common/connected to the training graph. 4.3 Evaluation Metrics 4.3.1 LLM taskEvaluation To evaluate LLM task’s ability to detect information communities, we use a comprehensive metric, which we refer to as Coverage . To compute it, we first determine the appropriate gold community. This is important, as LLM taskis only asked to predict one community, but the gold data has two. To evaluate, we choose the gold community as the one that has the largest number of overlapping users with LLM task’s predicted community. We then determine how many users were correctly predicted, out of all the users both predicted and missing. Mathematically: # of correct pred.\n# of correct +incorrect +missing pred.(1) 2Code and data released with this submission.This metric prioritizes both predicting the communities correctly, and not missing any users. 4.3.2 News Source Profiling For source profiling, we evaluate Accuracy and Macro F1 (the dataset is unbalanced) for news sources, using the dataset proposed by (Baly et al., 2020) and expanded by (Mehta and Goldwasser, 2023a) for the inductive test set settings. 4.4 Results 4.4.1 LLM taskEvaluation Tab. 4 shows our results when we use ChatGPT as LLM taskon the two Reddit datasets (Political and Economic) and TwiBot Bot Detection (Feng et al., 2021). Tab. 7 shows results when Llama 2 is used asLLM task, showing our framework generalizes across LLMs. We evaluate emerging news events, where test data is unseen and collected from time periods after the training data. On each dataset, focus areas lead to significant performance improvements, particularly our LLM prompt model after it is trained with RL and Curriculum Learning. When evaluated on the same (but future) domain as training, Reddit Political, LLM prompt ’s focus areas lead to a 5.84% performance improvement in Coverage, with RL providing ∼5% relative improvement. On a different domain, economic data, performance improves 3.33%, showing the benefit of our framework to transfer to different domains. On Bot Detection, focus areas lead to more than 5% improvement. Thus, focus areas improve LLM task community detection, even on unseen domains. 7\nModel FN\nAcc.FN\nF1Bias\nAcc.Bias\nF1\n(Mehta et al., 2022) 44.66 28.50 47.74 34.69\n(Mehta et al., 2022) +\nLLM task+ Focus Areas45.53 30.17 48.64 36.34\nTable 5: News Source Media Profiling: Fake News (FN) and\nPolitical Bias Detection. When added via edges to the graph\n(1444 edges), the communities formed using ChatGPT for\nLLM task+ focus areas lead to improvements, showing the\nusefulness of LLMs to form communities which help\ndownstream tasks, despite no training on this domain.\nReward Fn. Coverage # Samples\nNone: No Focus Area 42.01 Coverage 46.07 Entity Frequency 46.96 Informativeness 46.90 Length 45.58 All: Curriculum Learning 47.85 Table 6: ChatGPT + T5-Base Reward Function Ablation\nStudy on Reddit Political Data. Although each reward\nfunction leads to improvements, using all of them via\nCurriculum Learning performs the best.\n4.4.2 News Source Factuality Detection Tab. 5 shows results on news source factuality detection. We evaluate 444 sources for factuality (183 high, 131 mixed, 128 low) and bias (202 right, 109 left, 108 center, rest unknown), and 212 comms. We compare to Mehta et al., but in the emerging news events settings, using the public Black Lives Matter data from Mehta and Goldwasser. We see that using LLM taskwith Focus Areas to form communities leads to improvements (over 4% relative increase on Bias F1). This shows the benefit of using LLMs to form communities to improve downstream social media tasks, particularly when LLMs are prompted with focus areas. Details: App. C. 5 Discussion We analyze our best RL LLM prompt model, with ChatGPT. We do an ablation study of our reward functions, ( 5.1), then a human analysis of generated focus areas, (5.2), then case studies (App. H), then an analysis of LLM detected user communities for factuality detection (App. I), and finally discuss the real world impact of our approach ( 5.3). 5.1 Ablation Study App. J shows the benefit of RL, and Tab. 6 the results of our reward function ablation study. While we notice improvements compared to not using fo- 597cus areas, they are not as significant, showing the benefit of RL and learning the rewards together. Doing so enables each reward function to contribute to learning an overall useful focus area. 5.2 Human Analysis of Focus Areas We have 3 humans analyze 50 of LLM prompt ’s focus areas, comparing them to the ChatGPT generated ones. They score each focus area on a scale of 1-5, for grammatical correctness and usefulness (to identify divisive issues and user communities). On average, on grammar, ChatGPT scores 4.95, and LLM prompt 3.00. However, on usefulness, ChatGPT scores 3.07 and LLM prompt 3.26. From this, we see LLM prompt generates better focus areas to separate users into communities, which explains our results from Sec. 4. App. G provides details. 5.3 Real World Impact Our framework to generate focus areas can be utilized with any LLM taskin the real world, even without fine-tuning it. This is because, focus areas are just an additional input to the prompt of LLM task. Moreover, as we evaluated extensively on emerging news events, particularly on topics and tasks on which our models were not trained on (Reddit Economic, TwiBot, and Source Factuality Detection) our framework is very applicable in the real world on social media, where new topics arise daily. Most importantly, LLM prompt doesn’t have to be retrained every time a new topic arises. 6 Conclusion In this paper, we proposed to use large, frozen LLMs to detect user information communities on social media, particularly in the challenging settings of emerging news events, where test data features topics and time periods not seen at training time. We then improved this LLMs performance, by training a smaller LM ( LLM prompt ) to generate a focus area, an additional sentence to feed into the bigger LLM. This focus area focuses the LLM on the relevant aspects of users that would result in better information communities, such as divisive issues. Experimental results on Reddit and Twitter data showed performance improvements in detecting communities when using Focus Areas, even on emerging news events. Further, we learned meaningful communities, that lead to improvements on the downstream task of source profiling (factuality/bias detection). Our future work is to generate better focus areas, i.e exploring reward functions. 8\n7 Ethics Statement 7.1 Limitations In this paper, we proposed a framework to train and evaluate on social media data, specifically Reddit and Twitter data and English. The framework we presented, and the experimental results we achieved, are shown for these domains/tasks. We believe that they will generalize to other domains and tasks, but we leave the exploration of that to future work. In this paper, we focused on the emerging news events settings, where we evaluated when the test data was not seen at test time. These are some of the most challenging settings for social media tasks, as knowledge learned at training time can’t always be used at test time. This is also why we leveraged LLMs for this task. Our future work involves testing how our experiments in this paper can generalize to other domains of emerging news events. In this paper, we used two Large Language Models: ChatGPT and Llama 2. For ChatGPT, we used the API released publicly by OpenAI, and the details of the model are not known. For Llama 2, we ran it locally, using the Llama-cpp-python library. We specifically run the 70B parameter model, as detailed in Appendix E.1. While we use both of these models as black-boxes, and they perform well in numerous benchmarks (Qin et al., 2023), we understand that our frameworks build on these models and this could be a potential limitation. We believe it’s important to take caution when deploying these models. For experimental reasons, we set up our framework to detect communities in sets of 6 users. We hypothesize this can generalize, to number of users more than or less than 6. Specifically, it there are less than 6 users, generalizing is simple, just provide less users in the prompt. If there are more than 6, our framework can be used by either breaking the number of users into groups of 6, and then asking the LLM to detect communities, or by just passing in more than 6 users at once. While we did not test the latter, we hypothesize it may still work provided the LLM has the ability to handle the longer context, and leave it for future work. 7.2 Ethics We do not believe we violated any code of ethics in our experiments done in this paper. We release our full code and anonymized data, to make the re- 696implementation of our models as simple as possible. We also caution that our models are the output of a machine learning model, and this could be parameter/machine dependent. In our Reddit dataset release, we anonymized all the user data, to violate no code of ethics. Further, the data we scraped was released publicly by (Chang et al., 2020). Thus, all the data we used is previously publicly available. Our framework in general is to be used to analyze social media and form information communities along with LLMs. Our general experimental settings of forming focus areas may also be useful for other tasks, and we leave the investigation of this to future work. Our framework also has the potential to be used in malicious ways, along with positive ones. Specifically, identifying users that belong to specific communities can potentially impact those users, even in harmful ways, such as if this knowledge is made public. While there are clear positives to our community detection approach, such as downstream tasks or finding ‘friends’ for other users, this is one of the downsides. Thus, our framework must be used with caution. When considering our work, it’s important to consider these and other related things to make sure the usage of our framework and code/data release falls within appropriate and safe use. 725",
        "aspect_review": "The paper introduces a novel approach by employing reinforcement learning to enhance large language models' ability to identify user communities on social media, which is a significant advancement in the field. The study provides comprehensive experimental validation across multiple datasets, including Reddit and Twitter, demonstrating the robustness and applicability of the proposed method in real-world scenarios. The focus on emerging news events and the temporal and topic shift challenges in social media analysis is timely and relevant, addressing a critical need in understanding dynamic social media environments. The results show significant improvements in community detection and downstream tasks like news source profiling, indicating the practical impact and effectiveness of the proposed approach.\n\nThe experiments could benefit from additional clarity in the presentation of hyperparameters and implementation details to facilitate reproducibility. While the evaluation is thorough, minor improvements in the clarity of figure formatting could aid in better understanding the results. Some results could be presented with more detailed explanations to enhance comprehension. The paper could provide slightly more information on the code availability to ensure ease of access and replication by other researchers.",
        "general_review": "This paper presents a novel approach to improving the performance of large language models (LLMs) in detecting user communities on social media by training a smaller model to generate focus areas. The methodology is innovative, leveraging reinforcement learning to enhance the capabilities of LLMs without retraining them. The experimental validation is comprehensive, covering multiple datasets including Reddit and Twitter, and demonstrating improvements in both intrinsic community detection tasks and downstream applications like news source profiling. The practical impact is significant, as the approach can be applied to real-world scenarios involving emerging news events, offering a scalable solution to understanding social media dynamics.\n\n1. The paper could benefit from minor improvements in clarity, particularly in the explanation of the reinforcement learning setup and reward functions. 2. Some details on the hyperparameters used in the experiments are not fully specified, which could aid in reproducibility. 3. Figures and tables could be formatted more clearly to enhance readability. 4. The availability of code and data is mentioned, but further clarification on how to access these resources would be beneficial."
    },
    "19": {
        "paper": "Zero-shot Commonsense Reasoning over Machine Imagination\nAnonymous ACL submission\nAbstract\nRecent approaches to zero-shot commonsense reasoning have enabled Pre-trained Language Models (PLMs) to learn a broad range of commonsense knowledge without being tailored to specific situations. However, they often suffer from human reporting bias inherent in textual commonsense knowledge, leading to discrepancies in understanding between PLMs and humans. In this work, we aim to bridge this gap by introducing an additional information channel to PLMs. We propose IMAGINE (Machine Imagin ation-based R easoning), a novel zero-shot commonsense reasoning framework designed to complement textual inputs with visual signals derived from machine-generated images. To achieve this, we enhance PLMs with imagination capabilities by incorporating an image generator into the reasoning process. To guide PLMs in effectively leveraging machine imagination, we create a synthetic pretraining dataset that simulates visual questionanswering. Our extensive experiments on diverse reasoning benchmarks and analysis show thatIMAGINE outperforms existing methods by a large margin, highlighting the strength of machine imagination in mitigating reporting bias and enhancing generalization capabilities1. 1 Introduction Commonsense reasoning has been considered a crucial milestone in the pursuit of artificial general intelligence (Gunning, 2018). While Pre-trained Language Models (PLMs; Devlin et al., 2019; Brown et al., 2020) often exhibit near-human reasoning capabilities after being fine-tuned on specific commonsense datasets, they face challenges in zeroshot scenarios where examples differ significantly from their training data distribution (Mitra et al., 2019; Kim et al., 2022). Overcoming this limitation 1Our code and data are available at https://anonymous.\n4open.science/r/Imagine-C35A\nExistingModel (CAR)IMAGINE(ours)\nQ. How do you butter toast?[Physical Commonsense Reasoning (PIQA)]A.Sol1: Use a knife to grab the butter, and …Sol2: Dip the toast into a tub of butter\nText\nText+ ImaginationSol1Sol2\nPrediction\nSol1Sol2Prediction\nFigure 1: Example from the PIQA (Bisk et al., 2020)\nwith model predictions. Compared to the existing meth-\nods, I MAGINE performs reasoning with imagination.\nis crucial for achieving human-level proficiency in natural language understanding. One promising approach to this limitation is injecting commonsense knowledge from external Knowledge Bases (KBs; Sap et al., 2019a; He et al., 2022b) into PLMs. Specifically, this involves transforming knowledge entities into a questionanswering (QA) format, resulting in a synthetic QA dataset. This constructed dataset is then used to train PLMs similarly to the pre-training phase. Since the knowledge bases can cover a wide spectrum of commonsense knowledge, this approach leads to substantial improvements in reasoning ability across diverse situations without specializing in specific knowledge (Wang et al., 2023, 2024). However, they often suffer from human reporting bias (Gordon and Durme, 2013), as textual commonsense knowledge only captures the most frequently occurring scenarios, thereby neglecting 1\nless common but equally critical knowledge necessary for comprehensive reasoning. Figure 1 illustrates a case where a recent model (Wang et al., 2023) fails to accurately reason about the question \"How do you butter toast? \". Since the existing models rely solely on textual inputs, they often neglect contextual details, such as the fact that butter is typically too solid to be dipped. In contrast, humans can easily answer such questions by visually imagining the shape, solidity, and interactions of butter with other objects. This observation motivates us to explore additional modalities to complement textual commonsense knowledge. In this paper, we introduce IMAGINE (Machine Imagin ation-based R easoning), a novel zero-shot commonsense reasoning framework designed to circumvent the reporting bias inherent in textual inputs. Inspired by the cognitive studies highlighting the beneficial effects of visual imagery on language understanding (Gambrell and Bales, 1986; Dessalegn and Landau, 2013), IMAGINE is designed to leverage visual signals to complement textual inputs. To achieve this, we integrate PLMs with a conditional image generator, enabling machine imagination capabilities. To guide the model in learning to utilize visual and textual inputs jointly, we create a synthetic VQA dataset, which is then used to optimize PLMs. By acquiring a broad spectrum of commonsense knowledge along with visual signals, IMAGINE enhances reasoning capabilities while circumventing human reporting bias. To verify the effectiveness of IMAGINE , we perform extensive experiments, encompassing diverse reasoning benchmarks, architectures, and scales. The experimental results convincingly demonstrate thatIMAGINE surpasses existing methods, including large language models, in reasoning capabilities. Moreover, our in-depth analysis reveals that IMAGINE effectively enables PLMs to adaptively leverage machine imagination capabilities in a beneficial manner. The contributions of this paper include the following: •We introduce IMAGINE , a novel zero-shot commonsense reasoning framework, aimed at mitigating reporting bias and enhancing the generalizability of PLMs. •We construct a synthetic VQA dataset to enable PLMs to jointly utilize textual and visual signals while achieving commonsense reasoning ability. 107•We demonstrate that IMAGINE surpasses stateof-the-art zero-shot reasoning models across diverse reasoning tasks, highlighting the significance of machine imagination. 2 Related Work 2.1 Zero-shot Commonsense Reasoning There are two major approaches to zero-shot commonsense reasoning. The first approach involves utilizing the inherent capabilities of the off-theshelf PLMs without updating their parameters. For example, Trinh and Le (2018) utilized the perplexity of vanilla language modeling, and Li et al. (2022) leveraged PLMs with specifically-designed prompting. Shwartz et al. (2020) solicited the commonsense knowledge from the language models through an iterative self-talk. Similarly, Dou and Peng (2022) obtained additional knowledge for reasoning based on the cloze-style translation. The second approach involves leveraging external commonsense knowledge bases (e.g., ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017)) to provide language models with additional knowledge. Specifically, recent studies have transformed the knowledge entities (e.g., triplets of (head, relation, tail)) into synthetic QA pairs and trained the models with them (Banerjee and Baral, 2020; Ma et al., 2021). Recently, Wang et al. (2023) further improved the synthetic signals through a conceptualization process (Song et al., 2011) which abstracts a commonsense knowledge triplet to many higherlevel instances. Subsequently, Wang et al. (2024) injected the instantiation phase into the process of synthetic dataset generation with the help of the generation capabilities of LLMs. 2.2 Visual Information for Natural Language Understanding A few previous works have leveraged machine imagination to address Natural Language Understanding (NLU) problems. For example, Tan and Bansal (2020) proposed VOKEN, which introduces visual supervision into language model pre-training by incorporating external knowledge from images retrieved for the tokens. Instead of retrieving visual information, Lu et al. (2022) proposed generating synthetic images (i.e., imagination) based on a generative model to tackle downstream NLU tasks. In the context of commonsense reasoning, Liu et al. (2022) utilized visual information to comprehend spatial commonsense knowledge (e.g., how big is a 2\nEmory goes camping with friend. As a result, others felt:(a) exhausted(b) scared (c) relaxed\nKBsSynthetic QASynthetic VQAQA SynthesisMachineImagination(Goescamping withfriend, oReact, relaxed)(Brings the cake, oWant, eat it)…(takes guitar lessons, xReact, contented)Knowledge TriplesEmory goes camping with friend. As a result, others felt:(a) exhausted(b) scared(c) relaxedSynthesized QAsetSynthesized VQAset(a) Construction procedures of synthetic VQA dataset\nQ. Emory goes camping with friend. As a result, others felt:Machine ImaginationA.(a) exhausted(b) scared (c) relaxed Text-to-ImageGeneratorImaginationVisual EncoderITM AdapterLM AdapterMasked InputsOriginal InputsPre-trained Language ModelVisualfeaturesMulti-HeadAttentionFeedForward×NTextualfeaturesJoint features(c) relaxedTextual featureVisual featureITMscores\nLM scores\n(b) Inference and optimization procedures of I MAGINE (ours)\nFigure 2: Overall procedures for (a) constructing a synthetic VQA dataset and (b) the inference/optimization phase\nofIMAGINE (ours) using the given QA pair. The process starts with the textual pair consisting of a question and\nits answers, followed by the generation of visual signals (i.e., imagination) conditioned on the question. The two\ndistinct features from visual and textual models are then utilized to derive a comprehensive prediction.\nlion? ). Similar to the proposed method, Yang et al. (2022) introduced Z-LaVI, which integrated visual information with PLMs through both retrieval and synthesis to achieve zero-shot reasoning abilities. 3 Machine Imagination-based Reasoning In this section, we elaborate on the proposed method, namely IMAGINE (Machine Imagin ationbased R easoning), for zero-shot commonsense reasoning. The core strategy is to complement textual commonsense knowledge with visual signals derived from machine-generated images. To achieve this, we first couple the PLMs with a text-to-image generator (§3.1), enabling machine imagination in text-based PLMs. We then construct a large-scale synthetic VQA dataset to learn the joint use of textual and visual signals in the reasoning process (§3.2). By optimizing the model with additional signals that encapsulate commonsense knowledge, IMAGINE can effectively perform commonsense reasoning while avoiding human reporting bias inherent in textual inputs (§3.3, §3.4). The overall procedure is depicted in Figure 2. 3.1 Machine Imagination in PLMs We start by introducing the machine imagination in text-based PLMs. We denote PLMs as MT, which 181serve as the backbone for zero-shot commonsense reasoning. For machine imagination, we incorporate two additional models to process visual signals. Specifically, we introduce: (i) a text-to-image generator, MT2I, which creates relevant images by conditioning the textual inputs, and (ii) a visual encoder, MI, which acts as a feature extractor for the given images. The overall mechanism of machine imagination operates as follows: Given a textual input, the textto-image model MT2Iinitially generates an image that captures the essence of the text. With these generated images linked to textual inputs, both PLMs, MT, and the visual encoder, MI, jointly encode the textual input and the generated image. The resultant features are then utilized to derive the comprehensive predictions. 3.2 Synthetic VQA Construction Following the previous works (Ma et al., 2021; Wang et al., 2023), we achieve zero-shot commonsense reasoning ability by constructing the synthetic QA dataset from the knowledge base. On top of this dataset, we build a synthetic visual questionanswering (synthetic VQA) dataset with the help of machine imagination. The dataset is designed to: (i) instill commonsense reasoning abilities in 3\nQ:  Emory is walking home.  \n Emory is seen as...  \nA1: Bossy  \nA2: Tired \nA3:  Independent  \nQ:  Berkeley folds his tent.  \n Berkeley is seen as...  \nA1:  W ithdrawn  \nA2:  Dedicated  \nA3:  Adventur ous Q:  A group of people walking down a street.   \n Where is this scene from?  \nA1: This scene takes place in a university  \nA2: It looks like the middle east  \nA3: This scene is set before the nineteen hundreds  \nQ: A man and a woman sitting at a bar .  \n Is Sam currently drunk?  \nA1: Yes, Bali recently drank alcohol  \nA2: Yes, Sam is intoxicated  \nA3: Possibly , but not presently  \nFigure 3: Examples of the Synthetic VQA dataset. The examples on the left are sourced from AbstractATOMIC\n(Wang et al., 2023), while the two examples on the right are sourced from VCR (Zellers et al., 2019). Bold indicates\nthe correct answer, and underline denotes the generated image caption.\nPLMs and (ii) teach them to harmoniously utilize both textual and visual inputs. The objective of this process is to construct VQA pairs (Q, A, I ), where each pair includes a natural language question Q, a set of nanswer choices A=A1, A2, ..., A n, including one ground-truth answer and n−1distractors, along with an image Ithat corresponds to the question. Synthetic QA We first construct textual QA pairs from the KBs by following the recent work (Wang et al., 2023). Specifically, we transform the knowledge entities into the QA pairs through the conceptualized augmentation of the entities (Wang et al., 2023) with the pre-defined natural language templates (e.g., the relation of xWant is transformed toAs a result, PersonX wanted to ). This process results in textual synthetic QA pairs (Q, A). Synthetic VQA On the textual synthetic QA pairs, we input the textual question Qto the text-toimage model MT2Ito generate the visual counterpartIthat depicts the scenarios described in each question. These generated images provide an additional layer of information, offering a visual context that enhances the reasoning ability based not only on textual descriptions but also on visual evidence. This augmentation leverages the strengths of visual imagery on language understanding (Gambrell and Bales, 1986; Dessalegn and Landau, 2013), potentially improving the robustness and accuracy of the model predictions. However, relying solely on the synthetic relationships between QA pairs and generated images can introduce challenges related to the alignment of visual content since machines often fail to generate well-aligned images with textual inputs (Feng et al., 2023). Therefore, we augment the synthetic VQA pairs with the widely used Visual Commonsense 244Reasoning (VCR) dataset (Zellers et al., 2019). Each pair from this dataset consists of (Q, A, R, I ), where Ris a rationale for the correct answer; however, we omit Rsince our focus is on the QA pairs associated with relevant images. Additionally, to enrich the input and enhance visual comprehension for PLMs, we generate textual context information for each image using an image captioning model2, which we prepend as a prefix to each Q3. 3.3 Pre-training I MAGINE on Synthetic VQA Based on the synthetic VQA dataset, we integrate commonsense knowledge into the models. Since IMAGINE involves two distinct modalities (i.e., text and image), we introduce two separate objectives to select the best answer choice: Language Modeling (LM) and Image-Text Matching (ITM). To obtain the LM scores, we calculate the masked language modeling loss for the Transformer encoder-based model, formulated as: SLM(T) =−1\nmmX\nt=1logP(wt|...wt−1, wt+1...).\n(1) For the decoder-based model, we compute the autoregressive language modeling loss, defined as: SLM(T) =−1\nmmX\nt=1logP(wt|w1...wt−1),(2) where widenotes the i-th word, and mis the number of tokens in the sequence T. To compute the ITM scores, we first contextualize the visual features based on the textual sequences. Let the visual features from the visual encoder MIbe denoted as 2We use InstructBLIP (Dai et al., 2023) for captioning.\n3More details of synthetic VQA are in Appendix A.\n4\nV, we derive the contextualized visual features as follows: C=softmax (¯TV⊤\n√dv)V, (3) where ¯Tis the feature vector from the PLMs MT. For the encoder-based model, we use the final hidden state of the [CLS]token as the context vector, and for the decoder-based model, we use the hidden state of the last token as the context vector. dvis the dimension of visual features. We then achieve the ITM scores by calculating the similarity between contextualized visual features and textual features as follows: SI(T, V) =sim(⃗T, C), (4) where sim(·)denotes the cosine similarity function. By combining two different scores, we produce the joint scores SJas follows: SJ(T, V) =1\n2(SM(T) +SI(T, V)),(5) After calculating all scores S(1), S(2), ..., S(n)for nanswer candidates, we calculate the marginal ranking loss defined as: LQA(S) =1\nnnX\ni=1,i̸=ymax(0 , η−S(y)+S(i)),(6) where yindicates the index of the correct answer andηis the pre-defined margin. The overall objectives are as follows: L=LQA(SM) +LQA(SI) +LQA(SJ).(7) However, we have empirically observed that the ITM objective prevents the model from learning the LM objective, which is essential for developing reasoning capabilities. To mitigate the conflict between these two objectives, we introduce two distinct adapters (He et al., 2022a), LM adapter and ITM adapter. Each adapter is trained separately with a different focus. It is important to note that only the weights within these adapters are optimized during training; all other parameters remain frozen. By separating the parameters for objectives, we can effectively reduce conflicts between them. 3.4 Inference from I MAGINE For the zero-shot evaluation, we use the same strategy to compute the LM and ITM scores after synthesizing the image based on the question. However, we ensemble two scores to derive the model’s 314prediction after obtaining the probability distribution through softmax. P(S) =softmax (S(1), S(2), ..., S(n)),(8) P(A|Q) = (1 −λ)·P(SM) +λ·P(SI),(9) where λis an ensemble coefficient that controls the contributions between textual and visual features. 4 Experiments In this section, we demonstrate the effectiveness ofIMAGINE . Specifically, we conduct extensive experiments and analysis to answer the following research questions: Q1(Generalizability) Does IMAGINE offer better zero-shot performance across a broad range of reasoning benchmarks? (§4.2) Q2(Multimodality) Does IMAGINE effectively integrate visual signals (imagination) with textual knowledge? (§4.3, §4.4) Q3(Effectiveness) How effective are the components of IMAGINE in zero-shot commonsense reasoning? (§4.5) 4.1 Experimental Setup Dataset. Following the previous works on zeroshot reasoning (Ma et al., 2021; Yang et al., 2022), we evaluate our framework on commonsense reasoning tasks and science QA tasks to assess its generalizability. Specifically, we evaluate each baseline on the five reasoning benchmarks, including Abductive NLI ( αNLI; Bhagavatula et al., 2020), CommonsenseQA (CSQA; Talmor et al., 2019), PhysicalIQA (PIQA; Bisk et al., 2020), SocialIQA (SIQA; Sap et al., 2019b), and Winogrande (WG; Sakaguchi et al., 2020). These datasets vary significantly in format (e.g., natural language inference, QA, pronoun resolution) and required knowledge (e.g., social and physical knowledge for SIQA and PIQA, respectively), enabling a comprehensive evaluation of a wide spectrum of reasoning capabilities. For science QA tasks, we assess each baseline on the four benchmarks, including QA via Sentence Composition (QASC; Khot et al., 2020), Science Questions (SciQ; Welbl et al., 2017), and the AI2 Reasoning Challenge (ARC-Easy, ARCChallenge; Clark et al., 2018). Given that science QA datasets often contain various types of reporting bias, such as color and shape biases, we selected these datasets to verify the efficacy of IMAGINE in mitigating reporting bias. 5\nMethod KB αNLI CSQA PIQA SIQA WG Avg.\nGPT-2-L (Radford et al., 2019) - 56.5 41.4 68.9 44.6 53.2 52.9\nRoBERTa-L (Liu et al., 2019) - 65.6 45.0 67.6 47.3 57.5 56.6\nDeBERTa-v3-L (He et al., 2023) - 59.9 25.4 44.8 47.8 50.3 45.6\nRoBERTa-L (MR; Ma et al., 2021) AT 70.8 64.2 72.1 63.1 59.6 66.0\nZero-shot Fusion (Kim et al., 2022) AT, CN, WD, WN 72.5 68.2 72.9 66.6 60.8 68.2\nCAR-RoBERTa-L (Wang et al., 2023) AbsAT 72.7 66.3 73.2 64.0 62.0 67.6\nCAR-DeBERTa-v3-L (Wang et al., 2023) AbsAT 79.6 69.3 78.6 64.0 78.2 73.9\nCANDLE-DeBERTa-v3-L (Wang et al., 2024) CANDLE 81.2 69.9 80.3 65.9 78.3 75.1\nCANDLE-VERA-T5-xxl (Wang et al., 2024) CANDLE 73.8 64.7 77.6 59.4 71.3 69.4\nIMAGINE -GPT-2-L Synthetic VQA 61.5 63.9 68.9 53.0 55.2 58.5\nIMAGINE -RoBERTa-L Synthetic VQA 74.7 67.5 72.3 64.3 61.2 68.0\nIMAGINE -DeBERTa-v3-L Synthetic VQA 82.2 74.0 80.7 66.3 76.7 76.0\nHuman - 91.4 88.9 94.9 86.9 94.1 91.2\nTable 1: Zero-shot evaluation results on commonsense reasoning tasks (Accuracy %). Bold andUnderline indicate\nthe best and second-best results, respectively. AT, CN, WD, WN, and AbsAT refer to ATOMIC, ConcetNet,\nWikiData, WordNet, and AbstractATOMIC. The full comparison is presented in Table 13 (Appendix). The results\nare from each reference.\nMethod αNLI CSQA PIQA SIQA WG Avg.\nGPT-3.5 61.8 68.9 67.8 68.0 60.7 65.4\nChatGPT 73.2 75.7 81.7 69.7 64.1 72.9\nGPT-4 75.0 43.0 73.0 57.0 77.0 65.0\nLLaMA2 13B 55.9 67.3 80.2 50.3 72.8 65.3\nMistral 7B 51.0 59.6 83.0 42.9 75.3 62.4\nIMAGINE 82.2 74.0 80.7 66.3 76.7 76.0\nHuman 91.4 88.9 94.9 86.9 94.1 91.2\nTable 2: Zero-shot evaluation results of LLMs on com-\nmonsense reasoning tasks (Accuracy %). Bold and\nUnderline indicate the best and second-best results, re-\nspectively. Results are taken from Wang et al. (2024),\nandIMAGINE represents the results on DeBERTa-v3-L.\nMethod QASC SciQ ARC-E ARC-C\nSMLM∗26.6 - 33.4 28.4\nCAR-RoBERTa-L 56.7 60.7 57.0 36.5\nCAR-DeBERTa-v3-L 70.0 76.9 75.3 53.2\nOPT 30B∗39.7 72.7 58.2 34.8\nFLAN 137B∗- - 79.5 61.7\nZ-LaVI (RoBERTa-L)∗27.2 51.3 51.8 33.4\nZ-LaVI (BART-L)∗27.3 51.0 56.1 36.5\nZ-LaVI (OPT 30B)∗42.1 74.0 59.5 34.1\nIMAGINE -GPT-2-L 46.5 58.4 55.1 35.1\nIMAGINE -RoBERTa-L 57.1 63.7 57.9 39.1\nIMAGINE -DeBERTa-v3-L 72.4 78.9 76.0 56.2\nTable 3: Zero-shot evaluation results on four science\nquestion-answering tasks (Accuracy %). Bold and\nUnderline indicate the best and second-best results, re-\nspectively. Results (∗) are taken from references (Baner-\njee and Baral, 2020; Yang et al., 2022; Wei et al., 2022)\nBaselines. We mainly compare IMAGINE with the following zero-shot commonsense reasoning frameworks: MR (Ma et al., 2021), SMLM (Baner- 365jee and Baral, 2020), Zero-shot Fusion (Kim et al., 2022), CAR (Wang et al., 2023), and the stateof-the-art framework, CANDLE (Wang et al., 2024). To confirm the efficacy of training with machine imagination in IMAGINE , we also compare it with Z-LaVI (Yang et al., 2022), which leverages machine imagination but does not include the training process. Beyond the reasoning framework based on KBs, we evaluate the recent LLMs, which include LLaMA2 13B(Touvron et al., 2023), Mistral 7B(v0.1) (Jiang et al., 2023), OPT 30B (Zhang et al., 2022), FLAN 137B (Wei et al., 2022), and the GPT families (i.e., GPT-3.5, ChatGPT (gpt-3.5-turbo) , GPT-4). Backbones. To verify the general applicability of IMAGINE , we apply our method to the both encoder and decoder models. Specifically, following the previous works, we utilize RoBERTa-Large (Liu et al., 2019) and DeBERTa-v3-Large (He et al., 2023). Each model has 362M and 443M parameters, respectively. As for the decoder model, we use GPT-2-Large that involves 792M parameters. Implementation details are in Appendix B. 4.2 Main Results Tables 1, 2, and 3 show the results for the commonsense reasoning tasks and the science questionanswering tasks. Models based on IMAGINE reveal either superior or competitive performance on overall reasoning tasks. This demonstrates the effectiveness of IMAGINE and highlights the benefit of leveraging machine imagination for reasoning. In particular, compared to zero-shot common6\n[WG] \nQ. It is an article of faith that the paper is  \n more important than the exam, even though  \n the _ weighs less heavily on the grade.  \nA1.  Paper     A2. Exam[PIQA ] \nQ: Brush dust of f eyebrows  \nA1: Use toothbrush to gr oom \nA2: Use dental floss to groom[aNLI ] \nQ.  Everyone laughed at the funny video.  \nA1. They took a study break to film videos  \nA2. Beth found a funny cat video.\n[SIQA ] \nQ. After starting the bar fight Kai told Riley that  \n he had better go quickly .  \n How would you describe Riley?  \nA1.  A trouble maker   A2. Full of adrenaline  \nA3.  A peace maker[CSQA ] \nQ. What part of a table  \n would you put a ruler in?  \nA1. Drawer    A2.  Desk  \nA3.  The backside   A4.  Office \nA5.  Measure distanceExisting Model: A2( X)  IMAGINE: A1(O)\nExisting Model: A3( X)   IMAGINE: A3(X)Existing Model: A1(X)  IMAGINE : A2(O)\nExisting Model: A1(O)  IMAGINE : A1(O)\nExisting Model: A2(X)  IMAGINE : A2(X)\n[CSQA ] \nQ.  Where usually lacks an elevator but  \n sometimes has a telephone book?  \nA1.  At hotel     A2. Kitchen  \nA3.  Library      A4. Telephone booth  \nA5. House\nExisting Model: A4(O)  IMAGINE:  A5(X)Figure 4: Comparison of model predictions and the correctness from IMAGINE and the existing model (Wang et al.,\n2023) on five commonsense reasoning tasks.\nsense reasoning frameworks in commonsense reasoning tasks (Table 1), IMAGINE -DeBERTa-v3-L model surpasses the previous state-of-the-art by 0.9%p on average, and specifically by 4.1%p on the CSQA. This suggests that synthetic VQA significantly enhances generalization performance in zero-shot commonsense reasoning. Comparison results with LLMs (Table 2) also shows that IMAG INEoutperforms recent LLMs, including ChatGPT and GPT-4 (OpenAI, 2023). This result suggests the superior efficiency and effectiveness of IMAG INE’s multimodal approach. IMAGINE also proves effective for science QA tasks (Table 3). Compared to the models with KBs and larger models, IMAGINE presents better or competitive reasoning performance. These results confirm the effectiveness of the machine imagination capabilities on science-related contexts. We also highlight the comparison results with Z-LaVI (Yang et al., 2022) that leverages imagination similar to ours. IMAGINE outperforms this method by a significant margin (18.5%p on average), underscoring the importance of the pre-training phase in effectively utilizing machine imagination. 4.3 Impact of Imagination on Model Inference We analyze the inference results from the textbased model, CAR (Wang et al., 2023), and IMAG INEto confirm the impact of machine imagination on the model inference. The results are shown in 426KB αNLI CSQA PIQA SIQA WQ Avg.\nSynthetic VQA 74.7 67.5 72.3 64.3 61.2 68.0\nw/o VCR 71.7 65.7 72.3 65.7 60.3 67.1\nw/o AbsAT 75.6 67.5 71.7 56.2 58.8 66.0\nw/o VCR, AbsAT 65.6 45.0 67.6 47.3 57.5 56.6\nTable 4: Ablation results on Synthetic VQA. Bold and\nunderline indicate the best and second-best results.\nFigure 4. We draw three major findings regarding the impact of imagination: (i) When the text contains limited commonsense knowledge, imagination indeed helps the model to correctly infer the answer (First row in the Figure), i.e., positive impact on predictions (ii) When the generated images only partially capture the context of the text query, imagination does not affect the inference results (Second row in the Figure). (iii) When images deviate from the real world, imagination can lead to incorrect inferences (Third row in the Figure). Specifically, we empirically observe that longer text queries often result in such cases. These results suggest that incorporating a text-to-image model with better alignment capabilities could potentially mitigate the negative impacts of imagination4. 4.4 Contributions of Synthetic VQA To confirm the effectiveness of each component in Synthetic VQA, we evaluate the contribution of 4We provide more examples with the visualization of\nmodel attention in Appendix F.\n7\nLM ITM αNLI CSQA PIQA SIQA WG Avg.\n✓ ✓ 74.7 67.5 72.3 64.3 61.2 68.0\n✓ - 74.3 65.2 71.9 62.3 60.5 66.8\n-✓ 71.7 62.0 68.8 60.0 59.6 64.4\n- - 65.6 45.0 67.6 47.3 57.5 56.6\nTable 5: Ablation results on pre-training objective of\nIMAGINE . We use a RoBERTa-L as a backbone.\nInference αNLI CSQA PIQA SIQA WG Avg.\nEnsemble 74.7 67.5 72.3 64.3 61.2 68.0\nLM 74.1 66.9 71.8 63.8 61.1 67.1\nITM 71.7 63.1 68.3 59.8 59.4 64.0\nTable 6: Results of the different inference strategy (LM,\nITM). These strategies are evaluated on RoBERTa-L.\nAbsAT and VCR. Table 4 presents the results on commonsense reasoning tasks. The model trained only with AbsAT (i.e., w/o VCR) shows superior performance on datasets that contain longer sequences and require complex knowledge (e.g., PIQA, SIQA). In contrast, the model trained only with VCR (i.e., w/o AbsAT) shows its strength on the dataset that contain simpler questions ( αNLI, CSQA) which allows the better use of visual information. When combining these two components, the synthetic VQA results in well-generalized reasoners across diverse reasoning tasks, demonstrating the complementary effect of each component. 4.5 Component Analysis on I MAGINE Ablation on Training Objectives. IMAGINE employs two objectives (i.e., LM, ITM) to learn commonsense knowledge from different modalities. We perform ablations on these objectives to verify their contributions in enhancing zero-shot reasoning capabilities. Table 5 shows the ablation results. Notably, omitting the LM objective leads to a significant drop in performance, underscoring the crucial role of language understanding in commonsense reasoning. Furthermore, while ITM alone does not significantly impact reasoning effectiveness, combining ITM with LM results in improved reasoning performance. These findings suggest that integrating visual information in model optimization leads to better reasoning in commonsense situations. Effect of Ensemble Inference. IMAGINE performs reasoning based on ensemble of the LM and ITM scores. To investigate the contributions in scores obtained from these two different modalities, we evaluate each score independently. The results are presented in Table 6. We observe the lowest 480Model αNLI CSQA PIQA SIQA WG Avg.\nAdapter 74.7 67.5 72.3 64.3 61.2 68.0\nFull 73.0 65.4 71.1 61.5 61.2 66.4\nTable 7: Evaluation results of IMAGINE with full fine-\ntuning (Full) and adapter tuning (Adapter).\nperformance when evaluating only the ITM scores. However, ensembling LM scores with the ITM results in significant performance improvement across all tasks, even though the scores derived from images are much lower than those from text. This indicates that integrating machine-generated images can complement and enhance languagebased reasoning abilities5. Impact of Adapter. IMAGINE utilizes adapters (He et al., 2022a) to alleviate the conflicts between the two objectives (i.e., LM, ITM) during the pretraining. In this study, we examine whether separating parameters through adapters for distinct modality objectives is truly effective. Table 7 presents the ablation results on adapters. We observe a significant decline in reasoning performance when adapters are removed. This suggests that direct training of PLMs with images adversely affects the acquisition of textual knowledge. One plausible explanation for this phenomenon is possibly related to catastrophic forgetting (Kirkpatrick et al., 2017), where the model loses previously acquired knowledge (i.e., textual knowledge inherent in PLMs). This highlights the effectiveness of adapters in maintaining the model’s linguistic understanding when it learns from new modalities. 5 Conclusion In this paper, we have proposed IMAGINE , a novel zero-shot commonsense reasoning framework that leverages visual signals to mitigate reporting bias. To steer IMAGINE in effectively utilizing visual information, we have created a large-scale synthetic VQA dataset and optimized the model to jointly use both textual and visual information. We have conducted extensive experiments across a broad range of reasoning tasks. Comprehensive results have shown that IMAGINE establishes new state-of-theart results on zero-shot commonsense reasoning tasks compared to strong baselines, demonstrating the efficacy of machine imagination. 5More analysis on ensemble methods are in Appendix D.\n8\nLimitations We have demonstrated the efficacy of the machine imagination to improve zero-shot commonsense reasoning ability. However, we still have the following limitations: Additional Computations While machine imagination leads to performance improvement in PLMs, it necessitates additional computations for generating and processing visual signals. This limitation can be addressed by retrieving relevant images instead of synthesizing new ones, as demonstrated in previous work (Yang et al., 2022). We consider this approach a promising avenue for future research. Exploration of IMAGINE on LLMs In this work, we apply IMAGINE to only intermediate-size models (300M to 790M), as one of our objectives is to show the smaller models with machine imagination outperforms LLMs on a broad range of commonsense reasoning tasks. However, we believe that IMAGINE can be effectively combined with LLMs, given that the reporting bias is an inherent issue in the pre-training corpus and not the models themselves. We plan to explore the scaling of machine imagination in our future research. 545",
        "aspect_review": "IMAGINE demonstrates superior or competitive performance across diverse reasoning tasks, surpassing state-of-the-art models and highlighting the effectiveness of integrating machine imagination. The paper introduces a novel synthetic VQA dataset that enhances the generalization performance of PLMs in zero-shot commonsense reasoning. IMAGINE outperforms recent large language models, including GPT-4, demonstrating its superior efficiency and effectiveness in leveraging multimodal approaches. The paper is well-organized, with clear explanations of methodology and comprehensive experimental validation, making it accessible and informative for readers. The integration of machine imagination through a text-to-image generator and the use of adapters to mitigate conflicts between objectives is innovative and technically rigorous.\n\n1. The reliance on synthetic relationships between QA pairs and generated images can sometimes lead to misalignment issues, affecting reasoning accuracy. 2. While IMAGINE outperforms many models, its reliance on additional computational resources for generating and processing visual signals could be a limitation compared to retrieval-based methods. 3. The model's performance on longer text queries sometimes results in incorrect inferences due to partial or misaligned visual context.",
        "general_review": "The paper introduces IMAGINE, a novel framework for zero-shot commonsense reasoning that integrates visual signals with textual inputs to mitigate human reporting bias in Pre-trained Language Models (PLMs). IMAGINE showcases notable novelty by incorporating machine-generated images into the reasoning process, a significant departure from traditional text-only approaches. The technical depth and rigor are evident in the comprehensive construction of a synthetic Visual Question Answering (VQA) dataset and the use of adapters to manage modality-specific training objectives. The experimental validation is thorough, with extensive experiments demonstrating that IMAGINE surpasses existing state-of-the-art models across various reasoning benchmarks. This highlights the framework's significant practical impact, as it effectively enhances generalization capabilities and reasoning performance in diverse scenarios.\n\n1. Minor clarity improvements could be made in the explanation of the synthetic VQA dataset construction process. 2. Additional details on the hyperparameters used in the experiments would enhance reproducibility. 3. Some figures could benefit from minor formatting adjustments for better readability. 4. A slight clarification on the availability and access to the code and data would be beneficial for the community."
    },
    "20": {
        "paper": "Self-Training for Sample-Efficient Active Learning for Text Classification\nwith Pre-Trained Language Models\nAnonymous ACL submission\nAbstract\nActive learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification. While active learning has made considerable progress in recent years due to improvements provided by pretrained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. Here we investigate how selftraining, a semi-supervised approach where a model is used to obtain pseudo-labels from the unlabeled data, can be used to improve the efficiency of active learning for text classification. Starting with an extensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we devise HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks on which it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using only 25% of the data. 1 Introduction In supervised machine learning, a lack of labeled data is the main obstacle to real-world applications, since labeled data is usually non-existent, expensive to obtain, and sometimes even requires domain experts for annotations. One solution to create models despite the absence of labels, is active learning , where in an iterative process an oracle (usually realized through a human annotator) provides labels for unlabeled instances that have been deemed to be informative by a so-called query strategy . These labels are then used to train a model, which in turn is used by the query strategy during the next iteration. In this work, we investigate the combination Figure 1: Active learning (a), and active learning with\ninterleaved self-training (b). For active learning, the\nmost uncertain samples are labeled by the human anno-\ntator, while for self-training pseudo-labels are obtained\nfrom the current model using the most certain samples.\nof self-training and active learning to reduce the required amount of labeled data even further.1During recent years, transformer-based pretrained language models (Vaswani et al., 2017; Devlin et al., 2019) have successfully been applied for active-learning-based text classification, thereby considerably raising the state-of-the-art results (e.g., Margatina et al., 2021). The dominant paradigm here is pool-based active learning (Lewis and Gale, 1994) where the query strategy repeatedly selects batches of instances to be labeled next from the pool, the entirety of unlabeled data. While language models have successfully been adopted for active learning (e.g., by Ein-Dor et al. (2020), Yuan et al. (2020), and Margatina et al. (2021)), the total labeling effort, i.e. the number of queries and the number of instances per query, has remained similar to setups predating transformers. With regard to the size of queries, there are two prevailing setups: (1) Absolute query sizes (Yang et al., 2009; Sharma et al., 2015; Zhang et al., 2017; Ein1Code will be released on Github upon publication.\n1\nDor et al., 2020; Yuan et al., 2020; Schröder et al., 2022; Tonneau et al., 2022), where a fixed number of instances are queried during each iteration, and (2) relative query sizes (Lowell et al., 2019; Prabhu et al., 2019; Margatina et al., 2021), in which the number of queried instances is a percentage of the unlabeled pool. We argue that those query sizes of both aforementioned experiment setups are needlessly large. Previous works query up to 1000 instances (Yang et al., 2009) or up to 25% percent of the unlabeled pool (Lowell et al., 2019), where the former is of considerable size and the latter is clearly infeasible in practice as soon as datasets reach average contemporary sizes or annotation costs are high. When using language models that have been trained on billions (Devlin et al., 2019) or even trillions (Touvron et al., 2023) of tokens, there is no need to label hundreds or even thousands of instances. In this work, we present a sample-efficient active learning approach which incorporates self-training with the goal of reducing the data needs for our key task of text classification. Our contributions are as follows: (1) We reproduce and four existing self-training approaches, enabling a fair comparison among them despite strongly diverging settings and hyperparameter choices in the original works. (2) We propose a simple yet highly effective selftraining approach that complements high-quality active learning labels with high-quantity pseudolabels . (3) In extensive experiments, we compare the new approach to the four reproduced methods on four text classification benchmarks using two different query strategies. (4) Finally, we discuss possible consequences for active learning that result from the observed effectiveness and alternatives based on large language models (LLMs). The new approach complements active learning with pseudo-labels obtained from the current model. Using as few as 130 instances, we achieve scores competitive with regard to the state of the art on three out of four datasets. 2 Related Work In this work, we investigate the intersection of selftraining and active learning for text classification. Self-Training The idea of self-training (Scudder, 1965; Yarowsky, 1995) is to leverage unlabeled data in supervised tasks, by obtaining algorithmically-derived pseudo-labels that are subsequently used for training a model. In natural lan- 112guage processing (NLP), self-training is an established and well-studied semi-supervised approach (Clark et al., 2003; Mihalcea, 2004; Tomanek and Hahn, 2009; Ye et al., 2020) that provides additional data by generating soft- or hard-labels from unlabeled data. The key here is, similar to active learning, a selection of suitable unlabeled instances, however, unlike active learning there is no human in the loop, therefore self-training targets pseudolabels that are likely to be correct. Pseudo-labels, however, can also be noisy, and in the case of repeated self-training iterations, this error can propagate over the iterations, resulting in increasing levels of noise in subsequent pseudo-labels (Arazo et al., 2020; Yu et al., 2022). Therefore, a central issue is pseudo-label regularization, which prevents overfitting on incorrect pseudo-labels. The recently dominant class of pre-trained transformer models (Vaswani et al., 2017; Devlin et al., 2019) are well-known for their improved effectiveness which derives, among other things, from a high sample efficiency of the contextualized representation, and therefore each additional pseudolabeled instance can be highly valuable for selftraining a pre-trained model. Consequently, it is unsurpising that self-training has been investigated in NLP with recent model architectures (Meng et al., 2020; Mukherjee and Awadallah, 2020; Vu et al., 2021; Gera et al., 2022; Chen et al., 2022; Sosea and Caragea, 2022), however, it is still underresearched regarding active learning, where the additional labeled data could help to alleviate the data scarcity (Yu et al., 2022; Xu et al., 2023). Active Learning and Self-Training Despite the recent performance gains achieved by transformer models, active learning still uses a considerable amount of data. Recent work in transformer-based active learning for NLP, however, focused on query strategies (Ein-Dor et al., 2020; Margatina et al., 2021; Zhang and Plank, 2021; Wertz et al., 2023; Zeng and Zubiaga, 2023), which often raised the state-of-the-art results given the same labeling budget, but mostly disregarded translating these improvements into reduced annotation efforts. Despite the comparably slow adoption of selftraining, a few recent works already started to investigate the use of unlabeled data in order to improve data efficiency (Siméoni et al., 2020; Gonsior et al., 2020; Gilhuber et al., 2022; Tsvigun et al., 2022). In the context of active learning for text classification, both Yu et al. (2022) and (Xu et al., 2023) use 2\nPseudo-Label Selection Self-Training Setting\nSubs. Unc. Div. Cls. Bal. Weight. Regularization Data Domain\nApproach D P E T N\nUST / / (/) / / / /reve /reve /reve /reve Text Few-Shot\nAcTune /reve / / /reve / /reve / /reve / /reve Text Active Learning\nVERIPS / / /reve /reve /reve /reve /reve / / /reve Images Active Learning\nNeST /reve / /reve /reve /reve /reve / /reve / / Text Active Learning\nHAST (ours) / / (/) / / /reve /reve /reve / / Text Active Learning\nTable 1: Comparing the four most relevant self-training approaches in terms of pseudo-label selection, self-training,\nand experiment setting. Symbols: /: covered; ( /): implicitly covered; /reve: not covered. Abbreviations in the\nsection pseudo-label selection: subsampling (Subs.), uncertainty (Unc.), diversity (Div.). Abbreviations in the\nsection self-training: class balance (Cls. Bal.) and weighting (Weight.). Abbreviations in the section regularization:\ndropout (D), previous prediction (P), ensembling (E), thresholding (T), and embedding space neighborhood (N).\npre-trained language models for active learning for text classification, thereby outperforming regular active learning. Their pseudo-label selection, however, relies on the prediction of previous rounds, which renders subsampling, a common method for handling large datasets or expensive models, impossible. The work of (Xu et al., 2023) is closest to our work due to an intersection of self-training and active learning, and text classification. 3 Active Learning and Self-Training The goal for active learning is to minimize the annotation effort while maximizing performance regarding some task, such as text classification. Problem Formulation In pool-based active learning, the training data X={(xi)}n\ni=1is partitioned into two disjoint sets: unlabeled pool Uand labeled poolL(i.e.,U ∩ L =∅). During the active learning loop, the query strategy selects the best ranked instances Xq⊆ U, which are then removed from U, labeled by the oracle, and subsequently added toL. We refer to a model as M, and a model that is trained during query tasMt. We denote the predicted class distribution of instance xduring query t(using model Mt−1) asPt(y|x). 3.1 Incorporating Self-Training While active learning aims to obtain a small labeled subset, during training it disregards the the data in the unlabeled pool. Self-training is a semisupervised approach that in addition to the labeled pool’s data leverages (parts of) the unlabeled pool by assigning machine-generated pseudo-labels labels. Similar to active learning, it queries instances according to a criterion such as, amongst others, uncertainty. In contrast to active learning, however, 198the selected instances are pseudo-labeled according to some heuristic instead of labeled by a human annotator. Contrary to active learning, where the model uncertainty heuristic has been shown to be effective for query strategies (Lewis and Gale, 1994; Roy and McCallum, 2001; Schröder et al., 2022), using the most certain instances has been observed to be most beneficial for self-training across several NLP tasks (e.g., Mihalcea (2004); Tomanek and Hahn (2009); Mukherjee and Awadallah (2020)). While these approaches obviously contradict, they can complement each other, as shown in Figure 1: an active learning query selects by uncertainty, aiming to find instances that provide the most information to the model, while self-training selects instances by certainty, preferring instances whose pseudo-labels are likely to be correct. 3.2 Pseudo-Label Regularization The problem with self-training is that pseudo-labels are automatically generated, usually derived from a previous model, and therefore are not necessarily correct and can be noisy. In the case of repeated self-training iterations this error can propagate over the iterations, resulting in increased levels of noise in subsequent iterations (Arazo et al., 2020; Yu et al., 2022). For this reason, a central issue for self-training is pseudo-label regularization, where methods carefully select or weight pseudo-labels with the aim to reduce the expected loss. 3.3 Previous Approaches Similar to active learning, at the heart of each self-training approach is a strategy that decides which instances are selected—but in this case to be pseudo-labeled. In the following, we present the four most relevant self-training approaches. 3\nUST Uncertainty-aware self-training (UST; Mukherjee and Awadallah (2020)) uses dropoutbased stochastic sampling to obtain multiple confidence estimates per instance. Each instance is then scored using the BALD measure (Houlsby et al., 2011) and a set of instances is sampled that is as class-balanced as possible. AcTune AcTune (Yu et al., 2022) aims to obtain a diverse set of instances by preceding the sampling step with weighted K-Means clustering. To overcome the noise of per-instance label variation during self-training iterations, they aggregate pseudo labels over multiple iterations. VERIPS The verified pseudo-label selection method (VERIPS; (Gilhuber et al., 2022)) selects instances whose prediction confidence exceeds a fixed threshold. Before the selected instances are considered pseudo-labels, a verification step, where a second model, trained without any pseudo-labels, is used to verify or discard pseudo-labels. NeST Neighbourhood-regularized self-training (NEST; (Xu et al., 2023)) leverages the embedding space to obtain pseudo labels that closely match the predicted distribution of their k-nearest neighbours. The individual scores are averaged over multiple active learning iterations for additional stability. In Table 1, we compare the distinguishing features of all presented approaches, including the approach proposed in Section 4. A striking commonality is that all sample selections mechanism rely on uncertainty, which has been shown to be very effective both for active learning and self-training (Yu et al., 2022; Xu et al., 2023). The main difference is the pseudo-label selection and regularization. 3.4 Limitations of Previous Approaches We identified several shortcomings shared by multiple of the previous approaches that limit the conclusiveness of existing evaluations. Unrealistic Evaluation Settings (Mukherjee and Awadallah, 2020) use validation sets matching the size of the training data and (Yu et al., 2022) select the best model based on validation sets of sizes 500and1000 . Validation sets of these sizes are unrealistic for an active learning scenario, where even training sets of these size would exceed the amount of data that we deem to be necessary when evaluating on common text classification benchmarks. Moreover, these validations are used for 282extensive hyperparameter optimization (Yu et al., 2022; Xu et al., 2023), which would otherwise not be possible, and also might not generalize. Computational Efficiency Since transformer models are known to be computationally expensive, aggravated in the active learning context, UST and VERIPS include a subsampling mechanism, which select a subsample before applying the query strategy, thereby enabling the use of more computationally expensive models and query strategies, as well as larger datasets. When pseudo-label regularization relies on predictions of previous active learning or self-training iterations, this renders the use of subsampling impossible, since the predictions of the instances outside the subsampled set would be missing. This is a serious drawback for strategies relying on previous predictions, and renders them infeasible for many scenarios. Confidence Thresholding VERIPS, AcTune, and NeST apply a confidence threshold to control pseudo-label regularization, either by ignoring low-confidence instances or by adapting the loss function. A fixed threshold, however, relies on the assumption that confidence estimates are well-calibrated, and also that the data contains samples for which the model is highly certain. The limitations illustrated above, the differences in task (text and image classification) and setting (few-shot and active learning), and an overreliance on hyperparameters, raise doubt on the generalizability of these findings and also limit the comparability across studies, thereby demanding further investigation and motivating a reproduction study. 4 Hard-Label Regularized Self-Training Based on a methodological analysis (Sections 3.3 and 3.4) and a reproduction study (Section 4), we present hard-label neighborhood-regularized selftraining (HAST, pronounced “haste” ), a novel selftraining method that aims to complement active learning with large quantities of pseudo-labels. The idea of our proposed approach is to rely on the generalization capabilities provided by contrastive (representation) learning , which is trained onn-tuples of instances, in the following assumed to be pairs. A pair can be formed between any two instances, and consequently each additional pseudo-labeled instances increases the number of possible pairings. Obviously, when combining this 4\nwith self-training, this can considerably enhance the effective number of instances—at the risk of introducing noise due to incorrect pseudo-labels. Therefore our goal is to obtain as many pseudolabels as possible at a reasonable level of noise. 4.1 Contrastive Representation Learning Commonly used representations that are obtained from a language model’s layers, such as the [cls] token (Devlin et al., 2019), rely on the principle that semantically similar inputs will result in similar embedding vectors—in an otherwise meaningless embedding space. Representation learning (Bengio et al., 2013) on the other hand, aims to learn a meaningful space in which the dimensions capture explanatory factors in the data (Le-Khac et al., 2020) and distance metrics are rendered meaningful (Le-Khac et al., 2020). Moreover, in contrastive representation learning (Carreira-Perpiñán and Hinton, 2005) this is achieved by contrasting pairs of instances, where similar instances are pulled together and dissimlar instances are pushed apart in the embedding space. One recent approach is the fine-tuning paradigm SetFit (Tunstall et al., 2022), which uses a Siamese network to train embeddings that are then used as representations in downstream tasks. SetFit has shown incredible effectiveness in the few-show setting (Tunstall et al., 2022), making it an obvious choice for active learning. Algorithm 1 AL WITH SELF -TRAINING\nInput: unlabeled pool U; labeled pool L; initial model\nM0; number of queries Q; batch size B;\nself-training iterations T\n1:forq∈{1, ..., Q}do\n2:Xq←query batch of size BfromU\n3:Yq←labels provided by oracle\n4:L ← L ∪ { (xq,i, yq,i), ...,(xq,i+B, yq,i+B)}\n5:U ← U \\ { (xq,i, yq,i), ...,(xq,i+B, yq,i+B)}\n6:Mq←TRAIN (L)\n7:M∗\nq←SELFTRAIN (U,L, Mq,T)\nOutput: Final model M∗\nQ\n4.2 Active Learning and Self-Training We incoporate self-training into pool-based active learning by adding a subsequent self-training step after each training step as shown in Algorithm 1. After each query (line 2) a new model is trained (line 6), which is then followed a selftraining step (line 7), which may be HAST or one of the previous approaches. The training in line 6 367could be skipped after the first iteration, but this additional step “resets” the model, thereby counteracting model instability (Mosbach et al., 2021) and model collapse due to error propagation. Algorithm 2 SELFTRAIN (HAST)\nInput: unlabeled pool U; labeled pool L; current\nModel Mt0, number of self-training iterations T\n1:Lp=L;Up=U\n2:fort∈{1, ..., T}do\n3:Yq,t←Mt0(Up)\n4:X∗\nq,t← {xi|xi∈ Upand 1PL(x)}\n5:Lp← L p∪ {(xt,i, yt,i), ...,(xt,m, yt,m)}\n6:Up← U p\\ {(xt,i, yt,i), ...,(xt,m, yt,m)}\n7:Wq,t←Weights as described by Eq. 4.\n8:M∗\nq,t←TRAIN (Lp, Wq,t)\nOutput: Self-trained model M∗\nq,t\n4.3 HAST: Pseudo-Labels and Weighting The proposed approach is intended to exploit the current model to ideally provide larger amounts of pseudo-labels by leveraging the embedding space. Instead of relying on label distributions we use hard labels, which are obtained by a majority vote of the instance’s knearest neighbors (KNN). The proposed approach is shown in Algorithm 2. Our pseudo-label selection (line 4) takes all instances from the unlabeled pool Upwhere the most confident label crosses the decision threshold of 0.5 and the predicted label ˆyiagrees with the k nearest neighbors majority vote: 1PL(x) =(\n1ifsi>0.5∧ˆyknn\ni= ˆyi\n0otherwise(1) where si=P(yi= ˆyi|x)∈(0,1]is the confidence of the most confident predicted label ˆyiandˆyknnis the label given by a KNN majority vote. Since the predicted label ˆyi=argmax P(y|x)is obtained from the class with highest confidence, this strategy implicitly selects instances with high certainty. Weighting With the proposed pseudo-label selection strategy, we can potentially a large amount of instances. This can introduce (1) a class imbalance (Henning et al., 2023) among the pseudolabels and (2) an imbalance between the pseudolabels und human-annotated labels. To overcome these issues, we first introduce a weighting term to adjust for class imbalance: z=N/C−hc\nmax(1, hc)(2) 5\nwhere Nis the number of all pseudo-labels in Yq,t, Cis the number of classes, N/C the expected number of instances for a balanced class distribution, andhcis the count of class cin the histogram of Yq,tovercbins. This yields a term that is inversely proportional to the current class imbalance. Since the resulting values are unbounded and can potentially grow very large, we apply a sigmoid function to squash the values into the interval (0,10): αc=10\n1 +e−z(3) To reduce the effect of a possibly excessive number of pseudo-labels and retain some weight on the human-annotated labels, we introduce another term β∈(0,1], which is is the labeled-to-unlabeled ratio weight that penalizes pseudo-label weights iff β <1. The final weights are then given by: Wi=αˆyi·β (4) Human-annotated instances have a weight of Wi=1.0. Finally, the resulting weights are L1normalized, i.e.P\ni|Wi|=1, so that the perinstance loss of all instances can then by multiplied by the resulting normalized weight. 4235 Experiments In the experiments, we evaluate the proposed selftraining method HAST. Moreover, we reproduce the four most relevant previous self-training approaches, some of which have not yet been evaluated in the context of NLP or active learning before, and compare them against HAST. 5.1 Experiment Setup The key task in this work is active learning for single-label text classification. Using only 130 instances the experiments are designed to be both challenging and data efficient. Data We evaluate on four established text classification benchmarks, whose key characteristics are displayed in Table 3. AGN and IMDB exhibit a balanced, and DBP and TREC-6 an imbalanced class distribution. IMDB is a binary classification problem, the others are multi-class problems. Evaluation Following (Kirk et al., 2022), we report the classification performance in accuracy for balanced and in macro-F 1for imbalanced datasets. Classification We evaluate two different models: (1) the paraphrase-mpnet-base SBERT (Reimers Datasets\nQuery Strategy Classifier Self-Training AGN DBP IMDB TREC\nBreaking TiesBERTNo Self-Training 0.763 0.057 0.619 0.129 0.745 0.030 0.341 0.UST 0.798 0.016 0.645 0.042 0.764 0.100 0.333 0.AcTune 0.806 0.021 0.651 0.054 0.795 0.050 0.434 0.VERIPS 0.834 0.012 0.907 0.047 0.816 0.050 0.540 0.NeST 0.840 0.013 0.918 0.006 0.783 0.041 0.580 0.HAST 0.762 0.055 0.605 0.071 0.806 0.097 0.424 0.SetFitNo Self-Training 0.853 0.011 0.973 0.004 0.872 0.009 0.691 0.UST 0.658 0.030 0.483 0.033 0.851 0.028 0.491 0.AcTune 0.863 0.006 0.980 0.003 0.896 0.024 0.642 0.VERIPS 0.859 0.005 0.981 0.002 0.857 0.024 0.730 0.NeST 0.878 0.005 0.981 0.001 0.927 0.005 0.235 0.HAST 0.886 0.007 0.984 0.001 0.882 0.040 0.773 0.RandomBERTNo Self-Training 0.758 0.007 0.574 0.029 0.740 0.046 0.407 0.UST 0.797 0.039 0.693 0.083 0.794 0.014 0.298 0.AcTune 0.791 0.050 0.559 0.082 0.801 0.045 0.386 0.VERIPS 0.812 0.023 0.850 0.063 0.813 0.029 0.551 0.NeST 0.819 0.039 0.865 0.037 0.782 0.022 0.553 0.HAST 0.677 0.114 0.650 0.053 0.831 0.051 0.514 0.SetFitNo Self-Training 0.848 0.005 0.939 0.031 0.907 0.007 0.676 0.UST 0.659 0.038 0.476 0.039 0.871 0.031 0.491 0.AcTune 0.847 0.014 0.970 0.008 0.918 0.004 0.651 0.VERIPS 0.854 0.010 0.968 0.008 0.921 0.002 0.726 0.NeST 0.860 0.011 0.965 0.004 0.923 0.008 0.253 0.HAST 0.885 0.002 0.974 0.006 0.926 0.004 0.738 0.Table 2: Classification performance after the final iteration (in accuracy or macro-F 1) broken down per query\nstrategy, classifier, and self-training approach. The reported numbers are the average over five runs.\n6\nUST AcTune VERIPS NeST HAST0.000.250.500.751.00\n0.000.250.500.751.00\n0.000.250.500.751.00\n30 70 1100.000.250.500.751.00\n30 70 110 30 70 110 30 70 Number of InstancesBreaking  Ties Random\nBERT SetFit BERT SetFitAG News [Acc.] DBPedia [F 1] IMDB [Acc.] TREC-6 [F 1]Figure 2: Learning curves per model, query strategy, and dataset, showing the classification performance on the test\nset. The x-axis shows the number of instances, while the y-axis indicates classification performance.\nDataset Name (ID)Type Classes Training Test Metric\nAG’s News (AGN) N 4 120,000 7,600 Acc.\nDBPedia-14 (DBP) T 14 560,000 70,000 F 1\nIMDB (IMDB) S 2 25,000 25,000 Acc.\nTREC-6 (TREC-6) Q 6 5,500 500 F 1\nTable 3: Key information about the examined datasets.\nAbbreviations: N (News), S (Sentiment), Q (Questions).\nand Gurevych, 2019) model which is fine-tuned using SetFit (Tunstall et al., 2022), and, in order to verify its effectiveness in the non-contrastive setting, (2) a BERT-base model (Devlin et al., 2019) that is trained using vanilla fine-tuning. Both of these models consist of 110M trainable parameters. Active Learning Models are initialized with 30 instances. Active learning is performed over 10 iterations during each of which 10 more instances are labeled. Following (Hu et al., 2019) and (Yu et al., 2022), the model is trained from scratch after each active learning and self-training iteration. While we do not directly investigate query strategies, they are paramount to active learning. 460To verify if the query strategy has an effect on the self-training process we evaluate all configurations both using the breaking ties strategy (Scheffer et al., 2001; Luo et al., 2005) and a random baseline. Self-Training For HAST, we use k=5 and β=0.1. For all other strategies, we use the best hyperparameters as reported in the respective publications. A subsample of 16384 instances is drawn before obtaining pseudo-labels for all strategies supporting subsampling. To minimize the effect of error propagation (as shown in Section 3.2), but also for reasons of computational feasibility, we refrain from consecutive self-training iterations. 5.2 Results The final classification performance of each configuration is shown in Table 2. Comparing selftraining to non-self-training runs, we observe selftraining to be effective. HAST wins for most SetFit configurations, while NeST wins for most BERT configurations. This can be explained through the large number of pseudo-labels it obtains, but also due UST underperforming in many configurations. 7\nDataset Approach (Parameters) N Score\nAGN ReGen1(125M) 0 0.[Acc.]BERT3(336M) 525 0.HAST (110M; ours) 130 0.DBP DeBERTa4(355M) 0 0.[F1] UST2(110M) 420†0.HAST (ours) 130 0.IMDB RoBERTa (355M)40 0.[Acc.]UST2(110M) 60†0.HAST (110M; ours) 130 0.TREC GPT3.5 Turbo & RoBERTa50 0.[F1] BERT3(336M) 525 0.HAST (110M; ours) 130 0.Table 4: Comparison against previous low-resource\nmethods:1(Yu et al., 2023),2(Mukherjee and Awadal-\nlah, 2020),3(Schröder et al., 2022),4(Gera et al., 2022),\n5(Xiao et al., 2023).†:Excluding additional instances\nthat have been used for validation.\nWe conducted further investigation on this (Appendix Table 8), finding that this is indeed caused by a lack of pseudo-labels that is caused by an absolute confidence threshold. This shows that methods relying on fixed absolutes thresholds have difficulties to generalize in practical settings or whenever a calibrated model cannot be guaranteed. Besides the final performance, it is also crucial to investigate the performance after each active learning iteration, which can be seen in the learning curves depicted in Figure 2. HAST clearly outperforms in most settings as well, sometimes reaching a performance close to the final value already before the first query. While the breaking ties query strategy sometimes wins at the early iterations, the overall better final classification performance and are achieved by the random sampling strategy. The corresponding area under curve values can be found in Appendix Table 7. In Table 4, we compare the best result per dataset to results from literature achieved by sampleefficient methods (zero-shot, few-shot, or active learning). Except for TREC, HAST achieves results that are very close to state-of-the-art results, despite using a comparably small model. 6 Discussion The experiments have shown that HAST is effective at obtaining a large amount of pseudo labels through which it outperforms the other approaches both in classification performance and area under curve. Through the reproduction, we have investigated the relative strength of UST, AcTune, 514VERIPS, and NeST in the context of active learning for text classification, and the main issues of the previous approaches seem to be confidence threshold parameters (VERIPS, AcTune, NeST) that are difficult to adjust without hyperparameter optimization and result in little to no pseudo-labels. The problems caused by the confidence thresholding are a limitation here and limit the relative comparison among the previous approaches, but also serve as a reality check very close to a practical active learning setup, where methods fail, when hyperparameters are not carefully optimized. Further resulting questions are discussed in the following. Why did the experiments not incorporate the most recent LLMs of 1B or more parameters? With a total runtime of 2152 hours, the experiments are already computationally expensive–despite using models that are considered small by today’s standards, and would be infeasible with larger model sizes. Besides, research has demonstrated that smaller models can outperform larger ones when properly fine-tuned or distilled (Hsieh et al., 2023), and therefore, we prioritize model efficiency in our active learning research, which ultimately aims to support real-world annotation where smaller models offer a more accessible solution. Why did the experiments use only a single selftraining iteration? While increasing the number of self-training iterations may further increase the classification performance, this also runs the risk of degradation (Gera et al., 2022; Xu et al., 2023). For this reason, by using only a single self-training iteration we minimize the risk of degradation, thereby using self-training not to replace but to complement active learning, and in favor of real-world settings at only little additional computational costs. 7 Conclusions In this work, we reproduce four existing selftraining approaches and apply them to the task of active learning for text classification. We devise and evaluate a new self-training approach that is tailored to contrastive learning and generates more pseudo-labels, thereby supporting the superior efficiency of contrastive training. Using small language models of 110M parameters and only 25% the amount of instances as previous work, the proposed approach achieves results close to the state of the art on three out of four datasets . 8\nLimitations This study is a not a replication, but a reproduction with slight deviations that provide comparable conditions. While this makes previous approaches comparable for the first time, this also introduces the risk of deviations or errors in the code. While the overall approach has shown to be highly effective, for an active learning study, it is unfortunate that this seems to be largely caused by data-efficient models leveraging the additional pseudo-labels, and only to a minor degree by the instances selected by the query strategy. Nevertheless, this was previously unknown and motivates further research on finding a query strategy suitable for self-training. Finally, the proposed approach is targeted at single-label classification. Our heuristic for hardlabel decisions is not applicable to the multi-label settings and would need to use another heuristic. Ethical Considerations This work presents a method that reduces annotation efforts and could be used for good or bad—in the same manner as most methods. In both cases our method would help to reduce the annotation efforts, however, all of this could be also done, given enough labeling efforts, without our method. Moreover, since self-training relies on algorithmically assigned pseudo-labels, the obtained pseudo-label distribution is dependent on the unknown true distribution of the dataset, which could be biased towards certain classes. In this case, selftraining might not only be prone to error propagation, but also might propagate class biases. 595",
        "aspect_review": "The paper introduces a novel self-training strategy, HAST, which effectively combines self-training with active learning to enhance text classification performance using pre-trained language models. The paper provides comprehensive experimental validation across four text classification benchmarks, demonstrating the effectiveness of the proposed method in comparison to existing approaches. The results show that HAST achieves competitive classification results using significantly fewer labeled instances, highlighting its potential for practical applications in reducing annotation costs. The study addresses the critical issue of data scarcity in text classification by leveraging unlabeled data through self-training, which is a significant contribution to the field.\n\nThe performance of HAST on certain datasets, such as TREC, is not as strong as on others, indicating room for improvement. The study is limited to single-label classification tasks, which restricts the generalizability of the proposed method to multi-label scenarios. While the paper compares HAST with existing self-training approaches, the impact of query strategies on self-training performance is not deeply explored. The reliance on smaller models for computational feasibility limits the exploration of HAST's potential with larger, more recent language models. The use of a single self-training iteration, while practical, may not fully exploit the potential benefits of iterative self-training.",
        "general_review": "This paper presents a notable novelty by introducing HAST, a new self-training strategy that significantly improves the efficiency of active learning for text classification. The technical depth and rigor are evident in the comprehensive reproduction of four previous self-training approaches, allowing for a fair comparison and validation of the proposed method. The experimental validation is extensive, covering four text classification benchmarks and demonstrating that HAST achieves competitive results using only 25% of the data. The practical impact is significant as the method reduces annotation efforts, making it highly relevant for real-world applications where labeled data is scarce.\n\n1. Minor improvements in writing or clarity could enhance the readability of certain sections. 2. Small details on hyperparameters or implementation specifics could be expanded for better reproducibility. 3. Very minor formatting issues in figures could be addressed to improve visual clarity. 4. Slight clarification on code availability, as it is mentioned to be released upon publication, could be made more explicit."
    },
    "21": {
        "paper": "Reconsidering Sentence-Level Sign Language Translation\nAnonymous ACL submission\nAbstract Historically, sign language machine translation has been posed as a sentence-level task: datasets consisting of continuous narratives are chopped up and presented to the model as isolated clips. In this work, we explore the limitations of this task framing. First, we survey a number of linguistic phenomena in sign languages that depend on discourse-level context. Then as a case study, we perform the ﬁrst human baseline for sign language translation that actually substitutes a human into the machine learning task framing, rather than provide the human with the entire document as context. This human baseline—for ASL to English translation on the How2Sign dataset— shows that for 33% of sentences in our sample, our ﬂuent Deaf signer annotators were only able to understand key parts of the clip in light of additional discourse-level context. These results underscore the importance of understanding and sanity checking examples when adapting machine learning to new domains. 1 Introduction One of the key challenges in sign language processing is that methods from mainstream natural language processing (NLP) are tailored primarily to text and secondarily to speech. Much of the work in this space therefore focuses on generalizing these methods to video, in order to capture this oft-neglected dimension of linguistic diversity (Bragg et al., 2019; Yin et al., 2021). One such carryover is that sign language machine translation (MT) is framed as a sentence-level task. Although continuous sign language datasets are usually derived from long-form signed content (e.g., interpreted news broadcasts), they are preprocessed into short clips associated with each sentence in the spoken language transcript (which may not themselves correspond to discrete sentences in the continuously translated sign language version), and models are trained and evaluated on 042these clips in isolation. In this work, we examine the limitations of this task framing, which—like many other sign language modeling decisions (Desai et al., 2024)—was adopted somewhat uncritically, and ask: what is the right unit of translation for sign language? Machine translation between spoken languages is typically posed as a sentence-level task, and although it largely works, there are known intersentential dependencies like anaphora that are impossible to resolve in isolation (Bawden et al., 2018; V oita et al., 2019). These dependencies are especially troublesome for language pairs that have mismatches in grammatical features like pronoun dropping, tense marking, or gradations of register. The situation is perhaps even more pronounced for translation between spoken languages and sign languages. Sign languages are not just spoken languages produced with the hands: the grammar of sign languages is shaped by the nature of the visualspatial modality (Meier et al., 2002). While utterances produced by non-native signers tend to resemble the syntax of the region’s spoken language, native signing often expresses concepts in a fundamentally different way that is richly grounded in spatial world understanding and, more importantly here, the discourse context. When deprived of that context, the viewer may catastrophically fail to understand the meaning of an utterance and therefore be unable to translate it. We describe some linguistic phenomena relevant to cross-modal translation in Section 3. To the best of our knowledge, no sign language MT benchmarks provide baselines for human performance that actually ask humans to perform the same task that they expect of the model. Reference translations are given in the dataset by construction, either as the source text or by discourse-level translation. Human judgments are used at the discourse level to quality-check preprocessing or to evaluate model-generated outputs, but not to sanity check 1\nthe task framing itself. We therefore provide in Section 4 the ﬁrst such human baseline, for American Sign Language (ASL) to English translation on the How2Sign dataset (Duarte et al., 2021), as a case study. How2Sign consists of informal instructional (“how to”) narratives, which is a particularly illustrative domain. Before even scoring results against ground truth references, we ﬁnd that for 33.3% of instances in our sample, our ﬂuent Deaf signer annotators felt that they could not fully perform the translation given only the sentence-level clip—but could, given additional discourse-level context. Most of these errors were due to features of sign languages that lack analogues in spoken languages. When we do compute metrics, we get a surprisingly low score of 19.8 BLEU (56.6 BLEURT) for the sentencelevel task, which increases with additional context but only to 21.5 (59.5). We disaggregate these results for each of ﬁve distinct interpreters in the How2Sign test set, and ﬁnd that sentence-level scores vary from 5.2 BLEU (45.7 BLEURT) to 39.5 (70.0). Scores are higher for interpreters who hew closer to English; context is more important for those who don’t. We hope that these results and analysis will encourage the sign language MT ﬁeld to reconsider whether computational beneﬁts of the sentencelevel task framing outweigh its quality and alignment limitations, and to continue to pare back unfounded modeling assumptions by understanding datasets more deeply and crafting benchmarks more deliberately. 2 Background & Related Work 2.1 Sign Languages See Bragg et al. (2019), Yin et al. (2021), Coster et al. (2023), and Desai et al. (2024) for excellent surveys of the social and technical aspects of sign language processing. In brief, in contrast to spoken languages, which are articulated with the vocal tract, sign languages are articulated with the upper body (including the face). These two modalities impose different constraints on the grammar of languages within them. Sign languages are minority languages primarily used by the Deaf/Hard of Hearing communities of various regions; they are natural languages that are genealogically unrelated to but often considerably inﬂuenced by the dominant spoken language of the region. Within a single sign language, there is a 133great deal of variation due to geographic and social factors. For example, in the US and Canada there is a diglossic spectrum from American Sign Language (ASL), a fully-ﬂedged independent language; to Manually Coded English (MCE), a system to transcribe spoken English into the sign lexicon of ASL; with Conceptually Accurate Signed English (CASE) vaguely in between (Supalla and McKee, 2002; Rendel et al., 2018). Across all of these, there is regional variation in vocabulary, analogous to “soda” vs. “pop” in American English but perhaps more pronounced (Shroyer and Shroyer, 1984). And there is social variation, like Black ASL, analogous to Black English (McCaskill et al., 2011). Less than 6% of deaf children in the US and less than 2% of deaf children worldwide are exposed to a sign language in early childhood (Murray et al., 2019), so there are also different levels of proﬁciency even among Deaf signers. Sign language MT should handle all these dimensions of variation. 2.2 Sign Language Translation Because the full task involving video to text translation was unapproachable at the time, early work on sign language translation focused on generation cascaded through glosses , which are nonstandardized linguistic annotations representing signs. This allowed the task to be formulated as a special case of (sentence-level) text-to-text translation and reuse methods from mainstream MT (Chapman, 1997; Veale et al., 1998; Zhao et al., 2000). Unlike MT for written languages, translation from sign language glosses as a source representation is not immediately useful, because signers in general do not use them—only linguists and to some extent students do. Therefore the other half of the cascaded sign language understanding pipeline is sign language recognition (SLR), the task of predicting glosses from videos of people signing. Isolated SLR classiﬁes a single gloss from a short clip (Charayaphan and Marble, 1992; Joze and Koller, 2019; Li et al., 2020; Desai et al., 2023; Starner et al., 2024), and continuous SLR predicts a sequence of glosses from a clip of an entire sentence (Koller et al., 2015; Cui et al., 2017). This sentence granularity is inherited from translation above and by analogy to automatic speech recognition (ASR), but is not especially harmful here: context is not strictly necessary because the task is to transcribe form, not understand meaning. 2\nThe modern framing for end-to-end video-totext sign language MT originates in Camgoz et al. (2018). The paper does not phrase the sentencelevel framing as an explicit decision point, but rather inherits it again, from mainstream machine translation and continuous SLR. Because videos (and more generally, long sequences) are computationally difﬁcult to work with, there is also an unstated pressure to use shorter clips. The provided dataset, RWTH-PHOENIX-Weather 2014T, is constructed on top of an existing (sentence-level) continuous SLR dataset, RWTH-PHOENIX-Weather 2014, of weather forecasts interpreted into German Sign Language. There is no human baseline provided for the task, but even if there were, it would likely be uneventful due to the dataset’s limited domain and non-native interpreters. As subsequent datasets have expanded into more sign languages and broader domains (and deemphasized glosses, because they are a lossy bottleneck with limited availability), the datasets have retained the sentence-level framing—despite being constructed from long video corpora, where full discourse context is available and where there is not necessarily a sentence-level correspondence between the speech and sign tracks. Human annotations have been used to preprocess/quality check the dataset (Camgoz et al., 2021; Albanie et al., 2021; Shi et al., 2022; Joshi et al., 2023; Shen et al., 2023; Uthus et al., 2023) or evaluate model outputs (Müller et al., 2022, 2023; Duarte et al., 2021), but not to explore the sentence-level framing itself. See Appendix A for a dataset-by-dataset analysis. While surveying gloss-based translation methods, Müller et al. (2022) note that only sentencelevel systems had been studied at the time, and they give spatial indexing as one example of a grammatical feature that may be truncated in sentence-level systems. We are aware of only one work that has studied sign language translation beyond the sentence level since then, Sincan et al. (2023). Their work examines the empirical gains from providing models with prior text context—either full sentences or sign spottings—without speciﬁc sign linguistic motivation. Quality improves signiﬁcantly but is still extremely low in absolute, so it is possible that the context is being used as a shortcut rather than an essential part of the task framing. Our work is complementary in that we analyze a wide variety of linguistic phenomena, and study a setting (human performance) where we are not bottlenecked by limitations of current training datasets 236and can more easily interpret results qualitatively. 2.3 Document-Level Translation While the majority of work on machine translation focuses on (and has been very successful within) the sentence-level task framing, there is a body of work that highlights the aspects that are lost between sentences. Automatic reference-based metrics are relatively insensitive to discourse-level problems that stand out to human raters (Hardmeier, 2012; Läubli et al., 2018), such as issues with lexical consistency, formality, and gender/number agreement (V oita et al., 2019; Fernandes et al., 2023). Therefore many works create contrastive test sets where several candidate translations are ranked among themselves, rather than translations being generated from a blank slate, to measure these properties (Bawden et al., 2018; Müller et al., 2019; Nagata and Morishita, 2020). These works mostly evaluate model outputs rather than ideal (human) performance, but e.g. Matsuzaki et al. (2015) provides a human baseline for English !Japanese translation of short dialogues, in which the rate of correct translations is 18 percentage points higher given full document context vs. only an isolated sentence. We extend this line of work to sign languages by surveying extra linguistic phenomena related to the visual-spatial modality, then evaluate the empirical importance of discourse-level effects in this domain using a combination of automatic metrics and human ratings in the ideal (human) setting. Historically, the bottleneck for training document-level MT has been the availability of document-level parallel corpora (V oita et al., 2019); only a small fraction of translation data was natively document-level, such as video content with subtitles in multiple languages (Lison et al., 2018; Duh, 2018).1The situation is markedly different for sign languages: virtually all sign language corpora are natively discourse-level (with minor exceptions like SP-10 (Yin et al., 2022) and WMT-SLT Signsuisse (Müller et al., 2023), which consist of isolated dictionary example sentences) but are preprocessed into isolated clips. Why not use this extra structure? 1Recently, with the rise of self-supervised pretraining and\nLLMs this is less of a concern, since document-level mono-\nlingual data is abundant. (Siddhant et al., 2020; Wang et al.,\n2023).\n3\nTHATCAUSEPRESSURECL-[move joystick]WILLCL-[wing flap moves]Figure 1: Example of the interaction between classiﬁers and long-range context. It isn’t clear in isolation that\nthe ﬁst moving back and forth represents a ﬁst controlling a joystick, or that the arm represents an airplane wing\nand the hand represents a ﬂap (aileron) on the wing. Interpreter’s head omitted here for privacy.\n3 Long-Range Linguistic Dependencies In this section, we outline a number of long-range dependencies in the grammar of sign languages, primarily ASL, which may be truncated with sentencelevel clipping. These features are not necessarily universal to all sign languages, but they are relatively common insofar as they are motivated by the visual-spatial modality (Aronoff et al., 2005). We create example ﬁgures using clips from the How2Sign dataset (Duarte et al., 2021); we omit the signers’ faces in the ﬁgures for privacy but note that facial expressions and mouthing are important in sign language. 3.1 Spatial Referencing Perhaps the most salient feature that distinguishes sign languages from spoken languages is the ability to use space in a way that is grammatically structured (as opposed to in co-speech gesture) (Emmorey, 1996). Pronouns Whereas spoken languages use third-person pronouns to refer to entities that were previously introduced in the discourse, sign languages use spatial indexing , i.e., they establish that a locus in space refers to a particular entity and then reference that entity by pointing (Emmorey, 1996). Because spoken languages tend to have a small set of thirdperson pronouns, they become ambiguous as the number of entities under discussion grows. But the number of unambiguous referents in sign languages is only limited by the granularity at which space can be comfortably partitioned. Therefore there can be less pressure to reintroduce referents in sign languages than in spoken languages. So it may be the case that a spatial index in a sign language should be translated into a named entity in a spoken language (rather than a pronoun), or vice versa—but without context, it’s impossible to know what name corresponds to that spatial index, or where that named entity lies in 321space. This is like a more severe version of translation between languages that have gendered vs. ungendered (or omissible) pronouns (Frank et al., 2004; Savoldi et al., 2021). Directional Verbs Some verbs in sign languages are directional , i.e., their movement is inﬂected to agree with the spatial loci of their arguments (Liddell, 1990). This is analogous to polypersonal agreement in spoken languages (verb agreement with respect to multiple arguments), but more ﬂexible (and more contextdependent) for the same reason as pronouns above. Classiﬁers In certain spoken languages, the term “classiﬁers” refers to words that agree with nouns of different semantic categories, and are often obligatory when counting nouns with numerals (Allan, 1977). In sign languages, classiﬁers are more expansive: like with spoken classiﬁers, different handshapes represent different categories of objects, but they can also be inﬂected in classiﬁer predicates , where the location and movement of the classiﬁer take on an extremely ﬂexible, iconic predicative meaning (Frishberg, 1975; Liddell, 1980). A classic example is the 3 handshape in ASL (extended thumb, index, and middle ﬁnger) oriented with the thumb up, which represents a number of vehicles, especially cars. The classiﬁer can be repeated across space to describe a packed parking lot, swerved side to side to depict a car driving down a winding road, slammed into another surface to represent a car crash, etc. Because classiﬁers can refer to many objects in a particular category, and the referent needs only be clear from context (either explicitly introduced or just implied by the situation), the subject or entire meaning of a classiﬁer predicate may not be clear in isolation. For example, in Figure 1 it is only clear from context that the classiﬁers are referring to a joystick & wing ﬂaps in an airplane. 4\nRole Shift When describing interactions between two or more characters, signers will often role shift , i.e., they physically embody and act out the different characters (Padden, 1986). This is analogous to quotes in spoken languages, except that turn-taking is not marked explicitly with words like “he said”: instead, it’s marked by shifting the angle and position of the body and head. In sentence-level clips, it may not be clear who is referenced by each role— or even that role shift is being used at all—because each turn in the role shift is considered its own sentence and clipped in isolation. 3.2 Out-of-Vocabulary Terms With languages in the same modality, it is straightforward to translate out-of-vocabulary terms like proper nouns by copying them directly from the source into the target context (perhaps with some phonological tweaks and transliteration, complicated somewhat by acronyms). But this strategy breaks down across modalities. Because spoken languages are socially dominant over sign languages, virtually every sign language can productively borrow terms from spoken languages, through ﬁngerspelling (spelling the word with a manual alphabet) or mouthing (silently saying the word while producing a related sign). But the reverse isn’t true: spoken languages have no mechanism for borrowing signs. Context is important for strategies that reconcile this mismatch. Abbreviated Fingerspelling When introducing a ﬁngerspelled term for the ﬁrst time in a discourse, signers will spell it clearly to make sure that it can be understood. But when returning to that term later, they may speed through it amorphously to save time, with the understanding that the viewer can recognize the shape of the word in context. For example, in Figure 2 the letters of the word “basil” are ﬁngerspelled simultaneously and out of order. This is described as “careful” vs. “rapid” ﬁngerspelling in the literature (Patrie and Johnson, 2011; Thumann, 2012; Wager, 2012).2If the signer anticipates that they will refer to the term repeatedly, especially for proper nouns, they may even declare a temporary acronym upfront and use it for the remainder of the discourse. For example, in an instance from the human baseline 2A similar reduction happens for repeated spoken words\ntoo, but the effect is smaller (Jacobs et al., 2015).\nB A I S L \nB A/I \n A/I/L \n L/S Figure 2: Example of the interaction between rapid\nﬁngerspelling and long-range context. Top is the ﬁrst\n“basil” in the narrative (itself spelled slightly out of or-\nder), and bottom is the version from the test sentence:\nhighly coarticulated, with multiple letters produced si-\nmultaneously. The labels indicate the relevant letters\ngiven the ground truth, but without context other letters\nsuch as Y , X, and T could be perceived.\nthe trading card “Whalebone Glider” is abbreviated “WG” after its ﬁrst mention. Absent context, it is difﬁcult or impossible for someone viewing sentence-level clips to know what these abbreviated terms refer to, and copying the abbreviations directly would be unnatural in the target spoken language. The other translation direction is perhaps less problematic, because one could guess whether a proper noun is being used for the ﬁrst time based on local cues and translate appropriately. Name Signs In American Deaf culture, in addition to their full legal names, signers use sign names given to them by other members of the Deaf community. If their name is short enough, a person’s sign name may be a ﬁngerspelled version of their legal name, but otherwise it is an idiosyncratic sign based on factors like their personality, appearance, and interests; name signs are perhaps even more idiosyncratic than names in spoken languages (Supalla, 1992). When talking to an unfamiliar audience, a signer will often ﬁngerspell a person’s name and give their name sign, then refer to them using their name sign for the rest of the discourse. Training on isolated clips that include name signs will encourage the model to hallucinate. Challenges with name signs are not necessarily universal across sign languages; for example, in Japanese Sign Language, name signs are often a function of the kanji in a signer’s legal name (Nonaka et al., 2015), and therefore could more easily be translated without context. 5\nNonstandard signs For a variety of historical reasons—lack of a writing system, the very recent development of video calling, historical exclusion of sign languages from education—ASL lacks standardized vocabulary in certain academic ﬁelds (McKee and Vale, 2017).3When introducing a nonstandard or niche sign, the signer will often ﬁngerspell it to ensure that it is understood by a less familiar audience. When translating from a sign language into a spoken language, like with name signs the model may be able to guess the meaning but is generally encouraged to hallucinate. When translating from a spoken language into a sign language, if the model knows multiple nonstandard signs it is unclear how it could coordinate their usage across independently translated sentences, like issues with lexical cohesion in text MT (V oita et al., 2019). 3.3 Generic Context Dependence In addition to the aforementioned features speciﬁc to sign languages and the visual-spatial modality, sign languages can be context-dependent in similar ways to spoken languages. For example, in terms of grammar: ASL can drop pronouns (LilloMartin, 1986) and has a variety of strategies for expressing tense (Jacobowitz and Stokoe, 1988) and deﬁniteness/indeﬁniteness (Irani, 2019). In terms of vocabulary: lexical signs can be ambiguous or dialectal (making them harder to understand without context). 4 Case Study In order to explore how these phenomena surface in real sign language translation datasets, we perform a human baseline for ASL to English translation on How2Sign (Duarte et al., 2021) across different amounts of provided context. To the best of our knowledge, this is the ﬁrst time human performance has been measured for the sentence-level sign language machine translation task. How2Sign (CC BY-NC 4.0) was constructed by having 11 signers—5 Hearing, 4 Deaf, and 2 Hard of Hearing—watch English-captioned instructional “how to” videos from the earlier How2 dataset (Sanabria et al., 2018) a ﬁrst time to understand the content, then a second time at 0.75x speed while performing a live interpretation. The captions 3There are efforts underway to invent standardized vocab-\nulary, but currently each school or even each class tends to\ninvent its own signs as needed.(from the original speech track) were manually realigned to the signing, with an average sentence duration of 8.67 seconds. 4.1 Setup First, we describe the human baseline test instances and settings. Here in the context of ASL to English translation, we use sto refer to the source ASL clips for a particular video and tto refer to its target English captions. iis the index of a particular clip/caption within that video. We collect translations across four different context settings: •si: The source clip alone. This is the classic sign language machine translation framing. •si\u00001:i: The source clip extended backwards to include the previous clip. •si\u00001:i,ti\u00001: The previous and current source clip, plus the ground truth text for the previous clip.4•si\u00001:i,t0:i\u00001: The previous and current source clip, plus the ground truth captions for the entire video up to this point. Note that each of these settings strictly expands upon the prior one, so it is valid for a single annotator to perform all four in sequence. (Some of these translations may be identical to those for prior settings, if the annotator does not want to adjust their translation in light of new context.) However, it is not valid for an annotator to translate multiple clips iwithin a single video due to leakage. On top of these four translation settings, we also ask the annotators to describe how well they understood the sentence in isolation vs. after seeing additional context, and to rate the naturalness of the signing on a scale from 0-2, where higher is more natural.5To select our human baseline instances we start with How2Sign’s test set, which consists of 184 ASL translations of 149 How2 narratives, sliced into 2,322 clips. We discard narratives that are translated multiple times by different signers (to avoid cross-instance leakage) and videos that seem generally malformed (e.g., large spans of the video 4Using the ground truth is slightly unrepresentative of what\nis possible at test time; the ideal would have been to translate\nusing the entire source video up to this point as context, but\nevaluating this setting would have been prohibitively time-\nconsuming. These settings that condition on previous captions\nare more similar to how we initially expect machine learning\npractitioners to incorporate context in light of sequence length\nconstraints, like in Sincan et al. (2023).\n5Speciﬁcally, they were asked to answer “Is it natural\nASL?”, with 0=“no”, 1=“eh”, and 2=“yes” as the options.\n6\nlack captions or captions extend beyond the duration of the video). For each remaining narrative, we sample a clip uniformly at random, excluding the ﬁrst clip in each narrative because results for the context settings would be trivial.6Some clips within narratives are not contiguous because the signer made an error between sentences, which breaks the si\u00001:icondition; we reject these cases and resample until success. The result is a set of 102 test instances, at most one per narrative. Second, we describe the actual execution of the human baseline: Our annotators were the two middle authors, who are Deaf signers who use both ASL and English as primary languages7; the other authors set up the test instances. Each annotator spent several hours performing the translations and ratings for a random nonoverlapping split of the data, leaving additional commentary as they went for use in our qualitative analysis. The annotators were allowed to slow down or repeat the video, but were told not to agonize over it frame by frame. See Appendix B.1 for annotator instructions. 4.2 Results Following prior works that evaluate on How2Sign (Álvarez et al., 2022; Lin et al., 2023; Tarrés et al., 2023; Uthus et al., 2023), we report BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020) as our quantitative metrics. We compute BLEU using SacreBLEU (Post, 2018) version 2 with all default options, and BLEURT using checkpoint BLEURT-20 (Pu et al., 2021). See Table 1 for scores, Table 2 for ratings, and Appendix B.2 for the complete set of translations comprising the human baseline. Effect of context. Human performance on the sentence-level translation task is 19.8 BLEU (56.6 BLEURT) and increases monotonically with extra 6This means that our metrics will slightly overestimate the\neffect of context, because they ignore initial sentences that are\nmeant to be understood without context.\n7Note that these annotators are not professional transla-\ntors, which may harm the quality of the translated outputs\n(and automated metrics computed on them). However, the\nEnglish captions in How2Sign (originally from How2) are not\nespecially polished themselves, since they are transcriptions\nof spontaneous speech with disﬂuencies etc., so we expect\nthis to be less of an issue than if we were comparing to ref-\nerence translations by professional sign language interpreters\nof originally signed content. These annotators also know the\nresearch purpose (and could have inferred it from the sequence\nof context settings, even if they hadn’t had foreknowledge),\nwhich may bias the translations and ratings. We were more\nconcerned with getting a good qualitative understanding of\nthe data amongst the authors.BLEU s isi\u00001:isi\u00001:i,ti\u00001si\u00001:i,t0:i\u00001\nAverage 19.8 20.4 21.1 21.5\nInterpreter A 5.2 6.0 6.1 6.3\nInterpreter B 18.4 19.1 20.5 21.0\nInterpreter C 7.4 7.2 8.2 8.7\nInterpreter D 39.5 40.9 41.3 41.1\nInterpreter E 19.4 19.5 19.8 20.7\nBLEURT s isi\u00001:isi\u00001:i,ti\u00001si\u00001:i,t0:i\u00001\nAverage 56.6 58.1 59.4 59.5\nInterpreter A 45.7 48.7 49.3 48.6\nInterpreter B 57.3 57.1 57.6 58.1\nInterpreter C 47.7 50.0 54.3 55.0\nInterpreter D 70.0 70.6 71.1 70.3\nInterpreter E 59.7 61.3 61.3 61.8\nTable 1: BLEU (top) and BLEURT (bottom) scores ( \")\nfor the human baseline for ASL to English translation,\nacross different amounts of provided context and differ-\nent interpreters featured in the videos.\ncontext, but only up to 21.5 BLEU (59.5 BLEURT). This consistent but relatively small difference in automatic metrics belies the annotators’ perception of the gap: for 33.3% of test instances, the annotators judged that they were unable to understand key details of the signed content from the sentence in isolation which they later understood from additional context (veriﬁed with their actual translations across settings compared to the ground truth). Of these failure cases, 47% featured classiﬁers with unclear referents, 38% grammatical features like prodrop/lack of overt tense markings, 26% rapid ﬁngerspelling, 9% acronyms, 6% ambiguous signs, and 6% dialectal sign variation.8In addition to translations that improved given past context, there were several examples where the translation was incorrect across all settings because future context was needed to understand the sentence. We did not anticipate this, so there was no experimental setting to measure it. Variation across interpreters. We observe qualitatively that there is enormous variation in signing style between the ﬁve interpreters (which we label A-E) featured in the test videos, across the spec8We didn’t come across any How2Sign instances of several\nlinguistic phenomena described in Section 3, for a variety of\npresumed reasons. Spatial indexing, directional verbs, and\nrole shift are relevant when discussing third-person characters\n(especially multiple ones interacting), but How2Sign is largely\nﬁrst-person or second-person given the instructional narrative\ndomain. Name signs are generally only used in originally\nproduced Deaf content. Nonstandard signs are used primarily\nby domain experts, so they are unlikely to be introduced in\ncontent translated from English without much preparation.\n7\ntrum from ASL to CASE to MCE. It is hard to disentangle this from the shallow translations that are typical of live interpreting. Inspired by prior work on disaggregated evals (Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019; Barocas et al., 2021; Kaplun et al., 2022), we therefore break down our results by interpreter. We ﬁnd that the human baseline metrics match our subjective impressions: they vary from 5.2 BLEU (45.7 BLEURT) for Interpreter A to 39.5 (70.0) for Interpreter D. The interpreters with lower scores perform deeper translation closer to ASL, and those with higher scores are bordering on MCE (which inﬂates n-gram overlap, because the task approaches sign recognition rather than translation). The interpreters signing with more English inﬂuence also tend to mouth more prominently, so sometimes the translation is clear from lipreading even when the signing itself is odd and hard to understand. The annotators rated the average naturalness of the content at 1.05 on a scale from 0-2 ( \"), ranging from 1.93 for Interpreter A to 0.64 for Interpreter D; generally, the more natural the content, the worse the sentence-level translation metrics.9When we look at the other three settings, we see that context has a proportionally larger effect for interpreters where the translation metrics were originally lower (and naturalness is rated higher): Interpreter A increases from 5.2 BLEU (45.7 BLEURT) to 6.3 (48.6) and Interpreter C from 7.4 (47.7) to 8.7 (55.0), vs. Interpreter D from 39.5 (70.0) to 41.1 (70.3). This bears out in the annotator ratings as well: translation failed due to missing context 73.3% of the time on Interpreter A and 44.0% of the time on Interpreter C, but only 13.6% of the time on Interpreter D. This conﬁrms our suspicion that the effect of discourse context is obscured by evaluating on live (and especially hearing) interpreters. Even though there is a clear improvement in metrics due to context, the average effect size is obscured by the fact that we are essentially evaluating on multiple domains at once. 9We emphasize that these naturalness judgments are sub-\njective from the perspective of the annotators. This may\nbe biased by social factors like the perception that a hyper-\ncorrect “pure” form of ASL is the most prestigious, as op-\nposed to signing with more inﬂuence from English—or vice\nversa (Stokoe Jr, 1969; Vicars, 2023). Sign language transla-\ntion models should still understand this content (especially to\nthe extent that this reﬂects real variation in how people sign,\nas opposed to performance effects of live interpreting), but it\nis important to know what we are actually evaluating so that\nwe do not e.g. test on artiﬁcially easy content and overstate\nperformance for actual Deaf signers.% ctxt failure naturalness (0-2, \")\nAverage 33.3 1.05\nInterpreter A 73.3 1.93\nInterpreter B 14.3 1.0\nInterpreter C 44.0 1.24\nInterpreter D 13.6 0.64\nInterpreter E 26.9 0.73\nTable 2: Annotator ratings for the human baseline —\n% of instances where they failed to understand key de-\ntails from the sentence in isolation but later succeeded\nwith context, and naturalness of the signed content on\na scale from 0-2 ( \")—broken down by interpreter.\nMisalignment. Despite How2Sign’s use of manually realigned captions (and despite us having excluded apparently malformed videos earlier), 5% of the sentence-level clips in our baseline still do not contain the relevant content. Even more clips lack signiﬁcant parts of the ground truth translation or have extra content beyond it. On top of this, the onset of a sentence usually begins earlier on the face than the hands, so with even with “accurate” clipping the sentence may either start with a leftover handshape from the previous sentence or truncate the start of the sentence on the face. These all combine to make it difﬁcult for annotators (or models) to know which parts of the input clip they should and shouldn’t translate. In a discourse-level framing, misalignment matters less because the offset is a smaller fraction of the overall content. 5 Conclusion In this paper, we argued that the costs of the sentence-level sign language MT task framing are higher than many might expect from experience with spoken languages, because features of the visual-spatial modality and cross-modal translation make discourse context especially important for sign languages. We supported this with a case study: the ﬁrst human baseline for sentence-level sign language MT, from ASL to English on the How2Sign dataset. We found that discourse context was necessary to fully understand and translate a large fraction of sentences (33.3%), and this effect is itself attenuated by the prevalence of signing data that does not represent the more challenging aspects of ASL due to its use of non-native or live interpreters. We hope that this inspires more indepth analysis grounded in ﬁrsthand experience with sign languages, to avoid perpetuating systemic bias in the way we conceptualize sign language tasks (Desai et al., 2024). 8\nLimitations Our results are limited in that we empirically evaluate one language pair (ASL and English), one translation direction (ASL to English), and one domain (instructional narratives from the How2Sign dataset). Extrapolating from our analysis in Section 3: •We expect the aforementioned long-range dependencies to exist in other sign languages, because they are generally motivated by features of the visual-spatial modality. •We expect English to ASL translation (translation from a spoken language into a signed language) to suffer similar problems. Sometimes, source sentences would not include enough grounding to perform a natural translation with classiﬁers. And even when source sentences do include all necessary information to perform a faithful translation, even a perfect sentence-level translation model would result in unnatural discourse-level translations when concatenating clips due to inconsistent use of space and other discourse phenomena across sentences. •Direct translation between two sign languages may be less problematic than translation between a sign language and a spoken language, because similarities in use of space or classiﬁers may allow for a shallower translation. •Results from How2Sign may not be representative of results on other domains. Informal instructional narratives are relatively well-suited to showing the inadequacies of sentence-level translation, because they are grounded in a single scene for the duration of the narrative and use relatively short sentences. However, they are also light on description of multiple thirdperson entities interacting with each other, which use other context-dependent structures described above. We expect stories/ASL literature to require more context, and content with stronger inﬂuence from English (or the respective dominant spoken language for other regions) to require less. Ethics Statement The ethical considerations of this work are those for sign language processing as a whole. Namely, ma- 714chine understanding of sign languages would improve access to information, communication, and other technologies for underserved signing communities. However, there is a risk that—rather than supplement existing resources to strictly improve access—entities who currently provide services in sign languages might replace a high-quality solution that uses human interpreters with a lowerquality automated one. This work tries to expose deﬁcits in the current task framing so that automatic solutions will be less ﬂawed. Inclusion in modern NLP also brings with it a number of well-known risks (misinformation, bias, etc. at scale). Future works that release trained models should mitigate these potential harms. 729",
        "aspect_review": "The paper is well-structured and clearly presents the motivation and objectives of the study, making it accessible to readers. The paper provides a comprehensive discussion of the linguistic phenomena in sign languages that require discourse-level context, offering valuable insights into the limitations of sentence-level translation. The analysis of human baseline performance provides a novel perspective on the importance of context in sign language translation, highlighting the gap between machine and human capabilities.\n\nThe study is limited to a single dataset (How2Sign) and language pair (ASL to English), which may restrict the generalizability of the findings. Some sections of the paper could benefit from more concise language to improve readability. While the paper discusses the importance of discourse context, additional experiments across different datasets and languages could strengthen the validation of the claims.",
        "general_review": "This paper presents a novel exploration into the limitations of sentence-level framing in sign language translation, which is a significant departure from traditional methods. It provides strong technical depth by integrating human baseline studies that directly substitute humans into the machine learning task framing, which is a rigorous approach that has not been previously applied in this domain. The comprehensive experimental validation is evident through the detailed case study on ASL to English translation using the How2Sign dataset, highlighting the importance of discourse-level context. The paper has significant practical impact by challenging existing assumptions in sign language machine translation and encouraging the field to reconsider task framing for better model performance and alignment.\n\n1. The paper could benefit from minor writing improvements for clarity in some sections, particularly where complex linguistic phenomena are described. 2. There are small details missing on hyperparameters or implementation specifics that could aid in reproducibility. 3. Very minor formatting issues are present in some figures, which could be improved for better visual clarity. 4. The availability of the code used for the experiments is not explicitly mentioned, which could be clarified to enhance transparency and reproducibility."
    },
    "22": {
        "paper": "COMP ACT: Compressing Retrieved Documents Actively for\nQuestion Answering\nAnonymous ACL submission\nAbstract\nRetrieval-augmented generation supports language models to strengthen their factual groundings by providing external context. However, language models often face challenges in locating and integrating extensive information, diminishing their effectiveness in solving complex questions. Query-focused compression tackles this issue by filtering out information irrelevant to the query, but current methods still struggle in realistic scenarios where crucial information may not be located with a single-step approach. To overcome this limitation, we introduce COMP ACT, a novel framework that employs an active strategy to condense extensive documents without losing key information. COMPACTflexibly operates as a cost-efficient plug-in module with any off-the-shelf retriever or reader model, achieving extremely high compression rates (44x). Our experiments demonstrate that COMPACT brings significant improvements in both compression rate and QA performance on multihop question-answering datasets. 1 Introduction Retrieval-augmented generation empowers language models to solidify their factual groundings, presenting relevant contexts to answer questions (Khandelwal et al., 2019; Lewis et al., 2020; Karpukhin et al., 2020a; Izacard et al., 2023). While this approach extends the knowledge scope of language models beyond their inherent capabilities, it also introduces a number of challenges when it comes to handling long contexts (Li et al., 2024; An et al., 2024; Qian et al., 2024). First, models often struggle to find key information from these extensive contexts, which diminishes their abilities to reference documents (Liu et al., 2024). Also, models often fail to integrate information across multiple documents, which is a natural situation in real-world scenarios (Cheng et al., 2024). To this 5 10 20 30 40\nT op-k4045505560Metric\nRecall@k\nRECOMPRaw docs\ngold docsCompAct (Ours)Figure 1: Performance of HotpotQA with different top-\nkdocuments. We set the reader as LLaMA3-8B. Our\nCOMP ACTframework demonstrates solid performance\nimprovements that align with those of gold documents.\nThis highlights that COMPACTeffectively leverages the\nbenefits of increased top- k, unlike other methods which\nstruggle to maintain their performance due to increased\nnoisy context.\nend, there is a growing need for methods that can assist models with handling long contexts. One way to overcome these challenges is by compressing contexts into more compact forms (Li et al., 2023; Pan et al., 2024). The main goal of compression is to reduce the amount of tokens from the original text without losing too much information. As it focuses on retaining all the crucial contexts from the source documents, the compressed output can be applied to various tasks without compromising the integrity of the information. However, simply compressing retrieved contexts can be suboptimal for question-answering (QA) tasks (Joshi et al., 2017; Kwiatkowski et al., 2019), where important details may be filtered out during the compression process (Li et al., 2023). Maintaining redundant information without compres1\nsion can also harm performance, as they may serve as distractors that can induce models to generate incorrect responses. To handle these limitations, query-focused compression emerges as an effective approach in QA. This approach reduces the context length by focusing on information relevant to the question (Xu et al., 2024; Cao et al., 2024). However, existing query-focused compressors (Jiang et al., 2023c; Xu et al., 2024) still struggle to take advantage of information located behind lengthy contexts, leaving out potential opportunities for reader models to improve their answers. Figure 1 highlights the difficulty of utilizing retrieved documents of extensive lengths. The increase in retrieval recall parallel to the number of retrieved documents shows that even lower-ranked documents may still include valuable information. Simply using more documents can result in a significant amount of noisy context, making it challenging for language models to effectively leverage additional information. Furthermore, existing methods lack the ability to integrate information across multiple documents, which is required in real-world scenarios (Gutiérrez et al., 2024). Figure 2 depicts an example: the question is \"What ‘Virtual Choir’-noted conductor has created works for the Austin-based ensemble Conspirare?\" . To answer this, not only do we need to retrieve information implied within the question ( \"conductors worked for the Austin-based ensemble Conspirare\" ), we should also holistically connect and synthesize information across multiple documents ( \"‘Virtual Choir’-noted conductor\" ). In other words, the quality of answers hinges on the ability of models to dynamically integrate information across multiple documents, which is an underexplored area in compression. To this end, we propose COMP ACT, a novel framework that can address these challenges by using an active strategy to compress extensive documents and retain crucial information. This approach has two key components: active compression and early termination. During compression, the model actively encapsulates input documents by jointly analyzing previously compressed contexts with newly provided segments. This ensures that only the most relevant information to the question is preserved at each step, creating a dense and compact context. At each step, the model then decides whether to terminate the compression process. This decision is made based on the relevance and completeness of the information gathered to 109answer the query. Our approach offers two distinct advantages. First, it effectively captures essential context from long documents by incorporating segments along with the previously compressed context. This is crucial for complex QA tasks that require in-depth reasoning and synthesis of information such as multi-hop QA. Second, it condenses large volumes of documents with a high compression rate, but without missing essential information. We conduct experiments on five question-answering datasets to evaluate our COMPACTframework. The results demonstrate that our framework brings significant improvement in compression rate and end-QA performance in several multi-document benchmarks. This represents the effectiveness of our compression method, as it preserves necessary context without losing critical information. Our contributions are as follows: (1) We propose COMP ACT, a novel framework that employs an active strategy for compressing extensive documents. Our framework dynamically filters and preserves relevant information by jointly considering previously compressed contexts with newly provided segments. (2) We address the limitations of existing compression methods by ensuring the integration of information across multiple documents. (3) Our approach effectively manages the challenges associated with handling long contexts, particularly in complex QA tasks that require indepth reasoning and synthesis of information. (4) Our framework achieves a high compression rate (44x) which demonstrates its cost-efficiency, especially when collaborating with API calls such as GPT-3.5-turbo. (5) We demonstrate the effectiveness of our COMPACTframework through comprehensive experiments on five question-answering QA benchmarks. 2 Preliminaries 2.1 Multi-Document Question Answering Multi-document (or multi-hop) question answering (QA) involves the task of answering questions that require gathering information from multiple documents. (Yang et al., 2018; Ho et al., 2020b; Chen et al., 2020; Trivedi et al., 2022; Mavi et al., 2022) This is more complex than single-document QA, since models must find and combine information from different sources. However, limited context windows hinder performance, in spite of the need for models to reference multiple sources. 2\nQuestion :What “Virtual Choir ”-noted \nconductor has created works for the \nAustin -based ensemble Conspirare?Answer :\nEric Edward \nWhitacreRetriever\n Reade rLLM\n Compressor\n1stSegment 2nd Segment N-th Segment\nThe summary (...) It mentions that Conspirare \nhas commissioned works from several \ncomposers, including Eric Whitacre , but does \nnot directly link Whitacre to Johnson. \n[INCOMPLETE]3k toks\nSegment -size\ndocuments\nEvaluationInputs\nCompressed TextQuestion\nCompressor\nSegment -size\ndocumentsInputs\nQuestionPrevious Output\nCompressor\nEvaluationCompressed Text\nThe summary does not provide sufficient details \n(...) about which \"Virtual Choir\" -noted conductor \nhas created works (...)  It mentions Eric Whitacre \nbut does not directly link Whitacre to the creation \nof works for Conspirare. [INCOMPLETE]\nSegment -size\ndocumentsInputs\nQuestionPrevious Output\nEvaluationCompressed Text\nThe summary provides all necessary details to answer the \nquestion. It identifies Eric Whitacre as a Grammy -winning \nAmerican composer, conductor , and speaker, known for his \n\"Virtual Choir\" projects , and mentions that Conspirare has \ncommissioned works from him .[COMPLETE]\nCompressor\n< 200 toks\nConspirare is a choral ensemble based in Austin, Texas. (...) \ncomposers including David Lang, Tarik O'Regan, Jocelyn \nHagen, Donald Grantham, Eric Whitacre , (...) Eric Whitacre \nis aGrammy -winning American composer, conductor , (...) \nHe is also known for his \"Virtual Choir\" projects , (...)Conspirare is a choral ensemble based in Austin, \nTexas .  They have commissioned works from \ncomposers including David Lang, Tarik O'Regan, \nJocelyn Hagen, Donald Grantham, Eric Whitacre ,  \n(...)Conspirare is a choral ensemble based in \nAustin, Texas . They have commissioned works\nfrom composers including  David Lang, Tarik \nO'Regan, Jocelyn Hagen, Donald Grantham, \nEric Whitacre , (...) \nFigure 2: Overall COMPACTframework as a plug-in module between the retriever and the reader LLM. After\nsplitting retrieved documents into segments, our model sequentially compresses these segments into compacted\ncontexts. By checking the termination condition at each step, we actively incorporate the information of the newly\nprovided segment while preserving essential backgrounds in compressed contexts. If the segments do not offer\ncomplete information to answer the question (1st, 2nd segments), the model continues to the next step to acquire\nnew information. Once all supporting clues are fully captured ( N-th segment), the iteration ends.\n2.2 Compression To alleviate the cost of inference, several studies have proposed compression methods. Mu et al. presents a compression method, called gisting, which allows models to compress prompts into shorter transformer activations. Ge et al. (2024) proposes training objectives related to compression that enable language models to learn to restore contexts. Several works have focused on compressing long context inputs. For example, Chevalier et al. (2023) progressively compress long documents into intermediate summary vectors. Li et al. (2023) and Jiang et al. (2023b) utilize conditional probabilities of LLMs to assess the importance of information. Concurrent with our work, Zhang et al. (2024) have developed an iterative framework using Chain-ofAgents to enable information aggregation and context reasoning over long-context tasks. However, our work focuses on addressing a crucial aspect: capturing pivotal information between segments in retrieved documents while compressing contexts. 2.3 Task Formulation In retrieval-augmented generation, a model Mpredicts an output yconditioned on an input xand 183kretrieved passages Dk={d1, ..., d k}k\ni=1. For the task of question answering, the input xtypically consists of a question qwith an instruction I. Thus, Mgenerates an answer ybased on xand the retrieved documents Dkas follows: M(y|x, D k). To mitigate the costs of Mcaused by processing\na large number of tokens, several approaches have\nbeen recently proposed to compress the documents\ninto a shorter context (Wang et al., 2023; Xu et al.,\n2024). Building on these approaches, our goal is\ndescribed as follows:\nargmax\nπPM(y|Cπ, x)\nandCπis defined as:\nCπ=π(q, D k)with l(Cπ)≪l(Dk)\nwhere lrepresents the number of tokens and π is a function that compresses documents Dkinto a shorter context Cπbased on the question q. It is important to note that we do not aim to optimize the model Mor the retriever. Instead, our primary focus is on compressing the provided contexts into a concise format, ensuring the essential information is retained to answer the question. 3\nDataset[COMPLETE] [INCOMPLETE]\nTotalPrevious\nOutput (O)Previous\nOutput (X)Previous\nOutput (O)Previous\nOutput (X)\nHotpotQA 7.2K 7.2K 7.2K 7.2K 28.8K\nTable 1: Statistics of our Dataset Construction.\n3 C OMP ACT We introduce COMP ACT, a novel compression framework that actively compresses documents until it finds all necessary evidence for answering a question. To condense a large amount of information from documents, we devise an iterative architecture where the compressed contexts are updated at each iteration. In this section, we initially provide a comprehensive explanation of our framework. Subsequently, we detail the data construction for training our model. 3.1 How to Compress We reconsider compression as sequential updates\nof compressed contexts based on the previous in-\nformation. Figure 2 clearly shows the concept of\nour framework. Given a question and documents\nDk={d1, ..., d k}k\ni=1from a retrieval system, we\nfirst group the documents as follows:\nSt={dt×j+1, dt×j+2, ..., d (t+1)×j}\nwhere Stis at-th segment consisting of jdocu-\nments, and jrepresents the predefined number of\ndocuments to be compressed at each iteration. We\nthen begin compressing each segment iteratively\nuntil it satisfies the end condition. It can be formu-\nlated as follows:\nCt, Et=π(q, St, Ct−1)\nHere, qis a given question to answer. CtandEt represent the compressed context and a condition token at step t, respectively. Ctis used as part of the input for the next step. During compression, the model actively integrates information related to the question by analyzing both the previously compressed context and the newly provided segments. This approach ensures that only the most relevant information is preserved at each stage, resulting in a compact context. As the resulting context is designed to retain query-related information, it serves as a comprehensive memory of all iterations up to the current step. 1 2 3 4 5 6\nn-th iteration01020304050Frequency (%) 70.6%\n67.9%69.5%\n69.2%68.8%\n68.7%67.3%\n68.8%67.8%\n69.1%67.3%\n69.6%GPT-4o [COMPLETE] %\nGPT-4o Correct %\nCompAct [COMPLETE] %\nCompAct Correct %Figure 3: Distribution of iterations where models de-\ntermine the compressed contexts to be complete. We\ncompare the distribution between GPT-4o (Yellow) and\nCOMPACT(Green). We also measure the percentage of\ncorrectness in complete cases.\n3.2 Early Termination Instead of completing all iterations, we introduce a specific end condition that early terminates the iterations. We implement this by including a condition token Ein the generation process. The purpose of the condition token Eis to assess whether an input segment St, combined with the previous context Ct−1, provides sufficient details to answer the question. If the token indicates the provided context is sufficient, the iteration terminates; otherwise, the iterations continue to gather lacking information until all missing details are obtained. This adaptive termination offers three primary benefits. First, we prevent redundant contexts from entering the compressed contexts or acting as a distraction. Second, we can avoid meaningless iterations, thereby drastically lowering the computational burdens of our iterative architecture. Moreover, the adaptive feature allows the model to dynamically adjust to the complexity of the question and the information density of the passages. This flexibility allows the model to be both effective and efficient across a wide range of scenarios, from simple queries to more complex, multi-hop questions requiring extensive context integration. 3.3 Dataset Construction Our model aims to compress documents into querycompacted contexts while concurrently determining the termination of the iterations. To cultivate this capability, we instruct a superior LLM to follow the three-step processes. Sentence-Level Selection. We begin by asking the LLM to identify sentences, particularly focus4\ning on relevant clues that may help the answer the question. If certain sentences provide relevant information or implicitly clarify ambiguous points, the LLM is prompted to generate these sentences from the provided documents. Query-focused Compression. We generate a summary of the selected sentences, emphasizing clues that can help answer the question. we explicitly restrict the LLM from making assumptions or attempting to draw conclusions without supporting evidence, as instructed: \"DO NOT make assumptions or attempt to answer the question; your job is to summarize only.\" This restriction is crucial because our main objective is to condense relevant information from the provided documents, not to answer the questions directly. Skipping logical steps to directly answer the question, as if relying on parametric knowledge, can harm the compression performance by increasing the likelihood of missing necessary information. Determining the Early Termination. We also prompt the LLM to evaluate its own compressed contexts based solely on the provided information, without any additional background context. We direct the LLM to generate a condition token (e.g., [COMPLETE] or [INCOMPLETE]) along with the rationale for its judgment. Overall, we construct a synthetic dataset for training using the LLM with the instructions that describe the three-step processes. we conduct the data construction in two scenarios: realistic and distractor. In realistic scenarios, the provided documents are the results of a retrieval system. However, it is difficult to collect early termination cases due to the infrequent appearance of gold documents. To address this issue, we also conducted data collection in distractor scenarios which include predefined documents that contain all supporting facts needed to answer the question. After filtering the collected datasets from both scenarios, we build a training dataset consisting of 28k instances categorized into four distinct groups. Table 1 shows the categories of the dataset. 4 Experiment 4.1 Experimental Setup Dataset Construction We employ GPT-4o API (2024-05-13) as the LLM to collect our dataset. We only use a subset of HotpotQA (Yang et al., 2018) train set for data collection. To retrieve doc- 303uments, we use Contriever (Izacard et al., 2022), fine-tuned on MS-MARCO (Bajaj et al., 2016), as our retrieval system on the 2018 Wikipedia corpus (Karpukhin et al., 2020b). We set the default number of documents per segment as 5. Since it is rare to find additional evidence beyond the top 30 documents, we set the top- kto 30, allowing for a maximum of 6 iterations per query. To prevent lengthy API responses, the maximum number of generated tokens is limited to 700. Training & Inference Leveraging the collected dataset, we perform supervised fine-tuning to train our model. Without using specific labeling or methods for particular iterations, we focus on teaching the model to effectively update the previous context based on the question and given documents at the current steps. we use instruction-tuned Mistral7B (Jiang et al., 2023a) as our base model. At inference, we process the same number of segments and inputs as training. Further information is provided in the Appendix A.2. 4.2 Datasets We evaluate COMPACTon both single-document and multi-document question-answering (QA) datasets. For single-document QA, we use Natural Question (NQ) (Kwiatkowski et al., 2019) and TriviaQA (TQA) (Joshi et al., 2017). For multidocument QA, we evaluate on HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and 2WikiMultiHopQA (Ho et al., 2020a). The evaluation is conducted on the dev set of each dataset, except for TriviaQA, which is evaluated on the test set. As mentioned, we comprise the training data only from HotpotQA. Therefore, we conducted zero-shot evaluation on the other datasets without accessing their training set. 4.3 Baselines In Table 2, we compare COMPACTto several baseline methods. To ensure a fair comparison, we feed compressed contexts from each baseline to the same reader model, LLaMA3-8b (AI@Meta, 2024). We consider the following baselines: •Oracle. We provide the reader with documents that contain an answer of questions. if such documents are not available, we include five documents as a default. •Raw Document. We simply concatenate the top-k retrieved documents. 5\nBaselinesHotpotQA MuSiQue 2WikiMQA NQ TriviaQA\nComp. EM F1 Comp. EM F1 Comp. EM F1 Comp. EM F1 Comp. EM F1\nOracle 10.8x 39.9 51.2 10.3x 14.21 23.66 11.0x 37.4 43.2 - - - - - -\nRaw Document 1x 32.5 43.1 1x 6.8 16.0 1x 31.6 37.2 1x 40.0 52.1 1x 70.7 77.5\nLong-Context LLM\nInternLM2-chat-7B 1x 8.0 20.3 1x 1.0 6.8 1x 9.3 19.5 1x 7.6 22.6 1x 12.1 31.5\nMistral-7B-Instruct-v0.2 1x 9.5 22.6 1x 1.0 7.9 1x 1.2 15.4 1x 4.3 20.9 1x 35.3 50.4\nFILM 1x 32.4 43.7 1x 6.9 15.7 1x 26.4 31.7 1x 38.2 50.8 1x 62.7 71.7\nGPT-3.5-turbo 1x 32.8 43.8 1x 7.3 16.1 1x 28.6 33.9 1x 40.8 54.6 1x 69.9 77.4\nCompressor\nAutoCompressors 35.4x 18.4 28.4 34.7x 3.9 11.9 36.2x 19.0 24.5 34.4x 17.3 31.8 34.5x 55.3 64.3\nLongLLMLingua 3.4x 25.6 35.3 3.4x 4.8 13.5 3.6x 27.9 32.9 3.5x 27.7 40.6 3.3x 64.0 70.8\nRECOMP (extractive) 34.3x 29.7 39.9 32.7x 6.7 15.7 35.9x 29.9 34.9 32.7x 34.6 45.1 39.2x 67.6 74.1\nCOMPACT(Ours) 44.7x 35.0 46.5 36.0x 8.4 17.7 48.5x 30.1 35.9 44.3x 37.8 49.6 46.9x 65.6 74.8\nTable 2: Main results. We set the reader as LLaMA3-8b (AI@Meta, 2024) for a fair comparison. We retrieve top-30\ndocuments to compute the scores. We use three Multi-hop and two single-hop question-answering datasets. Since\nour training datasets consist of HotpotQA dataset, we perform zero-shot evaluation on the rest of the datasets. Comp.\nrefers to the compression rate which is denoted as follows: compression rate =# of tokens in retrieved documents\n# of tokens in compressed text.\n•Long-Context LLM. We select a number of LLMs that support long context windows, including Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a), GPT-3.5-turbo (OpenAI, 2023), InternLM2-chat-7B (Cai et al., 2024), and FILM (An et al., 2024). •Compressor. We compare COMPACTwith three compression-based methods. AutoCompressors (Chevalier et al., 2023) process segments of long context into soft prompts, which are prepended to the next segment as summary vectors. RECOMP (Xu et al., 2024) suggests extractive/abstractive methods to compress documents into textual summaries. LongLLMLingua (Jiang et al., 2023c) takes a perplexity-based approach to filter out tokens with less importance. 4.4 Results We assess the performance of COMPACTusing three metrics: Compression rate (Comp.), Exact Match (EM), and F1 score (F1). Overall, COMPACTexhibits strong performance across all benchmarks, while achieving the highest compression rate across all baselines. Specifically, COMPACT outperforms other compression-based methods in all three metrics, demonstrating its ability to process abundant information efficiently. Compared to long-context LLMs, COMPACTshows comparable performance in NQ and TriviaQA, while outperforming all three multi-document question answering benchmarks. This shows how COMPACT excels at tasks that require integrating information within multiple documents. 5 10 20 30 40\nT op-k404550556065Metric\nRecall@k\nRECOMPRaw docs\nGold docsCompAct (Ours)\n5 10 20 30 40\nT op-k3035404550Metric\nRecall@k\nRECOMPRaw docs\ngold docsCompAct (Ours)Figure 4: End QA performance using diverse retriever\nsetups. We use BM25 (Left) and Contriever (Right) to\nretrieve top- kdocuments.\n5 Analysis We also analyze the quality of our compressed text, both qualitatively and quantitatively, to evaluate its usefulness through diverse reader LLMs. Finally, we investigate ways to facilitate the usage of compressors by reducing the inference time of COMPACTin both realistic and benchmark experimental setups. 5.1 Compressor as a Plug-in Module In Figure 2, we depict the compressor as a plug-in module. Our design highlights the ease of replacing it with new models as the retriever or reader evolves. Our goal is to determine if our COMPACT can flexibly perform compressing the context provided by diverse retrievers and efficiently preserve useful information regardless of various readers. Generalizability across Retrievers. In Figure 4, we describe the overall results. We use BM25 (Robertson et al., 2009) and Contriever (Izacard et al., 2022), the most common and useful setups to replace our retrieved documents. 6\n5 10 20 30 40\nT op-k42.545.047.550.052.555.057.560.0Metric\nRecall@k\nRaw docsgold docs CompAct (Ours)Figure 5: Performance of HotpotQA with different top-\nkdocuments. We set the reader as GPT-3.5-Turbo.\nWe use the HotpotQA (Yang et al., 2018) dev set to compute the recall and F1 performances. Surprisingly, in the top-5 documents, we observe that our COMPACTframework achieves a higher score compared to the gold documents given BM25 retrieved documents. Our framework also shows a saturated performance while retrieving up to 40 documents. However, we want to highlight that the score (48.7) shows significantly higher performance compared to other compressor models. Additionally, for the Contriever setup, where the retriever initially fails to retrieve useful documents, increasing the top- kleads to performance improvements. As we intended, these observations demonstrate that our COMPACTframework shows robustness across various retriever setups. Generalizability across Readers. We try to figure out that our COMPACTframework truly provides useful compressed text to solve the multi-hop question answering. Thus, we assess our compressed text with diverse reader LLMs to prove it provides useful information regardless of specific reader models such as LLaMA2 13B (Touvron et al., 2023), LLaMA-3-8b (AI@Meta, 2024), and GPT-3.5-Turbo (OpenAI, 2023). We randomly sample 500 instances from the HotpotQA dataset and describe the reader performance of using top30 retrieved documents. We compare our results with several baselines: providing pivotal information to solve questions (gold documents), prepending top-30 documents (raw documents), and other compression methods such as RECOMP (Xu et al., 2024) and LongLLMLingua (Jiang et al., 2023c). 438ComponentsHotpotQA MuSiQue\nComp. EM F1 Comp. EM F1\nEval. 124.8x 30.8 41.5 119.8x 6.8 14.8\nCT 44.7x 35.1 46.6 36.0x 8.3 17.7\nCT + Eval. 31.9x 34.8 46.3 27.1x 8.1 17.2\nTable 3: Results of each component effectiveness. Eval.\nrefers to the evaluation text containing rationale and\ncondition token. CT refers to the compressed text.\nDatasetsN-th Iterations\n1 2 3 4 5 6\nHotpotQA 78.1 114.1 128.5 126.5 135.9 147.5\nMuSiQue 77.5 110.6 135.2 91.6 145.6 124.0\nTable 4: Average Length of Compressed Text per Itera-\ntion for HotpotQA and Musique.\nIn Figure 5, we demonstrate that our COMPACT framework sufficiently provides high-quality compressed text to solve multi-hop questions. Furthermore, our framework proves its effectiveness on the widely used top- kretrieved documents such as k∈{5, 10, 20, 30, 40}. Notably, until providing top-20 documents, there is little difference between the raw documents and our score. However, at the top-30 and top-40, performance degradation occurs as more documents are included as input, increasing irrelevant context. In contrast, our COMPACT framework shows lower performance degradation even with increased context. Additionally, COMPACTframework achieves a higher compression rate (44x) signifies that the number of input tokens is significantly reduced resulting in cost-efficiency using API calls. We provide LLaMA2-13B and LLaMA3-8B performance in Table 6. 5.2 Ablation Studies Component Effectiveness. Our key point of COMPACTframework is using the compressed text (CT) and evaluation (Eval.) from previous outputs. The evaluation consists of the rationale for the compressed text and the condition token that determines early termination based on this rationale. We describe the detailed performance in Table 3. If we only provide the evaluation text, the compression rate increases dramatically, but the end performance (F1) significantly drops (Row 1). Additionally, when comparing cases where both compressed text and evaluation are provided versus compressed text only, there is no significant difference in performance (Row 2 & 3). 7\nWe also identify that as the reader LLM advances, the rationale provided by our COMPACT framework can negatively impact the performance. For instance, when using the LLaMA2-7B (Touvron et al., 2023) as the reader LLM, the performance achieves 45.0 given the previous output. However, when only the compressed text is provided, the performance drops to 44.2. To accurately analyze the trend of these findings, a detailed exploration through comparison with different reader LLMs will be reserved for future work. Average Length of Compressed Text per Iteration. In Table 4, we provide the detailed length information of compressed text per iteration. From a compression perspective, our COMPACTframework compresses text from 30 retrieved documents into under 200 tokens. We observe that it maintains a high compression rate on average throughout iterations. To ensure the practicality of providing context with fewer tokens, we also provide an additional point. Among the models with over 1 million downloads on Huggingface1, 102 out of 154 are language models. Of these, 77.5% can feed inputs of 512 tokens or fewer. Despite ongoing research on LLMs capable of handling long contexts, it is evident that many users still frequently employ models with smaller token inputs. Therefore, it seems like a positive direction to examine how the compressed text and evaluation provided by our framework can enhance the performance of classification models like BERT (Kenton and Toutanova, 2019), which accept inputs with fewer tokens. 5.3 Inference Latency While COMPACToffers a significant cost-saving advantage by reducing the token usage in the reader, we also consider a potential increase in inference latency due to the active iteration of our framework. To investigate this, we measure the time taken to answer the question with our framework and other baselines. Given that the inference speed can vary depending on the composition of retrieved documents and types of queries, we assess the time on the multi-hop (e.g., HotpotQA (Yang et al., 2018)) question-answering dataset. In Table 5, we measure diverse inference time: inference GPU time, compression GPU time, total GPU time (inference + compression), throughput (examples per second), and corresponding F1 score. We agree that our COMPACTframework has a cru1https://huggingface.co/ModelsBaselinesInference\nGPU TimeCompression\nGPU TimeTotal\nGPU TimeThroughput\n(example/sec)F1\nNo Documents 1.5m - 1.5m 5.58 31.7\nRaw Documents 11.5m - 11.5m 0.72 42.5\nLongLLMLingua 3.3m 32.3m 35.6m 0.23 35.5\nRECOMP 1.9m 2.1m 4.0m 2.10 41.5\nCOMPACT(5 docs) 1.4m 178.7m 180.1m 0.05 47.3\nCOMPACT(10 docs) 1.9m 92.9m 94.8m 0.09 45.4\nTable 5: Inference time for HotpotQA dataset. Our it-\nerative inference lets us consider the trade-off between\nmassive inference time and high performance (compres-\nsion rate and end QA performance).\ncial limitation for inference time in processing the compression. However, our COMPACTframework provides high-quality compressed text regardless of retriever and reader. Furthermore, we prove its effectiveness through the trained HotpotQA dataset and other zero-shot evaluations. How to Speed up Inference in COMP ACTFramework (Varying Segment Size). Extending the segment size is a way to improve inference speed efficiently. Instead of retraining the model to handle more documents, we simply increase the number of documents provided per iteration. Specifically, we apply 10 documents to COMPACT, which was originally trained to compress only 5 documents. We observe performance degradation when looking at segments of different sizes than those seen during training, but we still observe a high level of performance. Adopting this approach can yield advantages during inference time in our framework. 6 Conclusion We introduce COMP ACT, a novel framework that employs an active strategy to compress extensive documents. Our framework effectively captures important context and compresses documents of large volumes without losing pivotal information. COMPACTcan serve as a convenient plug-in module that can fully collaborate with advanced off-the-shelf retrievers and readers. Our framework achieves a high compression rate (44x), which significantly increases the cost-efficiency when collaborating with external API calls. Our experiments show that COMPACTshows significant improvements in compression rates and QA performance on multi-hop question-answering datasets such as HotpotQA and MuSiQue. Limitations Our main concern is about the inference time required to compress top- kretrieved documents. 8\nWe acknowledge that our COMPACTframework spends considerable time compressing the retrieved documents. However, with the combination of a strong retrieval system, there is also potential for significant time savings if sufficient contexts are provided earlier. Also, our effort to address complex question types through compression is pioneering in this field. This aspect makes our research valuable, as it sets the foundation for future work to build upon and potentially resolve these issues. We hope that subsequent research will continue to refine these methods, further enhancing the efficiency of inference latency of our framework. Additionally, we found that there are non-trivial errors when judging the completeness of compressed contexts. We use GPT-4o API to collect the training data with our custom instructions. For example, during data collection, even when the documents provide oracle evidence to answer the question, GPT-4o outputs a [COMPLETE] condition token at a rate of 39.88%. This indicates that even GPT-4o, which we believed to perform the best in our situation, struggles with accurately determining completeness of contexts. Although we attempt to address the issue by filtering the error cases, there may still be instances where the model incorrectly judges completeness. Moreover, we only train our model in Mistral7B-Instruct-v0.2 due to resource limitations. We need to verify whether our COMPACTframework works well across a range of model sizes, both smaller (< 7B) and larger (> 7B). It is challenging to assert that our framework operates efficiently when the compression model is significantly larger than the reader LLM used afterward. It would be beneficial to conduct experiments during the rebuttal period to confirm these aspects. Ethics Statement In environmental cost, our training process can use a significant amount of energy as the process is computationally expensive. In our manuscript, we attempt to minimize these effects by pre-training on one Mistral model and only do the necessary supervised fine-tuning to minimize the computation cost. Furthermore, a potential risk of this work is that the generated dataset can contain biases of API calls such as stereotypes of racism and gender. To our knowledge, there haven’t been significant issues reported when creating datasets related to question answering. However, it would be benefi- 608cial to apply methods that robustly train or validate against such concerns. 610",
        "aspect_review": "The paper introduces COMP ACT, a novel and flexible framework for compressing retrieved documents actively, which can be integrated as a cost-efficient plug-in module with any off-the-shelf retriever or reader model. This methodology shows strong technical depth and rigor, offering a significant improvement in compression rate and QA performance, particularly in complex multi-hop QA tasks. The empirical results are impressive, demonstrating that COMP ACT achieves extremely high compression rates (44x) and brings significant improvements in QA performance across multiple datasets. This comprehensive experimental validation highlights the framework's practical impact and effectiveness in real-world scenarios.\n\nThe paper could benefit from a more detailed comparison with existing query-focused compression methods, specifically addressing how COMP ACT differs in handling long contexts and integrating information across multiple documents. While the experiments are thorough, there is a need for more clarity on the evaluation metrics and the specific conditions under which the experiments were conducted to better understand the results. The paper focuses primarily on multi-hop QA datasets, which might limit its perceived impact on single-document QA tasks. Expanding the evaluation to a broader range of tasks could enhance its contribution. The comparison with baseline methods could be strengthened by providing more detailed insights into why COMP ACT outperforms other methods, particularly in terms of its unique features and advantages. While the paper makes a notable contribution, the discussion could be expanded to emphasize the broader implications of the framework beyond the specific datasets used. The methodology, though robust, could be further detailed, particularly regarding the active compression strategy and early termination process, to enhance reproducibility and understanding. Some sections of the paper, such as the explanation of the framework's components and the interpretation of results, would benefit from additional detail and clarity to improve reader comprehension.",
        "general_review": "The paper introduces a novel framework, COMP ACT, which provides significant contributions to the field of retrieval-augmented generation for question answering. The framework offers notable novelty by employing an active strategy for document compression, which is a transformative approach in handling long contexts. The technical depth and rigor are evident through the comprehensive experimental validation across multiple datasets, demonstrating significant improvements in compression rates and QA performance. The practical impact is significant, as COMP ACT can be integrated as a cost-efficient plug-in module with existing retrievers and readers, enhancing their performance without losing crucial information.\n\n1. Minor improvements are needed in the writing for clarity, particularly in sections explaining the methodology. 2. More details on hyperparameters and implementation specifics could enhance reproducibility. 3. Some figures require minor formatting adjustments for better readability. 4. Slight clarification on code availability would be beneficial to encourage adoption and further research."
    },
    "23": {
        "paper": "TRIP NEGOTIATOR : A Travel Persona-aware Reinforced Dialogue\nGeneration Model for Personalized Integrative Negotiation in Tourism\nAnonymous ACL submission\nAbstract A sophisticated negotiation dialogue system for tourism should engage in negotiations beyond mere price considerations, encompassing various other aspects and amenities inherent in the tourism package. To ensure such tailored interaction, it is imperative to understand the intricacies of traveler preferences, constraints, and expectations. Incorporating these personality facets allows for customizing negotiation strategies, resulting in a more personalized and integrative experience. With this aim, we take a pivotal step in advancing automated dialogue systems for personalized integrative negotiation tasks. We develop DEAL , a pioneering Dialogue datas Et for person ALized integrative negotiation task in the tourism domain. Further, we propose TRIP NEGOTIA TOR, a novel Travel persona-aware Reinforced dIalogue generation model for Personalized iNtegrative n EGOTIAT ion within the t OuRism domain. TRIP NEGOTIATOR is built to discern the traveler’s persona and intent, systematically adjusts negotiation strategies, and directs the negotiation toward a pertinent phase to ensure effective negotiation. Through reinforcement learning with Proximal Policy Optimization (PPO), we guide TRIP NEGOTIATOR to generate coherent and diverse responses consistent with the traveler’s personality. Extensive qualitative and quantitative analyses demonstrate the effectiveness of TRIP NEGOTIATOR in generating personalized responses during negotiation1. 1 Introduction E-tourism’s substantial growth over the past decade (Laz ˘ar et al., 2019) has increased online trip bookings, necessitating the development of automated negotiation dialogue systems (NDS) for travelers and travel agencies. The tourism industry must comprehend travelers’ travel persona to interpret their travel preferences and facilitate personalized 1Sample dataset and code can be accessed at this link.interactions (Park et al., 2010). Besides, it needs to establish trust and rapport with travelers to foster collaborative and long-term relationships (Assaker and Hallak, 2013). Research has shown that modeling users’ personality leads to successful negotiation (Yang et al., 2020), and an integrative approach to negotiation further elevates negotiation by promoting user inclusivity in the commercial arena (Guttman and Maes, 1998). Existing NDS (Dhingra et al., 2017; He et al., 2018; Zhou et al., 2019) fall short in modeling users’ detailed persona encompassing their likes, dislikes, or priorities toward certain options, which hinders the generation of tailored strategies and utterances for diverse users. To address this gap, we attempt to consolidate personality and strategic modeling in NDS and propose a novel task of personalized integrative negotiation within the tourism domain. Personalized integrative negotiation aims to integrate travelers’ travel persona and needs to ensure personally appealing and mutually beneficial negotiation consequences. Travel persona defines travelers’ preferences regarding a trip, spanning both fundamental (hotel types, meals, etc.) and optional (entertainment, outdoor activities, etc.) aspects and amenities. Beyond primary price considerations, travelers are interested in distinct fundamental aspects and optional amenities to varying extents. The negotiation on these factors depends on their interests. For instance, in Figure 1, a traveler interested in adventure negotiates for packages including activities like snowboarding and nature photography, while a mountain enthusiast emphasizes visits to scenic viewpoints and enjoying authentic local food. The agent’s recommendations aligned with travelers’ likes, dislikes, and priorities ensure a personalized experience and contribute to successful negotiation. Conversely, failing to capture this information leads to generic or unsuitable suggestions that can frustrate travelers and reduce the likelihood of a successful booking. 1\nT: Hi! Can you suggest an exciting vacation filled  \nwith adventure sports  and outdoor activities . I love \nsnowboarding and nature photography . (Greet-\nAsk)\nA: Amazing! For a snowboarding adventure with  \nserene mountain views, I recommend staying in  \nZermatt. It's known for great snowboarding on the  \nMatterhorn Glacier and stunning landscapes  \nperfect for photography . (Problem identification)  \n(Relational positioning)\nT: I want to add the snowboarding  as I would like  \nto experience it during the trip. (Negotiate AddX)\nA: Absolutely , adding the amenity snowboarding  \nwill surely enhance your trip experience. The final  \ntour cost would be $20150. (Strategic Proposal)\n(Problem solving)T: Hi! Can you suggest an exciting vacation filled  \nwith adventure sports  and outdoor activities . I \nlove snowboarding and nature photography . \n(Greet-Ask)\nA: How about staying in Zurich, Switzerland? It’ s \na vibrant city with great shopping, nightlife, and  \ncultural attractions. You can also take day trips to  \nnearby cities or visit some local museums. Does  \nthat interest you? (Problem identification)  \n(Relational positioning)\nT: God! I'm actually interested in snowboarding . \nI'm not really looking for a city experience.  \n(Negotiate AddX)\nA: Oh, I see. W ell, Zurich might have some  \noptions for snowboarding nearby . (No strategy)\n(Relational positioning)\nT: Hi! I'm planning a peaceful vacation in the  \nmountains. I love scenic viewpoints  and \nexperiencing local food , especially farm-to-table  \ndining . (Greet-Ask)\nA: Of course! For a peaceful mountain retreat with  \nstunning views and amazing local cuisine, I  \nrecommend the Bernese Oberland. It’ s known for  \nits breathtaking mountain views and authentic  \nSwiss farm-to-table dining experiences. (Problem  \nidentification)  (Relational positioning)\nT: I really want to enjoy the top viewpoints  and \nlocal food . How is it to include farm-to-table  \nexperience ? I propose a budget of $9415, hope   \nit’s fine. (Negotiate price no-change)\nA: Sure, will include farm-to-table experience to  \nyour package. However , it is dif ficult at budget of  \n$9415. But I can of fer you a discounted rate of  \n$7201. (Flexible pricing) (Offer generation)T: Hi! I'm planning a peaceful vacation in the \nmountains. I love top viewpoints  and \nexperiencing local food, especially farm-to-table  \ndining .  (Greet-Ask)\nA: How about a city trip to Geneva? It has nice \nshopping places and great restaurants. You can \nalso enjoy  the lake views and some cultural  \nattractions. (Problem identificati on) (Relational  \npositioning)\nT: Oh!!! I really want to enjoy the top viewpoints  \nand local food. How is it to include farm-to-table  \nexperience ? (Negotiate AddX)\nA: We can offer you a travel package at a rate of \n$12,500, which includes city tours and gourmet  \nrestaurant dining experiences. (No strategy)  \n(Offer generation)Adventure Enthusiast Mountain Enthusiast \nWith Travel Persona Without Travel Persona\nFigure 1: Example illustrating how personality and\nstrategic modeling ensures personalized integrative ne-\ngotiation and overlooking these facets results in negoti-\nation failure. Green and grey text depict the traveler’s\npersona and intent, while pink and blue text indicate\nthe agent’s negotiation strategy and phase, respectively.\nHere, Tis Traveler and Ais Travel agent.\nThe negotiation dialogues exhibit unique peculiarities like proposing prices, offering proposals, accepting or rejecting them, and/or formulating counter-offers (Thompson et al., 2010). Hence, during negotiation, the travel agent must employ an appropriate negotiation strategy as per the conversation context to resolve conflicts and foster a collaborative ambience. Further, the travel agent should discern the travelers’ specific goals (intents) to recognize the key signals (proposals, acceptances, rejections, or counter-offers) and understand their requirements, eventually navigating the conversation toward adequate negotiation phase. To exemplify, in Figure 1, the agent adapts negotiation strategies based on the context. Also, based on the traveler’s intent to add an amenity or negotiate on price, the agent smoothly transitions the conversation to the relevant phase (problem-solving or offer-generation). The inability to do so leads to traveler dissatisfaction and negotiation breakdown. Driven by these considerations, in this work, we propose TRIP NEGOTIATOR , a novel Travel persona-aware Reinforced d Ialogue generation model for Personalized i Ntegrative n EGOTIAT ion within the t OuRism domain. To develop TRIP NEGOTIATOR , we first curate DEAL , a pioneering Dialogue datas Et for person ALized integrative negotiation. DEAL is created through prompting Large Language Model (LLM), MPT-7B (Team et al., 2023) under few-shot settings followed by 112human interventions to ensure high-quality dialogues. The traveler’s utterances in DEAL are then annotated with travel personas and intents, and the agent’s utterances are annotated with negotiation strategies and phases. Using DEAL ,TRIP NEGO TIATOR is built upon the DialoGPT (Zhang et al., 2020a) in a reinforcement learning (RL) framework. To train the end-to-end TRIP NEGOTIATOR , we use a novel reward function to ensure that the responses align with the traveler’s persona and intent and adhere to the appropriate negotiation strategy and phase while maintaining fluency, contextual coherence, and diversity. Finally, we optimize a policy via RL using the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) based on the reward assigned to the generated responses. In conclusion, the key contributions of this work are:(i)Introduce a novel task of personalized integrative negotiation within the tourism domain, which considers traveler’s unique travel persona and needs to generate personalized integrative responses; (ii)Curate a novel dialogue dataset, DEAL through prompting for personalized integrative negotiation; (iii) Annotate DEAL with travel personas, negotiation strategies, intents, and negotiation phases; (iv)Propose an RL-based robust dialogue system, TRIP NEGOTIATOR to generate personalized integrative responses during negotiation focused on tourism domain by devising a pioneering reward function; (v)Conduct thorough automatic and human evaluations on the DEAL dataset to establish the efficacy of the TRIP NEGOTIATOR . 2 Related Work Negotiation Dialogue Datasets. The development of negotiation dialogue datasets is crucial for advancing research in NDS. Existing datasets for distributive negotiation, such as CraigslistBargain (He et al., 2018), NegoCoach (Zhou et al., 2019), AntiScam (Li et al., 2020), and for integrative negotiation, like STAC (Asher et al., 2016), DealorNoDeal (Lewis et al., 2017), CaSiNo ( ?), JobInterview ( ?), DinG (Boritchev and Amblard, 2022) have provided foundational insights into human negotiation behavior. These datasets focus on specific scenarios, such as price bargaining, privacy protection, strategic games, item assignment, or job interviews, featuring simplified negotiation scenarios that may not fully capture the nuances of real-world negotiations. A recent dataset, IND (Ahmad et al., 2023), has introduced richer, more diverse integrative ne2\ngotiation scenarios, including negotiation on price and other factors like item addition/removal from a deal bundle in an e-commerce setup. In contrast to these datasets, we introduce a novel dialogue dataset for the tourism domain, comprising integrative negotiation conversations on varied aspects and amenities of tourism packages, like price, destination, entertainment, transportation, and more. Negotiation Dialogue Systems. Research in NDS has seen significant progress in recent years (Yamaguchi et al., 2021; ?; Fu et al., 2023; Deng et al., 2023b). Zhou et al. (2019); Deng et al. (2023a) proposed a distributive strategic approach aimed at recommending strategies to sellers to improve their deal outcomes. Zhao et al. (2019); Yamaguchi et al. (2021) introduced a collaborative strategy framework to mutually negotiate on the target issue. He et al. (2018) proposed dialogue acts tracking to model the behavior of buyers and sellers. Yang et al. (2020) improved the NDS by capturing the partners’ personalities using a one-step dialogueact look ahead during negotiation. Zhang et al. (2020b); ?presented an opponent behavior modeling method to estimate opponent action during negotiation. Recently, Ahmad et al. (2023) introduced an integrative negotiation agent (INA) to negotiate item prices and tailor deals to customers’ needs. Existing research emphasized solely the objective outcomes of negotiation, such as points scored or the final agreed price. Also, the limited understanding of individual differences to accommodate varying preferences in these works restricts highquality personalized conversations. Unlike these studies, the current work presents a novel personalized integrative negotiation system for tourism. This system utilizes the travelers’ detailed travel persona to grasp their trip preferences for generating personally relevant responses. Our work further differentiates in that we propose novel integrative negotiation strategies to assist the agent in determining the next action based on the negotiation context and guiding the negotiation toward the desired phase whilst aligned with the travelers’ intent. Following (Shi et al., 2021; Ahmad et al., 2023), we train our system with standard PPO loss using six novel rewards customized for the proposed task. To our comprehension, our work pioneers the development of such a personalized integrative NDS for the tourism domain. 2123 Dataset To develop the personalized trip negotiator, we crafted DEAL , an innovative Dialogue datas Et for person ALized integrative negotiation task. We focus on the dialogues considering the traveler’s persona information and requirements to facilitate the best travel packages and deals. The ultimate goal is to offer a seamless and collaborative planning and booking experience during negotiation, thereby contributing to an enriched and positively transformed landscape within the realm of tourism. 3.1 Dataset Creation DEAL comprises interactions between the travel agent and the traveler negotiating on price, destination, various aspects, and amenities of the tourism package. To alleviate the reliance on expensive human resources, the dataset is developed by leveraging the extensive knowledge embedded in the LLM, MPT-7B (Team et al., 2023). Specifically, the dataset is created by prompting the MPT-7B model, followed by human intervention to ensure quality control. The entire dataset creation process encompasses four key stages: (a) Background Data Acquisition, (b) Negotiation-specific Intent Formulation, (c) Dialogue Flow Generation, (d) Traveler-Agent Dialogue Curation. (a) Background Data Acquisition. This stage involves gathering relevant details about diverse facets of the tourism domain, such as tourist destinations, attractions, accommodations, transportation options, and other related factors. This information helps formulate 20 distinct travel packages, each accompanied by various aspects, amenities, and services to cater to diverse preferences and needs. Overall, the background database used to create dialogues comprises essential details like the package names and their description, list of aspects, amenities, and services with their descriptions. The list of packages is given in §A.1 of the appendix. (b) Negotiation-specific Intent Formulation. To build robust integrative negotiation dialogue systems, it is crucial to formulate intents encompassing a broad spectrum of negotiation scenarios, including price and feature-based negotiation. To comprehensively address these diverse scenarios, we formulate the 17 intents, namely Greet, Ask, Inform, Ask price, Tell price, Ask clarification-Y, Provide clarification-Y, Negotiate price increase, Negotiate price decrease, Negotiate price no-change, Negotiate add-X, Negotiate remove-X, Provide consent, Consent response, Accept, Reject, andAc3\nknowledge acceptance . The definitions and examples of intents are detailed in Table 6 and Table 7 in the appendix, respectively. (c) Dialogue Flow Generation. The dialogue flow is essentially a sequence of intents occurring during negotiation. We devise a Dialogue Flow Generator (DFG) module to automatically generate the dialogue flows. The DFG module assumes that dialogue flow can exhibit randomness; however, to ensure consistency in generated dialogues, we impose a constraint of initiating a dialogue by the traveler with a ‘Greet’ intent, which may be followed by a clarification request or one of the intents designed to negotiate on certain attributes (price, aspects/amenities). The travel agent can then respond by the ‘Inform’ intent or one of the intents demonstrating negotiation on distinct attributes. We also maintain negotiation information on the ongoing deal with the traveler for each travel package. This information comprises the agent’s minimum price and current price, the traveler’s current price, and the tolerance threshold ( ψ). To uphold the win-win outcomes of the negotiation, we confine price-based negotiations to predefined number of dialogue turns, after which the intents ‘Negotiate add-X’ or ‘Negotiate remove-X’ come into play. Further, to propose the price for the subsequent utterance, we assume a decline in the price difference (increment for the traveler and decrement for the agent) across dialogue turns. This approach mirrors the method of Faratin et al. (1998), where a comparable function models price negotiations between customers and sellers. The proposed prices by the traveler ( Prt) and agent ( Pra) at the current turn,i, are computed as follows: Prai=Prti−1+ (Prai−1−Prti−1)e−kt(1) Prti=Prai−1+ (Prai−1−Prti−1)e−kt(2) Here, kis a constant governing the rate of price change from one utterance to the next. A higher value of kresults in a higher concession rate, whereas a lower value indicates a reduced concession rate from the agent. In our specific scenario, we assume a higher value of kfor the agent and a lower kfor the traveler, reflecting the traveler’s tendency to adhere closely to their budget constraints. Also the value of kfor agent depends upon the initial price gap between traveler and agent. For low-budget, where price gap is high, we choose higher value of kand for high-budget, where price gap is low, we choose lower value of k(c.f. Table 4 in appendix). During negotiation, the agent 314will opt for the ‘Accept’ intent if the offered price from the traveler ≤(Prai−ψ∗Prai). Conversely, the traveler will select the ‘Reject’ intent when the negotiation deadline has been surpassed and the agent is unwilling to reduce the package price further. The dialogue concludes upon acknowledging with either the ‘Accept’ or ‘Reject’ intent. (d) Traveler-Agent Dialogue Curation. The traveler-agent dialogue curation involves prompt designing and dialogue generation. Prompt Designing. To begin the few-shot dialogue generation using the MPT-7B model, we design few-shot prompts (Brown et al., 2020) for each intent. Each few-shot prompt consists of three distinct components: a task description, a concise summary of the pertinent information from the dialogue, and an utterance aligned with the specified intent. The task description elucidates the intent’s scenario and objective; for instance, the task description for the ‘Acknowledge acceptance’ intent might entail the traveler agreeing to opt for a package, causing the agent to express gratitude towards the traveler and proceed with the transaction. The summary of the pertinent information is devised considering the sequential flow of intents from the preceding dialogue utterances. The utterance aligned with the specified intent is manually created using the task description and the information summary of the few-shot prompt. A sample prompt is provided in §A.2 of the appendix. Dialogue Generation. Subsequently, the DFG module generates an ordered list of intents accompanied by relevant details for each intent. For example, the proposed price adjustment is articulated for the intent ‘Negotiate price decrease’, while for ‘Negotiate remove-X’, the item to be removed is specified. We leverage this generated list of intents to augment the prompt corresponding to each intent. This augmented prompt is then used to prompt the MPT-7B model to generate the utterance. Due to space constraints, dataset cleaning and quality control (§A.3), and dataset quality assessment (§A.4) with statistics of the final dataset (Table 3) are provided in the appendix. 3.2 Dataset Annotation The traveler’s utterances in the DEAL dataset are annotated with ‘travel persona’ using Persona Extraction Module (PEM), while the agent’s utterances are annotated with negotiation strategies and phases employing a semi-supervised approach. Personalized trip negotiation dialogues require 4\ncreating detailed, domain-dependent travel personas that reflect the traveler’s destination choice, holiday priorities, and other travel-related decisions like accommodation, travel groups, and leisure activities. Hence, we leverage travel personas that focus on travelers’ preferences and priorities, ensuring that their trip preferences are adequately met. Further, at each step of negotiation, the travel agent often needs to employ an appropriate negotiation strategy to choose among different possible actions to achieve successful negotiations and foster collaborative outcomes. Hence, we devise a set of negotiation strategies for the travel agent using the negotiation theories and a preliminary assessment of 60 randomly selected dialogue samples. The three human experts2independently labeled sampled dialogues, analyzed discrepancies and modified the strategies accordingly. The final inter-expert agreement (McHugh, 2012) surpassed 0.83 for all categories. Specifically, eight different negotiation strategies, namely problem identification (pi) ,strategic proposal (sp) ,firm pricing (fp) , definitive decision making (dd) ,collaborative proposal (cp) ,flexible pricing (flp) ,co-operative decision making (cd) andno strategy (ns) are identified. Also, negotiation typically unfolds through distinct phases or stages. Negotiation phases provide a narrative explanation of the negotiation process, i.e., they identify sequences of events that constitute the entire negotiation. Thus, to apprehend the negotiation progress from inception to conclusion, we devise the four negotiation phases, namely relational positioning ,problem solving ,offer generation, and decision making . Due to space constraints, definition and example utterances of negotiation strategies and phases are given in Table 8 and Table 9, respectively, and the annotation procedures are provided in §A.6 in the appendix. 4 Proposed TRIP NEGOTIATOR To facilitate effective negotiation between the dialogue agent and travelers having unique personas and intentions, we introduce TRIP NEGOTIATOR , a novel Travel persona-aware Reinforced d Ialogue generation model for Personalized i Ntegrative nEGOTIAT ion within t OuRism domain. The architecture of TRIP NEGOTIATOR is depicted in Figure 2. It is built in three stages, viz.Supervised Finetuning, Reward Modeling, and RL Fine-tuning. 2Two possess Ph.Ds in Linguistics, and one has an M.Sc. in Computer\nScience. They have two years of experience in related tasks and are paid\naccording to institute norms.\nT: Hello, I am interested in exploring the natural\nbeauty of mountains, what's the cost of\nMountain Resort package?\nA: Sure sir , you will get to enjoy the amenities of\nmountain biking, rock climbing, snowboarding at\nthe cost of $18900. \nT: This is way out of my budget; can you remove\nrock climbing and give this package at $1500.\nPEM\nTravel persona:  [natural\nbeauty , mountains]Dialogue\nI am sorry but I can't go below this  price .\nHow about adding gourmet dining at \nthe cost of $16500.\nOkay sir , we are removing rock climbing,\nbut how about you get to enjoy scenic\nrides  at the cost of $1730.\nRL-PolicyOkay sir , we are removing rock\nclimbing, but how about you get to\nenjoy scenic rides at the cost of $1630.\nCandidates  Reward Modeling StagePPO Gradients Optimization\nFM BS-F1 JS\nLanguage-focussed Rewards\nPersona-\nNLIMNSCM IPGM\nTrait-focussed Rewards\nRL Fine-tuning Stage Supervised Fine-tuning Stage\nPPO LossFigure 2: Overall architecture of personalized integra-\ntive negotiation dialogue system - TRIP NEGOTIATOR .\nSupervised Fine-tuning Stage. Let,D= {t1, a1, t2, a2, ..., t T, aT}be a multi-turn dialogue between the traveler ( t) and travel agent ( a), where tiandaidenote the traveler’s and the agent’s ithutterance, respectively, and Tdenote the total number of utterances in D. Let the traveler’s current utterance tiwith the corresponding travel persona Pibe represented as tPi= [ti⊕Pi]. Further, let πa andπtPdenote the probability distributions of the agent’s utterance and the traveler’s utterance with corresponding travel persona, respectively. Then, for a given context, a persona-aware supervised fine-tuned model (PSFT) is obtained by maximizing the likelihood over Equation 3. πPSFT(D) =TY\ni=1πtP(ti|tP<i, a<i)πa(ai|tP≤i, a<i)(3) Reward Modeling Stage. The reward model aims to generate persona-aligned responses that adhere to pertinent negotiation strategies and drive the negotiation toward the desired phase, considering the travelers’ intents while maintaining fluency, contextual coherence, and diversity in the generated responses. To achieve this, a reward model comprising Trait-focused Rewards (TFR), viz.Persona Alignment, Negotiation Strategy Consistency, and Intent-Phase Agreement rewards, and Languagefocused Rewards (LFR), viz. Response Fluency, Dialogue Coherence, and Diversity rewards is devised. TFR guides the agent toward the responses exhibiting pertinent negotiation strategy and phase while taking into account the travelers’ persona and intent, and LFR ensures the syntactic and semantic richness of the generated responses. Persona Alignment Reward (PAR). Aligning the negotiation agent’s response to match the traveler’s persona encourages personalized interactions. 5\nWe approach the problem of response alignment with traveler’s persona, P={p1, p2, ..., p l}as a Natural Language Inference (NLI) task defined as fNLI(ˆri, pk)→ {E, N, C }(E: entailment, N: neutral , andC: contradiction ). Entailed responses align with the persona, whereas contradictory responses are inconsistent and thus warrant penalization. To assess the persona alignment of the generated responses, a BERT-based (Devlin et al., 2018) Persona-NLI model (Persona-NLIM) is first finetuned on Dialogue NLI dataset (Welleck et al., 2018)3. This fine-tuned model is then run on each (ˆri, pk)pair to obtain raw outputs as Oi,k= {oE\ni,k, oN\ni,k, oC\ni,k}, which is used to attain the respective class probability as ρi,k=softmax (Oi,k) = {ρE\ni,k, ρN\ni,k, ρC\ni,k}. These probability scores are finally used to compute PAR as Rpa=Pl\nk=1(1−ρC\ni,k). It can be inferred that Rpawill be more when the generated response does not contradict the persona. Negotiation Strategy Consistency Reward (NSCR). During negotiation, travel agents must employ a well-thought-out strategy to manage conflicts and ensure win-win outcomes. Thus, we penalize the responses deviating from the groundtruth negotiation strategy. To design NSCR, we track class probabilities score from the negotiation strategy classification model (NSCM) and compute NSCR as: Rnsc=NSCM (ti)−δ×NSCM (ˆri). NSCM is built by fine-tuning RoBERTa-large (Radford et al., 2019) on the DEAL dataset4. Here, δ≥1serves as the penalization factor. Intent-Phase Agreement Reward (IPAR). The travel agent should steer the negotiation toward the decision phase, empowering travelers to make informed choices for a more satisfying and mutually beneficial outcome. With the evolving traveler’s utterance and its intent, travel agent should negotiate differently and advance the conversation to the optimal phase. However, it is challenging to model the association between travelers’s dynamically changing intents and the travel agent’s ability to guide the conversation toward the intended phase. Hence, to enable the agent to employ intent-aware correct negotiation phase at ithturn, we formulate the IPAR as Ripa=σ(J(P(ai, phi|ti, ini))−η× J(P(ˆri,ˆphi|ti, ini))), where, P(·)signifies the intentphase generation model (IPGM) developed by finetuning DialoGPT-medium (Zhang et al., 2020a) on DEAL5. It takes traveler’s utterance and its intent 3We attain an accuracy of 86.76% on the test set.\n4We obtain an accuracy and macro-F1 of 91.75% and 81.34%, respectively.\n5We attain PPL, BLEU, METEOR, and BS-F1 as 3.71, 0.29, 0.63, andas input and generates travel agent’s utterance and corresponding phase as output. iniandphirepresent the traveler’s intent and agent’s phase, respectively. ˆphidenotes the generated response’s phase predicted by the Negotiation Phase Classification Model (NPCM) obtained by fine-tuning RoBERTalarge6.J(·)denotes negative log-likelihood loss function and σ(·)denotes the tanh()7activation function, which scales down the values to [0,1]. Response Fluency Reward (RFR). The generated response should be fluent (grammatically correct). Hence, we design RFR as the inverse of the perplexity of generated response ˆriasRrf= FM( ˆri)1/m. We use PLM, DialoGPT-medium as fluency model (FM), and mis the word count in ˆri. Dialogue Coherence Reward (DCR). The generated response should be coherent with the dialogue context to ensure a smooth flow of conversation. Hence, we devise the DCR, which penalizes the generated response ( ˆri) deviating from the context ( ci) and the traveler’s utterance ( ti) asRdc=MIN([h(ci,ˆri) +h(ti,ˆri)],1)/2, where, h(·) denotes the similarity measure computed using BERTScore-F1 (BS-F1) (Zhang et al., 2019). To mitigate the occurrence of similar responses, a threshold value of 1 is employed. Diversity Reward (DR). During negotiation, the generated response may become generic and repetitive ( e.g., I can offer the best package at best price ), which affects the overall conversation quality (Li et al., 2015; See et al., 2019). To ensure diverse and engaging responses, we formulate DR utilizing the Jaccard similarity (JS) between the responses, ˆriandˆri−1atithand(i−1)thturns, respectively as Rd= 1−(ˆri−1∩ˆri)/(ˆri−1∪ˆri). Reward Function. To train the entire system, we formulate a reward function Ras the weighted sum of all the rewards, i.e., R=wpa∗ Rpa+wnsc∗ Rnsc+wipa∗Ripa+wrf∗Rrf+wdc∗Rdc+wd∗Rd. All reward values are between 0 and 1 andP\nr∈Wwr= 1, where W={wpa, wnsc, wipa, wrf, wdc, wd}is a set of weights (hyperparameters) that are tuned to optimize and maximize the reward. RL Fine-tuning Stage. The final reward Ris employed within an RL policy loss PPO, which offers feedback to the agent, motivating it to produce high-quality responses corresponding to the desired outcomes. During the RL fine-tuning phase, the RL 0.82, respectively.\n6We attain an accuracy and macro-F1 of 93.24% and 84.02%, respectively.\n7Thetanh()maps values between -1 and 1, but in this case, the loss\ncannot be negative, so the resulting values will always be between 0 and 1.\nFurther, sigmoid ()is not chosen as it would map values between 0.5 and 1.\n6\npolicy PΘis initialized πPSFT. Due to space constraints, training details and RL policy optimization with PPO are furnished in §B of the appendix. 5 Experimental Details Baselines. We evaluate TRIP NEGOTIATOR against eight baselines, viz.GPT-2-small (Radford et al., 2019), DialoGPT-small (Zhang et al., 2020a), ARDM (Wu et al., 2021), GPT-Critic (Jang et al., 2022), PersRFI (Shi et al., 2021), INA(Ahmad et al., 2023), ProCoT (Deng et al., 2023a), and PSFT: DialoGPT fine-tuned in a supervised fashion with traveler’s persona. Baselines (§C.1) and implementation details (§C.2) are given in appendix. Evaluation Metrics. We conduct automatic and human evaluations to assess the performance of the TRIP NEGOTIATOR . We evaluate classifiers using Weighted Accuracy (W-Acc.) and Macro-F1, and intent-phase generation model using Perplexity (PPL) (Brown et al., 1992), BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and BERTScore-F1 (BS-F1) (Zhang et al., 2019). TRIP NEGOTIATOR is evaluated using traitfocused metrics (TFM) to measure the travel agent’s proximity to the goal from the perspective of persona modeling and negotiation, and language-focused metrics (LFM) to emphasize the language realization and generation ability for negotiation dialogue. For automatic evaluation, Persona Alignment (PA), Negotiation Strategy Consistency (NSC), and Negotiation Phase Agreement (NPA)8are used as TFM, and PPL, BLEU, METEOR, BS-F1, and response length (R-LEN) are used as LFM. For human evaluation, besides PA, NSC and NPA, Win Times (WT), Deal Rate (DR), and Pareto Optimality (PO) (Lewis et al., 2017) are utilized as TFM, and Fluency (F), Contextual Coherence (CC), and Engagingness (E) are used as LFM. PA, NSC, NPA, F, CC, and E are rated on a scale of 1-5 (the higher, the better). The details of evaluation metrics and human evaluation process are given in §C.5 of the appendix. 6 Results and Analysis Automatic Evaluation Results. Table 1 presents the results of TRIP NEGOTIATOR and baselines. The superior performance of PSFT compared to other baselines, viz. GPT-2, DialoGPT, ARDM, PersRFI, GPT-Critic, INA, and LLMs’ promptingbased ProCoT underscores the significance of the travel persona. The integration of these attributes 8PA, NSC, NPA are computed using Persona-NLIM, NSCM, and NPCM\naccuracy scores.inherently guides PSFT towards generating more personalized and interactive responses. Further, the evaluation results show that TRIP NEGOTIATOR obtains the best scores compared to all baselines across all the metrics. Evidently, there is a significant decline in PPL. The proposed system obtains PPL of 2.13, which is -41.96% compared to the best baseline (PSFT). TRIP NEGOTIATOR also generates longer responses, which can be inferred from the highest score of R-LEN. This could be attributed to the TFR, RFR, and DCR that drive the model to build a connection with travelers by generating personalized integrative responses tailored to travelers’ preferences and requirements while being contextually adequate and fluent. This results in the generation of interactive and engaging responses. The proposed model also shows an improvement in TFM. TRIP NEGOTIATOR reports improved PA, NSC, and NPA scores of 82.31%, 84.62%, and 81.34%, respectively, which are 10.36%, 7.65%, and 9.38% better than PSFT. This could be due to the TFR that forces the model to generate personaaligned responses that adhere to pertinent negotiation strategies and drive the negotiation toward the desired phase based on the traveler’s intent. Further, the proposed system yields 0.35, 0.87, and 0.81 scores of BLEU, METEOR, and BS-F1, respectively, with an increase of +0.07, +0.12, and +0.08 points compared to PSFT. These scores signify high lexical and semantic similarity of generated responses with the gold responses. This strengthens the design of rewards in generating customized responses aligned with gold responses. It can also be deduced that the performance of TRIP NEGOTIATOR decreases when any of the rewards are removed. Notably, the PPL score increases in the order TRIP NEGOTIATOR -(Rpa<Ripa< Rnsc<Rdc<Rrf<Rd). The performance further drops when either all the TFR, or LFR, or both are ablated. These results signify the need for both TFR and LFR to generate fluent, coherent, and diverse responses grounded in appropriate negotiation strategy and phase and aligned with the traveler’s persona and intents. Due to space constraints, results of classifiers and intent-phase model are given in §C.3 and §C.4 of the appendix. Human Evaluation Results. Table 2 presents human evaluation results for TRIP NEGOTIATOR . We compare our proposed model against GPT-2, DialoGPT, ARDM, PersRFI, GPT-Critic, INA, ProCoT, PSFT, TRIP NEGOTIATOR −(Rpa+Rnsc+Ripa), TRIP NEGOTIATOR −(Rrf+Rdc+Rd)and TRIP NE 7\nModels PPL BLEU METEOR BS-F1 PA NSC NPA R-LEN\nBaselinesGPT-2 (Radford et al., 2019) 5.90 0.18 0.58 0.62 64.52% 71.24% 68.72% 18.61\nDialoGPT (Zhang et al., 2020a) 5.27 0.20 0.59 0.64 66.83% 73.47% 69.23% 19.82\nARDM (Wu et al., 2021) 4.82 0.23 0.65 0.69 71.25% 75.34% 72.46% 24.92\nPersRFI (Shi et al., 2021) 4.26 0.25 0.65 0.70 71.69% 75.05% 71.46% 25.03\nGPT-Critic (Jang et al., 2022) 4.12 0.24 0.67 0.70 71.01% 75.60% 71.69% 23.89\nINA (Ahmad et al., 2023) 3.98 0.27 0.69 0.71 70.28% 74.10% 70.15% 25.19\nProCoT + Llama-2-7b-chat (Deng et al., 2023a) 36.87 0.11 0.33 0.29 36.14% 39.90% 32.86% 10.75\nProCoT + ChatGPT (Deng et al., 2023a) 29.30 0.19 0.50 0.43 42.76% 46.30% 39.42% 19.46\nPSFT 3.67 0.28 0.75 0.73 74.58% 78.60% 74.36% 28.16\nProposed System TRIP NEGOTIATOR 2.13 0.35 0.87 0.81 82.31% 84.62% 81.34% 31.84\nAblationsTRIP NEGOTIATOR -Rpa 2.51 0.32 0.83 0.78 79.62% 82.61% 79.82% 29.63\nTRIP NEGOTIATOR -Rnsc 2.46 0.33 0.85 0.80 80.16% 81.37% 80.36% 30.43\nTRIP NEGOTIATOR -Ripa 2.48 0.33 0.85 0.79 81.04% 82.99% 78.01% 30.19\nTRIP NEGOTIATOR -Rrf 2.38 0.34 0.86 0.80 82.03% 84.16% 80.73% 30.72\nTRIP NEGOTIATOR -Rdc 2.41 0.33 0.85 0.79 81.27% 83.28% 80.54% 30.76\nTRIP NEGOTIATOR -Rd 2.37 0.34 0.85 0.80 81.64% 83.54% 80.59% 30.83\nTRIP NEGOTIATOR - (Rpa+Rnsc +Ripa) 2.58 0.31 0.81 0.77 77.24% 79.82% 78.52% 28.07\nTRIP NEGOTIATOR - (Rrf+Rdc+Rd) 2.46 0.34 0.85 0.78 81.76% 84.03% 80.65% 30.72\nTRIP NEGOTIATOR -R 3.06 0.29 0.76 0.73 73.10% 77.92% 74.81% 29.17\n∆TRIP NEGOTIATOR −BEST Baseline -41.96% ↓ 25.00% ↑ 16.00% ↑ 10.95% ↑ 10.36% ↑ 7.65%↑ 9.38%↑ 20.17% ↑\nTable 1: Automatic evaluation results of the TRIP NEGOTIATOR system on the proposed DEAL dataset. All results are statistically\nsignificant based on Welch’s t-test (Welch, 1947) conducted at a 5% significance level.\nModels PA NSC NPA WT DR PO F CC E\nBaselinesGPT-2 (Radford et al., 2019) 2.13 2.32 2.18 53.75% 58.44% 60.23% 2.05 2.16 1.87\nDialoGPT (Zhang et al., 2020a) 2.48 2.86 2.53 56.25% 61.39% 62.01% 2.34 2.43 2.08\nARDM (Wu et al., 2021) 2.76 2.63 2.56 67.50% 65.99% 67.48% 2.67 2.7 2.18\nPersRFI (Shi et al., 2021) 2.93 2.78 2.72 70.25% 67.21% 68.50% 2.82 2.85 2.24\nGPT-Critic (Jang et al., 2022) 2.86 2.71 2.69 70.00% 66.80% 68.33% 2.74 2.79 2.22\nINA (Ahmad et al., 2023) 2.04 2.88 2.79 71.10% 68.00% 69.05% 2.92 2.91 2.26\nProCoT + ChatGPT (Deng et al., 2023a) 1.80 1.56 1.49 30.12% 36.29% 35.01% 3.50 3.60 1.20\nPSFT 3.61 3.78 3.91 75.00% 71.32% 72.48% 3.91 3.55 3.24\nProposed System TRIP NEGOTIATOR 4.32 4.21 4.53 88.75% 82.76% 87.24% 4.73 4.71 4.19\nAblationsTRIP NEGOTIATOR - (Rpa+Rnsc +Ripa) 3.86 3.81 4.12 77.50% 80.14% 84.17% 4.2 4.14 3.71\nTRIP NEGOTIATOR - (Rrf+Rdc+Rd) 3.95 3.89 4.38 81.25 81.52 85.49 4.33 4.2 3.86\nTRIP NEGOTIATOR -R 3.64 3.55 3.97 76.25 78.23 84.46 3.99 3.92 3.61\nTable 2: Human evaluation results of the TRIP NEGOTIATOR on the proposed DEAL dataset. All results are statistically\nsignificant based on Welch’s t-test (Welch, 1947) conducted at a 5% significance level.\nGOTIATOR −Ronly as manual evaluation is expensive. It is evident that TRIP NEGOTIATOR achieves better scores of 4.32, 4.21, 4.53, 88.75%, 82.76%, 87.24%, 4.73, 4.71, and 4.19 for PA, NSC, NPA, WT, DR, PO, F, CC, and E, respectively, with an improvement of +0.71, +0.43, +0.62, +13.75, +11.44, +14.76, +0.82, +1.16, and +0.95, points for these metrics, compared to PSFT. The highest PA, NSC, NPA, WT, DR, and PO scores emphasize that incorporating PAR, NSCR, and IPAR helps TRIP NE GOTIATOR to offer personalized experience during negotiation by generating engaging and interactive responses. The highest scores of F, CC, and E suggest that FR, DCR, and DR have played a crucial role in generating consistent, fluent, and non-repetitive responses. Also, TRIP NEGOTIA TOR−(Rpa+Rnsc+Ripa)shows a marginal improvement on PSFT and TRIP NEGOTIATOR −R. This further implies that LFR are insufficient to ensure persona-aware responses in negotiation conversations. Notably, a minimal performance difference can be seen between TRIP NEGOTIATOR and TRIP NEGOTIATOR −(Rpa+Rnsc+Ripa). This signifies 669that TFR are crucial for generating persona-aware responses with the correct negotiation strategy and phase. It can also be noted that TRIP NEGOTIA TOR achieves better scores than TRIP NEGOTIA TOR−(Rrf+Rdc+Rd)for all metrics, indicating the importance of LFR. 7 Conclusion This work introduces a new task of personalized integrative response generation considering the travelers’ travel persona and requirements in negotiation conversations. For this task, a novel dialogue dataset DEAL is created by prompting MPT-7B with manual interventions and is annotated with travel persona, intents, negotiation strategies, and phases. Then, a novel personalized integrative negotiation dialogue system, TRIP NEGOTIATOR , is developed in an RL framework by designing unique rewards to facilitate the generation of travel persona-aware responses with appropriate negotiation strategy and negotiation phase aligned with the pertinent intent. The findings emphasize the necessity of each reward to facilitate a personalized integrative experience during negotiation. 8\nLimitations The present work has limitations regarding the proposed DEAL dataset creation approach and modeling of the proposed TRIP NEGOTIATOR framework. Regarding data curation, using the MPT-7B model for dialogue generation requires substantial GPU memory, specifically 40 GB. Another constraint arises from MPT-7B’s context window, which is limited to 2,048 tokens. This restriction impacts our prompting mechanism, as within this window, we must accommodate background data alongside dialogue history and few-shot examples. Consequently, we are constrained to a maximum of 4 shots during prompting, leading to some hallucinations in the generated data, thus requiring manual correction. Regarding the dialogue system, TRIP NEGOTIA TOR also, one limitation is the requirement of substantial GPU memory capacity, specifically 40 GB for training the end-to-end RL model. Another challenge stems from optimizing reward weights, potentially extending both training and validation times. To mitigate this, heuristic approaches are employed to select specific combinations of reward weights. Additionally, in cases involving continuous, brief, or direct responses (e.g., ‘ Yeah ’, ‘Agree ’, ‘No’, ‘$600 ’, ‘Okay ’,Can you decrease? , etc. the model initially tends to comprehend the traveler’s need by requesting information; however, after a few turns, it may generate repetitive or inconsistent responses. This discrepancy arises from the training data, which comprises interactive dialogues with longer utterances, leading to confusion when handling concise inputs. Furthermore, continuous out-of-context responses may prompt the model to generate inadequate responses, given that language models inherently possess extensive knowledge in their memories. Ethics Statement Given the inherent bargaining nature of negotiations, developing and utilizing personalized negotiation dialogue systems must adhere to ethical conduct. Our Institutional Review Board (IRB) evaluated and approved this study. Our personalized integrative negotiation approach emphasizes traveler’s flexibility, fostering a win-win outcome. Contrary to a zero-sum scenario, where one party’s gain is the other’s loss, the proposed negotiation dialogue system prioritizes mutual benefit. Throughout the conversation, travelers retain the autonomy 742to reject the deal, ensuring they are not obliged to proceed if it doesn’t align with their preferences. The ethical considerations should also be taken care of in case of the dataset. Due to the synthetic nature of the dialogues, users should approach them with sensitivity and respect and be cognizant of the potential risks of cultural appropriation or misrepresentation when generating data using language models. Given that language models are trained on web-based data, there is a potential for these models to incorporate biases that reinforce stereotypes, contribute to discrimination, or marginalize particular communities. Previous research has also reported the potential influence of synthetic data in contributing to feedback loops that intensify the occurrence of biased language generation (Taori and Hashimoto, 2023). Hence, it is imperative to engage in collaboration with linguists, language experts, and community representatives to prevent the inadvertent perpetuation of stereotypes and cultural insensitivity. To ensure proper use, access to the created dataset will be granted upon completion and signing an agreement that the data will exclusively be used for research purposes. Human experts, regular employees of our research group engaged in the annotation, filtering/editing of data, and manual evaluations are compensated in accordance with institute policies. 771",
        "aspect_review": "The paper provides a comprehensive review of existing negotiation dialogue systems and datasets, highlighting the gaps in current research that the proposed model addresses. The study conducts extensive qualitative and quantitative analyses, including both automatic and human evaluations, to demonstrate the effectiveness of the proposed model. The proposed TRIP NEGOTIATOR model shows significant improvements in various metrics over baseline models, indicating its effectiveness in generating personalized integrative negotiation dialogues. The introduction of the DEAL dataset for personalized integrative negotiation in the tourism domain is a valuable contribution, providing a novel resource for future research. The paper compares the proposed model against a range of strong baselines, showing clear improvements in performance across multiple metrics. The work introduces a novel task and dataset, along with a robust dialogue generation model, contributing significantly to the field of negotiation dialogue systems. The paper is well-structured and clearly presents the methodology, experiments, and results, making it accessible to the audience. The use of reinforcement learning with Proximal Policy Optimization (PPO) and the development of a novel reward function are notable methodological strengths.\n\nThe paper could provide more detailed analysis on specific failure cases of the model to offer deeper insights. While the results are promising, a more detailed discussion on the limitations of the model's applicability to other domains could enhance the analysis. The dataset creation process involves manual intervention, which might introduce biases that are not fully addressed. Although the model outperforms baselines, the paper could benefit from including more diverse baselines for a broader comparison. The optimization of reward weights in the reinforcement learning framework requires heuristic approaches, which could be more rigorously justified. Some sections, particularly those describing the dataset creation and annotation processes, could benefit from additional detail to improve clarity.",
        "general_review": "The paper introduces a novel task of personalized integrative negotiation within the tourism domain, which is a significant advancement in negotiation dialogue systems. The creation of the DEAL dataset through prompting and human intervention ensures high-quality dialogues and provides a solid foundation for future research. The TRIP NEGOTIATOR model is technically robust, employing reinforcement learning with Proximal Policy Optimization to tailor responses to traveler personas, which is a notable contribution to personalized dialogue systems. The extensive qualitative and quantitative analyses demonstrate the model's effectiveness, with significant improvements over baselines in both automatic and human evaluations. The paper also emphasizes the practical impact of the proposed system in enhancing personalized negotiation experiences, which is highly relevant for the tourism industry.\n\n1. The requirement for substantial GPU memory for both dataset creation and model training is a limitation that might hinder reproducibility for some researchers. 2. The context window limitation of the MPT-7B model affects the prompting mechanism, leading to some hallucinations in the generated data that require manual correction. 3. The model may generate repetitive or inconsistent responses when handling concise inputs, which could be addressed with further refinement. 4. Optimizing reward weights in the reinforcement learning framework can be time-consuming, which could be streamlined with more efficient heuristic approaches."
    },
    "24": {
        "paper": "Unveiling Hallucination in Text, Image, Video, and Audio Foundation\nModels: A Comprehensive Review\nAnonymous ACL submission\nAbstract\nThe rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research and development in this pivotal area. 1 Introduction The rapid progress in large-scale foundation models (FMs), spanning language, image, audio, and video domains, has revolutionized the field of artificial intelligence (AI). Models such as GPT3 (Brown et al., 2020), MiniGPT-4 (Zhu et al., 2023), AudioLLM (Borsos et al., 2023), and LaViLa (Zhao et al., 2022) have demonstrated remarkable abilities across diverse tasks, from text generation to multimodal understanding. However, as these models are increasingly deployed in highstakes domains, understanding and mitigating their potential to generate hallucinated outputs – content that appears plausible but is factually incorrect or inconsistent with the input – has become a critical priority. Hallucination, often unintended, can 042arise due to factors like biases in training data, limited access to current information, or the model’s constraints in understanding and generating contextually precise responses. Deploying these models without addressing their hallucination tendencies may result in misinformation, incorrect conclusions, and adverse consequences. Thus, it is imperative to comprehensively study and understand the hallucination behavior of FMs across different modalities. 1.1 Motivation and Contributions Most of the existing survey papers have explored hallucination in the context of large language models (LLMs) (Huang et al., 2023), (Tonmoy et al., 2024). Recent studies have shown that hallucination can also occur in vision, audio, and video foundation models, highlighting the need for a comprehensive understanding of this challenge across multiple modalities (Liu et al., 2024a), (Sahoo et al., 2024), (Rawte et al., 2023b). To address this gap, the present survey aims to provide a holistic and multimodal perspective on the hallucination challenge in FMs. This review comprehensively examines existing research across language, vision, video, and audio domains to understand the mechanisms, detection methods, and mitigation strategies for hallucination in FMs. It serves as a vital resource for researchers and practitioners, aiding in the development of more robust AI solutions. Additionally, it includes a detailed taxonomy diagram in Fig. 1 and a summarized Table 1 illustrating recent advancements across different modalities. Please refer to Table 9.1 of the appendix. The contributions of this survey paper are as follows: •Establish a precise definition and structured taxonomy of hallucination in the context of large-scale foundation models. •Identify the key factors and mechanisms that contribute to the emergence of hallucination 1\nacross different modalities. •Explore the various detection and mitigation strategies that have been proposed to address the hallucination problem in a multimodal setting. •Highlight the open challenges and future research directions in this critical area. 2 Hallucination in Large Language Models Despite the progress of LLMs, a notable challenge persists in their proneness to hallucinate, impeding their practical implementation. For instance, the illustration in Figure 2 exemplifies the generated response by the LLM, showcasing indications of hallucination. 2.1 Hallucination Detection and Mitigation Detecting hallucinations in LLMs is crucial for ensuring the credibility and reliability of their results, especially in scenarios requiring factual correctness. Existing fact-checking methods often rely on complex modules or external databases, requiring either output probability distributions or interfacing with external sources. The SelfCheckGPT method proposed by (Manakul et al., 2023) offers a zeroresource black-box solution for detecting hallucinations in any LLM without relying on external resources. This method operates on the principle that an LLM familiar with a topic will produce consistent and comparable facts in its responses. In contrast, randomly sampled responses from an unfamiliar topic are likely to contain contradicting and hallucinated facts. Continuing the exploration of methods for passage-level hallucination detection, (Yang et al., 2023) proposed a novel selfcheck approach based on reverse validation, aiming to automatically identify factual errors without external resources. They introduced a benchmark, Passage-level Hallucination Detection(PHD) , generated using ChatGPT and annotated by human experts, to assess different methods. Assessing the accuracy of long text generated by LLMs is challenging because it often contains both accurate and inaccurate information, making simple quality judgments insufficient. To address this, (Min et al., 2023) introduced FACTSCORE (Factual Precision in Atomicity Score), a new evaluation method that breaks down text into individual facts and measures their reliability. The study (Huang and Chang, 1292023) introduced a unique strategy to mitigate hallucination risks in LLMs by drawing parallels with established web systems. They identified the absence of a \"citation\" mechanism in LLMs, which refers to acknowledging or referencing sources or evidence, as a significant gap. Addressing the need to identify factual inaccuracies in LLM-generated content, (Rawte et al., 2024b) developed a multi-task learning (MTL) framework, integrating advanced long text embeddings like e5-mistral-7b-instruct, along with models such as GPT-3, SpanBERT, and RoFormer. This MTL approach demonstrated a 40% average improvement in accuracy on the FACTOID benchmark compared to leading textual entailment methods. Hallucination mitigation efforts have predominantly relied on empirical methods, leaving uncertainty regarding the possibility of complete elimination. To tackle this challenge, (Xu et al., 2024b) introduced a formal framework defining hallucination as inconsistencies between computable LLMs and a ground truth function. Through this framework, the study examines existing hallucination mitigation strategies and their practical implications for real-world LLM deployment. The study (Rawte et al., 2024c) introduces the Sorry, Come Again (SCA) prompting technique to address hallucination in contemporary LLMs. SCA enhances comprehension through optimal paraphrasing and injecting [PAUSE] tokens to delay LLM generation. It analyzes linguistic nuances in prompts and their impact on the hallucinated generation, emphasizing how prompts with lower readability, formality, or concreteness pose challenges. (Rawte et al., 2023a) investigates how LLMs respond to factually correct and incorrect prompts, categorizing their hallucinations into mild, moderate, and alarming subcategories. Additionally, the paper introduces the Hallucination eLiciTation dataset, comprising 75,000 text snippets annotated by humans, and introduces a novel Hallucination Vulnerability Index metric. Benchmark Evaluation: In certain instances, LLMs engage in a phenomenon termed \"hallucination snowballing,\" where they fabricate false claims to rationalize prior hallucinations, despite acknowledging their inaccuracy. To empirically explore this phenomenon, (Zhang et al., 2023a) devised three question-answering datasets spanning diverse domains, wherein ChatGPT and GPT-4 often furnish inaccurate answers alongside explanations fea2\nHallucinationsText §2DetectionFACTOID (Rawte et al., 2024b)\nFACTOR (Muhlgay et al., 2023)\nFactCHD (Chen et al., 2023b)\nHalluQA (Cheng et al., 2023)\nHaluEval (Li et al., 2023b)\nSelfCheckGPT (Manakul et al., 2023)\nFACTSCORE (Min et al., 2023)\nFacTool (Chern et al., 2023)\nMitigationDoLa (Chuang et al., 2023)\nCoK (Li et al., 2023d)\nMixAlign (Knowledge)\nSELF-FAMILIARITY (Luo et al., 2023)\nCoVe (Dhuliawala et al., 2023)\nHALOCHECK (Elaraby et al., 2023)\nInstructional Prompting (Varshney et al., 2023)\nPURR (Chen et al., 2023a)\nLLM-AUGMENTER (Peng et al., 2023)\nImage §3DetectionHallusionBench (Guan et al., 2023)\nGA VIE (Liu et al., 2023)\nPhD (Liu et al., 2024b)\nVHTest (Huang et al., 2024)\nM-HalDetect (Gunjal et al., 2024)\nChartBench (Xu et al., 2023b)\nMSG-MCQ (Lu et al., 2024)\nMMVP (Tong et al., 2024)\nREVO-LION (Liao et al., 2023)\nCIEM (Hu et al., 2023)\nFAITHSCORE (Jing et al., 2023)\nPOPE (Li et al., 2023f)\nVQA (Changpinyo et al., 2022)\nHaELM (Wang et al., 2023)\nNOPE (Lovenia et al., 2023)\nMMHAL-BENCH (Sun et al., 2023)\nTouchStone (Bai et al., 2023)\nMitigationHalluciDoctor (Yu et al., 2023a)\nLURE (Zhou et al., 2023)\nMARINE (Zhao et al., 2024)\nFDPO (Gunjal et al., 2024)\nViGoR (Yan et al., 2024)\nHA-DPO (Zhao et al., 2023)\nVIGC (Wang et al., 2024)\nData Centric Approach (Lu et al., 2024)\nMoF (Tong et al., 2024)\nInternLM-XComposer (Zhang et al., 2023b)\nVCD (Leng et al., 2023)\nFactually Augmented RLHF (Sun et al., 2023)\nObjMLM (Dai et al., 2022)\nVideo §4Detection EMScore (Shi et al., 2022)\nMitigationFactVC (Liu and Wan, 2023)\nCLearViD (Chuang and Fazli, 2023)\nMGAT (He et al., 2022)\nAudio §5DetectionPAM (Deshmukh et al., 2024)\nCompA (Ghosh et al., 2023)\nMitigationMusicLDM (Chen et al., 2024)\nRe-AudioLDM (Yuan et al., 2024)\nSECap (Xu et al., 2024a)\nRECAP (Ghosh et al., 2024)\nCacophony (Zhu and Duan, 2024)\nEnCLAP (Kim et al., 2024)\nFigure 1: Taxonomy of hallucination in large foundation models, organized around detection and mitigation\ntechniques.\nturing at least one false claim. Significantly, the study suggests that the language model can discern these false claims as incorrect. Another benchmark dataset FactCHD (Chen et al., 2023b), was introduced to detect fact-conflicting hallucinations within intricate inferential contexts. It encompasses a range of datasets capturing different factuality patterns and integrates fact-based evidence chains to 188improve assessment accuracy. The study by (Li et al., 2023b) introduced a dataset to assess the performance of LLMs in recognizing hallucinations. The outcomes highlighted ChatGPT’s inclination to produce hallucinated content, particularly on certain topics, introducing unverifiable information. 3\nFigure 2: LLM responses showing the types of hallu-\ncinations, highlighted in red, green, and blue (Zhang\net al., 2023d).\n3 Hallucination in Large Vision-Language Models Large Vision-Language Models (LVLMs) have garnered significant attention in the AI community for their capacity to handle visual and textual data simultaneously. Nonetheless, similar to LLMs, LVLMs also confront the issue of hallucination. Figure 3 illustrates an example of visual hallucination. 3.1 Hallucination Detection and Mitigation (Dai et al., 2022) investigate the issue of object hallucinations in Vision-Language Pre-training (VLP) models, where textual descriptions generated by these models contain non-existent or inaccurate objects based on input images. (Li et al., 2023f) reveal widespread and severe object hallucination issues and suggests that visual instructions may influence hallucination, noting that objects frequently appearing in visual instructions or cooccurring with image objects are more likely to be hallucinated. To enhance the evaluation of object hallucination, they introduce a polling-based query method called POPE, which demonstrates improved stability and flexibility in assessing object hallucination. The absence of a standardized metric for assessing object hallucination has hindered progress in understanding and addressing this issue. To address this gap, (Lovenia et al., 2023) introduce NOPE (Negative Object Presence EvaluFigure 3: Four IVL-Hallu examples in Prompted Hal-\nlucination Dataset(PhD) (Liu et al., 2024b) including\nvisuals and the matching question-answer pairs and hal-\nlucination elements (HE). While words annotated in red\ndo not exist or do not match within the image, words\nannotated in green have correspondences within the im-\nage. Question, Answer, and Statement are denoted by\nthe letters Q, A, and S respectively.\nation), a novel benchmark for evaluating object hallucination in vision-language (VL) models through visual question answering (VQA). Utilizing LLMs, the study generates 29.5k synthetic negative pronoun (NegP) data for NOPE. It extensively evaluates the performance of 10 VL models in discerning the absence of objects in visual questions, alongside their standard performance on visual questions across nine other VQA datasets. Most existing efforts focus primarily on object hallucination, overlooking the diverse types of LVLM hallucinations. The study by (Liu et al., 2024b) delves into Intrinsic Vision-Language Hallucination (IVL-Hallu) and proposes several novel IVL-Hallu tasks categorized into four types: attribute, object, multi-modal conflicting, and counter-common-sense hallucination. To assess and explore IVL-Hallu, they introduce a challenging benchmark dataset and conduct experiments on five LVLMs, revealing their incapacity to effectively address the proposed IVLHallu tasks. To mitigate object hallucination in LVLMs without resorting to costly training or API reliance, (Zhao et al., 2024) introduces MARINE, which is both training-free and API-free. MARINE enhances the visual understanding of LVLMs by integrating existing open-source vision models and utilizing guidance without classifiers to integrate 4\nobject grounding features, thereby improving the precision of the generated outputs. Evaluations across six LVLMs reveal MARINE’s effectiveness in reducing hallucinations and enhancing output detail, validated through assessments using GPT4V . HalluciDoctor (Yu et al., 2023a) tackles hallucinations in Multi-modal Large Language Models (MLLMs) by using human error detection to identify and eliminate various types of hallucinations. Through rebalancing data distribution via counterfactual visual instruction expansion, they successfully mitigate 44.6% of hallucinations while maintaining competitive performance. Despite proficiency in visual semantic comprehension and meme humor, MLLMs struggle with chart analysis and understanding. Addressing this, (Xu et al., 2023b) proposed ChartBench, a benchmark assessing chart comprehension. ChartBench exposes MLLMs’ limited reasoning with complex charts, prompting the need for novel evaluation metrics like Acc+ and a handcrafted prompt, ChartCoT. The study (Zhang et al., 2023b) introduced InternLM-XComposer, an LVLM aimed at designed to address the challenge of hallucination in image-text comprehension and composition. The performance of InternLM-XComposer’s text-image composition is evaluated through a robust procedure involving both human assessment and comparison to GPT4-Vision, with the model demonstrating competitive performance against solutions like GPT4-V and GPT3.5. 3.2 Benchmark Evaluation The current methods of developing LVLMs rely heavily on annotated benchmark datasets, which can exhibit domain bias and limit model generative capabilities. To address this, (Li et al., 2023e) proposed a novel data collection approach that synthesizes images and dialogues synchronously for visual instruction tuning, yielding a large dataset of image-dialogue pairs and multi-image instances. Another study (Huang et al., 2024) introduced VHTest, a benchmark dataset with 1,200 diverse visual hallucinations (VH) instances across 8 VH modes. Evaluation of three SOTA MLLMs showed varying performance, with GPT4V exhibiting lower hallucination than MiniGPTv2. The study (Rawte et al., 2024a) categorizes visual hallucination in VLMs into eight orientations and introduces a dataset of 2,000 sam- 300ples covering these types. They propose three main categories of methods to mitigate hallucination: data-driven approaches, training adjustments, and post-processing techniques. (Wang et al., 2024) propose the Visual Instruction Generation and Correction (VIGC) framework to address the scarcity of high-quality instruction-tuning data for MLLMs. VIGC enables MLLMs to generate diverse instruction-tuning data while iteratively refining its quality through Visual Instruction Correction (VIC), mitigating hallucination risks. The framework produces diverse, high-quality data for fine-tuning models, validated through evaluations, improving benchmark performance, and overcoming language-only data limitations. 4 Hallucinations in Large Video Models Large Video Models (LVMs) represent a significant advancement, allowing for the processing of video data at scale. Despite their potential for various applications like video understanding and generation, LVMs face challenges with hallucinations, where misinterpretations of video frames can result in artificial or inaccurate visual data. This issue arises due to the complexity of video data, which requires the model to thoroughly process and comprehend it. Figure 4 demonstrates the instances of hallucination observed in LVMs. 4.1 Hallucination Detection and Mitigation The intricate task of dense video captioning, involving the creation of descriptions for multiple events within a continuous video, necessitates a thorough understanding of video content and contextual reasoning to ensure accurate description generation. However, this endeavor faces numerous challenges, potentially resulting in instances of inaccuracies and hallucinations (Iashin and Rahtu, 2020), (Suin and Rajagopalan, 2020). Traditional methods detect event proposals first, then caption subsets, risking hallucinations due to overlooking temporal dependencies. To address this, (Mun et al., 2019) introduces a novel approach to modeling temporal dependencies and leveraging context for coherent storytelling. By integrating an event sequence generation network and a sequential video captioning network trained with reinforcement learning and two-level rewards, the model captures contextual information more effectively, yielding coherent and accurate captions while minimizing the risk of hallucinations. Another study (Liu and Wan, 2023) in5\nFigure 4: A video featuring descriptions generated by VLTinT model and ground truth (GT) with description errors\nhighlighted in red italics. (Chuang and Fazli, 2023).\ntroduces a novel weakly-supervised, model-based factuality metric called FactVC, which outperforms previous metrics. Furthermore, they provide two annotated datasets to promote further research in assessing the factuality of video captions. (Wu and Gao, 2023) proposed a context-aware model that incorporates information from past and future events to conditionally influence the description of the current event. Their approach utilizes a robust pre-trained context encoder to encode information about the surrounding context events, which is then integrated into the captioning module using a gateattention mechanism. Experimental findings on the YouCookII and ActivityNet datasets demonstrate that the proposed context-aware model outperforms existing context-aware and pre-trained models by a significant margin. To enhance dense video captioning, (Zhou et al., 2024) introduced a streaming model comprising a memory module for long video handling and a streaming decoding algorithm enabling predictions before video completion. This approach notably boosts performance on dense video captioning benchmarks such as ActivityNet, YouCook2, and ViTT. Video infilling and prediction tasks are crucial for assessing a model’s ability to comprehend and anticipate the temporal dynamics within video sequences (Höppe et al., 2022). To address this, (Himakunthala et al., 2023) introduced an inferencetime challenge dataset containing keyframes with dense captions and structured scene descriptions. This dataset contains keyframes supplemented with unstructured dense captions and structured FAMOUS: (Focus, Action, Mood, Objects, and Setting) scene descriptions, providing valuable contextual information to support the models’ understanding of the video content. They employed various language models like GPT-3, GPT-4, and Vicuna with greedy decoding to mitigate hallucination 388risks. Prominent developments in video inpainting have been observed recently, especially in situations where explicit guidance like optical flow helps to propagate missing pixels across frames (Ouyang et al., 2021). However, difficulties and constraints occur from a lack of cross-frame information. The study (Yu et al., 2023b) aims to tackle the opposite issue rather than depending on using pixels from other frames. The suggested method presents a Deficiency-aware Masked Transformer (DMT), a dual-modality-compatible inpainting framework. This approach improves handling scenarios with incomplete information by pre-training an image inpainting model to serve as a prior for training the video model. Understanding scene affordances, which involve potential actions and interactions within a scene, is crucial for comprehending images and videos. (Kulal et al., 2023) introduced a method for realistically inserting people into scenes. The model seamlessly integrates individuals into scenes by deducing realistic poses based on the context and ensuring visually pleasing compositions. (Chuang and Fazli, 2023) introduced CLearViD, a transformerbased model that utilizes curriculum learning techniques to enhance performance. By adopting this approach, the model acquires more robust and generalizable features. Furthermore, CLearViD incorporates the Mish activation function to address issues like vanishing gradients, thereby reducing the risk of hallucinations by introducing nonlinearity and non-monotonicity. Extensive experiments and ablation studies validate the effectiveness of CLearViD, with evaluations on ActivityNet Captions and YouCook2 datasets showcasing significant improvements over existing SOTA models in terms of diversity metrics. 6\n4.2 Benchmark Evaluation The study (Zhang et al., 2006) created a novel two-level hierarchical fusion method to hallucinate facial expression sequences from training video samples using only one frontal face image with a neutral expression. To effectively train the system, they introduced a dataset specifically designed for facial expression hallucination, which included 112 video sequences covering four types of facial expressions (happy, angry, surprise, and fear) from 28 individuals, resulting in the generation of reasonable facial expression sequences in both the temporal and spatial domains with less artifact. In the realm of video understanding, the development of end-to-end chat-centric systems has become a growing area of interest. (Zhou et al., 2018) assembled the YouCook2 dataset, an extensive set of cooking videos with temporally localized and described procedural segments, to facilitate procedure learning tasks. The study by (Li et al., 2023c) introduces \"VideoChat\", a novel approach integrating video foundation models and LLMs through a learnable neural interface to enhance spatiotemporal reasoning, event localization, and causal relationship inference in video understanding. The researchers constructed a video-centric instruction dataset with detailed descriptions and conversations, emphasizing spatiotemporal reasoning and causal relationships. To counteract model hallucination, they employed a multi-step process to condense video descriptions into coherent narratives using GPT-4 and refined them to improve clarity and coherence. To explore the challenge of deducing scene affordances, (Kulal et al., 2023) curated a dataset of 2.4M video clips, showcasing a variety of plausible poses that align with the scene context. 5 Hallucinations in Large Audio Models Large audio models (LAMs) have emerged as a powerful tool in the realm of audio processing and generation, with a wide range of applications like speech recognition, music analysis, audio synthesis, and captioning (Latif et al., 2023), (Hussain et al., 2023), (Ghosal et al., 2023). Although these models have demonstrated remarkable capabilities across various domains, they are susceptible to hallucinations. These anomalies can take several forms, from creating unrealistic audio by piecing together fabricated snippets to injecting false information, such as quotes or facts, into summaries. Additionally, they may fail to accurately capture the 475inherent features of audio signals, such as timbre, pitch, or background noise (Shen et al., 2023). 5.1 Hallucination Detection and Mitigation In the realm of audio captioning, where natural language descriptions for audio clips are automatically generated, a significant challenge arises from the over-reliance on the visual modality during the pre-training of audio-text models. This reliance introduces data noise and hallucinations, ultimately undermining the accuracy of the resulting captions. To address this issue, (Xu et al., 2023a) introduced an AudioSet tag-guided model designed to bootstrap large-scale audio-text data (BLAT). Notably, this model sidesteps the incorporation of video, thus minimizing noise associated with the visual modality. The experimental findings across a range of tasks, including retrieval, generation, and classification, validate the effectiveness of BLAT in mitigating hallucination issues. Speech emotions play a crucial role in human communication and find extensive applications in areas such as speech synthesis and natural language understanding. However, traditional categorization approaches may fall short of capturing the nuanced and intricate nature of emotions conveyed in human speech (Jiang et al., 2019), (Han et al., 2021), (Ye et al., 2021). SECap (Xu et al., 2024a), a framework designed for speech emotion captioning. It aims to capture the intricate emotional nuances of speech using natural language. SECap utilizes various components, including LLaMA as the text decoder, HuBERT as the audio encoder, and Q-Former as the Bridge-Net, to generate coherent emotion captions based on speech features. Audio-language models, despite their capability for zero-shot inference, confront challenges like hallucinating task-specific details despite strong performance. To address this, (Elizalde et al., 2024) introduces the Contrastive Language-Audio Pretraining (CLAP) model. Pre-trained with 4.6 million diverse audio-text pairs, CLAP features a dual-encoder architecture, enhancing representation learning for improved task generalization across sound, music, and speech domains. 5.2 Benchmark Evaluation To address the scarcity of data in the specific domain of music captioning, (Doh et al., 2023) introduced LP-MusicCaps, a comprehensive dataset comprising 0.5 million audio clips accompanied by approximately 2.2 million captions. Leverag7\nFigure 5: Audio hallucination examples for each classes - Type A: Involving hallucinations of both objects and\nactions Type B: Featuring accurate objects but hallucinated actions Type C: Displaying correct actions but\nhallucinated objects (Nishimura et al., 2024).\ning LLMs, they train a transformer-based music captioning model with the dataset and assess its performance under zero-shot and transfer-learning scenarios, demonstrating its superiority over supervised baseline models. Another author (Nishimura et al., 2024) investigates audio hallucinations in large audio-video language models, where audio descriptions are generated primarily based on visual information, neglecting audio content. They have classified these hallucinations into three distinct types such as Involving hallucinations of both objects and actions ,Featuring accurate objects but hallucinated actions , and Displaying correct actions but hallucinated objects as represented in Fig. 5. In their investigation, they gathered 1000 sentences by soliciting audio information and then annotated them to determine whether they contained auditory hallucinations, further categorizing the type of hallucination if detected. To assess compositional reasoning in LAMs, (Ghosh et al., 2023) introduced CompA, consisting of two expertannotated benchmarks primarily focused on realworld audio samples. This benchmark is employed to fine-tune CompA-CLAP with a novel learning approach, enhancing its compositional reasoning skills and demonstrating substantial improvement over all the baseline models in tasks requiring compositional reasoning. 6 Hallucination: Good or Bad? Hallucinations in large-scale models present a complex interplay between creativity and uncertainty. On one hand, the ability to traverse beyond conventional data boundaries can lead to the generation of novel and innovative outputs. Hallucinations can spark exploratory learning, revealing unexpected patterns and features within the data. They can also serve as a form of stress testing, improving the model’s robustness and adaptability. Furthermore, these unexpected outputs can even inspire human 564creativity, serving as a springboard for new ideas and perspectives (Rawte et al., 2023b). However, this dual nature of hallucinations also introduces significant drawbacks. The quality and coherence of hallucinatory outputs can be questionable, posing challenges in applications where accuracy and reliability are paramount. Hallucinations can also propagate misinformation and biases present in the model’s training data, potentially reinforcing existing prejudices and eroding user trust. The reduced interpretability of these outputs can further undermine the model’s credibility and adoption. Ethical concerns arise when hallucinations produce inappropriate, offensive, or harmful content. Careful monitoring and control mechanisms are essential to prevent the generation of outputs that could cause harm or distress to users. Navigating this intricate balance between exploration and fidelity is crucial for maximizing the utility of large models while mitigating the risks associated with unexpected outputs. Overall, the phenomenon of hallucinations in large-scale models highlights the need for a nuanced understanding and strategic management of these capabilities. 7 Conclusion and Future Directions This survey paper systematically categorizes existing research on hallucination within FMs, providing comprehensive insights into critical aspects such as detection, mitigation, tasks, datasets, and evaluation metrics. It addresses the pressing issue of hallucination in FMs, acknowledging its widespread impact across various domains. By examining recent advancements in detection and mitigation techniques, the paper underscores the importance of addressing this challenge, given FMs’ indispensable role in critical tasks. Its primary contribution lies in presenting a structured taxonomy for classifying hallucination in FMs, spanning text, image, video, and audio domains. 8\n8 Limitation Previous survey papers primarily focused on hallucination in Large Language Models and did not extensively cover hallucinations in vision, audio, and video modalities. In this survey paper, our aim is to provide a comprehensive overview of hallucinations across all modalities, considering that hallucinations can occur in any large foundation model. Despite our efforts to provide a comprehensive summary of recent advancements related to hallucination techniques in all foundational models, we acknowledge that we may miss some relevant work in the field. 616",
        "aspect_review": "The paper establishes a precise definition and structured taxonomy of hallucination in large-scale foundation models, providing a comprehensive framework for understanding hallucination across text, image, video, and audio modalities. It synthesizes recent advancements in detecting and mitigating hallucination, offering valuable insights for researchers and practitioners, and includes detailed taxonomy diagrams and tables illustrating advancements across modalities. The paper provides a holistic and multimodal perspective on the hallucination challenge in foundation models, identifying key factors and mechanisms contributing to hallucination and exploring various detection and mitigation strategies.\n\nThe paper acknowledges that previous surveys focused primarily on hallucination in Large Language Models and may miss some relevant work in vision, audio, and video modalities. While the paper provides a comprehensive overview, the practical impact of the proposed taxonomy and strategies on real-world applications remains to be fully assessed. The analysis could benefit from more in-depth exploration of the effectiveness of different mitigation strategies in practical scenarios. Some figures and tables could be better integrated into the text to enhance readability and comprehension. The paper could further elaborate on the potential implications of hallucination in high-stakes applications and suggest more concrete future research directions. While the paper offers a detailed taxonomy, it could improve clarity by providing more examples and case studies to illustrate different types of hallucinations.",
        "general_review": "The paper offers a comprehensive and novel overview of hallucination in foundation models across multiple modalities, including text, image, video, and audio. It establishes a well-structured taxonomy and provides insightful empirical analysis of detection and mitigation strategies. The paper is well-organized, synthesizing recent advancements and presenting a clear framework that could significantly impact future research and development in this area. The inclusion of detailed tables and figures enhances understanding, making it a valuable resource for researchers and practitioners.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of complex methodologies. 2. There is a need for small details on hyperparameters or implementation specifics in some sections to aid reproducibility. 3. Some figures require minor formatting adjustments for better readability. 4. Slight clarification on code availability would enhance the paper's practical utility."
    },
    "25": {
        "paper": "From Text Segmentation to Enhanced Representation Learning: A Novel\nApproach to Multi-Label Classification for Long Texts\nAnonymous ACL submission\nAbstract\nMulti-label text classification (MLTC) is an important task in the field of natural language processing. Most existing models rely on highquality text representations provided by pretrained language models (PLMs). They hence face the challenge of input length limitation caused by PLMs, when dealing with long texts. In light of this, we introduce a comprehensive approach to multi-label long text classification. We propose a text segmentation algorithm, which guarantees to produce the optimal segmentation, to address the issue of input length limitation caused by PLMs. We incorporate external knowledge, labels’ co-occurrence relations, and attention mechanisms in representation learning to enhance both text and label representations. Our method’s effectiveness is validated through extensive experiments on various MLTC datasets, unraveling the intricate correlations between texts and labels. 1 Introduction Multi-label text classification (MLTC) aims to assign multiple relevant labels to a given text. It plays a crucial role in various applications such as content recommendation (Wu et al., 2023), sentiment analysis (Sentiment, 2020), and information retrieval (Henzinger et al., 2023). The pre-trained language models (PLMs), e.g., BERT (Devlin et al., 2018) and Roberta (Liu et al., 2019) significantly improve the performance of various natural language processing tasks including MLTC. However, PLMs have an inherent limitation in handling long texts due to their maximum input sequence length of 512 tokens. This limitation could lead to information missing when dealing with long texts. More than this, text representations directly obtained from PLMs should be enhanced further, by incorporating e.g., domain knowledge (Liu et al., 2023). Another difficulty for MLTC arises from intricate labellabel and word-label relations. These complicated 041relations, if effectively captured, are expected to improve prediction performance. To overcome the input length limitation of PLMs, investigators have proposed a series of techniques that are based on e.g., text truncation (Park et al., 2022), sparse attention mechanism (Zaheer et al., 2020; Beltagy et al., 2020), and text segmentation (Ding et al., 2020; Jaiswal and Milios, 2023). Specifically, Park et al. (2022) proposes two methods Bert+Rank and Bert+Random. Bert+Rank concatenates the first 512 tokens of a text representation with the representations of the top-ranked sentences (up to 512 tokens). Unlike Bert+Rank, Bert+Random concatenates the first 512 tokens of a text representation with at most 512 tokens randomly selected from the text corpus. These methods often perform poorly due to the loss of text information. The reason why PLMs limit the input length is that they incorporate a self-attention mechanism, whose computational complexity is in quadratic time of the input length. Therefore, Bigbird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020) utilize the sparse self-attention mechanism to handle longer input sequences. Nevertheless, they haven’t fundamentally solved the issue and can not process sequences of arbitrary length. Ding et al. (2020) proposes a dynamic programming method, named CogLTX, for text segmentation. Jaiswal and Milios (2023) splits a long text into fixed-length chunks, encodes these chunks by BERT, and concatenates the representations of chunks as the representation of the long text. However, the methods above do not take into consideration the semantic change before and after segmentation, hence may damage the semantic coherence of a text. The above methods mainly focus on the content of texts alone, ignore the integration of external knowledge. As pointed out by Liu et al. (2023), the utilization of high-quality external knowledge can enhance text representations, and then improve 1\nprediction accuracy. However, to incorporate external knowledge, two crucial questions have to be answered: (a) how to establish the connection between texts and external knowledge? (b) how to develop a model to produce text-related knowledge representations? If the above issues were not effectively resolved, external knowledge would become noise and may worsen prediction performance. Label correlations have been explored in MLTC. A typical model is LDGN Ma et al. (2021). The model first captures labels’ co-occurrence relation and then employs the Graph Convolutional Network (GCN) to update label representations based on the co-occurrence relation. However, LDGN initializes label representations randomly without considering the labels’ semantic information. Indeed, when the semantic information is incorporated, not only label representations can be improved, but also the importance of words to labels can be captured. To tackle the issues above, we propose a model (SKFRL ) that integrates dynamic Segmentation, Knowledge Fusion, and Representation Learning for multi-label long text classification. SKFRL is equipped with a text segmentation method to partition a long text into sub-texts. It also fuses text representations with external knowledge as enhanced text representations. It further enhances both text and label representations by capturing intricate label-label and word-label relations. In summary, our contributions are as follows: •We propose a text segmentation method, named as TXTSEGto address the input length limitation of PLMs. TXTSEGmeasures the cost of a segmentation on a long text by taking context information of each cut point into consideration, and moves towards the optimal solution via dynamic programming. •We introduce a knowledge fusing method that fuses text representations from PLMs with concept representations from external knowledge, to obtain updated text representations. •We achieve enhanced label and text representations learning by integrating external knowledge, label co-occurrence relation and max attention schema. •We conduct extensive experiments on benchmark datasets to validate the superiority of our model SKFRL compared with other baseline models. The source code of SKFRL is 131available on GitHub1. 2 Related Work Multi-Label Text Classification. MLTC involves assigning multiple relevant labels to a single text, which is more complex than single-label classification. Traditional deep learning methods such as Convolutional Neural Networks (CNNs) (Liu et al., 2017) and Recurrent Neural Networks (RNNs) (Nam et al., 2017) are widely used in MLTC. However, the methods above often overlook the correlation between texts and labels. To address this issue, a method named CORE is introduced by Zhang et al. (2021). CORE takes a text and all the labels as input simultaneously, hence obtains the representations of the text and all the labels in the same space. However, CORE struggles with long texts or largescale label datasets. Zhang et al. (2023) introduces a two-stage label reduction method that reduces the number of labels through association rules and label merging. The label reduction method heavily depends on the distribution of high-frequency labels in a dataset despite its effectiveness. You et al. (2019) and Xiao et al. (2019) utilize attention mechanisms to construct label-specific text representations, which capture the importance of each word to each label. The correlation among labels is also crucial and plays a key role in MLTC (Yang et al., 2018; Ma et al., 2021). Yang et al. (2018) explores the label-label correlations by treating labels as a sequence and then using the sequence generation model to effectively capture high-order correlations among labels. However, label sequence that is derived from an unordered set, inevitably influences the model’s performance. Although LDGN Ma et al. (2021) captures the co-occurrence relation among labels in prediction, it overlooks labels’ semantic information but simply initializes label representations with a randomized strategy. Extensions on PLMs. Pre-trained language models like BERT (Devlin et al., 2018), Roberta (Liu et al., 2019) significantly advance the field of natural language process. However, one limitation of PLMs is their fixed input sequence length, typically capped at 512 tokens. This limitation causes a significant challenge when dealing with long texts. Sun et al. (2019) directly truncates a text and discards the remaining part. Pappagari et al. (2019); Jin et al. (2021) simply split a long text into small segments to meet the input length of PLMs. 1https://anonymous.4open.science/r/SKFRL-12\n( a )  L o n g  t e x t  s e g m e n t a t i o n  ( b )  E x t e r n a l  k n o w l e d g e  f u s i o n  \n( c )   E n h a n c e d  l a b e l  r e p r e s e n t a t i o n  l e a r n i n g( d )                   f o r  w o r d s  a n d  l a b e l s\nBERT\nFC\nGCN\ncatcat\ncatExt ernal\nKno wledgeFigure 1: The Architecture of SKFRL .SKFRL consists of four parts. (a) Long text segmentation ( TXTSEG):\nTXTSEGsplits the long text Tiinto several sub-texts x1\ni, xm\ni, xn\niby dynamic programming. ( Wicontains the words\nof sub-texts.) (b) External knowledge fusion: External knowledge Eg\niis incorporated into text representation H(w)\ni,\nresulting in the fused text representation H(w)′\ni. (c) Enhanced label representation: Similar to (b), the external\nknowledge of label Elis incorporated into label representation hland obtains the fused label representation Hl.\nGCN is employed to obtain the enhanced label representation H′\nl. (d) MAXATTfor words and labels: A max\nattention mechanism ( MAXATT) is used to calculate the importance of each word to each label. In addition, H(w)′′\ni\nis the label-weighted text representation obtained through (d), H(cls)\ni is the representation of the special token [CLS].\nDing et al. (2020) proposes an objective function, that measures the goodness of a segmentation purely by cut points, and cuts a long text into sub-texts via optimizing the objective function. Jaiswal and Milios (2023) splits a long text into fixed-length chunks and processes these chunks by BERT independently. However, the segmentation of the above methods does not consider the semantic information and the coherence of a text, hence may damage the semantic information of the original text. Chen et al. (2022) proposes a digest algorithm to extract more important sentences at the beginning and end of a text. Recently, extended transformer architectures enable PLMs to handle longer sequences by introducing sparse attention mechanisms. Typical works include TransformerXL (Dai et al., 2019), Reformer (Kitaev et al., 2020), Longformer (Beltagy et al., 2020), and BigBird (Zaheer et al., 2020). Specifically, Dai et al. (2019) develops a segment-level recurrence mechanism along with a novel positional encoding scheme. Kitaev et al. (2020) replaces the dotproduct attention by one that uses locality-sensitive 203hashing, to reduce computational complexity. Longformer (Beltagy et al., 2020) combines local windowed attention with a task motivated global attention. BigBird (Zaheer et al., 2020) designs a sparse attention mechanism to reduce the quadratic dependency to linear. Though they attempt to reduce complexity by introduce various attention mechanisms, they still face challenges in balancing computational efficiency and model performance. Our goal is threefold. (1) We develop a text segmentation method that segments a long text by considering semantic coherence, to address the input length constraints of PLMs. (2) We incorporate external knowledge to enrich both text and label representations. (3) We leverage label-label and wordlabel relations to further enhance text and label representations. By doing so, we aim to obtain a robust model for multi-label long text classification. 3 Method In this section, we introduce an approach to addressing the issue of MLTC on long texts. As shown in Figure 1, our approach consists of four 3\nparts, i.e.,Long text segmentation, External knowledge fusion, Enhanced label representation learning, and Representation enhancing with attention (MAXATTfor words and labels). Overall, give a long text, SKFRL first cuts it into several sub-texts with the text segmentation algorithm TXTSEG. After segmentation, all the sub-texts can be encoded via BERT directly, thereby addressing the issue of input length limit. SKFRL then introduces external knowledge to enhance the representation of each sub-text. Meanwhile, SKFRL processes the labels along the similar line as that on texts and gains the fused label representations. It then updates the fused label representations through GCN and obtains the enhanced label representations. Finally, to capture the importance of words to different labels, SKFRL utilizes the max attention mechanism ( MAXATT) to obtain the label-weight text representations. We next elaborate on each of them, starting from the problem formulation. Problem Formulation: LetD={(Ti, Yi)}N\ni=1be the given training text set containing total N texts with corresponding labels. Here Tirefers to the i-th text in D,Yi={l1\ni,···, lL\ni}is the corresponding label set of 0,1 for Ti, andLdenotes the total number of labels. The goal is to train a classifier, which predicts the most relevant labels for a given text, which might be quite long and exceeds the input length limit of a PLM . 3.1 Long text segmentation To overcome the input length limitation of PLMs, we develop an algorithm TXTSEG, for long text segmentation. The pseudo-code of TXTSEGis shown in Algorithm 1. Its core idea is as follows. Core idea. TXTSEGworks in three steps. It first identifies a set of cut points, along with their costs, and maintains them in poses . Based on poses , it finds the set of cut points that has the least total cost, by following dynamic programming. Algorithm . We start from the cost metric, followed by the algorithm details. Cost metric . Specifically, for each character sin a text sequence T, we define a metric ν(s)to evaluate its cost when sis chosen as a cut point. ν(s) =pc(s) +sc(s) (1) Here, function pc(s)sets a value for a character sbased on the type of s; in this work, period, question mark, and exclamation mark have a cost of 1, 273Algorithm 1 TXTSEG\nInput: Long text T, maximum length max\nOutput: Listsegments containing the cut points\n1:Initialize poses\n2:Initialize arrays dp,split ; a list segments\n3:dp[0]←0\n4:fori←1tolen(poses )−1do\n5: forj←0toi−1do\n6: ifposes [i][0]−poses [j][0]≤maxthen\n7: cost←dp[j] +poses [i][1]\n8: ifcost < dp [i]then\n9: dp[i]←cost\n10: split[i]←j\n11: end if\n12: end if\n13: end for\n14:end for\n15:i←len(poses )−1\n16:while i >0do\n17: segments. append (poses [i][0])\n18: i←split[i]\n19:end while\n20:segments. reverse ()\n21:return segments\ncomma has a cost of 2, and the cost of other characters is set as 8, by following Ding et al. (2020). Function sc(s)is defined as⃗ a·⃗b\n|⃗ a||⃗b|, where ⃗ a,⃗bare the vectors including 20 tokens before and after s inT, respectively. Given a long text Tand the maximum length max(set as 511), TXTSEGfirst identifies all the punctuation as cut points. For each pair of consecutive cut points cps,cpe, if the length of the sub-text T(cps, cpe)exceeds max,TXTSEGrepeatedly performs the following task to identify more cut points. That is, it identifies a new cut point cpn, that is max distance away from cpswithin T(cps, cpe), and updates T(cps, cpe)withT(cpn, cpe). When all the cut points are recognized, TXTSEGcalculates their costs by following Eq. (1)and maintains cut points and their costs in array poses (line 1). The i-th entry in poses is a pair of position and cost. TXTSEG also initializes two arrays dpandsplit (line 2). The array dpkeeps track of the minimum cost from the first cut point to the current cut point; while the arraysplit is used to store the index of the previous best cut point for the current cut point. Afterwards, TXTSEGidentifies the best set of cut points by following dynamic programming (lines 3-14). Specifi4\nGCN\nP P M I  M a t r i xC o n c e p t  G r a p hE n h a n c e d  C o n c e p t  G r a p hFigure 2: The working flow of enhancing the concept\nrepresentation. Each item PPMI (wi, wj)in the PPMI\nMatrix denotes the PPMI value of words wiandwj\nin the dataset. kis the concept node, and n1, n2, n3\nare the first-order neighbors of concept k.k′is\nthe enhanced concept node, and n′\n1, n′\n2, n′\n3are the\nenhanced first-order neighbors of concept k′.\ncally, for the i-th (i∈[1, len(poses )−1]) cut point poses [i][0],TXTSEGiterates through all previous cut points poses [j][0]and evaluates the cost of the j-th and the i-th cut points. If the length of the subtext does not exceed max,TXTSEGcalculates the cost and updates dpandsplit accordingly. After completion of the above process, TXTSEGbacktracks to reconstruct the cut points (lines 16-19). Finally, TXTSEGreturns segments that store the best cut points (line 21). Using TXTSEG, a long text Tican be divided into several sub-texts, i.e., Ti={x1\ni, ..., xm\ni, ..., xn\ni}, where ndenotes the number of sub-texts, which will be used for subsequent operations e.g., external knowledge fusion, and MAXATTcalculation of words and labels. Hence, we illustrate subsequent operations on sub-texts, for the sake of brevity. 3.2 External knowledge fusion To enhance the text representation, we fuse it with external knowledge. In this work, a publicly available knowledge graph CONCEPT NET(Speer et al., 2017) that is composed of a large number of triples like ‘ entity -relationship -entity ’ is applied. Given a sub-text xm\ni={w1\ni, ..., wj\ni, ..., wQ\ni} withQwords, we match each word in xm\niwith the corresponding concept in ConceptNet and obtain the concept sequence Ki={k1\ni, ..., kj\ni, ..., kQ\ni}. If no concept corresponding to wj\niis found, kj\niis set as [PAD]. After recognizing all the concepts in a subtext, we identify the first-order neighbors from CONCEPT NETand construct a concept graph Gj\nibased on the first-order neighbors for each concept kj\ni. Later on, TRANS E(Bordes et al., 2013) is employed to train these concept graphs, and the representation of each concept graph is used as the external knowledge representation Et\ni, where 336Et\ni={e(t,1)\ni, ..., e(t,j)\ni, ..., e(t,Q)\ni} ∈RQ×D,e(t,j)\niis the representation of the concept kj\ni, and D denotes the dimension of word embedding. We next incorporate the co-occurrence relationship of text words into the external knowledge representation, to further enhance their representation. The process is shown in Figure 2. Specifically, we first construct the positive pointwise mutual information (PPMI) of the text words with the following formulas: PMI(wi, wj) = log\u0012P(wi, wj)\nP(wi)P(wj)\u0013\n(2) PPMI (wi, wj) = max( PMI(wi, wj),0) (3) where P(wi)andP(wj)are the probabilities that words wiandwjappear in the training set, respectively; P(wi, wj)is the co-occurrence probability that words wiandwjappear together in a sliding window of size 20 of a text. The size of the sliding window is suggested by Lin et al. (2021). We then assign edge weights to concept graphs as follows: for each edge (va, vb) in a concept graph Gj\ni, we update its weight w(va, vb)with PPMI (wa, wb)if(wa, wb)is in the matrix, otherwise, we set w(va, vb)as 0. Here, wa(resp. wb) is the corresponding word of va (resp. vb). After updating the edge weights, we obtain a normalized adjacency matrix ˆAof each concept graph, with Eq. (5)(will be given in Section 3.3). Lastly, we employ GCN to update the representation of a concept graph Gj\niwith its corresponding ˆA, extract the representation of each concept e(g,j)\ni from Gj\niand obtain an enhanced representation Eg\ni={e(g,1)\ni, ..., e(g,j)\ni, ..., e(g,Q)\ni} ∈RQ×Das the enhanced representation of Ki. In addition, we encode the sub-text xm\nivia a PLM and obtain two representations. One is the output of the sequence of the last hidden layer H(w)\ni={h(w1)\ni, ..., h(wj)\ni, ..., h(wQ)\ni} ∈RQ×Dand the other one is the output of [CLS] token H(cls)\ni∈R1×D, which will be used later. We incorporate H(w)\niwith external knowledge representation Eg\niwith Eq. (4)and obtains the final representation of xm\ni. H(w)′\ni=H(w)\ni+Eg\ni (4) where H(w)′\ni∈RQ×Dis the fused text representation. 5\n3.3 Enhanced label representation In MLTC, a label, e.g.,‘international affairs ’, often carries semantic information. The application of semantic information has been proven effective in improving classification performance. Hence, we first encode all the labels in a dataset through a PLM and obtain their initial representations. Along the similar line to the knowledge fusion on texts, we also fuse the label representations with external knowledge. In contrast to the fusion operation on texts, we use GraphSAGE (Hamilton et al., 2017) instead of GCN to update label representations, since label words are much less than text words. After fusion, we obtain the label representations Hl∈RL×D, where Lis the number of labels. Prior works show that the co-occurrence relation among labels is useful in classification. To this end, we further enhance the label representations by incorporating the co-occurrence relation of labels. Specifically, we first construct a label co-occurrence graph, where each node represents a label and each edge indicates the label cooccurrence between two labels, from the training set. We next obtain the adjacency matrix Afrom the label co-occurrence graph and normalize it as ˆAwith Eq. (5): ˆA=D−1\n2AD−1\n2 (5) where D∈RL×Lis a diagnal degree matrix. We finally feed the label representations Hland ˆAinto GCN and obtain the enhanced label representations (Eq. (6)): H′l=LeakyReLU (ˆAH lW1) (6) where W1∈RD×Dis a learnable transformation matrix, and H′l∈RL×Dis the label representations with the labels’ co-occurrence relation incorporated. 3.4 Enhancing text representation with max attention mechanism In MLTC, the importance of each word in a text is different for labels. Taking “ The football club’s stock soared after signing a major sponsorship deal with a top bank ” as an example, it is clear that the word ‘ football ’ is the most important word for the label ‘ sports ’. It is hence necessary to capture the importance of a word to a label and enhance text representation with the attention. To this end, we introduce an 429Algorithm 2 MAXATT\nInput: Text and label representations H(w)′\ni,H′l.\nOutput: Updated label representation.\n/*calculate the importance of word-to-label */\n1:M←H(w)′\ni·H′T\nl\n2:Ma← {tanh(max( M[i]))|i∈[1 :Q]}\n3:⃗m←SOFTMAX(Ma)\n/* compute the label-weighted text representation */\n4:H(w)′′\ni←mean (⃗m·H(w)′\ni)\n/* obtain the final text representation */\n5:return H(w)′′\ni\nattention mechanism, which is named MAXATT and works as Algorithm 2. Taking a text representation H(w)′\niand a label representation H′las input, MAXATTfirst obtains a word-to-label weight matrix Mby multiplying H(w)′\niwithH′l(line 1). MAXATTthen performs max pooling and nonlinear transformation on each row of M, to obtain the max attention vector ⃗mof the text (lines 2-3). Next, MAXATTmultiplies ⃗m byH(w)′\niand applies average pooling to obtain the label-weighted text representation H(w)′′\ni (line 4). Model learning & testing . Given a label-weighted representation H(w)′′\ni,SKFRL concatenates it with H(cls)\nito obtain the final text representation H(w,f)\ni, and feeds H(w,f)\niinto a fully connected layer to produce the predicted results. To train the model, we select the Binary Cross-Entropy (BCE), which is commonly used in MLTC task, as the loss function (Ma et al., 2021; Xiao et al., 2019). Lij\nbce=−(yj\nilog(ˆyj\ni) + (1−yj\ni) log(1 −ˆyj\ni))(7) Lbce=−1\nNNX\ni=1LX\nj=1Lij\nbce(8) where NandLdenote the total number of texts and labels, respectively; yj\niandˆyj\nirepresent the ground truth (0 or 1) and the prediction result of thej-th label for the i-th text, respectively. To predict the labels of a text Ti,SKFRL first outputs predicted results {ˆy1\ni, ...,ˆym\ni, ...,ˆyn\ni}for each of its sub-text, performs max pooling (Eq. (9)) over the results of sub-texts, and picks labels whose predicted values ˆym\ni(m∈[1, n]) are above a threshold (0.5 in this work). ˆyi=max(ˆy1\ni, ...,ˆym\ni, ...,ˆyn\ni) (9) 6\nwhere ˆyiis a vector of dimension Lthat serves as the label prediction result of the text Ti. 4 Experiment 4.1 Experimental Setup Datasets. To ensure a fair comparison with the baseline, we select four MLTC benchmark datasets. Table 1 shows the details of the four datasets. CMU Book Summary is an MLTC dataset containing 12788 samples (texts) with multiple labels (up to 227). Pairs of book summaries are created by combining text pairs from the CMU Book Summary dataset. EURLEX-57K is a large-scale MLTC dataset containing 57,000 European Union legal texts with labels (up to 4271). Inverted EURLEX57K is a modified version of the EURLEX-57K dataset in which the order of chapters is reversed to ensure that the key information focuses on the end of the text. Baselines. The following baselines are compared with our proposed method. •Bert (Devlin et al., 2018) is adopted by (Park et al., 2022) to conduct MLTC, which achieves a good result. •Bert+TextRank (Park et al., 2022) obtains the features of MLTC by concatenating the representation of the first 512 tokens of the sentence with the representation of the topranked sentences. •Bert+Random (Park et al., 2022) concatenates the representation of the first 512 text tokens with the other 512 text tokens randomly selected from the text to conduct MLTC. •Longformer (Beltagy et al., 2020) utilizes a sparse attention mechanism that scales linearly with sequence length, to make it easy to process more tokens (up to 4,096 tokens). •ToBERT (Pappagari et al., 2019) adopts a hierarchical approach to processing long texts for the MLTC task. •CogLTX (Ding et al., 2020) splits long text into chunks by dynamic programming. The chunks with high scores are used for MLTC. •ChunkBERT (Jaiswal and Milios, 2023) splits a long text into chunks with fixed-length. The concatenated chunk representation is used for MLTC. 507Parameter setting. We implement our model in PyTorch and train it with 1 NVIDIA 4090. BERT is used for the PLM. The maximum text length and batch size are set to 512 (including the [CLS] token) and 2 respectively. The learning rate is 3e-5 for EURLEX and Paired Summary, 5e-5 for Inverted EURLEX, and 7e-6 for Book Summary. We train our model for 20 epochs. The checkpoint with the best performance is used to test our proposed model. Evaluation metrics. We use Micro- F1(%) as the evaluation metric. 4.2 Results and Analysis Our proposed SKFRL is evaluated with various baseline methods across four benchmark datasets. To evaluate the performance of SKFRL on long text, the texts (tokens exceeding 512) of four benchmark datasets are picked out as a new test set. In addition, the results on the full test set ( i.e.,long texts are not picked out) are given in the Appendix (see the Appendix A.1 for details). Table 2 shows the results on long texts. Our proposed SKFRL achieves the state-of-the-art performance on 4 datasets. Specifically, SKFRL improves the previous best results from 67.03, 64.31, 62.34, 58.55 to 67.12 (+0.09), 66.04 (+1.73), 62.47 (+0.13), 62.92 (+4.37) respectively. In addition, the simple method, i.e.,Bert, performs well on the first three datasets, except for the Paired Summary. Compared with Bert, Bert+TextRank and Bert+Random achieve a better result by enhancing the text representation with incorporating additional information from TextRank or random tokens. In addition, while the performance of popular models such as Longformer, ToBERT, ChunkBERT, and CogLTX achieve encouraging results, their performance still is limited by long text. Different from the methods above, our proposed SKFRL leverages a novel long-text segmentation method and knowledge fusion strategy to improve the performance of MLTC on long texts. Experimental results also show that SKFRL achieves a remarkable improvement on MLTC of long texts. 4.3 Ablation Study We conduct ablation studies to verify the effectiveness of the sub-modules in SKFRL . As shown in Table 3, our observations are as follows: (1) Our proposed SKFRL outperforms all variants based on SKFRL. It is shown that each sub-module is effective on improving the performance. (2) Compared with Variant 1, Variant 2 performs better, 7\nDataset #Total #Train #Dev #Test #Labels #BERT Tokens %Long\nEURLEX-57K 57000 45000 6000 6000 4271 707.99 ±538.69 51.3\n- Inverted 57000 45000 6000 6000 4271 707.99 ±538.69 51.3\nBook Summary 12788 10230 1279 1279 227 574.31 ±659.56 38.46\n- Paired 6393 5115 639 639 227 1148.62 ±933.97 75.54\nTable 1: Statistical information of the datasets. #BERT Tokens indicates the average token number obtained via the\nBERT tokenizer. %Long means the proportion of texts exceeding 512 BERT tokens.\nModel EURLEXInverted\nEURLEXBook\nSummaryPaired\nSummary\nBert 66.76 62.88 60.56 52.23\nBert+TextRank 66.56 64.22 61.76 56.24\nBert+Random 67.03 64.31 62.34 56.77\nLongformer 44.66 47.00 59.66 58.55\nToBERT 61.85 59.50 61.38 58.17\nCogLTX 61.95 63.00 60.71 55.74\nChunkBERT 64.94 62.94 57.80 57.73\nSKFRL 67.12 66.04 62.47 62.92\nTable 2: Model performances on long texts. The highest\nand second-highest scores are bolded and underlined.\nVariant EKF ELR ATT EURLEXInverted\nEURLEXBook\nSummaryPaired\nSummary\n1 × × × 65.99 64.76 61.37 61.23\n2 ✓ × × 66.21 65.02 61.58 61.53\n3 ✓ × ✓ 66.91 65.77 62.10 62.57\nSKFRL ✓ ✓ ✓ 67.12 66.04 62.47 62.92\nTable 3: The results of the ablation study. EKF and\nELR denote the external knowledge fusion and en-\nhanced label representation learning respectively. ATT\nrefers to the MAXATTfor word and label. ’ ✓’ (resp.\n’×’) denote the Variant with (resp. without) the corre-\nsponding operations ( i.e.,EKF ,ATT , andELR ).\nwhich indicates the effectiveness of EKF . (3) Compared with Variant 2, Variant 3 adds the max attention mechanism MAXATT. As we mentioned in Section 3.4,MAXATTcan help the model learn the importance of words for different labels, resulting in a great improvement. (4) The performance ofSKFRL is better than Variant 3, verifying the effectiveness of ELR .ELR enhances the label representation through GCN, which enables the model to capture the label semantic information and the label co-occurrence relation. In addition, in Section 3.1, different from Ding et al. (2020), we incorporate text semantic similarity as a cost of segmentation. In Section 3.2, we introduce PPMI and employ GCN to obtain the enhanced text external knowledge representation, which differs from the GraphSAGE method used by Liu et al. (2023). To verify the effectiveness of our improved methods, we conduct ablation study. The results are shown in Table 4. The ex- 577Variant EURLEXInverted\nEURLEXBook\nSummaryPaired\nSummary\nSegp+KGs 66.67 65.56 61.93 62.19\nSegp+s+KGs 69.91 65.88 62.36 62.68\nSegp+s+KGp(SKFRL) 67.12 66.04 62.47 62.92\nTable 4: Ablation study results of long text segmentation\nand external knowledge training method. Seg pdenotes\nthat the text segmentation is conducted by punctuation.\nSeg p+sdenotes that the text segmentation is performed\nby punctuation and text semantic similarity. KG sde-\nnotes that text representation is enhanced by external\nknowledge with GraphSAGE. KG pdenotes that the\ntext representation is enhanced by external knowledge\nwith PPMI and GCN.\nperimental results show that our proposed method Segp+s+KGpachieves the best performance compared with other variant methods ( i.e.,Segp+KGs andSegp+s+KGg). Specifically, the model with Segp+KGscan achieve good results across the four datasets. Once the text segmentation method (i.e.,Segp+s+KGs) is applied, the performance of the model is improved. The performance of the model is further improved and optimized on all datasets by introducing the external knowledge representation with PPMI and GCN method ( i.e., Segp+s+KGp). 5 Conclusion In this work, we propose a novel model for multilabel long text classification. The model achieves superior text segmentation by following dynamic programming, thereby addressing the issue of input length limitation caused by PLMs. The use of external knowledge enriches the text representation encoded by PLMs effectively. To further enhance the text representation, GCN and max-attention mechanisms are utilized to obtain the label-weighted text representation. Experimental results show that our model outperforms baselines, which confirms the effectiveness of our model in text segmentation and incorporation of external knowledge. 8\nLimitations There are some limitations for our work. Firstly, due to the limitation of computational resources, we only verify our method on the BERT-base. Extending our experiments to larger PLMs would make our work more convincing. Secondly, the performance of the model could be further improved by employing the correlation among sub-texts. We will solve the limitations above in the future. 612",
        "aspect_review": "The paper introduces a novel text segmentation algorithm (TXTSEG) that considers semantic coherence, which is groundbreaking in addressing the input length limitations of pre-trained language models. The integration of external knowledge and attention mechanisms further enhances representation learning, showcasing strong technical depth and rigor. The comprehensive experimental validation across multiple benchmark datasets demonstrates the significant practical impact of the proposed approach. The model consistently outperforms existing methods, indicating its robustness and effectiveness in multi-label long text classification.\n\nWhile the results are impressive, the paper could benefit from minor writing or clarity improvements to better highlight the significance of the findings. Small details on hyperparameters or implementation specifics are not thoroughly discussed, which could aid in reproducibility. The comparison with baseline models is strong, but a slight clarification on code availability would enhance the paper's utility for the community. Very minor formatting issues in figures could be addressed to improve the overall presentation. While the methodology is robust, minor details on the hyperparameters used in the experiments could be provided for completeness.",
        "general_review": "The paper presents a notable novelty in addressing the input length limitation of pre-trained language models for multi-label long text classification by introducing a dynamic text segmentation algorithm. It demonstrates strong technical depth and rigor through the integration of external knowledge, label co-occurrence relations, and attention mechanisms to enhance text and label representations. The comprehensive experimental validation across multiple benchmark datasets highlights the model's superior performance against existing methods. The significant practical impact is evident in its potential applications across various natural language processing tasks that involve long texts.\n\n1. Minor writing or clarity improvements: Some sections of the paper could benefit from clearer explanations, particularly in the methodology section. 2. Small details on hyperparameters or implementation: The paper could provide more detailed information on the choice of hyperparameters and implementation details. 3. Very minor formatting on figures: Some figures could be formatted more clearly to enhance readability. 4. Slight clarification on code availability: Although the paper mentions the availability of the source code, providing a direct link or more specific instructions could be beneficial."
    },
    "26": {
        "paper": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided\nReward Ensemble\nAnonymous EMNLP submission\nAbstract\nEmploying large language models (LLMs) to enable embodied agents has become popular, yet it presents several limitations in practice. In this work, rather than using LLMs directly as agents, we explore their use as tools for embodied agent learning. Specifically, to train separate agents via offline reinforcement learning (RL), an LLM is used to provide dense reward feedback on individual actions in training datasets. In doing so, we present a consistencyguided reward ensemble framework ( COREN), designed for tackling difficulties in grounding LLM-generated estimates to the target environment domain. The framework employs an adaptive ensemble of spatio-temporally consistent rewards to derive domain-grounded rewards in the training datasets, thus enabling effective offline learning of embodied agents in different environment domains. Experiments with the VirtualHome benchmark demonstrate that CORENsignificantly outperforms other offline RL agents, and it also achieves comparable performance to state-of-the-art LLM-based agents with 8B parameters, despite CORENhaving only 117M parameters for the agent policy network and using LLMs only for training. 1 Introduction Developing embodied agents capable of understanding user instructions and executing tasks in physical environments represents a crucial milestone in the pursuit of general AI. Recent advancements in large language models (LLMs) have demonstrated their remarkable reasoning capabilities, paving the way for their application in embodied agents (Yang et al., 2023; Padmakumar et al., 2023; Pantazopoulos et al., 2023; Yun et al., 2023; Logeswaran et al., 2022; Ichter et al., 2022). Yet, deploying an LLM directly as part of an embodied agent presents several inefficiencies, such as the need for sophisticated environmentspecific prompt design, substantial computational 041resource demands, and inherent model inference latency (Hashemzadeh et al., 2024). These factors can limit the practical application of LLMs, particularly in scenarios where embodied agents are required to respond rapidly and efficiently. In the literature of reinforcement learning (RL), data-centric offline learning approaches have been explored (Kumar et al., 2020a). These offline RL approaches are designed to establish efficient agent structures, necessitating datasets that include wellannotated agent trajectories with reward information. However, the characteristics of instructionfollowing tasks assigned to embodied agents, particularly their long-horizon goal-reaching nature, often conflict with such dense data requirements of offline RL. Embodied agents normally can produce trajectories with sparse reward feedback, because their instruction-following tasks are evaluated based on binary outcomes of success or failure, which directly align with the specific goals of the instructions. In offline RL, this sparse reward setting poses significant challenges in achieving effective agent policies (Park et al., 2023; Ma et al., 2022). In this work, we explore LLMs for offline RL. By employing capable LLMs as a reward estimator that provides immediate feedback on agent actions, we augment the trajectory dataset with dense reward information. This method, LLM-based reward estimation is capable of significantly enhancing the effectiveness of offline RL for embodied agents. To do so, we address the limitations inherent in LLM-based reward estimation. A primary challenge arises from the limited interaction with the environment in an offline setting, which complicates the LLMs’ ability to acquire essential environmental knowledge. The offline setting makes it difficult to ensure that the generated rewards are properly grounded in the specific domain of the environment. For instance, without explicit knowledge that bread is typically stored in a pantry in the target environment, an LLM might struggle to 1\nAction 𝒍𝒕\nLow LatencyResource Efficiency\n✓𝑟̂𝑟!𝑟\"𝑟#\n[Deployment]Domain-GroundedRewardOffline  DatasetInstruction 𝒊“You can see table, …”\nEmbodied Agent\nfOffline RL(i) Spatio-temporally Consistent Rewards(ii) Reward Ensemble[Offline Training]\nReward-Augmented Dataset\nObservation 𝒐𝒕\nTemporalContextualStructural“Open microwave”“Relax onsofa”Figure 1: COREN, a framework for LLM-based reward\nestimation and offline learning. In (i), an LLM estimates\nrewards based on spatio-temporal (i.e., contextual, struc-\ntural, and temporal) consistencies; In (ii), these rewards\nare integrated into a single domain-grounded reward\nvia an ensemble. Using the reward-augmented dataset,\noffline RL can be conducted effectively to achieve em-\nbodied agents with resource efficiency and low latency.\naccurately assign rewards for actions like “go to pantry” versus “go to fridge” when tasked with retrieving bread. While both actions might seem reasonable from a commonsense perspective, the optimal action depends on specific conditions of the target environment that the LLM may not have access to in the offline setting. In response to these challenges, we present COREN, a consistency-guided reward ensemble framework, specifically designed for robust LLMbased reward estimation and effective agent offline learning. It adopts a two-staged reward estimation process, as depicted in Figure 1. (i) An LLM is first queried to estimate several types of rewards for actions, each considering a distinct spatio-temporal consistency criterion of the LLM to have coherent and domain-grounded rewards. (ii) Then, these rewards are further orchestrated, being unified into domain-specifically tuned rewards via an alignment process with the sparse rewards of given trajectories. The resulting agent, trained on the unified dense rewards by offline RL, is capable of performing instruction-following tasks with high efficiency and minimal latency at deployment. This offline RL scheme, enhanced by LLM-based reward estimation, overcomes the limitations faced by the agents that rely on the online exploitation of LLMs. The contributions of our work can be summarized as follows: (i)addressing a practical yet chal- 111lenging problem of embodied agent offline learning using LLMs for the first time; (ii)proposing a twostaged reward estimation algorithm guided by a spatio-temporal consistency ensemble; and (iii)extensive evaluation on the VirtualHome benchmark, demonstrating performance comparable to state-ofthe-art LLM-based online agents. 2 Preliminaries 2.1 Goal-POMDPs For an embodied agent that follows user-specified instructions, we model their environment as a goal-conditioned partially observable Markov decision process (Goal-POMDP). A Goal-POMDP is represented by a tuple ( S,A,P,R,γ,Ω,O, G) (Song et al., 2023a; Singh et al., 2023) with states s∈ S, actions a∈ A , a transition functionP:S × A × S −→ ∆(S), a reward function R:S×A×Gω17→R, a discount factor γ∈[0,1), observations o∈Ω, an observation transition functionO:S ×A −→ Ω, and goal conditions G∈ G. Given this Goal-POMDP representation, we consider a user-specified instruction ias a series of goal conditions G= (G1,···)⊆ G such that the embodied agent is tasked with completing each of the specified goal conditions for the instruction i. 2.2 Offline RL For a Goal-POMDP, its optimal policy is formulated by π∗= argmax\nπE\n(s,a)∼π,\nG∼G\"X\ntγtR(s, a,G)#\n.(1) To achieve the optimal policy, we explore offline RL approaches where the policy is derived by optimizing the Bellman error objective, relying exclusively on an offline dataset Dwithout any environment interaction. Offline RL is particularly beneficial for embodied agents, as it reduces the risks and costs associated with active exploration of the environment with physical objects. We utilize D={(ij, τj) :j}where τjis a trajectory corresponding to instruction ij. Unlike conventional offline RL, this dataset Dincorporates sparse rewards. This sparsity is reflected in a subset of trajectories that are marked by a success flag fs(ij, τj), indicating whether τjhas satisfied all the requisite goal conditions for the instruction ij. This sparse 1Xωfor a set Xis all possible finite products of X.\n2\n𝑜!𝑜\"…𝑟!#\"𝑟##\"𝑟!$\"𝑟#$\"Prompt 𝒫!\nMajority vote\n⋯\nTrajectory 𝜏forinstruction 𝑖.𝑯𝒏𝝉=\t{𝑙&!':𝑡}:High-value actions𝑙&\"'𝑙&#'…\n𝑙(\"'𝑙(#'\nBackward-Verificationq(𝑖,𝜏,𝑛):“Is performing 𝐻\"(𝜏)from observation 𝑜reasonable to accomplish the instruction 𝑖?”𝑹𝒏𝑻𝒐𝒕,𝒍𝒕\t𝒊)from 𝒫\"Prompt 𝒫$+\nObservation 𝑜&: “You can see …”𝒫$: “Your task is to estimate the reward for the action 𝑎%. You can see …”Prompt 𝒫'Instruction 𝑖: “Relax on sofa”Action 𝑙&: “Open microwave”⋯\n𝑹𝟏𝑪𝒐𝒕,𝒍𝒕\t𝒊)from 𝒫'𝑹𝒏𝑪𝒐𝒕,𝒍𝒕\t𝒊)from 𝒫\"𝑹𝑵𝑪𝒐𝒕,𝒍𝒕\t𝒊)from 𝒫!𝒓𝑪\n(c) Temporal Consistency(a) Contextual Consistency(b) Structural ConsistencyObservation 𝑜&Instruction 𝑖Action 𝑙&MDP QA-dataset𝒟QA={𝑞𝑜,𝑎𝑜:𝑜∈𝜏}MDP-specific query q(𝑜&):“Which objects in 𝑜&are relevant to the instruction 𝑖?”Prompt 𝒫$⋯⋯Prompt 𝒫&Prompt 𝒫'\n3𝑹𝟏𝑺𝒐𝒕,𝒍𝒕\t𝒊)\tfrom 𝒫' Φ𝑳𝑳𝑴(𝒫𝟏,𝒒(𝒐))\t3𝑹𝒏𝑺𝒐𝒕,𝒍𝒕\t𝒊)\tfrom 𝒫\" Φ𝑳𝑳𝑴(𝒫𝒏,𝒒(𝒐))3𝑹𝑵𝑺𝒐𝒕,𝒍𝒕\t𝒊)\tfrom 𝒫! Φ𝑳𝑳𝑴(𝒫𝑵,𝒒(𝒐))\t✓✓✗𝒓𝑺Majority voteX\n(𝑹𝟏𝑻,⋯,𝑹𝒏𝑻,⋯,𝑹𝒏𝑻)Majority vote (over 𝑛)𝒓𝑻(ii) Reward EnsembleOffline Dataset 𝒟={𝑖!,𝜏!:𝑗}𝑖:\tInstruction𝜏=𝑜',𝑙',𝑜.,⋯,𝑜/,𝑓0𝑖,𝜏𝑓0𝑖,𝜏=\t91\tif\t𝜏\tsuccesses\t𝑖0\tif\t𝜏\tfails\t𝑖\n𝒓𝑪𝒐𝒕,𝒍𝒕𝒊)\t𝒓𝑺𝒐𝒕,𝒍𝒕𝒊)\t𝒓𝑻𝒐𝒕,𝒍𝒕𝒊)𝒘𝑪𝒐𝒕,𝒍𝒕𝒊)\t𝒘𝑺𝒐𝒕,𝒍𝒕𝒊)\t𝒘𝑻𝒐𝒕,𝒍𝒕𝒊)(𝒐𝒕,𝒍𝒕,𝒊)𝕨𝒐𝒕,𝒍𝒕𝒊)=𝕣𝒐𝒕,𝒍𝒕𝒊)=𝑟̂𝑜%,𝑙%𝑖)=𝕨𝒐𝒕,𝒍𝒕𝒊)⋅𝕣𝒐𝒕,𝒍𝒕𝒊)Unified reward at timestep 𝒕ℒΨ,=||9𝑟̂𝑜%,𝑙%𝑖)%−𝑓-𝑖,𝜏||.RewardOrchestratorΨ\"∇,Φ112(𝑞(𝑖,𝜏,𝑛))∈{True,\tFalse}=\tK∅\tif\t𝑙&∈𝐻\"𝜏\t⋏False𝑟3!4\t\t\t\t\t\totherwise(i) Spatio-Temporally Consistent Reward Estimation\nSpatio-TemporallyConsistent RewardsFigure 2: Two-staged reward estimation in COREN. In (i), spatio-temporally consistent rewards, constrained by\ncontextual, structural, and temporal consistencies, are calculated. (a) Contextual consistency is achieved through\nmajority voting across the responses from different prompts Pn, resulting in contextually consistent rewards rC. (b)\nStructural consistency is achieved by presenting MDP-specific queries to the LLM. If the LLM incorrectly answers\nthese queries (indicated by a red ‘X’), the rewards estimated from these particular prompts are removed from\nmajority voting. The successfully verified rewards contribute to structurally consistent rewards rS. (c) Temporal\nconsistency involves collecting high-value actions Hn(τ)and subjecting them to backward verification through\nLLM queries. Actions that fail this verification are excluded from the candidates for majority voting. Otherwise, they\ncontribute to temporally consistent rewards rT. In (ii), a trajectory (i, τ)with success flag fs(i, τ)is sampled from\nthe given offline dataset D. The spatio-temporally consistent rewards (rC, rS, rT)in (i) are combined using weights\n(wC, wS, wT), which are generated by the reward orchestrator Ψθ. This combined result renders a unified stepwise,\nmore domain-grounded reward ˆr. The orchestrator Ψθis trained to align the trajectory’s return of accumulating\nstepwise rewards ˆrwith the sparse reward fs(i, τ)annotated on the trajectory.\nreward setup is inherent for embodied instructionfollowing tasks, as each instruction is treated as a series of goal conditions within Goal-POMDPs. However, this sparse reward setup poses significant challenges in adopting offline RL approaches (Ma et al., 2022; Park et al., 2023). Learning a robust policy in an environment with expansive state-goal spaces and restricted interaction is inherently difficult, and the difficulty is intensified by the sparsity of reward feedback. 3 Our Approach LLM-based reward estimation. Offline RL facilitates agent learning without direct environment interaction, but relying solely on sparse rewards to learn long-horizon instruction-following tasks is often inefficient. To improve this, we augment agent trajectories with stepwise intrinsic rewards through LLM-based estimation. Similar to LLM-based task 173planning (Singh et al., 2023; Ichter et al., 2022), LLMs can be used to approximate the reward of observation-action pairs in the dataset, providing more immediate and actionable dense feedback to enhance the effectiveness of offline learning. Not-grounded reward estimation. Intrinsic rewards estimated by LLMs at intermediate steps might not consistently align with the sparse rewards provided at the conclusion of individual instructionfollowing tasks. This discrepancy arises when the intrinsic rewards are not sufficiently grounded in the environment domain. This issue is exacerbated in a partially observable setting, where LLMs are forced to infer rewards based on incomplete snapshots of the environment. 3.1 Overall Framework To tackle the limitations of LLM-based reward estimation, we propose a spatio-temporal consistencyguided reward ensemble framework CORENwith 3\na two-stage process. As described in Figure 2, the first stage (i) incorporates contextual, structural, and temporal consistencies to fully harness the LLM’s reasoning ability and enhance the groundedness of reward estimates within the specific domain of the embodied environment. In the second stage (ii),CORENorchestrates an ensemble of distinct rewards generated during the first stage based on the trajectories’ success. This allows for the derivation of domain-specifically tuned rewards, which can be effectively utilized for the offline learning of embodied agents. 3.2 Spatio-Temporally Consistent Rewards For reward estimation, we employ Ndistinct prompts P1,···,PNwith an LLM ( ΦLLM), where a prompt is distinguished by its unique explanations, in-context demonstrations, as well as the use of a chain-of-thought (CoT). Specifically, each prompt Pncombined with observation o, action l, and instruction iis used to generate rewards Rn through ΦLLMinferences. Rn(o, l|i) = Φ LLM(Pn,(o, l, i)) (2) Spatial consistency is intended to ensure that the domain-grounded LLM’s reward estimation remains consistent across different prompt-induced contexts as well as it is based on a comprehensive understanding of the environmental structure. We achieve this using the implementation of two consistency mechanisms. Contextual consistency. This mechanism aims to mitigate biases stemming from specific prompt contexts used in LLM-based reward estimation. By employing multiple Nprompts, each with a different contextual frame, we ensure that the rewards, which remain consistent across these variations, reflect a consensus in reasoning. For contextually consistent rewards rC, we integrate the responses RC\nn(o, l|i)of prompts Pnby rC(o, l|i) =argmax\nr∈ˆRNX\nn=11(RCn(o,l|i)=r)(3) where RC\nn(o, l|i) = Φ LLM(Pn,(o, l, i)). Structural consistency. This is intended to ensure that the reward estimation incorporates a comprehensive understanding of the environment physical structure, such as objects, their relationships, and their relevance to the given instruction. We inquire ΦLLMwith MDP-specific queries q(o)relevant to 238observation osuch as “Which objects in oare relevant to the instruction i?”. Exploiting the response ΦLLM(q(o))to these queries, we integrate the rewards RS\nn(o, l|i)of prompts Pn: rS(o, l|i) =argmax\nr∈ˆRNX\nn=11(RSn(o,l|i)=r).(4) We rewrite Eq. (2)for query violation cases, obtaining RS\nn(o, l|i) =(\n∅ ¬E(a(o),ΦLLM(Pn, q(o))\nΦLLM(Pn,(o, l, i))otherwise .\n(5) Here, for an MDP-specific query q(o), Eevaluates whether the response ΦLLM(q(o))matches its corresponding ground-truth answer a(o). Details of prompts Pnand the dataset construction for MDP-specific queries and answers DQA= {(q(o), a(o)) :o∈τ∈ D} are in Appendix. Temporal consistency. This is designed to ensure that the value assigned to an action remains coherent throughout its whole decision-making process. With temporal consistency, if forward reasoning by the LLM assesses certain actions as having high values, backward verification must confirm that these high-value actions can collectively accomplish the given instruction. To achieve this backward verification, we inquire ΦLLMwith the query q(i, τ, n ): “Is performing the high-value actions Hn(τ)from observationoreasonable to accomplish the instruction i?”. The reward is then contingent on the response ΦLLM(q(i, τ, n ))∈ {True,False}to this query, and Eq. (2) is rewritten as RT\nn(o, l|i) = (\n∅ l∈Hn(τ)∧ ¬ΦLLM(q(n, i, τ ))\nΦLLM(Pn,(o, l, i))otherwise\n(6) for the cases of query violation, i.e., l∈Hn(τ)∧ ¬ΦLLM(q(n, i, τ )). Here, for all trajectory observations o∈τ, high-value actions are defined as Hn(τ) ={argmax\nlΦLLM(Pn,(o, l, i))}.(7) Given Nprompts, we then integrate the rewards in Eq. (6)from each by employing the majority voting to establish temporally consistent rewards. rT(o, l|i) =argmax\nr∈ˆRNX\nn=11(RT\nn(o, l|i) =r)(8) 4\nAlgorithm 1: Two-staged C OREN\n1:Trajectory dataset D, MDP-QA dataset DQA\n2:Prompts P1,···,PNfor LLM ΦLLM\n3:Reward orchestrator Ψθ\n4:Reward-augmented dataset ¯D=∅\n/* Spatio-Temp. Consistent Rewards in 3.2 */\n5:for(i,(o, l, o′))∈(i, τ)∈ D do\n6: Reward-augmented trajectory ¯τ=∅\n7:rC←−rC(o, l|i)using Eq (3)\n8:rS←−rS(o, l|i)usingDQAand\nEq (5), (4)\n9:rT←−rT(o, l|i)using Eq (6), (8)\n10: ¯τ←−¯τ∪ {o, l, o′,(rC, rS, rT)}\n11: iflen(τ) = len( ˆτ)then\n12: ¯D ←− ¯D ∪ { (i,¯τ)}\n13: end if\n14:end for\n/* Domain-Grounded Rewards in 3.3 */\n15:repeat\n16: Sample (i,¯τ)∼¯D\n17:∀t, compute r(ot, lt|i)using Eq (9)\n18:∀t, compute w(ot, lt|i)using Eq (10)\n19:∀t,ˆr(ot, lt|i)←− ⟨r(ot, lt|i),w(ot, lt|i)⟩\n20:L(Ψθ)←− ∥P\ntγtˆr(ot, lt|i)−fs(i, τ)∥2\n21: Ψθ←−Ψθ− ∇ θL(Ψθ)\n22:until converge\n3.3 A Domain-Grounded Reward Ensemble From the spatio-temporally consistent rewards rC, rS, and rTcalculated above, we derive domaingrounded rewards through their ensemble based on the alignment with given offline trajectories. We model unified rewards ˆras \n\nr(o, l|i) = (rC(o, l|i), rS(o, l|i), rT(o, l|i))\nw(o, l|i) = (wC(o, l|i), wS(o, l|i), wT(o, l|i))\nˆr(o, l|i) =⟨r(o, l|i),w(o, l|i)⟩\n(9) where⟨·,·⟩is an inner product and wC, wSandwTare learnable weights. These ware generated by the reward orchestrator Ψθ. It takes observation o, action l, and instruction ias input, producing a softmax distribution for w. The orchestrator Ψθis used to align the predicted return of a trajectory with the labeled return, i.e., the sparse reward fs(i, τ): \n\nw(ot, lt|i) = Ψ θ(ot, lt, i)\nL(Ψθ) = E\n(i,ot,lt)∼\n(i,τ)∈D\u0014\n∥P\ntγtˆr(ot, lt|i)−αfs(i, τ)∥2\u0015\n(10) 291where αis a hyperparameter. Finally, using the augmented trajectory dataset that contains unified rewards ˆrin Eq. (9), an agent can be trained via offline RL algorithms such as CQL (Kumar et al., 2020b). The two-staged reward estimation in C ORENis outlined in Algorithm 1. 4 Experiments 4.1 Experiment Settings Environment and dataset. For evaluation, we use VirtualHome (VH) (Puig et al., 2018), a widely used realistic benchmark for household activities. VH features a diverse array of interactive objects (e.g., apples, couch) and basic behaviors (e.g., grasp, sit), enabling us to define 58 distinct actions for embodied agents. We use 25 distinct tasks including activities such as sitting on a couch with several fruits, microwaving salmon, and organizing the bathroom counter. To construct a training dataset Dfor offline RL, we begin with a single expert trajectory for each of these 25 tasks. We then augment each with random actions at intermediate steps that lead to failed trials. For each expert trajectory, a sparse reward of 1is annotated to indicate success, while for sampled failed trajectories, a sparse reward of 0is annotated to denote failure. This follows Goal-POMDP representations used in long-horizon instruction-following tasks. Evaluation instruction. We employ two distinct instruction types to assess the agent’s ability to handle different goal representations. A Fine-grained instruction type provides a detailed task description, often including specific actions performed to achieve certain goal conditions pertinent to the instruction-following task. An Abstract instruction type provides a more abbreviated and generalized task description, focusing on broader objectives without detailing each action. Each of the 25 tasks is assessed using 5 fine-grained and 5 abstract instructions, resulting in a total of 250 distinct instructions being tested. These instructions have not been included within the offline training dataset. Evaluation metrics. We use three metrics, consistent with previous works (Singh et al., 2023; Song et al., 2023b). SRmeasures the percentage of tasks successfully completed, defining success as the completion of all goal conditions for a task; CGC measures the percentage of completed goal conditions; Plan measures the percentage of the action sequence that continuously matches with the ground-truth sequence from the beginning. 5\nBaselines. We compare CORENwith different categories of agents: RL agents , in which an LLM is solely used for estimating rewards to train a separate RL agent, without directly using the LLM for online interaction; LM agents , in which either a small language model (sLM) or LLM is used to directly interact with the environment as an online agent. These are in contrast to the RL agents that use LLMs solely for agent training. In this LM agent category, to provide an evaluation under the compatible computational efficiency conditions with the RL agent category, we include sLM-based agents as well as LLM-based agents . The RL agent category baselines include i) Lafite-RL (Chu et al., 2023), which evaluates actions as good (1), neutral (0), or bad (-1) using an LLM, and integrates the evaluations with environmental rewards; ii) RDLM (Kwon et al., 2023), which uses an LLM to evaluate trajectory returns using dynamically sampled in-context demonstrations; iii) Self-Consistency (Wang et al., 2023), which generates multiple reward candidates via a single CoT prompt, taking a majority vote on them; and iv) GCRL, which relies on given sparse rewards related to goal conditions. The LM agent category baselines include v) SayCan (Ichter et al., 2022), which employs an offline dataset to learn the affordance scores combined with an LM’s prediction; vi) LLMPlanner (Song et al., 2023b), which uses an expert dataset for retrieval-augmented task planning; vii) ProgPrompt (Singh et al., 2023), which uses engineered programmatic assertion syntax to verify the pre-conditions of action execution. Each LM agent baseline is configured with both sLMs and LLMs. For our CORENand the RL agent category, we use Gemini-1.0 for the reward estimator ΦLLMand adapt the GPT2-based model architecture having 117M parameters to implement the agent policies that learn from their respective rewards. We also employ the CQL (Kumar et al., 2020b) offline RL algorithm in conjunction with the DDQN (van Hasselt et al., 2016) to handle the discrete action space in our environment. Details of the experiments are in Appendix. 4.2 Main Results Instruction-following task performance. Table 1 presents a performance comparison of our COREN and the baselines from different categories, including RL agents, LLM-based agents, sLM-based agents across metrics such as SR,CGC , and Plan . 392• C ORENoutperforms all the RL agent baselines by a significant margin, achieving average gains of20.0%,15.2%, and 5.6%over the most competitive RL agent baseline Self-Consistency in SR,CGC , and Plan , respectively. •Furthermore, the performance of CORENis on par with the LLM-based agents, with only a slight performance drop compared to SayCanGemini and ProgPrompt-Gemini, while it surpasses the other LLM-based agents (i.e., all with LLaMA3 and LLM-Planner-Gemini). These results are especially noteworthy, considering the significantly different model sizes between COREN(GPT2-based-117M) and other LLMbased agents (i.e., Gemini-50T, LLaMA-8B). These demonstrate COREN’s ability to learn long-horizon instruction-following tasks within specific domains using minimal domain-specific knowledge, such as partially annotated rewards. •We observe that the sLM-based agents using 4bit quantized LLaMA3 (LLaMA3Q) and GPT2 exhibit lower performance than the others, including our COREN, due to their dependency on the limited reasoning capabilities of sLMs. •Additionally, CORENdemonstrates relatively robust performance across different instruction types compared to LLM-based agents. This can be attributed to COREN’s ability to learn from a broad range of semantically similar instructions, which are generated by the LLM and included in the offline dataset. This enables the framework to better generalize to abstract instructions. Cross-domain performance. Here, we extend our evaluation scenarios to include domain shifts in the environment; i.e., the locations of key objects related to the given instructions differ from those in the training dataset. Specifically, we sample a subset of trajectories from the training dataset Dand relabel their sparse rewards fs(i, τ)to reflect the altered object locations. While keeping the spatiotemporally consistent rewards unchanged, we then retrain the reward orchestrator in Eq. (10) using these newly labeled sparse rewards. This approach facilitates the generation of domain-specific unified rewards for RL without the need to recalculate the consistency-based rewards themselves through LLM inferences. We also incorporate this newly labeled dataset for the LM agent category. For instance, LLM-Planner adapts to this new environment domain by using the trajectories, which are 6\nFine-grained Abstract\nRL agent SR CGC Plan SR CGC Plan\nCOREN 66.4 74.5 69.5 57.6 68.3 64.8\nLafite-RL 30.4 50.9 35.1 17.6 37.8 23.1\nRDLM 20.0 42.0 31.7 4.0 23.3 23.9\nSelf-Consistency 43.2 56.9 61.4 40.8 55.6 61.7\nGCRL 5.6 26.0 22.3 8.0 28.4 17.1\nLLM-based agent\nSayCan-Gemini 72.0 78.2 73.8 6.9 25.2 23.2\nSayCan-LLaMA3 4.8 22.4 63.8 3.2 14.6 20.0\nProgPrompt-Gemini 72.8 80.4 80.2 32.0 49.2 24.3\nProgPrompt-LLaMA3 68.0 74.5 50.5 16.5 29.5 8.2\nLLM-Planner-Gemini 55.2 63.8 59.7 2.1 18.1 0.0\nLLM-Planner-LLaMA3 15.1 34.0 30.6 2.0 15.4 0.6\nsLM-based agent\nSayCan-LLaMA3Q 4.8 21.6 62.6 0.0 15.4 0.4\nSayCan-GPT2 0.0 14.7 0.0 0.0 14.7 0.0\nProgPrompt-LLaMA3Q 43.2 68.2 68.8 15.2 34.5 31.1\nProgPrompt-GPT2 0.6 16.7 6.0 0.0 8.8 0.4\nLLM-Planner-LLaMA3Q 12.4 31.1 8.9 0.6 13.9 0.2\nLLM-Planner-GPT2 0.0 12.6 0.0 0.0 12.6 0.0\nTable 1: Instruction-following task performance in SR,\nCGC, and Plan metrics\nrelabeled as success, as demonstrations for task planning. Since other RL agent baselines, except GCRL, lack the ability to utilize domain information represented as sparse rewards, they are evaluated with the same policy as in the single-domain experiments. •For this cross-domain assessment, as shown in Table 2, CORENoutperforms all the RL agent baselines, showing a minimal drop compared to the results in the single-domain experiments (in Table 1). Upon domain shifts, COREN’s twostaged process adjusts the reward estimates to align with the target domain by the second stage conducting the adaptive ensemble in Eq. (10). In contrast, the RL agent baselines, which rely solely on the rewards derived from the LLM’s commonsense reasoning, exhibit a diminished ability to adapt to specific domains, showing large drops compared to the results in the singledomain experiments. •We also observe that the LLM-based agent baselines experience large degradation in this crossdomain assessment; e.g., LLM-Planner relies on the LLM’s knowledge, which is difficult to ground in a specific environment using only a few examples, leading to suboptimal performance. 4.3 Ablation Studies Spatio-temporally consistent rewards. To verify that the contextual, structural, and temporal consistencies (in Section 3.2) effectively complement 472Fine-grained Abstract\nRL Agent SR CGC Plan SR CGC Plan\nCOREN 60.0 66.3 69.4 45.0 55.0 42.5\nLafite-RL 2.5 12.5 10.6 0.0 12.5 25.2\nRDLM 15.0 23.8 22.5 3.8 18.8 23.6\nSelf-Consistency 35.4 47.9 54.7 31.3 45.8 51.6\nGCRL 0.0 6.3 8.5 0.0 6.3 10.9\nLLM-based agent\nSayCan-Gemini 12.5 18.8 26.6 0.0 8.3 0.0\nSayCan-LLaMA3 5.0 21.3 1.9 0.0 10.0 1.3\nProgPrompt-Gemini 25.0 31.3 23.4 14.6 32.3 1.6\nProgPrompt-LLaMA3 25.0 32.3 22.6 8.3 24.0 3.1\nLLM-Planner-Gemini 45.8 55.2 67.7 0.0 13.7 0.0\nLLM-Planner-LLaMA3 6.3 20.2 16.8 0.0 40.6 0.0\nsLM-based agent\nSayCan-LLaMA3Q 8.3 21.9 1.5 0.0 12.5 1.9\nSayCan-GPT2 0.0 6.3 0.0 0.0 6.3 0.0\nProgPrompt-LLaMA3Q 7.5 15.5 18.3 0.0 31.3 0.0\nProgPrompt-GPT2 0.0 8.3 0.3 0.0 6.3 0.0\nLLM-Planner-LLaMA3Q 2.1 15.2 9.9 0.0 13.5 0.0\nLLM-Planner-GPT2 0.0 6.3 0.0 0.0 6.3 0.0\nTable 2: Cross-domain performance\neach other in LLM-based reward estimation, we test different combinations of these consistencies in the ensemble of rewards. Table 3 demonstrates thatCOREN, which utilizes all three, consistently outperforms the others. This specifies that the combination of wand rewards derived from partial consistencies alone is limited in generating unified rewards that significantly benefit RL, while the ensemble weights wcan be adjusted via Eq. (10). Fine-grained Abstract\nSR CGC Plan SR CGC Plan\nCOREN66.4 74.5 69.5 57.6 68.3 64.8\nCS 64.8 67.9 69.7 52.0 62.3 56.3\nST 57.6 70.1 65.4 50.4 60.8 61.7\nCT 53.6 66.1 67.8 51.2 59.1 68.9\nC 47.2 58.6 59.7 47.1 57.1 58.3\nS 52.0 67.1 57.9 45.6 60.0 58.9\nT 45.6 57.8 55.7 41.6 51.2 50.8\nTable 3: Ablation on spatio-temporally consistent re-\nwards. For example, CS denotes the use of contextual\nand structural consistencies, and T denotes the use of\ntemporal consistency only, while CORENemploys all\nthree consistencies in the ensemble.\nDifferent LLMs for reward estimation. To implement COREN, which uses an LLM for offline reward estimation, we test a variety of LLMs raging from open-source LLaMA3 to proprietary models GPT-4-turbo, Gemini-1.0, and PaLM. In Table 4, we observe that LLaMA3, which has significantly fewer parameters, does not achieve performance comparable to the proprietary models. Among the 7\nproprietary models, the more recent and advanced capable LLMs, such as GPT-4-turbo and Gemini1.0, demonstrate a strong ability in reward estimation that positively impact on agent offline learning. Fine-grained Abstract\nSR CGC Plan SR CGC Plan\nLLaMA3 12.0 28.7 39.4 9.6 27.9 40.1\nPaLM 16.8 32.9 35.8 10.4 27.7 25.4\nGPT-4-turbo 65.6 71.8 70.6 40.8 50.5 52.5\nGemini-1.0 66.4 74.5 69.5 57.6 68.3 64.8\nTable 4: Different LLMs for reward estimation\nReward ensemble scheme. We evaluate several approaches as alternatives to the reward ensemble scheme in Eq. (9). First, we consider taking the average of rewards rC,rS, andrTto obtain a unified reward. Second, we employ a majority voting mechanism over the three rewards. As shown in Table 5, both Avg and Majority V oting result in degraded performance compared to COREN. While the majority voting of spatio-temporally consistent rewards can provide a considerable degree of domain groundedness, CORENtakes it a step further by employing the reward ensemble process using sparse rewards as guidance. Fine-grained Abstract\nSR CGC Plan SR CGC Plan\nCOREN 66.4 74.5 69.5 57.6 68.3 64.8\nAvg 53.6 63.7 55.6 43.2 55.2 57.7\nMaj.Voting 60.8 70.2 68.9 55.2 62.7 63.7\nTable 5: Ablation on reward ensemble scheme\n5 Related Works LLMs for embodied environments. Leveraging LLMs as an instruction-following agent in embodied environments becomes a bedrock, capitalizing on LLM’s reasoning capabilities (Hu et al., 2023; Singh et al., 2023; Yang et al., 2023; Pantazopoulos et al., 2023; Yun et al., 2023). To overcome the limitation of LLMs’ insufficient knowledge about specific domain conditions of the environment, prior works incorporate domain-related information. (Ichter et al., 2022) utilizes an offline dataset to learn the value of actions, which is later combined with the LLM’s token generation probability to calibrate the LLM’s decision for different domains. (Song et al., 2023a) employs an expert dataset as a knowledge base for retrieval- 522augmented task planning. Unlike those directly employing LLMs as agent policies and requiring online LLM inferences, our study focuses on leveraging LLMs for reward estimation in offline RL, thus allowing for efficient agent structures. LLMs for reward design. In RL, reward engineering is a long-standing challenge, traditionally tackled through manual trial-and-error or by leveraging domain knowledge from human experts. Inverse RL, on the other hand, aims to infer the underlying reward function from reward-free expert demonstrations (Hadfield-Menell et al., 2016; Klein et al., 2012). With the advent of capable foundation models, recent works have exploited them to produce reward functions (Wang et al., 2024; Du et al., 2023; Rocamonde et al., 2023; Baumli et al., 2023). (Kwon et al., 2023) harnesses the in-context learning of LLMs to evaluate the episodes of highlevel tasks. (Ma et al., 2023) leverages the code generation ability of LLMs, given environmental programming code, producing multiple code-based reward functions to train RL agents online and enhance them via feedback from agent training statistics. Our CORENframework also leverages LLMs for reward design; however, the framework distinguishes itself by focusing on generating domaingrounded rewards without direct interaction with the environment, particularly in scenarios where the available information about the embodied environment is limited to sparse rewards. 6 Conclusion We presented the reward ensemble framework CORENto achieve robust LLM-based reward estimation for offline RL, specifically tailored for embodied instruction-following tasks. The framework utilizes a spatio-temporal consistency-guided ensemble method for reward estimation. It generates multiple stepwise rewards on offline trajectories, with each reward focusing on a specific consistency related to contextual, structural, or temporal aspects, and then it integrates the multiple rewards into more domain-grounded ones via the sparse reward-aligned ensemble. As this work is the first to adopt LLMs for offline learning of embodied agents, we hope it can provide valuable insights into the development of LLM-driven training acceleration techniques. This is particularly significant for embodied agents involved in longhorizon instruction-following tasks, which are typically constrained by sparse reward signals. 8\n7 Limitations Despite the robust performance achieved by COREN, we identify that its success heavily depends on the capabilities of LLMs engaging in reward estimation, as shown by the ablation study in Table 4. Our LLM-based reward estimation is conducted in an offline manner, i.e., without direct interaction with the environment. However, the dependency on the capabilities of an LLM can be problematic, especially when the target environment domain significantly differs from the pretrained knowledge of the LLM and the domain changes continuously over time after agent deployment. In these cases involving dynamic GoalPOMDP environments, the agent policy learned offline by the dense rewards on the training dataset can degrade in terms of its task performance. The benefits of our ensemble method with the notion of spatio-temporal consistency are attributed to the effective alignment with the training dataset, and they can be limited in such non-stationary environment conditions. We leave the exploration of methods to address this limitation as a direction for future work. 596",
        "aspect_review": "The paper presents a comprehensive experimental validation using the VirtualHome benchmark, demonstrating that COREN significantly outperforms other offline RL agents and achieves comparable performance to state-of-the-art LLM-based agents. Detailed ablation studies are conducted to verify the effectiveness of spatio-temporally consistent rewards and different LLMs for reward estimation, showing the robustness of the proposed method. The paper is well-organized, with clear explanations of the methodology and results, including figures and tables that effectively illustrate the framework and experimental outcomes. The introduction of a consistency-guided reward ensemble framework (COREN) for offline RL is novel and technically rigorous, addressing the challenge of grounding LLM-generated rewards in the target environment domain.\n\nThe paper could benefit from a deeper discussion on the intuition behind the choice of specific consistency criteria and their integration into the reward ensemble. While the experiments are extensive, the evaluation could include more diverse environments to further demonstrate the generalizability of the approach. The reliance on the VirtualHome benchmark may limit the perceived impact, as it is a simulated environment and may not fully capture real-world complexities. Some implementation details, such as the selection of hyperparameters, are not thoroughly discussed, which could be important for reproducibility. The description of the reward orchestrator's training process could be expanded to provide more clarity on its role and impact on the overall framework.",
        "general_review": "The paper introduces a novel approach by leveraging large language models (LLMs) for offline reward estimation in reinforcement learning, specifically tailored for embodied agents. The proposed COREN framework demonstrates strong technical depth and rigor through its innovative two-stage reward estimation process that incorporates spatio-temporal consistency. The comprehensive experimental validation showcases COREN's superior performance over existing offline RL and LLM-based agents, even with significantly fewer parameters. This work has significant practical impact as it efficiently enables embodied agents to learn long-horizon instruction-following tasks with minimal domain-specific knowledge, potentially transforming how LLMs are used in embodied environments.\n\n1. The paper could benefit from minor writing improvements for enhanced clarity in some sections. 2. There is a need for additional details on the hyperparameters used in the experiments for better reproducibility. 3. Some figures could be formatted more clearly to improve the reader's understanding. 4. The availability of the code should be clarified to facilitate replication of the results."
    },
    "27": {
        "paper": "AUTOSCRAPER : A Progressive Understanding Web Agent for Web Scraper\nGeneration\nAnonymous ACL submission\nAbstract\nWeb scraping is a powerful technique that extracts data from websites, enabling automated data collection, enhancing data analysis capabilities, and minimizing manual data entry efforts. Existing methods, wrappers-based methods suffer from limited adaptability and scalability when faced with a new website, while language agents, empowered by large language models (LLMs), exhibit poor reusability in diverse web environments. In this work, we introduce the paradigm of generating web scrapers with LLMs and propose AUTOSCRAPER , a two-stage framework that can handle diverse and changing web environments more efficiently. AUTOSCRAPER leverages the hierarchical structure of HTML and similarity across different web pages for generating web scrapers. Besides, we propose a new executability metric for better measuring the performance of web scraper generation tasks. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Our work is now open-source. 1 Introduction Web scraping is a process where software automates the extraction of data from websites, typically using bots or web scrapers to gather specific information (Thapelo et al., 2021). It is important because it allows for efficient data collection and aggregation, which can be crucial for market research, competitive analysis, and real-time data monitoring. Due to the diversity of sources and information on the internet, the construction of a web scraper requires substantial human effort. Consequently, two types of methods for automatic web information acquisition have been proposed, categorized as wrapper-based and language-agent-based (Sarkhel et al., 2023). The wrapper-based method entails complex sequences of operations within customized rule-based functions, which are designed Q1: What’s theage ofStephen Curry ?\n……\nWrapper -Based Method Language -Agent -Based Method\nAUTOSCRAPER:Wrapper -Based +Language -Agent -BasedQ1 Q1\n<html>\n ...\n    <div \nclass=\"entity-\ntitle\">\n     <div \nclass=\"title\">\n        \n<span>Stephen \nCurry</span> \n…………\n Response\nHeavy Manual Effort Heavy Time &Financial Consuming\nQ1: What’s ……Curry?\nQ2: What’s ……Harden?Response\nResponseWebpage -1\n<html>\n ...\n    <div class=\"entity-\ntitle\">\n     <div class=\"title\">\n        <span> Stephen \nCurry</span> \n…………Webpage -1Wrapper1Q2\n<html>\n ...\n    <div \nclass=\"entity-\ntitle\">\n     <div \nclass=\"title\">\n        <span>James \nHarden</span> \n………… ResponseWebpage -2\nHighly reusable with Great Performance<html>\n ...\n    <div \nclass=\"entity-\ntitle\">\n     <div \nclass=\"title\">\n        \n<span>Stephen \nCurry</span> \n…………Webpage -1\nQ2: What’s theage ofJames Harden?\n<html>\n ...\n    <div \nclass=\"entity-\ntitle\">\n     <div \nclass=\"title\">\n        \n<span>LEBRON \nJAMES</span> \n…………Webpage -2 Q1\nWrapper2\nResponse Response\n<html>\n…………Webpage -2Figure 1: An illustration of comparing wrapper-based\nmethods, language-agent-based methods and AUTO-\nSCRAPER .\nto efficiently access and retrieve desired data from websites, which is especially beneficial for structured websites with stable layouts (Kushmerick, 1997; Dalvi et al., 2011; Bronzi et al., 2013). Conversely, the language-agent-based method leverages powerful natural language processing capabilities of large language models (LLMs) to interpret free-text queries and directly extract data within websites to meet the demand, effectively handling both structured and dynamic web content (Whitehouse et al., 2023; Marco Perini, 2024). Although both types of methods facilitate web scraping to varying degrees, as shown in Figure 1, they exhibit significant shortcomings in terms of scalability. Wrapper-based method, while reusable, struggles with entirely new website structures, which necessitates extensive human effort to develop additional customized functions (Gulhane et al., 2011; Lockard et al., 2019). Conversely, although language-agent-based methods demonstrate superior performance in adapting to new content, their reliance on a limited number of super1\npowerful API-based LLMs for web scraping incurs considerable time and financial costs. Together, these challenges impede the broader adoption and scalability of current web scraping technologies, limiting their practicality in dynamic and diverse web environments. To address the shortcomings of the aforementioned two paradigms, the paradigm of generating web scrapers with LLMs would be the optimal solution. On one hand, compared to wrapper-based methods, it fully leverages the reasoning and reflection capacities of LLMs, reducing manual design on new tasks and enhancing scalability. On the other hand, compared to language-agent-based methods, it introduces repeatable extraction procedures, reducing the dependency on LLMs when dealing with similar tasks, and thereby improving efficiency when handling a large number of web tasks. However, there are several challenges associated with using LLMs to generate web scrapers: 1.Long HTML document. Although LLMs excel in comprehending long textual content, HTML, as semi-structured data, comprises both structured (tags and attributes) and unstructured (textual content) elements. Consequently, it is challenging for LLMs to generate executable web scrapers that strictly adhere to the hierarchical structure of web pages in complex markup contexts. 2.Reusability. A good scraper needs to be reusable across multiple web pages. However, the differences in content and structure between various web pages can lead to the creation of a scraper that references a webpage, which can only be applied to some web pages. 3.Appropriate evaluation metrics. For a scraper to be considered useful, it must be able to automatically extract the desired results from all web pages. However, existing evaluation metrics for web information extraction, which focus on the extraction results from individual web pages, do not adequately reflect the usability of the scraper. This can potentially mislead experimental conclusions. We introduce AUTOSCRAPER , a two-stage framework to address the web scraper generation task. Illustrated in Figure 2, AUTOSCRAPER comprises two main components: progressive generation and synthesis. The progressive generation 112stage leverages the hierarchical structure of HTML for progressive understanding to address the long HTML document. Subsequently, the synthesis module integrates multiple scrapers generated on different web pages to produce a cohesive, websitespecific scraper that functions universally within that site. Besides, we propose a new evaluation metric for web scraper generation tasks, called the executability metric. Unlike traditional information extraction metrics that measure single web pages, this metric measures multiple web pages within a website, accurately reflecting the reliability and reusability of the scraper. We evaluate AUTOSCRAPER on three available datasets with 7 LLMs. On all three datasets, AUTOSCRAPER consistently outperforms all baselines and achieves new state-of-the-art results in zero-shot settings. Also, AUTOSCRAPER can surpass supervised learning methods. Moreover, AUTOSCRAPER demonstrates superior efficiency on large-scale web information extraction task. Compared to traditional wrappers, AUTOSCRAPER adjusted more quickly according to different websites and task requirements. This flexibility enables scrappers to handle diverse and changing web environments more efficiently. Compared to the language agent paradigm, it introduces intermediate functions to enhance reusability and reduce the dependency on LLMs when dealing with similar tasks, thereby improving efficiency when handling a large number of web tasks. 2 Related Work Wrapper-based methods for web scraping utilize the hierarchical structure of the webpage. Method of this category includes rule-based (Zheng et al., 2008), learning wrappers (i.e a DOM-specific parser that can extract content) (Gulhane et al., 2011; Kushmerick, 1997; Dalvi et al., 2011), heuristic algorithm (Lockard et al., 2018, 2019) and deep learning neural network (Lin et al., 2020; Zhou et al., 2021; Li et al., 2022; Wang et al., 2022). These methods demand substantial human involvement, including creating wrapper annotations, applying heuristic scoring rules (such as visual proximity), crafting features for neural network input, and using prior knowledge for verification. Therefore, it is difficult for wrapper-based methods to automatically scale up when facing web scraping tasks across a large number of different websites. With the emergence of powerful LLMs (Ope2\nnAI, 2023; Touvron et al., 2023), language agent (Sumers et al., 2023) act in interactive environments with the help of LLM-based reasoning, grounding, learning, and decision making. Current language agents target the web mainly aim to streamline the web environment (Sridhar et al., 2023; Gur et al., 2023; Zheng et al., 2024) and to devise strategies for planning and interacting with the web (Sodhi et al., 2023; Ma et al., 2023). However, these frameworks mainly focus on the concept of the open-world web simulation environments (Shi et al., 2017; Yao et al., 2023; Deng et al., 2023; Zhou et al., 2023), encompassing a broad spectrum of tasks found in real-life scenarios, such as online shopping, flight booking, and software development. These task scenarios are oriented towards individuals, and there is a huge difference in the requirements for accuracy and efficiency compared to web scraping. Therefore, current language-agent-based methods, cannot effectively utilize the HTML structural similarities between multiple web pages, reducing the dependency on LLMs when performing repetitive operations, resulting in inefficiencies. 3 Preliminaries In this section, we first define the scraper generation task and then present the dataset collection process and its corresponding evaluation metrics. 3.1 Task Formulation First, we formulate our scraper generation task. Given a set of webpages on the same website w∈ W describing a subject entity s(also called topic entity in the previous literature), and its corresponding predefined target attribute r∈ R, the task objective is to generate an executable rule/action sequence Ato extract target information ofrom all webpages. 3.2 Datasets We adopt the semi-structure information extraction task as a testbed for the scraper generation task. SWDE (Hao et al., 2011) is a Structured Web Data Extraction dataset that contains webpages from 80 websites in 8 domains, with 124,291 webpages. Each of the websites from the same domains focuses on 3-5 attributes in the web pages. EXTENDED SWDE (Lockard et al., 2019) involves fine-grained manual annotation of 21 sites 209Dataset Num Case Num Task Num Web\nSWDE 320 32 32,EXTENDED SWDE 294 221 29,DS1 83 11 Table 1: The statistic of web scraping task benchmarks.\nWe report the number of the case ( Num Case), the number\nof the different extraction task ( Num Task) and the total\nnumber of webpages ( Num Web).\nin 3 domains from SWDE . While SWDE contains an average of 4,480 triples for 3 predicates per website, the EXTENDED SWDE dataset averages 41K triples for 36 predicates per site. DS1(Omari et al., 2017) contains 166 annotated webpages from 30 real-life large-scale websites categorized into books, shopping, hotels, and movies. We transform the dataset with the following settings. First, we design instructions for each of the domains, and for each of the attributes as the input information for LLMs1. Second, for each website in each domain, we sample 100 web pages as the whole test set. We consider the set of webpages on the same websites and the corresponding extraction instruction as a case. For example, for the ESPN websites2in NBA player domains, the sampled 100 detail webpage of players and the instruction Please extract the team of the player he plays now is a complete case of our scraper generation task. Third, we pre-process the web pages by removing irrelevant elements in a webpage. We use open-source BeautifulSoup library3and filter out all DOM element nodes with <script> and <style> , as well as delete all attributes in the element node except @class . We replace the original escape characters in the annotations to ensure consistency with the corresponding information on the web. The statistic of the dataset we transformed is shown in Table 1. 3.3 Evaluation Metrics Existing evaluation schemes for web page information extraction tasks still follow the traditional metrics of text information extraction tasks, namely precision, recall, and F1 score. They limit the assessment of methods for the scraper generation task to two aspects. First, it focuses on extraction with a single webpage, rather than considering the generalizability from the perspective of a collection 1Further details about the prompt is in Appendix D\n2https://global.espn.com/nba/\n3https://beautifulsoup.readthedocs.io\n3\nAction Sequence 3\nStep1:   //*[text()='PPG’]\nStep2:  ./ancestor\nStep3:  ./span[1]/text()\nAction Sequence 2\nStep1:   //*[text()='PPG’]\nStep2:  ./ancestor\nStep3:  ./span[1]/text()\nPhase1: Progressive Generation Phase2: Synthesis\nAction Sequence 1\nStep1:  //*[text()=‘ PPG’]/text() \nStep2:  ./ancestor \nStep3:  ./span[2]/text()\nResult of Action \nSequence 3\nWebpage1:   17.4\nWebpage2:  28.3\nWebpage3:   22.6\nResult of Action \nSequence 2\nWebpage1:   \nWebpage2:  \nWebpage3:   \nResult of Action Sequence 1\nWebpage 1:   6.7\nWebpage 2:  4.7\nWebpage 3:   2.9Which sequence is the best?\nAction Sequence 1 is best.\nInstruction:  What’s the average \nrebound of James Harden?\nStep1: Top-down\nStep2: Step -backStep3: \nTop-down\nWrong.……\nCorrect.(a) Progressive Generation:\nGenerate an action sequence \nthrough multiple rounds of interaction with a webpage.(b) Synthesis:\nChoose one of the best action sequence generated with different webpage.\nAction Sequence 2Action Sequence 1\nAction Sequence 3Final Action \nSequence\nSeed Webpages Sequence Set\nExtract text with \naction sequences \nStep1:   [Top -down] \nXPath: //*[text()=‘ PPG’]/text()\n# Get the text below the text node ‘PPG’. \nStep2:   [Step -back] \nXPath: ./ancestor\n# 16.6 is not the right answer, get the \nsub HTML with more context. \nStep3:   [Top -down] \nXPath: ./span[2]/text()\n# Get the text from the second node of current sub HTML.[Full HTML]\n[Sub HTML]\n[Extract the text]    16.6\n[Get the sub HTML]\n[Extract the text]    5.1Figure 2: A UTOSCRAPER framework of two phases: (a) progressive generation and(b) synthesis .\nof webpages. Second, it does not effectively measure the transferability when adopting the action sequence to other web pages. To address this issue, we transform the traditional IE task evaluation into an executable evaluation. Based on the traditional IE evaluation on a collection of web pages, we categorize the executability of action sequences into the following six situations. Specifically, for each extraction task on a website, the result is classified based on the extraction result on precision, recall, and f1-score. (1) Correct : both precision, recall and f1score equal 1, which indicates the action sequence is precisely; (2) Precision(Prec.) : only precision equals 1, which indicates perfect accuracy in the instances extracted following the action sequence, but misses relevant instances; (3) Recall(Reca.) : only recall equals 1, which means that it successfully identifies all relevant instances in the webpage but incorrectly identifies some irrelevant instances; (4)Un-executable(Unex.) : recall equals 0, which indicates that the action sequence fails to identify relevant instances; (5) Over-estimate(Over.) : precision equals 0, which indicates that the action sequence extracts the instances while ground truth is empty; (6) Else: the rest of the situation, including 273partially extracting the information, etc. Since the above classifications are mutually exclusive, we use the ratio metric to calculate the proportion of each result in our task. MR=#case of situation\n#total case(1) We are more concerned with success rate, so for the Correct metric, higher values indicate a better proportion of generated execution paths; whereas for theUn-executable metric, lower values are preferable. 4 A UTOSCRAPER In this section, we describe our framework AUTOSCRAPER for generating a scraper to extract specific information from semi-structured HTML. Our approach is divided into two phases: first, we adopt a progressive generation module that utilizes the hierarchical structure of web pages; second, we employ a synthesis module based on results from multiple web pages. The overall framework is presented in Figure 2. 4.1 Modeling Unlike the wrapper method that generates an XPath, we model the scraper generation task as an action 4\nsequence generation task. In specific, we generate an action sequence Aseqthat consists of a sequence of XPath4expression from a set of seed webpages (i.e., a small portion of webpages in the test case for generating the sequence). Aseq= [XPath 1,XPath 2, ...,XPath n](2) where ndenotes the length of the action sequence. We execute the XPath in the sequence using the parser in order. In the sequence, all XPath expressions except the last one are used for pruning the web page, and the last one is used for extracting the corresponding element value from the pruned web page. 4.2 Progressive Generation Dealing with the lengthy content and hierarchical structure of webpages, generating a complete and executable scraper in one turn is difficult. However, the HTML content is organized in a DOM tree structure, which makes it possible to prune irrelevant page components and hence, limit the length and height of the DOM tree to improve the performance of LLM generation. Specifically, we perform a traversal strategy consisting of top-down andstep-back operations. Top-down refers to starting from the root node of the current DOM tree, progressively refining down to the specific node containing the target information. Step-back refers to reassessing and adjusting selection criteria by moving up the DOM tree to choose a more reliable and broadly applicable node as a foundation for more consistent and accurate XPath targeting. At each step, we first employ a top-down operation, guiding the LLMs to directly write out the XPath leading to the node containing the target information and to judge whether the value extracted with XPath is consistent with the value it recognizes. If execution fails, then adopt a step-back operation to retreat from the failed node, ensuring the web page includes the target information, which is driven by LLMs. The detail is shown in Algorithm 1. 4.3 Synthesis Although we gain an executable action sequence within the progressive generation process, there are still differences in the specific location of the target information and the structure between different web pages. The action sequence may collect XPath 4https://en.wikipedia.org/wiki/XPathwith specific characteristics in a single HTML and lose generalizability. To enhance the reusability of the action sequence, we propose a synthesis phase. Specifically, we randomly select nswebpages from the case as seed webpages. Then, we generate an action sequence for each of them. Subsequently, we execute multiple different action sequences to extract information from the seed web pages, respectively. We collect all action sequences and their corresponding results and then choose one that can extract all the target information in the web pages as the final action sequence. 5 Experiment Intending to put AUTOSCRAPER to practical use, we investigate the following research questions: 1) Can AUTOSCRAPER outperform the state-of-theart scraper generation methods? 2) How does AUTOSCRAPER framework improve the performance of the scraper generation task? 3) Does AUTOSCRAPER meet the requirements for web scraping tasks, specifically being accurate and efficient? 5.1 Experimental Settings & Evaluation Metrics We conduct our experiment on various LLMs including closed-source LLMs: GPT-3.5Turbo (OpenAI, 2022), Gemini Pro (Team et al., 2023) and GPT-4-Turbo (OpenAI, 2023) as well as open-source LLMs: Mistral-7B (Jiang et al., 2023), CodeLlama-34B (Rozière et al., 2024), Mixtral 8 ×7B(Jiang et al., 2024) and Deepseek-Coder-33B (Guo et al., 2024). Furthermore, we apply different LLM-prompt-based web agents as our baselines, including COT (Wei et al., 2023) and Reflexion (Shinn et al., 2023) andAUTO SCRAPER to them. The comparison between them is discussed in Appendix B.1. Due to the limited-length context of LLMs, all experiments are conducted under zero-shot settings. We test them on three datasets: SWDE (Hao et al., 2011), EXTEND SWDE (Lockard et al., 2019) and DS1(Omari et al., 2017). The experimental result of the last two can be found in Appendix A.1 and A.2. We set the size of seed webpages ns= 3and max retry times dmax= 5. In addition to the execution evaluation metrics described in Section 3.3, we also employ traditional evaluation metrics to more comprehensively assess the quality of different action sequences. 5\nModels MethodEXECUTABLE EVALUATION IE E VALUATION\nCorrect( ↑) Prec Reca Unex.(↓)Over. Else Prec Reca F1\nClosed-source LLMs\nGPT-3.5-TurboCOT 36.75 8.83 6.71 43.46 0.71 3.53 89.45 50.43 47.99\nReflexion 46.29 11.66 2.83 37.10 0.71 1.41 94.67 55.85 55.10\nAUTOSCRAPER 54.84 11.83 8.96 19.35 1.08 3.94 85.85 73.34 69.20\nGemini ProCOT 29.69 10.94 7.50 47.19 1.25 3.44 81.21 45.22 41.81\nReflexion 33.12 6.56 4.06 52.50 0.63 3.12 87.45 42.75 40.88\nAUTOSCRAPER 42.81 11.87 4.69 34.38 1.25 5.00 85.70 57.54 54.91\nGPT-4-TurboCOT 61.88 12.50 7.19 14.37 0.94 3.12 87.75 79.90 76.95\nReflexion 67.50 13.75 4.37 10.94 0.94 2.50 93.28 82.76 82.40\nAUTOSCRAPER 71.56 14.06 5.31 4.06 0.63 4.37 92.49 89.13 88.69\nOpen-source LLMs\nMistral 7BCOT 3.44 0.31 0.63 95.31 0.00 0.63 94.23 4.55 4.24\nReflexion 2.19 0.00 0.31 97.19 0.00 0.31 95.60 2.78 2.49\nAUTOSCRAPER 2.87 0.00 0.00 96.77 0.36 0.00 98.57 3.23 2.87\nCodeLlamaCOT 17.98 3.75 2.25 74.53 0.00 1.50 79.75 21.98 21.36\nReflexion 18.08 4.80 2.95 73.06 0.00 1.11 78.96 23.26 22.44\nAUTOSCRAPER 23.99 8.12 1.48 64.94 0.00 1.48 78.59 28.70 28.41\nMixtral 8 ×7BCOT 28.75 8.13 4.37 57.81 0.31 0.63 89.79 38.23 37.26\nReflexion 36.25 6.88 3.12 51.25 0.00 2.50 89.35 44.57 43.60\nAUTOSCRAPER 46.88 10.62 7.19 30.31 0.63 4.37 87.32 62.71 59.75\nDeepseek-coderCOT 36.56 10.94 5.63 42.50 0.63 3.75 86.05 48.78 47.05\nReflexion 37.19 11.25 4.06 44.69 1.25 1.56 86.41 48.28 47.08\nAUTOSCRAPER 38.75 11.25 5.31 39.69 0.63 4.37 84.91 52.11 49.68\nTable 2: The executable evaluation and IE evaluation of LLMs with three frameworks in SWDE dataset. We examine\n7 LLMs, including 3 closed-source LLMs and 4 open-source LLMs.\nSpecifically, we adopt precision (P.), recall (R.), and macro-f1 (F1), which are calculated as the mean of the corresponding metrics for each case. 5.2 Main Results on S WDE Results in Table 2 show that: 1)With AUTOSCRAPER generating action sequence, LLMs can achieve better performance. Compared to the COT and Reflexion baseline, our method performs a higher ratio of correct and a lower ratio of unexecutable. Also, it should be noted that Mixtral 8×7B + AUTOSCRAPER can outperform ChatGPT + Reflexion, indicating the superiority of AUTOSCRAPER in the generation of executable action sequences in the scraper generation task. 2)Models with small parameter sizes have significant difficulties in understanding and writing executable paths, so they can be considered challenging to apply in this task. On the contrary, large-scale models demonstrate a more stable ability in instruction alignment, web structure comprehension, and reflection on execution results; 3)Traditional IE evaluation metrics cannot well describe the success rate of our task. Especially for the precision metric, it fails to reveal the performance gap among 416different methods with different models. This is because the extraction metrics only evaluate the results that have been extracted, ignoring that unexecutable or empty extractions also greatly damage the executability. 5.3 Generate with Golden Label To better illustrate the effectiveness of our framework in generating executable action sequences, we compare the performance of COT, Reflexion, andAUTOSCRAPER , while answering the instruction. By offering the same extraction targets, we can effectively detect the performance of different frameworks in generating action sequences. Table 3 shows experimental results, from which we can have the following observations: 1)Our proposed progressive understanding framework still effectively enhances the model’s performance under this setting; 2)LLMs still suffer in accurately understanding web page contents with semistructured markup languages, which illustrate the performance gap between Table 2 and Table 3; 3)Compared to closed-source LLMs, even provided with golden labels, Open-source LLMs are unable to achieve sustained performance improve6\nModels MethodEXECUTABLE EVALUATION\nCorrect( ↑) Prec Reca Unex.( ↓) Over. Else\nClosed-source LLMs\nGPT-3.5-\nTurboCOT 41.70 12.92 7.38 35.42 0.74 1.85\nReflexion 47.23 16.24 2.21 33.21 0.37 0.74\nAUTOSCRAPER 56.89 19.43 5.65 13.43 0.71 3.89\nGemini\nProCOT 33.44 9.38 9.06 44.69 0.94 2.50\nReflexion 35.31 9.38 6.88 43.75 1.56 3.12\nAUTOSCRAPER 45.31 13.44 6.25 30.31 1.25 3.44\nGPT-4-\nTurboCOT 61.88 11.56 9.06 11.56 1.25 4.69\nReflexion 71.25 7.19 4.69 14.37 0.94 1.56\nAUTOSCRAPER 75.31 10.94 4.37 4.06 0.63 4.69\nOpen-source LLMs\nMistral 7BCOT 2.19 0.00 0.31 97.19 0.00 0.31\nReflexion 2.19 0.00 0.00 97.50 0.31 0.00\nAUTOSCRAPER 2.19 0.00 0.00 97.19 0.31 0.31\nCodeLlamaCOT 21.40 6.27 2.21 66.79 0.74 2.58\nReflexion 22.21 4.93 3.94 66.95 0.49 1.48\nAUTOSCRAPER 26.20 12.55 5.54 53.51 0.00 2.21\nMixtral\n8×7BCOT 27.50 7.50 5.31 56.87 0.94 1.87\nReflexion 34.69 8.13 5.31 49.06 0.63 2.19\nAUTOSCRAPER 45.62 11.56 5.94 32.50 1.25 3.12\nDeepseek-\ncoderCOT 35.00 18.75 5.31 36.25 0.63 4.06\nReflexion 38.75 11.87 2.81 42.19 0.63 3.75\nAUTOSCRAPER 38.44 20.94 4.06 31.56 0.94 6.56\nTable 3: The executable and IE evaluation with 7 LLMs\non S WDE dataset with golden label.\n1 2 3 4 5\nNumber of Seed Websites020406080100Executable Metric (%)\nGPT-4-Turbo\nGPT-3.5-Turbo\nCorrect\nUnexecutableGPT-4-Turbo\nGPT-3.5-Turbo\nCorrect\nUnexecutable\nFigure 3: The performance of AUTOSCRAPER with\ndifferent number of seed websites in S WDE dataset.\nment. This phenomenon demonstrates that the bottleneck for these models lies not in understanding the webpage content but in understanding the webpage’s hierarchical structure itself. 5.4 Ablation Study To further justify the effectiveness of each component of AUTOSCRAPER , we perform an ablation study. The results are shown in Table 4. It shows that: 1) AUTOSCRAPER without a second module still beat the other two baseline methods among different LLMs. 2) The second module ofAUTOSCRAPER ,synthesis module, not only improves AUTOSCRAPER , but also improves the performance of other methods. Using more web pages for inference can make the generated scraper more stable and have better generalization. 456Models MethodEXEC EVAL IE E VAL\nCorrect( ↑) Unex.( ↓) F1\nGPT-3.5-\nTurboCOT 36.75 43.46 47.99\n-synthesis 27.56 57.24 34.44\nReflexion 46.29 37.10 55.10\n-synthesis 28.62 59.01 35.01\nAUTOSCRAPER 54.84 19.35 69.20\n-synthesis 44.52 29.33 58.44\nGemini\nProCOT 29.69 47.19 41.81\n-synthesis 27.56 57.24 33.09\nReflexion 33.12 52.50 40.88\n-synthesis 28.62 59.01 37.60\nAUTOSCRAPER 42.81 34.38 54.91\n-synthesis 39.46 31.56 56.48\nGPT-4-\nTurboCOT 61.88 14.37 76.95\n-synthesis 46.88 30.00 61.20\nReflexion 67.50 10.94 82.40\n-synthesis 56.87 25.31 69.78\nAUTOSCRAPER 71.56 4.06 88.69\n-synthesis 65.31 11.87 80.41\nTable 4: Ablation study on AUTOSCRAPER . We report\nCorrect ,Unexecutable from the executive evaluation,\nandF1score from the IE evaluation in S WDE dataset.\nModel F1\nRender-Full (Hao et al., 2011) 84.30\nFreeDOM (Lin et al., 2020) 82.32\nSimpDOM (Zhou et al., 2021) 83.06\nMarkupLM BASE (Li et al., 2022) 84.31\nWebFormer (Wang et al., 2022) 92.46\nReflexion + GPT-4-Turbo 82.40\nAUTOSCRAPER + GPT-4-Turbo 88.69\nTable 5: Comparing the extraction performance (F1) of\n5 baseline models to our method AUTOSCRAPER using\nGPT-4-Turbo on the SWDE dataset. Each value of the\nsupervised model in the table is trained on 1 seed site.\n5.5 Seed Websites In all previous experiments, we fixed the number of seed websites ns= 3, which demonstrates the effectiveness of the synthesis module. In this experiment, we offer different numbers of seed webpages and test the performance of AUTOSCRAPER . The result is shown in Figure 3. As the number of seed webpages increases, the correct ratio increases, while the unexecutable ratio decreases. It suggests that the performance of AUTOSCRAPER can still be further improved by providing more seed webpages. In addition, the performance improvement reduces as the number increases, which shows that there is an upper limit to improve the performance of AUTOSCRAPER by increasing the number of seed webpages. 7\n5.6 Comparison with supervised baselines To further demonstrate that AUTOSCRAPER is adaptive to different web information extraction tasks, we conduct a comparison with 5 baseline models in web information extraction on supervised learning scenarios: Render-Full (Hao et al., 2011) proposes a complicated heuristic algorithm for computing visual distances between predicted value nodes and adjusting the predictions. FreeDOM (Lin et al., 2020) and SimpDOM (Zhou et al., 2021) encode textual features of DOM tree node with LSTM, while MarkupLM (Li et al., 2022) is pre-trained on HTML with text and markup information jointly. WebFormer (Wang et al., 2022) leverages the web layout for effective attention weight computation. Table 5 shows the result. Although the comparison is unfair because our method is in zero-shot settings, AUTOSCRAPER beat most of them on F1 scores. It shows that by designing an appropriate framework, LLMs can surpass supervised learning methods in some web information extraction tasks. 5.7 Efficiency Analysis Suppose the number of seed webpages is ns, the number of webpages on the same website is NW, the time to generate a wrapper is Tg, the time of synthesis is Ts, and the time for extract information from a webpage with a wrapper is Te. The total time for extracting all information from all websites with A UTOSCRAPER is T1=TG+TE= (nsTg+Ts) +NWTe(3) Besides, the time for LLMs directly extracting information from a webpage is Td, and the total time for extracting all information from all websites directly is T2=NWTd (4) In a real-world scenario, there are many web pages from the same websites to be extracted. Although generating a wrapper takes more time than extracting directly from a single webpage, the extraction efficiency of subsequent web pages would be significantly improved. To explore how many webpages are needed to make AUTOSCRAPER more efficient in web IE, we calculate the threshold ofNW. Suppose T1≤T2, we have TG+TE= (nsTg+Ts) +NWTe≤NWTd(5) NW≥nsTg+Ts\nTd−Te(6) 519It should be noted that Tgdepends on dmax in Algorithm 1 and can be roughly considered as Tg≈dmaxTd. In our experimental settings, we set dmax= 5andns= 3. Also, under the approximation that Ts≈TdandTd≫Te,AUTOSCRAPER have better extraction efficiency when a website contains more than 16 webpages. 5.8 Error Analysis We perform an analysis by looking at the recorded action sequence of AUTOSCRAPER with GPT-4Turbo and identify the following common failure modes. We mainly focus on the case categorized as unexecutable, over-estimate, and else. Non-generalizability of webpages The target information and corresponding webpage structures exhibit variations across different webpages, leading to a lack of generalizability in AUTOSCRAPER (i.e., the inability to apply the same rules across all webpages in the same website). For instance, for the task \"Please extract the name of the company offering the job\" in the website job-careerbuilder, most webpages contain the company name, but there is one webpage where the company name is \"Not Available\" on another node of DOM tree. Miss in multi-valued Presented with the task of generating a scraper for extracting address in restaurant webpages or contact phone number from university websites, the target information is located in multiple locations in the webpage, such as the information bar, title, etc. Although AUTOSCRAPER is capable of generating action sequences to extract portions of information, crafting a comprehensive action sequence that captures all of the information remains a challenge. 6 Conclusion In this paper, we introduce the scraper generation task and the paradigm that combines LLMs and scrapers to improve the reusability of the current language-agent-based framework. We then propose AUTOSCRAPER , a two-phase framework including progressive generation and synthesis module to generate a more stable and executable action sequence. Our comprehensive experiments demonstrate that AUTOSCRAPER can outperform the state-of-the-art baseline in the scraper generation task. 8\nLimitation We introduce a paradigm that combines LLMs with scrapers for web scraper generation tasks and propose AUTOSCRAPER to generate an executable action sequence with progressively understanding the HTML documents. Though experimental results show the effectiveness of our framework, there are still some limits to our work. First, our framework is restricted to the paradigm in the information extraction task for vertical webpages. LLMs with scrapers provide high efficiency in open-world web IE tasks, but can hardly transfer to existing web environments such as Mind2Web (Deng et al., 2023), WebArena (Zhou et al., 2023). Second, our framework rely on the performance of backbone LLMs. Enhancing LLMs’ ability to understand HTML is a very valuable research question, including corpus collection and training strategy. We will conduct research on HTML understanding enchancement in future work. Ethic statement We hereby declare that all authors of this article are aware of and adhere to the provided ACL Code of Ethics and honor the code of conduct. Use of Human Annotations Human annotations are only utilized in the early stages of methodological research to assess the feasibility of the proposed solution. All annotators have provided consent for the use of their data for research purposes. We guarantee the security of all annotators throughout the annotation process, and they are justly remunerated according to local standards. Human annotations are not employed during the evaluation of our method. Risks The datasets used in the paper have been obtained from public sources and anonymized to protect against any offensive information. Though we have taken measures to do so, we cannot guarantee that the datasets do not contain any socially harmful or toxic language. 606",
        "aspect_review": "The paper is well-organized and clearly presents the novel AUTOSCRAPER framework, making it accessible to readers. The introduction of a new paradigm for web scraper generation with LLMs addresses significant challenges in the field. The two-stage framework of progressive generation and synthesis is innovative and effectively leverages LLMs for web scraper generation. Comprehensive experiments demonstrate the framework's superiority over existing methods, achieving state-of-the-art results in zero-shot settings.\n\nWhile the paper presents a novel approach, the impact on broader NLP tasks beyond web scraping is not fully explored. The evaluation could benefit from additional metrics to assess the scalability of the approach. The results, though strong, are primarily focused on specific datasets, limiting the generalizability of findings. The analysis of failure cases could be expanded to provide deeper insights into the limitations of the approach. The framework is currently limited to vertical web pages and may not transfer well to more complex web environments. The theoretical underpinnings of the new executability metric could be more thoroughly discussed. Minor improvements in clarity and formatting could enhance the paper's readability. Details on the hyperparameters and implementation specifics could be expanded for reproducibility. The paper could benefit from clearer definitions and explanations of technical terms and concepts for a broader audience.",
        "general_review": "The paper introduces a novel paradigm for generating web scrapers using large language models (LLMs), addressing the limitations of existing wrapper-based and language-agent-based methods. The proposed AUTOSCRAPER framework demonstrates strong technical depth through its two-stage process of progressive generation and synthesis, which efficiently handles diverse and changing web environments. The paper provides comprehensive experimental validation, outperforming state-of-the-art methods in various datasets and achieving new benchmarks in zero-shot settings. Additionally, the work has significant practical impact as it enhances the scalability and reusability of web scrapers, reducing manual effort and improving efficiency in web information extraction tasks.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in explaining the synthesis module's impact on different methods. 2. Details on hyperparameters and implementation specifics could be expanded to aid reproducibility. 3. Some figures may require minor formatting adjustments for better readability. 4. While the code is open-source, a slight clarification on its availability and usage instructions would enhance accessibility for practitioners."
    },
    "28": {
        "paper": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL\nAnonymous ACL submission\nAbstract\nLarge Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-theart (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. 1 Introduction The Text-to-SQL task involves the automatic generation of SQL statements from natural language (Qin et al., 2022). Prior research primarily focused on training encoder-decoder models on text corpora and database schemas to capture generation patterns (Xu et al., 2021). Given the impressive capabilities of Large Language Models (LLMs) in various Natural Language Processing (NLP) tasks, numerous studies have endeavored to apply LLMs to this task (Li et al., 2024a; Zhang et al., 2024). Recent investigations have proposed enhancing the reasoning capabilities of LLMs in the Text-to-SQL task, yielding substantial progress. Diverse methods such as the few-shot Chainof-Thought (CoT) (Wei et al., 2022), selfQ: Find the names of the artists who are from UK and have produced English songs.A: SELECTartist_nameFROM artist WHERE country  = \"UK\" INTERSECTSELECTartist_nameFROM song WHERE languages  =  \"english”Q: What are the ids of  songs that are available in either mp4 formatorhave resolution above 720?A: SELECTf_idFROM files WHERE formats = \"mp4\" UNIONSELECTf_idFROM song WHERE resolution  >  720”                                       ⋯Q: Find the names of the artists who have produced English songsbut have neverreceived rating higher than 8.A: SELECTDISTINCT artist_nameFROM song WHERE languages  =  \"english\" EXCEPTSELECTDISTINCT artist_nameFROM song WHERE rating  >  8Targeted Drilling Bank\nFind the last name of students who is either female (sex is F) and living in the city of code BAL or male (sex is M) and in age of  below 20.\nI have seen similar problems before! I guess I should use 'union' to connect the two subqueries.SELECTlnameFROM student WHERE sex  =  'F' AND city_code=  'BAL' UNIONSELECTlnameFROM student WHERE sex  =  'M' AND age  <  20Figure 1: Demonstration of targeted drilling prompt on\nmulti-set problems.\nconsistency (Wang et al., 2022), and the decomposition prompt that emphasizes dissecting complex problems and solving them sequentially (Khot et al., 2022) have been introduced. A leading method, DIN-SQL (Pourreza and Rafiei, 2024), breaks down the task into several subtasks, classifies the complexity based on the nested logic of the problem, and applies different prompt strategies accordingly. However, like other studies, it overlooks the unique characteristics of SQL statements, which differ from math word problems and other code tasks. For calculations involving multiple sets, keywords like ’INTERSECT’ or ’UNION’ are often used to combine statements of several subproblems, making these queries naturally suitable for decomposition. Counting and sorting problems typically rely on ’GROUP BY’ operations to identify objects to be aggregated and use ’ORDER BY’ to sort other objects. Just like during a test with various question types, the knowledge points and problem-solving experiences that emerge in our minds are different. 1\nMotivated by the brief overview of SQL question types above, we consider whether it is feasible to guide LLMs, akin to training human students for specific question types to master key concepts, by focusing on type-related examples during reasoning (Zhou et al., 2024). Accordingly, we randomly select 100 multi-set operation questions from the training set, which require the use of keywords like ’INTERSECT’ or ’EXCEPT’. We adopt two different prompt strategies: one from DIN-SQL, where these questions are classified as nested-level questions, providing samples of various question types under this complexity level; and another, as depicted in Figure 1, where we only provide LLM multi-set question examples with the same number. With these strategies, we achieve execution accuracy rates of 39.0 and 55.0 using ChatGPT, respectively. The former exhibits more sub-query errors and logical confusion. Drawing on the above observation, we propose thePartitioning and Targeted Drilling (PTD-SQL) framework to enhance LLMs’ reasoning capabilities in Text-to-SQL tasks. This strategy mirrors the human learning process, where students typically first identify the group of question and then search for the most relevant knowledge points to answer it. Initially, we categorize the types of textual queries in the training set based on the keywords in the ground-truth SQL statements. Informed by previous studies, we opt not to rely solely on the LLM’s few-shot discrimination ability, but instead delegate a small LLM with fine-tuning for this task (Juneja et al., 2023; Zhuang et al., 2023). In the second step, we design prompts with different emphases for various categories of problems in the training set, and automatically generate problem sets and reference answers – the areas that the LLM needs to learn. Both of these operations are performed offline and avoid invoking GPT during testing, thus achieving cost-efficiency. Finally, during the inference stage, we classify the original textual query and design an automatic selection module to compose a few-shot prompt in the corresponding group of the problem set (An et al., 2023a). We extensively validate the effectiveness of PTDSQL on the Spider-dev, Spider-realistic, and BIRDdev datasets using three powerful LLMs, where it outperforms state-of-the-art frameworks such as DIN-SQL and DAIL-SQL. We also find that the model becomes more capable of achieving breakthroughs at the capability boundaries when equipped with PTD-SQL, which may potentially Multi-set problems! What are the drivers' first, last names and id who had more than 8 pit stops orparticipated in more than 5 race results?! Which are the car makers which produce at least 2 models andmore than 3 car makers ? List the id and the maker.     ⋯                    \nCombination problems! How many gymnasts are from eachhometown? ! Show the status of the city that has hosted the greatestnumber of competitions.      ⋯                    \nFiltering problems! Find the number of concerts happened in the stadium with the highest capacity.! What are the locations and names of  all stations with capacity between 5000 and 10000?\"     ⋯                    \nOther simple problems! How many countries exist?! What type of  pet is the youngest animal, and how much does it weigh?! List names of conductors in descending order     ⋯                    \nFigure 2: Some samples of proposed partition.\nextend to other reasoning tasks. Furthermore, our approach adheres to a one-time query paradigm, showing advantages in terms of token consumption and inference time, also allowing many methods targeting schema linking or database content alignment to be seamlessly integrated, thereby anticipating even higher performance. 2 Related Work LLM Reasoning Nowadays, the development of reasoning models based on LLM has become a popular and critical field. Many efficient prompting methods have been proposed, such as Chain-ofThought (Wei et al., 2022), which guides LLM in step-by-step thinking; Least-to-Most (Zhou et al., 2022), which makes the model adapt to the difficulty gradient; and Decomposition-based prompting (Khot et al., 2022; Ye et al., 2023), which breaks down difficult problems to solve them separately. In addition, Self-Consistency (Wang et al., 2022) demonstrates the overall tendency of LLM towards the correct answer through voting, Self-discover (Zhou et al., 2024) allows the model to make different problem-solving plans according to different types of questions, and Selfrefine (Madaan et al., 2024) enables LLM to learn from the feedback of its problem-solving process. Besides, many works also strengthen the weaker aspects of LLM at the code level, such as PAL (Gao et al., 2023b) and PoT (Chen et al., 2022). LLM-based Text-to-SQL Nowadays, many studies are focusing on utilizing LLMs to complete Text-to-SQL tasks, primarily involving more efficient prompt design and advanced process deployment. Strategies that have proven effective in common sense reasoning and mathematical reasoning, such as CoT and self-consistency, have also been applied to enhance Text-to-SQL reasoning. C3 (Dong et al., 2023) and StructGPT (Jiang 2\nMulti-set Problem\nQuery: What are the drivers' first, last names and id who had more than 8 pit stops or participated in more than 5 race results?Let's think step by step.<1> Question Decomposition: The query emphasizes union logic, so we can decompose the question into two subproblems: 1. what are the drivers' first, last names, and id who had more than 8 pit stops; 2. what are the drivers' first, last names, and id who participated in more than 5 race results.<2> Schema Linking: To complete the first subproblem, we need to use tables ['drivers', 'pitStops']. To complete the second subproblem, we need to use tables ['drivers', 'results’].<3> Operation: For the first subproblem, we need to perform a 'GROUP BY' operation on the column 'driverId' and filter by performing 'HAVING COUNT()' on the column 'stop'. For the second subproblem, we need to perform a 'GROUP BY' operation on the column 'driverId' and filter by performing 'HAVING COUNT()' on the column 'raceId’.<4> SQL Generation: Use 'union' operation to connect the queries of subproblems to form the final SQL statement.SQL query: SELECT T1.forename ,  T1.surname ,  T1.driverid FROM drivers AS T1 JOIN pitstops AS T2 ON T1.driverid  =  T2.driverid GROUP BY T1.driverid HAVING count(*)  >  8 UNIONSELECT T1.forename ,  T1.surname ,  T1.driverid FROM drivers AS T1 JOIN results AS T2 ON T1.driverid  =  T2.driverid GROUP BY T1.driverid HAVING count(*)  >  5Combination Problem\nQuery: Show the status of the city that has hosted the greatest number of competitions.Let's think step by step.<1> Operation: The query requires the city that has hosted greatest number of competitions, so we should apply the 'count'operation to table 'farm_competition', and sort itin descending order. Since the unit to which the competitions being counted in the query belongis city and only table 'farm_competition' has column 'Host_city_ID', so we should apply the 'group by'operation to column 'Host_city_ID' in table 'farm_competition’.<2> Schema Linking: Due to the direct foreign key connection between table 'city' and 'farm_competition'. We need to use tables ['city', 'farm_competition’].<3> SQL Generation: The query requires the status of the city that has hosted the greatest number of competitions, so we should select the 'Status' column in the 'city' table. The query does not require the count of most competitions, so it is only used for filtering and not selected. SQL query: SELECT T1.Status FROM city AS T1 JOIN farm_competitionAS T2 ON T1.City_ID  =  T2.Host_city_ID GROUP BY T2.Host_city_ID ORDER BYCOUNT(*) DESC LIMIT 1Figure 3: Prompt demonstrations for Multi-set and Combination problem.\net al., 2023) have introduced effective zero-shot strategies based on GPT, along with meticulous interface settings. DIN-SQL (Pourreza and Rafiei, 2024) divides the Text-to-SQL task into phased subtasks and assigns different LLMs to specialize in completing each stage, as well as categorizes the difficulty of questions to provide varying prompt strategies. DAIL-SQL (Gao et al., 2023a) has conducted a comprehensive evaluation of many prompt-based methods and proposed a more precise samples matching approach to improve results. Recent approaches have also concentrated on addressing issues not yet considered in the data itself. For instance, PET-SQL (Li et al., 2024b) focuses on leveraging prior knowledge within databases to enhance the accuracy of responses at the token level. SQL-CRAFT (Xia et al., 2024) suggests allowing models to engage in interactive refinement to improve reasoning accuracy. DEA-SQL (Xie et al., 2024) integrates multiple optimization techniques to propose workflow agents. 3 Pipeline of PTD-SQL In this section, we present the process of the PTDSQL framework as illustrated in Figure 4, which includes: i. The design and implementation of the proposed Query Group Partition (QGP) sub-task; ii. The automatic construction of distinct query group question banks, each containing its unique reasoning process; iii. The inference process. 1823.1 Query Group Partition In this section, we first provide the definition of the QGP sub-task and then describe the process of finetuning the small LLM using PEFT to accomplish the QGP task. Problem Formulation SQL queries differ from math word problems and other code problems, such as Python, as their textual labels often contain highly characteristic expressions, making problem group identification convenient. We cluster them based on label keywords: multi-set, combination, filter, and other simple problems. Multi-set problems frequently involve two or more layers of logic and require keywords like ’INTERSECT’, ’UNION’, ’EXCEPT’ for connection. Combination problems necessitate the use of ’GROUP BY’ operation to group data, followed by sorting, taking extreme values, and other purposeful operations. Filter problems involve constructing conditional statements and using them for target screening. The remaining problems are classified as other simple problems, as depicted in Figure 2. Considering that some queries may have implicit labels of other types, we provide prioritized classification criteria in the prompt to alleviate the impact of model bias. Specific examples are shown in the Appendix E.3. The task objective is explicitly defined as: given a text query q, we need to output its problem group ˆg. It is formulated as: 3\nMethod Exact Match\nLlama-2-7b + LoRA 85.0%\nChatGPT + 10-shot 68.0%\nTable 1: Performance on validation set of QGP sub-task.\nˆg=f(q|θ) (1) where f(· |θ)can present a model with parameters θ. We randomly select the training set STfor the QGP task on the original training set and separate the validation set SV. Fine-tuned LLM Classifier Inspired by previous works (Juneja et al., 2023; Zhuang et al., 2023), we consider delegating the ability to determine categories to the fine-tuning process of the small LLM rather than directly trusting the discrimination capability of LLM. With the rapid advancement of PEFT technology, we choose Low-Rank Adaptation (LoRA) (Hu et al., 2021) to fine-tune the Llama-2-7b model to solve the QGP problem. For a pre-trained weight matrix W0∈Rd×k, LoRA adds a bypass using two decomposition matrices A∈Rd×randB∈Rr×k, where r≪min(d, k). The forward process of single weight matrix is modified to: h=W0x+BAx (2) During finetuning with LoRA, we freeze the original weights of LLM and only update low-rank matrices AandB. For annotated labels Gand outputs of LLM, the objective loss is defined as : L=CrossEntropy (G|f(q|θ+δθ)) (3) Finetuned Small Model vs. Few-shot GPT The performance of the fine-tuned Llama-2-7b model and the few-shot prompting ChatGPT on the QGP task is presented in Table 1. This highlights the superiority of PEFT in downstream tasks, and prompts us to use the former on test set. 3.2 Targeted Drilling Bank Auto-construction In this section, we explain how to construct targeted drilling banks for different question groups in PTD-SQL, which can be compared to the specialized training and reference ideas and answers designed by teachers for students before examinations. Previous works grade the difficulty based on whether the problem requires nesting and design 251corresponding prompt templates. However, this approach only focuses on the surface logic of SQL queries and does not consider the distinct thinking paths required by the essence of different question groups for LLM. Given that selecting irrelevant examples may also be detrimental to LLM’s thinking, in PTD-SQL, we can benefit from the proposed QGP. That is, for test queries of specific question groups, we can directly and accurately locate the problem banks with similar thinking paths. Multi-set problems often require breaking down a complex problem into multiple subqueries, integrating the different results through connecting keywords. For filtering problems, we can often prompt LLM to first propose the organization of filtering conditions and then process the selection target. Therefore, these two types of problems are naturally suitable for the design inspiration of decomposed prompting. We show an example of prompt construction for a multi-set problem, as depicted in Figure 3. For filtering problems, our decomposition focuses on the division of conditional statements and the extraction of target columns, and the specific prompts are shown in the Appendix E.1. It is worth mentioning that we treat schema linking as a byproduct of LLM’s thinking process, thereby achieving the purpose of one-time generation, which reduces the query cost. For combination problems and other simple problems, we construct concise CoT templates. For the former, the model is required to distinguish the objects that need to be counted (sorted or taking extreme values) and the groups they belong to, thus improving the ability to organize answers under this question type. An example is shown in Figure 3. For the remaining simple problems, we choose to use the ground truth SQL query directly as the composition of the few-shot prompt, without introducing other thinking processes. After creating four different types of few-shot prompts, we apply them separately to their respective problem groups in the training set to generate the thinking process and the final SQL query. We select the samples with correct execution results of the SQL query to form four targeted drilling banks because we believe that the thinking paths in the examples with correct final answers are highly likely to be reasonable and enlightening. These are the sources of the examples that LLM refers to during the inference phase. The specific statistics of different targeted drilling banks are shown in Appendix A.1. 4\nTraining Set What are the names of movies that get 3 star and 4 star?  Multi-Set operationReport the number of students in each classroom. Combination Operation…\nPEFT\nTraining Set \n…Group 1Group 2Group 4\nShot Bank 1Shot Bank 2Shot Bank 4\n……\n## TablesTable Dogs (dog_id,owner_id,abandoned_yn,breed …)Table Owners (owner_id,first_name,last_name,street ...）Table Professionals (professional_id,role_code,first_name …)…Table Sizes (size_code,size_description)Table Treatment_Types (treatment_type_code …)Table Treatments (treatment_id,dog_id,professional_id …)## Foreign KeysDogs.owner_id= Owners.owner_id, … Treatments.professional_id= Professionals.professional_id,Treatments.treatment_type_code= Treatment_Types.treatment_type_code## QueryWhich professionals live in the state of Indianaor have done treatment on more than 2 treatments? List his or her id, last name and cell phone.\nShots auto-selection\n## Example 1## Tables:…## Foreign keys:…## queryWhat are the names and ids of all stations that have more than 14 bikes available on average or had bikes installed in December?Let’s think step by step.<1> Question Decomposition: 1. names and ids of all stations that have more than 14 bikes available on average; 2. names and ids of all stations that had bikes installed in December. <2> Schema Linking: … <3> Operation: GROUP BY, HAVING AVG(), UNION SQL: SELECT T1.name, T1.id FROM station AS T1 JOIN status AS T2 ON T1.id = T2.station_id GROUP BY T1.id HAVING avg(T2.bikes_available) > 14 UNIONSELECT name, id FROM station WHERE installation_dateLIKE '12/%'\nI guess it’s a multi-set operation.\nOriginal text\nN-shots\nSELECT professional_id,  last_name,  cell_numberFROM Professionals WHERE state  =  ‘Indiana’ UNIONSELECT T1.professional_id ,  T1.last_name ,  T1.cell_number FROM Professionals AS T1 JOIN Treatments AS T2 ON T1.professional_id  =  T2.professional_id GROUP BY T1.professional_id HAVING count(*)  >  2\n(a)(b)\n(c)Figure 4: Overflow of PTD-SQL. (a) QGP sub-task. (b) Targeted drilling bank auto-construction. (c) Reasoning\nstep.\n3.3 Few-shot Selection Few-shot example construction is a crucial step in prompt engineering because LLMs are sensitive to few-shot samples. In PTD-SQL, we perform QGP on each textual query and then automatically select shots in the corresponding targeted drilling bank. Semantic matching Previous work has verified the effectiveness of methods based on semantic vector matching (An et al., 2023b). We calculate and store sentence embeddings for all textual queries in the targeted drilling bank using OpenAI textembedding-ada-0021, resulting in an offline bank matrix M. For test queries, we encode them with text-embedding-ada-002 and calculate the cosine similarity with Mto measure the degree of semantic matching as some previous works do. sim 1(s, si) =Emb (s)Emb (si)T\n|Emb (s)||Emb (si)|(4) Syntactic matching Considering that textual SQL queries have strong syntactic features, such as counting problems often having phrases like \"how many\", and extreme value demands often accompanied by comparative adjectives like \"largest\" or \"lowest\". Therefore, we use token overlap counts to rank the syntactic relevance of samples in the corresponding targeted drilling bank. 1https://platform.openai.com/docs/guides/embeddingssim 2(s, si) =set(tokenize (s)) &set(tokenize (si))\nset(tokenize (s))\n(5) Mix-of-matching Similar to the idea of multiway recall, we mix an equal amount of examples selected by the two strategies above, for instance, choosing the top 2 most relevant examples from each in a 4-shot scenario, in order to provide as rich and relevant samples as possible within the same problem group, thus guiding effective thinking. 4 Experiments 4.1 Experimental Setup Datasets Spider (Yu et al., 2018) is the most widely used cross-domain dataset. This dataset has 7,000 training data in the training set and 1,034 data in the development set, covering 200 different databases and spanning 138 domains. Spiderrealistic (Deng et al., 2020) is a more challenging dataset containing 508 test data points which manually mask the specific column selections in the text query. BIRD (Li et al., 2024a) dataset contains 95 large-scale real databases, convering 37 professional domains. More details and usage of the data can be found in Appendix A.2. Evaluation Most previous work adheres to two common evaluation metrics: 1) Exact Match Accuracy (EM): It requires that each subcomponent of 5\nMethods Type EX\nT5-3B + PICARD†(Scholak et al., 2021) Fine-tuning 79.3\nRESDSQL + NatSQL†(Li et al., 2023) Fine-tuning 84.1\nC3 + ChatGPT†(Dong et al., 2023) Zero-shot 81.2\nChatGPT (Liu et al., 2023) Zero-shot 70.1\nGPT-4 (Achiam et al., 2023; Gao et al., 2023a) Zero-shot 72.3\nDIN-SQL + ChatGPT§(Pourreza and Rafiei, 2024) Few-shot 76.8\nDIN-SQL + GPT-4§Few-shot 80.6\nDIN-SQL + Deepseek-coder-6.7b-instruct‡Few-shot 73.6\nDAIL-SQL + ChatGPT†(Gao et al., 2023a) Few-shot 79.1\nDAIL-SQL + GPT-4†Few-shot 83.1\nDAIL-SQL + GPT-4 + Self-Consistency†Few-shot 83.6\nDAIL-SQL + Deepseek-coder-6.7b-instruct‡Few-shot 75.7\nPTD-SQL + ChatGPT ours Few-shot 80.3\nPTD-SQL + GPT-4 ours Few-shot 85.7\nPTD-SQL + Deepseek-coder-6.7b-instruct ours Few-shot 76.7\nTable 2: EX on Spider-dev set. Results of methods with\n†are taken from original paper or open-source code\nrepository. Results with label ‡are implemented by us.\nResults with §are obtained from the running results files\nprovided by (Pourreza and Rafiei, 2024) and evaluation\nprogram (Zhong et al., 2020).\nthe SQL query generated by the model matches the gold SQL query provided in the dataset. 2) Execution Accuracy (EX): EX judges correctness based on whether the answer returned by executing the predicted SQL query in the database is consistent with the gold query. Since a textual query may correspond to several correct but stylistically different SQL query formulations, it is a more accurate measure of Text-to-SQL methods. Besides, Valid Efficiency Score (VES) is used to demonstrate the efficiency of valid SQLs provided by models. Baselines We compare three different path Textto-SQL methods, including fine-tuning, zero-shot, and few-shot prompting methods. Among them, the fine-tuning method includes PICARD (Scholak et al., 2021) and the current SOTA RESDSQL+NatSQL (Li et al., 2023). The zero-shot method C3 (Dong et al., 2023) focuses on schema linking filtering and removing GPT’s inherent bias for SQL generation. DIN-SQL (Pourreza and Rafiei, 2024), which breaks down the textual query into multiple staged questions. DAIL-SQL (Gao et al., 2023a) considers optimizing sample selection and organization to further enhance LLM’s reasoning ability in Text-to-SQL. Implementation Details In order to comprehensively evaluate the performance of the framework on closed-source and open-source models and demonstrate its effectiveness, we employ three LLMs for comparison purposes: OpenAI GPT3.5-turbo-0613 for ChatGPT, GPT-4-0613, and 384Methods Type EX\nChatGPT Zero-shot 67.3\nGPT-4 Zero-shot 66.5\nDIN-SQL+ChatGPT Few-shot 70.3\nDIN-SQL+Deepseek-coder-6.7b-instruct Few-shot 68.3\nDAIL-SQL+ChatGPT Few-shot 69.3\nDAIL-SQL+ Deepseek-coder-6.7b-instruct Few-shot 68.9\nPTD-SQL+ChatGPT Ours Few-shot 72.2\nPTD-SQL+Deepseek-coder-6.7b-instruct ours Few-shot 69.9\nTable 3: EX on Spider-realistic dataset.\nMethods EX VES\nCodeX 34.4 41.6\nChatGPT+CoT 36.6 42.3\nGPT-4 46.4 49.8\nDIN-SQL + ChatGPT 41.0 51.4\nDIN-SQL + GPT-4 50.2 58.1\nDIN-SQL + Deepseek-coder-6.7b-instruct 40.7 49.0\nDAIL-SQL + ChatGPT 41.2 49.2\nDAIL-SQL + GPT-4 53.6 56.5\nDAIL-SQL + Deepseek-coder-6.7b-instruct 42.4 50.2\nPTD-SQL + ChatGPT ours 44.2 53.3\nPTD-SQL + GPT-4 ours 57.0 57.7\nPTD-SQL + Deepseek-coder-6.7b-instruct ours 45.4 55.0\nTable 4: EX and VES comparison on BIRD dataset.\nDeepseek-coder-6.7b-instruct2(Guo et al., 2024). The latter is pretrained on high-quality code corpora and has attained the current state-of-the-art performance among open-source code models in the realm of code generation. Maximum context length is limit to 4096 for OpenAI LLMs and 2048 for open-source LLMs. 4.2 Main Results As shown in Table 2, PTD-SQL + GPT4 achieves the best EX metric on the Spider-dev dataset. Additionally, PTD-SQL surpasses DIN-SQL and DAILSQL when using ChatGPT and Deepseek-coder6.7b-instruct as base models. Compared to the more advanced DAIL-SQL framework, PTD-SQL achieves relative increases of 1.5%, 3.1%, and 1.3% on ChatGPT, GPT-4 and Deepseek-coder6.7b-instruct respectively. When compared with previous fine-tuning and prompting methods, PTDSQL also attains a comparative performance. Besides, as shown in Table 3, ChatGPT equipped PTD-SQL also outperforms previous methods and GPT-4 using zero-shot. Furthermore, the results shown in Table 4 indicate that all three powerful models equipped with PTD-SQL demonstrate stronger EX. In terms of VES indicators, PTD-SQL 2https://huggingface.co/deepseek-ai/deepseek-coder-\n6.7b-instruct\n6\nBase Model Method Easy Medium Hard Extra All\nDeepseek-coder\n-6.7b-instructDIN-SQL 86.3 81.2 59.8 48.8 73.6\nDAIL-SQL 86.7 81.6 59.2 50.0 75.7\nPTD-SQL 87.1 78.9 74.9 57.2 76.7\nChatGPTDIN-SQL 90.7 82.3 62.1 56.6 76.8\nDAIL-SQL 91.5 83.8 71.2 56.0 79.1\nPTD-SQL 90.7 83.1 80.6 56.6 80.3\nGPT-4DIN-SQL 89.9 84.3 78.2 57.8 80.4\nDAIL-SQL 90.7 89.7 75.3 62.0 83.1\nPTD-SQL 94.8 88.8 85.1 64.5 85.7\nTable 5: Performance comparison on three LLMs across\ndifficulty levels on Spider-Dev dataset.\nalso has a certain competitiveness. A case study on Spider is given in Appendix B.6. Furthermore, we discuss the advantages of PTD-SQL in terms of token consumption and inference time in Appendix D. 5 More Discussion In this section, we investigate the efficacy of PTDSQL, taking into account both the challenges posed by the database itself ( RQ1 ) and the performance across various problem groups ( RQ2 ). Concurrently, we delve into the insights that PTD-SQL contributes to the LLM-based Text-to-SQL domain. Furthermore, we perform ablation studies on the employed modules, primarily focusing on the effectiveness of introduced QGP task ( RQ3 ), and the influence of shot selection strategies within the same targeted drilling bank ( RQ4 ). 5.1 RQ1: Performance from a Difficulty-level In this subsection, we evaluate the superiority of PTD-SQL over existing state-of-the-art frameworks based on the difficulty levels defined by the database, respectively. As depicted in Table 5, PTD-SQL outperforms DIN-SQL and DAIL-SQL across different base LLMs, particularly at hard and extra difficulty levels, indicating that LLM can specialize in a problem group and demonstrate enhanced targeted reasoning ability after imitating and delving into problems within the same group. Moreover, we illustrate the performance variations of PTD-SQL in comparison to DIN-SQL across different problem types, thereby discerning the disparities between problem group partitioning strategies and difficulty grading strategies. As inferred from Figure 5, LLMs have made great progress at their respective capacity limits under PTD-SQL. For instance, ChatGPT, akin to a diligent student, achieves a 29.8% improvement in hard difficulty by focusing on similar problems, but fails to progress in extra difficulty, possibly due e a s y m e d i u m h a r d e x t r a− 1 0− 5051 01 52 02 53 03 54 0\nDeepseek-coder -6.7b-instruct\nChatGPT\nGPT -4\nD i ﬃ c u l t y\nS i m p l e M o d e r a t e C h a l l e n g i n g− 1 0− 5051 01 52 02 53 03 54 0\nDeepseek-coder -6.7b-instruct\nChatGPT\nGPT -4\nD i ﬃ c u l t yFigure 5: Under different difficulty levels, the per-\ncentage gain (%) in EX metric on Spider (left) and\nBIRD (right) obtained by the three models using PTD-\nSQL compared to DIN-SQL.\nModel QGP Method Easy Medium Hard Extra All\nChatGPTw/o QGP 84.7 76.5 71.8 52.4 73.8\nChatGPT + 10-shot 86.7 78.9 74.1 56.0 76.3\nLlama-2-7b + LoRA 90.7 83.1 80.6 56.6 80.3\nDeepseek-coder\n-6.7b-instructw/o QGP 84.7 76.5 71.8 52.4 73.8\nChatGPT + 10-shot 84.3 79.1 69.5 54.8 74.9\nLlama-2-7b + LoRA 87.1 78.9 74.9 57.2 76.7\nTable 6: EX performance based on partition with differ-\nent accuracy levels on the Spider-dev dataset.\nto inherent model limitations. The deepseek-coder6.7b-instruct model, with capabilities comparable to ChatGPT, also shows the most significant improvement in hard difficulty (25.3% vs 17.2% on extra). However, GPT-4, resembling an elite student, achieves the most substantial breakthrough in extra difficulty and refines its responses across other difficulty levels through referencing and absorption. The results on the BIRD dataset show that GPT-4 achieves the largest increase in performance on the challenging group, while the other two models focus on simple and moderate difficulties. This suggests that LLMs with different levels of reasoning capability can guarantee their upper limit by practicing questions. Detailed results on BIRD are depicted in Appendix B.1. 5.2 RQ2: Performance under Problem Groups As depicted in Figure 6, PTD-SQL demonstrates a more pronounced advantage in multi-set problems and combination problems when employing three different baseline models. These problem types entail more intricate reasoning and perplexing conditions. Apart from when using GPT-4, the other two models yield very similar results in the filtering problem across the three methods. This suggests that this category of problem relies more on the inherent ability of the model to effectively organize the filtering conditions, rather than emphasizing the logical level. Besides, we consider the detailed performance of queries with multiple question type 7\n4050607086070809060708090100Multi-setSimpleFilteringCombinationSimpleCombinationSimpleCombination\nFilteringFilteringMulti-setMulti-set\nDeepseek-coder-6.7b-instructChatGPTGPT-4DIN-SQLDAIL-SQLPTD-SQLFigure 6: EX of three LLMs on Spider-dev dataset when equipped with DIN-SQL, DAIL-SQL and PTD-SQL.\nfeatures in Appendix B.5, and propose findings and directions for further improvement. 5.3 RQ3: Effectiveness of QGP In this section, we examine the impact of the QGP subtask. As shown in Table 1, the Few-shot method does not align well within a specific context, resulting in weaker performance compared to the fine-tuned model. To further investigate this, we conduct additional experiments involving problem groups classified by ChatGPT, as well as experiments that eliminate the QGP stage and directly recall shots from all targeted drilling banks. The findings presented in Table 6 indicate that a decline in QGP accuracy adversely affects the final outcomes, with a relative decrease of 5.0% when testing on ChatGPT. Besides, ChatGPT exhibits a slight reduction in extra difficulty, while Deepseek demonstrates tolerance for classification accuracy at medium to easy difficulty levels. However, upon removing the QGP, the model surpasses the zeroshot performance, but there is a substantial decline in the results. This observation implies that incorporating various types of questions during similarity retrieval might introduce confusion and burden to the model, and also validates the relevance of the QGP stage. 5.4 RQ4: Ablation on Few-shot Selection In this section, ablation experiments are conducted for three distinct shot selection strategies within the same problem group. As illustrated in Figure 7, the hybrid strategy demonstrates a favorable integration effect beyond the ’easy’ category, resulting in an overall improvement. This finding suggests that taking into account both query keywords and Easy Medium Hard Extra All5060708090Syntactic matching\nSemantic matching\nMix-of-matching\nDiﬃculty L evelExecution A ccuracy (%)Figure 7: Ablations on few-shot selection strategy on\nthe Spider-dev dataset. (Detailed data in Table 14)\nsemantic similarity can yield a more comprehensive prompting effect. Additionally, an ablation study on the few-shot number is demonstrated in Appendix B.4, revealing its effect. 6 Conclusion In this article, a novel method called PTD-SQL is proposed for LLMs to conduct targeted drilling on specific groups of questions after partitioning. This approach addresses the category tendency of SQL queries, which has been overlooked in previous work. By focusing on the thinking logic of specific types, LLM can effectively enhance its reasoning capabilities. Empirical observations from our comprehensive ablation studies reveal that PTDSQL significantly reduces the likelihood of LLM making errors within its distinct capability range while demonstrating substantial gains across various question groups. Furthermore, it is posited that this approach can be extended to other domains, such as math word problems and different types of code problems, paving the way for future research. 8\n7 Limitations The limitations of this article lie in the exploration of its effectiveness on larger-scale databases with a broader domain span. Moreover, even SQL statements with strong structural characteristics may have different types of divisions. Therefore, a more detailed investigation of performance under these different divisions can be further improved and optimized. Besides, as stated in Appendix B.5, for queries with multiple question types, we can also recall example questions from multiple shot banks to comprehensively consider the model and improve the fault tolerance of QGP subtasks. This may be an interesting topic that can be improved in the future. In addition, due to space constraints, this article doesn’t optimize for more detailed issues such as schema linking and database content alignment. However, the optimization methods for these issues can be relatively easily integrated into PTD-SQL as a downstream optimization method. Due to our greater focus on the improvement of LLM’s reasoning ability for the question answering itself in this article, we are confident that we can achieve better results by adding the aforementioned sub-optimization methods. 559",
        "aspect_review": "The paper presents a comprehensive experimental validation on multiple datasets, demonstrating that PTD-SQL outperforms state-of-the-art methods like DIN-SQL and DAIL-SQL across various difficulty levels and problem types. The proposed PTD-SQL framework introduces a novel approach by employing query group partitioning and targeted drilling, which mirrors human learning processes to enhance the reasoning capabilities of LLMs in Text-to-SQL tasks. The results show significant improvements in execution accuracy and valid efficiency score on datasets like Spider and BIRD, highlighting the practical impact and effectiveness of the proposed method.\n\nThe paper could benefit from additional details on the hyperparameters used in fine-tuning the LLMs, as well as minor clarifications on the availability of the code and implementation specifics.",
        "general_review": "The paper presents a novel and impactful framework, PTD-SQL, which significantly enhances the reasoning capabilities of Large Language Models (LLMs) in Text-to-SQL tasks. The methodology introduces a unique approach of query group partitioning and targeted drilling, allowing models to focus on specific problem types, mirroring human learning processes. This results in substantial improvements in model performance, particularly at the boundaries of their capabilities. The experimental validation is comprehensive, demonstrating that PTD-SQL outperforms state-of-the-art methods on multiple datasets. The paper also highlights the practical impact of the framework, showing advantages in terms of token consumption and inference time, and its potential extension to other reasoning tasks.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the experimental setup and results. 2. Some small details on hyperparameters and implementation specifics are missing, which could aid in reproducibility. 3. There are very minor formatting issues in some figures that could be addressed for better readability. 4. Slight clarification on the availability of the code and datasets used in the experiments would be helpful for the community."
    },
    "29": {
        "paper": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for\nLarge Language Models\nAnonymous ACL submission\nAbstract\nLarge language models (LLMs) are increasingly used for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks mainly focus on single-turn evaluations, overlooking the models’ capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark to evaluate the multiturn conversational abilities of LLMs. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or creating new examples using GPT-4 with a human-in-the-loop process to avoid data leakage. To study the factors impacting multiturn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 10 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in specific tasks. We observe significant performance degradation in multi-turn settings compared to single-turn settings in most models, which is not correlated with the models’ fundamental capabilities. Moreover, we identify the distance to relevant content and susceptibility to error propagation as the key factors influencing multi-turn performance. MT-Eval is released publicly to encourage future research towards more robust conversational models1. 1 Introduction The rise of large language models (LLMs) is transforming our daily lives and professional endeavors with their growing capabilities. Individuals increasingly rely on LLM-based AI assistants for diverse tasks, such as coding assistance, summarizing text from documents, and devising business strategies 1Code and data will be released upon acceptance.(Zheng et al., 2023a). These tasks often require understanding complex instructions and adapting to evolving needs through multiple user interactions. Moreover, it is crucial for LLMs to generate contextually coherent responses by retaining and recalling historical information. The ability of LLMs to engage in multi-turn conversations is often overlooked in existing evaluation frameworks. For instance, MMLU (Hendrycks et al., 2020) evaluates language understanding in multiple tasks using single queries, and MT-Bench (Zheng et al., 2023b) evaluates conversational ability using twoturn interactions without considering more turns and various conversation types. To bridge the gap, we propose MT-Eval, an evaluation benchmark to measure the capabilities of LLMs to conduct coherent multi-turn conversations. Our analysis of interactions in LMSYS-Chat1M (Zheng et al., 2023a) reveals four predominant patterns when users interact with AI assistants: Recollection, where the assistant must recall information from earlier turns; Expansion, involving the exploration of varied topics within the main subject; Refinement, where initial instructions are clarified or revised; and Follow-up, consisting of questions based on the assistant’s previous responses (see Figure 1). These patterns are reflective of the majority of real-world multi-turn interactions with assistants. We then construct evaluation sets for each interaction type by augmenting existing datasets or creating new ones to cover real-world applications. We employ the GPT-4 with a human-in-the-loop process for generating new instances to avoid data contamination. The human annotators rigorously review and revise each instance to ensure quality, difficulty, relevance, and originality. A performant multi-turn conversational model should perform well in multi-turn interactions and demonstrate minimal performance difference from the corresponding single-turn scenario. Therefore, we compare models using both response quality 1\nin the multi-turn setting and performance differences for identical queries in single vs. multi-turn settings. Combining these two results provides a comprehensive view of their multi-turn conversational capabilities. We evaluate 10 popular LLMs, including both open-source and closed-source models. Beyond general evaluation, we conduct in-depth analysis and ablation studies revealing how LLMs conduct multi-turn interactions and what affects their performance. Our findings include: 1) The closed-source models still dominate in multi-turn conversational abilities, but some open-source models have comparable performance to GPT-3.5-Turbo in some tasks. 2) Most LLMs perform worse in the multiturn setting than in single-turn. The performance gap between the two settings is not related to the model’s fundamental capacities. 3) Increasing distance to relevant content negatively impacts performance. 4) Models are prone to error propagation due to sensitivity to dialogue history. We summarize our contributions as follows: •We propose a comprehensive multi-turn conversational capabilities evaluation benchmark that covers a wide range of real-world scenarios. •We provide an in-depth analysis of the performance of 10 popular LLMs across our benchmark, offering insights into their capabilities in multi-turn conversations. •We identify key factors that influence LLM multi-turn performance, such as the distance to relevant content and error propagation. •We demonstrate the importance of evaluating LLMs in multi-turn settings, highlighting the performance discrepancies that can arise when compared to single-turn evaluations. 2 Related Work Recent advancements in LLMs (OpenAI et al., 2023; Touvron et al., 2023a; Chiang et al., 2023) have significantly improved their ability to engage in human-like, multi-turn conversations. These models can now understand instructions, intentions, and context from human prompts, offering valuable responses (Zhao et al., 2023). However, a limited number of studies have delved into the multi-turn conversation capabilities of LLMs. Zheng et al. (2023b) developed MT-Bench, a dataset comprising 80 meticulously crafted multi-turn questions 129designed to evaluate the conversational flow and instruction-following capabilities of LLMs. Nevertheless, the dataset’s limited sample size poses a challenge, with each conversation consisting of only two turns. This constraint hinders the ability to broaden the evaluation scope or capture the intricacies of more extended conversational contexts. Lee et al. (2023) proposed HALIE, a framework for evaluating human-AI interaction. but its reliance on human participation limits its scalability and efficiency across different tasks. In specific domains, Liao et al. (2023) designed an automatic evaluation framework for multi-turn medical consultations capabilities of LLMs. Moreover, Wang et al. (2023) proposed MINT to evaluate LLMs’ ability to solve tasks with multi-turn interactions. It focuses on LLM’s ability to use tools and utilize feedback during multi-turn conversations. In contrast, our work evaluates LLM’s comprehensive ability to conduct multi-turn conversations, possibly involving multiple types of dialogue in one session. Statistics Number\nAvg. # Turns per Dialogue 6.96\nAvg. # Words in Prompt † 760.41\nMax. # Words in Prompt † 2Avg. # Words in Response † 99.31\nMax. # Words in Response † Avg. # Words per Turn 60.63\nMax. # Words per Turn Total # Dialogues Total # Turns 1Table 1: Key statistics of MT-Eval. Detailed statistics\nfor individual tasks are provided in the Appendix. †:\nEstimated using GPT-4 responses.\n3 MT-Eval MT-Eval is designed to comprehensively evaluate the multi-turn conversation capabilities of LLMs across a wide range of real-world application contexts. By reviewing the existing authentic AIhuman conversation datasets (like ShareGPT and LMSYS-Chat-1M dataset (Zheng et al., 2023a)), we have identified and categorized four primary modes of engagement in user-assistant interactions: Recollection : Users present queries or tasks that necessitate the assistant’s capacity to retrieve information from prior interactions, relying on the assistant’s global context awareness and long-term memory capabilities. For instance, a user may instruct the model to initiate all the following responses with words starting with the letter \"c.\" 2\nFollow-u p\n1.\"The Hobbit\" is a\nchildren's fantasy\nnovel written by\nJ.R.R. Tolkien . It\nwas published . . .\n2.. . .\nWhat are some other \nbooks written by J.R.R. \nTolkien ?Refinement\nFormat your answer in \nJSON. Recollection\nTell me five facts about \nthe book “The Hobbit”.Begin all your \nresponses in the \nupcoming conversation \nwith the letter C.\nOk.Expansion\nTell me five facts about \nthe book “The Hobbit”.\nTell me five facts about \nthe book “The Great \nGatsby”.\n1. Creation: \"The\nHobbit\" was created by\nJ.R.R. Tolkien as a\nstandalone children's\nstory . . .\n2. . . .\n1.\"The Hobbit\" is a\nchildren’s . . .\n1.\"The Great Gatsby\"\nis a novel written by\nF. Scott Fitzgerald . .\n.\n2.. . .\nTell me five facts about \nthe book “The Hobbit”.\n{\n\"facts\": [\n\"\\\"The Hobbit \\\" is a \nchildren's . . .\",\n. . .,\n]\n}\nTell me five facts about \nthe book “The Hobbit”.\n\"The Lord of the Rings\" \ntrilogy . . .\n1.\"The Hobbit\" is a\nchildren’s . . .\nFigure 1: Illustration of the four dialogue tasks in MT-Eval: Recollection, Expansion, Refinement, and Follow-up.\nRecollection accesses the model’s ability to recall information from previous conversations. Expansion evaluates\nthe model’s capacity to address queries surrounding the same topic. Refinement gauges the model’s adherence to\nprogressively complex instructions. Follow-up examines the model’s proficiency in responding to queries that build\nupon its preceding response. A more detailed description of these tasks can be found in Section 3.\nExpansion : Users delve into various subjects while staying within the confines of the same topic. For example, the user might ask different questions concerning one specific topic “Michael Jordan”. Typically, the user will not refer to any specific details from previous dialogues. Refinement : Users clarify or modify their previous instructions. For instance, users might add more detailed constraints, like specifying the desired output format, or provide feedback to clarify their instructions. This requires the assistant’s ability to keep track of the instruction changes and leverage feedback to refine its responses. Follow-up : Users ask questions that build upon the assistant’s last response, often referencing specific details or opinions mentioned in that response. For instance, a user may seek additional information about a person mentioned in the assistant’s prior response. This assesses the assistant’s capacity to engage in coherent conversations. MT-Eval includes test instances targeting these four conversation categories (see Figure 1), while mirroring everyday scenarios of document processing, content creation, and information retrieval (Zheng et al., 2023a). It comprises 168 dialogue sessions with 1,170 turns to assess models’ competence in handling such realistic multi-turn interactions. 3.1 Construction To prevent data leakage in MT-Eval, we either extend existing datasets or construct new instances using GPT-4 with human-in-the-loop verification. Our preliminary findings indicate that this method generates test instances comparable to, or even sur- 200passing, human-generated ones, while significantly reducing cost and time. Human annotators verify each instance for difficulty, relevance, and originality. They revise or replace instances as needed and ensure the correctness and quality of the answers. Annotators modify or replace 82% of the queries generated by GPT-4. Appendix D provides further details about the human annotation process. For Refinement and Expansion tasks, we convert four and seven document-based NLP tasks, respectively, into a dialogue format. In this format, the first turn presents the context document and the initial query while subsequent turns provide the remaining queries. These NLP tasks, motivated by their prevalence in real-world AI-human interactions (Zheng et al., 2023a), include summarization, question answering, rewriting, etc.2. The full list of the NLP tasks used in these tasks is in Appendix I. We create new context documents for these tasks using GPT-4. The Refinement task introduces an additional constraint on each dialogue turn based on the previous instructions within the same NLP task. Each NLP task contains six queries. To increase complexity, each test instance in the Refinement task spans two NLP tasks. The first six turns correspond to the first task, and the remaining six turns correspond to the second. The Expansion task presents queries from seven different NLP tasks at each turn based on the same context document introduced in the first turn. Each test instance thus spans seven dialogue turns. 2As shown in Figure 3 of Zheng et al. (2023a), approxi-\nmately 20% of conversations involve NLP tasks\n3\nModel Avg. Recollection Expansion Refinement Follow-up\nGPT-3.5-Turbo 7.72 6.90 7.87 6.92 9.21\nChatGLM3-6B 5.49 2.92 5.90 4.73 8.39\nQwen-chat-7B 6.55 5.25 7.02 5.47 8.49\nVicuna-7B-v1.5 6.44 5.45 6.70 5.31 8.31\nLlama-2-chat-7B 6.11 3.86 5.87 6.20 8.53\nMistral-Instruct-7B 7.46 7.22 6.98 6.58 9.05\nVicuna-13B-v1.5 7.01 6.27 6.70 6.37 8.68\nLlama-2-chat-13B 6.31 3.66 6.37 6.37 8.82\nQwen-chat-14B 7.26 6.21 7.58 6.11 9.12\nMixtral-Instruct-8x7B 7.47 6.17 7.42 6.77 9.52\nTable 2: Multi-turn performance in four dialogue tasks. The highest score in each column is highlighted in bold ,\nwhile the second-highest score is underlined. Closed-source models outperform open-sourced models generally.\nWe create two sub-tasks for the Recollection task with varying difficulties. The easier subtask is a document classification task. The set of available class labels is given in the first turn, and the model predicts the class label for the document provided at each subsequent turn. The harder sub-task is a global instruction following task, where the model must adhere to an instruction given in the first turn (e.g., formatting, content restrictions) throughout the dialogue (Zhou et al., 2023). At each subsequent turn, the user poses a content creation or open-ended question. Table 9 lists the instructions used in this task. For the Follow-up task, we extend MT-Bench (Zheng et al., 2023b) by adding three extra turns, each asking a question that extends the assistant’s response. All questions are human-created due to the difficulty of GPT-4 generating sufficiently challenging follow-up questions. GPT-4 generates the initial answers, which are then verified by humans. Details for constructing each task can be found in Appendix I. For all tasks except Follow-up, we also establish corresponding single-turn instances for all dialogue turns. Constructing equivalent single-turn instances for the Follow-up task is difficult because it inherently depends on the assistant’s previous response. Appendix E provides details for constructing the single-turn instances. 4 Experiment 4.1 Models We evaluate 10 popular LLMs, including ChatGLM3-6B (Du et al., 2022), Vicuna-v1.5 (7B, 13B) (Chiang et al., 2023), Llama-2-chat (7B, 13B) (Touvron et al., 2023b), Qwen-chat (7B, 14B) (Bai et al., 2023), Mistral-Instruct-7B (Jiang et al., 2023), Mixtral-Instruct-8x7B (Jiang et al., 2024), 269and GPT-3.5-Turbo (Ouyang et al., 2022)3. We exclude GPT-4 (OpenAI et al., 2023) from our main analysis to avoid potential bias as indicated in Appendix H. 4.2 Evaluation Evaluating LLM responses poses challenges due to the inclusion of additional contents in the generated responses, such as introductions, conclusions, or supplementary explanations (Yue et al., 2023; Zhou et al., 2023). This hinders accurate quality assessment using rule-based automatic evaluation metrics. Recent research shows that using LLMs for evaluation, especially GPT-4, aligns closely with human judgment (Zheng et al., 2023b; Bitton et al., 2023). Therefore, we use GPT-4 to evaluate all responses, except for classification and recollection tasks, which can be scored with simple rules. To evaluate the responses, we provide GPT-4 with a zero-shot chain-of-thought (Wei et al., 2022) to assign an integer rating from 1 to 10 based on the relevant context. Previous work has shown that utilizing chain-of-thought in evaluation enhances the quality (Liu et al., 2023). The evaluation prompt can be found in Figure 9. We evaluate the global following sub-task in the Recollection task using heuristics and rules (Zhou et al., 2023), calculating the average number of dialogue turns adhering to the global instruction and normalizing the result to a maximum score of 10. For the document classification task, we measure the classification accuracy directly and normalize it to a full score of 10. A good multi-turn conversational model should demonstrate strong capacity in multi-turn interactions and exhibit a minimal performance gap between single-turn and multi-turn settings. Therefore, we also evaluate the corresponding single-turn 3We utilized gpt-3.5-turbo-0613 and gpt-4-0613.\n4\nModel ST Avg. MT Avg.Recollection Expansion Refinement\nST MT ST MT ST MT\nGPT-3.5-Turbo 8.07 7.23 (-0.84) 8.75 6.90 8.39 7.87 7.08 6.92\nChatGLM3-6B 5.71 4.52 (-1.19) 5.05 2.92 7.20 5.90 4.89 4.73\nVicuna-7B-v1.5 6.31 5.82 (-0.49) 6.35 5.45 6.99 6.70 5.60 5.31\nLlama-2-chat-7B 7.21 5.31 (-1.90) 7.26 3.86 7.36 5.87 7.00 6.20\nQwen-chat-7B 6.86 5.91 (-0.95) 7.17 5.25 7.46 7.02 5.96 5.47\nMistral-Instruct-7B 7.69 6.93 (-0.76) 8.47 7.22 7.60 6.98 7.00 6.58\nVicuna-13B-v1.5 7.10 6.45 (-0.65) 6.98 6.27 7.67 6.70 6.66 6.37\nLlama-2-chat-13B 7.55 5.47 (-2.08) 7.51 3.66 7.86 6.37 7.29 6.37\nQwen-chat-14B 7.62 6.64 (-0.98) 8.40 6.21 7.90 7.58 6.58 6.11\nMixtral-Instruct-8x7B 8.28 6.78 (-1.50) 7.86 6.17 9.50 7.42 7.48 6.77\nTable 3: Performance of various models across different dialogue tasks in both single-turn and multi-turn settings.\nSTandMTdenote single-turn and multi-turn respectively. The best score in each column is highlighted in bold and\nthe second-highest score is underlined . Bracketed numbers indicate the change in score between the single-turn and\nmulti-turn scenarios. The Follow-up task is omitted since there is no equivalent single-turn setting. Most models\nexhibit a substantial performance gap between the single-turn and multi-turn settings.\nperformance, in addition to multi-turn settings, to measure the gap between them. 4.3 Human Verification Task Pearson Spearman\nRefinement 0.74 0.58\nExpansion 0.67 0.65\nFollow-up 0.72 0.70\nAvg. 0.71 0.64\nTable 4: The correlation scores between human ratings\nand GPT-4 ratings for different tasks.\nTo verify that GPT-4’s evaluation mostly aligns with human preference, we recruited five annotators to evaluate 60 randomly selected responses from each multi-turn dialogue task, excluding the Recollection task which uses automatic evaluations. They evaluated in total of 180 responses. More details regarding the human evaluation can be found in Appendix G. Table 4 shows Pearson’s correlation and Spearman’s rank correlation coefficient between human ratings and GPT-4 ratings. It shows that GPT-4 ratings have an average Spearman correlation of 0.64 and a Pearson correlation of 0.71. The results indicate that GPT-4 ratings align well with human ratings, consistent with recent findings (Zheng et al., 2023b; Bitton et al., 2023; Liu et al., 2023). 4.4 Result Table 2 shows the multi-turn performance of the evaluated LLMs across all four task categories. All models achieve an average score lower than 8, indicating that MT-Eval poses a considerable challenge for multi-turn capabilities. In particular, most models perform worst in the Recollection task, failing 332to obey the global instruction stated initially in successive turns. All models also perform poorly in the Refinement task, often ignoring constraints from previous turns. Overall, MT-Eval comprises tasks of varying difficulty, targeting various aspects of multi-turn interaction and effectively highlighting the strengths and weaknesses of LLMs across diverse multi-turn scenarios. We provide a more detailed discussion of the results below. Performance of Closed- versus Open-Source LLMs. Table 2 illustrates that closed-source LLM, GPT-3.5-Turbo, generally outperforms opensource ones in multi-turn dialogues. Although GPT-3.5.Turbo stands out with an average score of 7.72, open-source models like Mistral-Instruct7B and Mixtral-Instruct-8x7B demonstrate exceptional performance in specific tasks, making them comparable to or even surpassing GPT-3.5-Turbo’s performance. For instance, Mixtral-Instruct-8x7B achieves a score of 9.52 in Follow-up dialogues, outperforming GPT-3.5-Turbo’s score of 9.21. These results align with recent research, suggesting that open-source LLMs can achieve comparable or even superior performance to closed-source LLMs in certain domains (Chen et al., 2023). MistralInstruct-7B surpasses all 7B models and exhibits performance comparable to 13B models, consistent with the findings reported in Jiang et al. (2023). However, Mixtral-Instruct-8x7B, despite its strong performance in most tasks, faces challenges in adhering to global instructions in recollection tasks, resulting in a similar average score as MistralInstruct-7B. 5\nFirst Second Difference\nGPT-3.5-Turbo 6.98 6.85 -0.12\nChatGLM3-6B 5.25 4.21 -1.03\nVicuna-7B-v1.5 5.40 5.21 -0.19\nLlama-2-chat-7B 6.97 5.42 -1.55\nQwen-chat-7B 5.80 5.13 -0.67\nMistral-Instruct-7B 6.53 6.62 0.09\nVicuna-13B-v1.5 6.62 6.12 -0.50\nLlama-2-chat-13B 6.99 5.74 -1.25\nQwen-chat-14B 6.30 5.92 -0.38\nMixtral-Instruct-8x7B 6.90 6.63 -0.26\nTable 5: Performance of the first task (the first six turns)\nand the second task (last six turns) in Refinement task.\nThe performance difference between the two tasks is\nalso shown.\n0 2 4 6 8\nTurnstart_char\njson_format\nbullet_list\nnumber_sentences\ncapital_letter\nno_comma\ntwo_responses\nplaceholder\nVicuna-13B-v1.5\nQwen-chat-14BMistral-Instruct-7B Mixtral-Instruct-8x7B\nFigure 2: The average number of turns that different\nmodels can adhere to the instructions in the Recollection\ntask. Each instruction consists of two dialogue sessions\nwith ten dialogue turns. The description of the instruc-\ntions can be found in Table 15.\nInferior Performance in Multi-Turn Dialogues. Table 3 shows a performance gap between multiturn and single-turn scenarios. It indicates that most models exhibit a substantial decline in the performance of multi-turn dialogues compared to single-turn instances. This performance gap therefore serves as a valuable indicator of a model’s multi-turn capabilities. Notably, the observed gap between the two scenarios does not appear to be directly correlated with the fundamental capabilities of the models. For instance, while Llama2-chat models outperform Vicuna models in the single-turn setting, they noticeably lag in multi-turn dialogues. This observation underscores the importance of including multi-turn evaluation when 380conducting a comprehensive evaluation of LLMs. Challenges in Long-Distance Information Retrieval for LLMs. Our study reveals that LLMs often underperform in tasks requiring information from earlier dialogue turns. In the Recollection task, all LLMs struggle to adhere to the initial global instructions as the conversation length, i.e., distance from their initial instruction, increases. Table 5 also supports this trend, revealing that most models perform better on the first task (i.e., the first six turns) compared to the second (i.e., the final six turns), as the turns in the second task are further from the given document at the beginning. Our error analysis confirms that LLMs commonly overlook prior instructions. A detailed discussion is presented in Section 4.5. 1 3 5 7 9 11\nTurn456789ScoreSwitch TaskPerformance in Refinement Task\nLlama-2-chat-7B\nQwen-chat-7B\nVicuna-7B-v1.5\nMistral-Instruct-7BLlama-2-chat-13B\nQwen-chat-14B\nVicuna-13B-v1.5Mixtral-Instruct-8x7B\nGPT-3.5-Turbo\nChatGLM3-6B\nFigure 3: Performance across turns in Refinement task.\nEach dialogue has two NLP tasks with each task com-\nprising six increasingly complex instructions. The tran-\nsition to the second NLP task occurs at the seventh turn\nas denoted by the grey dashed line. The performance of\nall models declines as more instructions are added.\n4.5 Further Analysis This section presents further analyses of four topperforming models. We meticulously examine the ten responses with the largest score difference in multi-turn and single-turn scenarios generated by each model across the four dialogue tasks, resulting in an analysis of a total of 160 responses. The objective is to understand the factors that degrade model performance in multi-turn dialogues, rather than assessing their foundational capabilities. Our analysis reveals that 80 responses (50%) did not comply with earlier instructions, 77 responses (48.1%) were misdirected by the errors accumulated in the earlier context, and 3 instances (1.9%) were at6\ntributed to evaluation errors. A comprehensive analysis of these findings is provided below. Noncompliance with Earlier Instructions (50%). Figure 2 reveals significant variation in how models follow the initial instructions in the Recollection task. Open-source models face challenges with specific instructions, particularly those prohibiting comma usage or requiring generating two distinct responses simultaneously. Mixtral-Instruct-8x7B, despite its strong performance in other multi-turn tasks, struggles to follow many global instructions, such as formatting responses as JSON. Our case studies also show that LLMs often forget previous instructions. An example is illustrated in Figure 13. All models encounter difficulties with countingrelated instructions, such as limiting responses to a specific number of sentences or including a set number of placeholders in the response. This issue also arises in the Refinement task, where models often struggle to identify the correct paragraph for tasks such as translation or noun identification in the n-th paragraph. Error Propagation (48.8%). Accumulated errors from preceding dialogue turns often confuse the models, leading to more incorrect responses. A notable example of this issue is the misidentification of the correct paragraph in the initial turn of many refinement tasks. The models persist in fulfilling new instructions based on this incorrect paragraph, which further accumulate errors and result in consistently low scores throughout the dialogue. We explore this phenomenon in greater depth through ablation studies detailed in Section 4.6. Evaluation (1.2%). GPT-4 occasionally misinterprets instructions, leading to inaccurate evaluations. This issue primarily surfaces in the Refinement task, where GPT-4 struggles to identify relevant constraints within a series of instructions. Figure 14 illustrates an instance of this behavior. Despite these minor errors, using GPT-4 to evaluate is still a highly accurate and efficient method that aligns well with human judgment as shown in Table 4. 4.6 Ablation Study Inspired by the insights gained from the earlier sections, we proceed to conduct two ablations studies to investigate the effects of varying dialogue con- 459texts on the model’s performance. Additionally, we conduct another ablation study to explore how the distance between the relevant context and the current query affects performance. Gold Context vs. Self-Predicted Context. Table 7 presents the results of three dialogue tasks, conditioned on dialogue history of self-generated responses (i.e. the main results) or gold responses from human-verified GPT-4 outputs. The results indicate that models conditioned on gold context exhibit significant improvement in Recollection and Refinement tasks. We attribute this performance gap to two factors. Firstly, using gold context prevents the error propagation from earlier turns. Secondly, the gold responses serve as in-context examples, providing valuable knowledge for the model (Brown et al., 2020). Notably, using gold responses in the Expansion task yields only a slight improvement. This is likely because each dialogue turn in this task is a distinct NLP task, thus not benefiting from these examples of other tasks. Influence of Dialogue History as In-context Examples. We investigate the impact of dialogue history as in-context examples on model performance in document classification, following previous work (Min et al., 2022). We manipulate dialogue history in four settings and vary the number of dialogue turns (either four or nine). Each turn includes a document and a category depending on the setting used. The Gold setting involves random documents with their correct labels. The Diverse Gold Class setting is similar to Gold but excludes documents sharing the current turn’s label. In the Single Gold Class setting, documents from a randomly chosen category are provided, avoiding the current turn’s label. The Random Class setting assigns random labels to the randomly selected documents. Random Class (5) andRandom Class (10) denotes the performance in turn 5 and 10 respectively. Table 8 reveals that incorporating gold labels of randomly selected documents ( Gold ) improves performance compared to zero-shot setting, underscoring the value of in-context examples in the dialogue history (Brown et al., 2020; Min et al., 2022). The diverse gold class setting yields similar improvements, even with the documents belonging to the same label as the current turn excluded in the dialogue history. However, dialogue history limited to a single class can negatively impact weaker models, suggesting that biased examples may be 7\nWithout 1 Between 3 Between 6 Between 1 Front 3 Front 6 Front\nMistral-Instruct-7B 6.53 6.44 6.25 6.08 6.66 6.68 6.83\nVicuna-13B-v1.5 6.62 5.91 5.47 5.56 6.25 6.16 5.89\nQwen-chat-14B 6.30 5.89 5.76 5.17 6.22 6.01 6.18\nMixtral-Instruct-8x7B 6.90 6.47 6.57 6.33 7.01 6.58 6.89\nTable 6: Performance of various LLMs in Refinement task with varying numbers of distracting turns (1, 3, or 6)\ninserted at the front (Front) or in between (Between) the document and query turns.\nModelRecollection Expansion Refinement\nPredicted Gold Predicted Gold Predicted Gold\nMistral-Instruct-7B 5.25 7.29 6.98 7.02 6.58 7.38\nVicuna-13B-v1.5 4.64 7.32 6.70 6.87 6.37 7.15\nQwen-chat-14B 4.43 7.00 7.58 7.63 6.11 6.95\nMixtral-Instruct-8x7B 3.21 7.11 7.42 7.47 6.77 7.17\nTable 7: Comparison of model performance in three dialogue tasks, conditioned on dialogue history with self-\ngenerated responses versus gold responses.\nGold DGC SGC RC RC (5) RC (10) ST\nVicuna-13B-v1.5 81.00 84.00 70.00 45.00 62.00 28.00 75.00\nQwen-chat-14B 94.00 95.00 86.00 69.00 68.00 60.00 94.00\nMistral-Instruct-7B 96.00 95.00 95.00 75.00 80.00 70.00 94.00\nMixtral-Instruct-8x7B 95.00 95.00 94.00 57.00 60.00 54.00 88.00\nTable 8: Performance in classification task using various dialogue contexts. Gold : Randomly select documents\nwith their proper labels. DGC : The diverse Gold Class setting. Similar to Gold , but exclude documents with the\nsame labels as the current turn. SGC : The Single Gold Class setting, which randomly chooses documents from the\nsame category, avoiding the current turn’s label. RC: The Random Class setting, which randomly selects documents\nand assigns random labels to them. The (5)and(10) refer to the performance at turn 5 and 10 respectively. ST:\nSingle-Turn, the single-turn performance with no dialogue context.\nharmful. Contrary to previous findings (Min et al., 2022), the Random Class setting significantly reduces performance. Furthermore, the performance at the 10th turn is even worse than the 5th turn, indicating the presence of error propagation. We extend our analysis to the Follow-up task with a more complicated setup. The detailed analysis can be found in Appendix M. Impact of Irrelevant Context on Performance. We conduct further experiments to examine how irrelevant context, placed at different positions, affects the performance in multi-turn dialogues. We insert varying numbers of dialogue turns, randomly sampled from LMSYS-Chat-1M (Zheng et al., 2023a), either at the beginning or between the document and the query turns in the Refinement task. Table 6 shows that inserting these turns at the beginning results in mixed outcomes. Notably, Mistral-Instruct-7B and Mixtral-Instruct8x7B even show improved performance, while other models show slight declines. This suggests that models are capable of switching topics in a multi-turn dialogue without being affected by pre- 534vious discussions. Conversely, inserting distracting turns between the document and query turns consistently degrades performance. This further supports that the increasing distance between the document and the queries negatively impacts performance in multi-turn dialogues. 5 Conclusion MT-Eval represents an important first step in systematically evaluating and understanding LLMs’ multi-turn conversational abilities. Our experiment shows a pronounced gap between single-turn versus multi-turn performance across current models, a phenomenon that persists irrespective of the underlying capabilities of the models. Our comprehensive analysis reveals that the distance to relevant content and susceptibility to error propagation are the key factors that cause a decline in multi-turn performance. We believe this work not only sheds light on the current limitations of LLM’s multiturn conversational abilities, it also paves the way for further efforts to close the identified gap and develop robust conversational models capable of multi-turn interactions. 8\nLimitations This work focuses on constructing a multi-turn evaluation benchmark and exploring factors that contribute to performance differences in single-turn and multi-turn scenarios. Therefore, we use a simple but effective method: prompting GPT-4 with chain-of-thought (Wei et al., 2022) to perform evaluations (Liu et al., 2023). Our study confirms that GPT-4’s evaluations closely align with human ratings. However, our analysis suggests a potential bias in LLM towards its generated outputs. While more complex approaches like multi-agent debate (Chan et al., 2023) could mitigate this bias, we opt for a simpler, more cost-effective method. This choice aligns with our focus on benchmark development and analysis rather than exploring evaluation methods. Due to computational limits, our experiments do not include any larger open-source models like Llama-2-chat-70B. Further studies can investigate whether larger LLMs exhibit similar findings outlined in this paper. 579",
        "aspect_review": "The paper introduces a novel benchmark, MT-Eval, which comprehensively evaluates the multi-turn conversational capabilities of large language models (LLMs). The methodology includes categorizing interaction patterns into four types and constructing multi-turn queries using a human-in-the-loop process, ensuring quality and relevance. MT-Eval provides a diverse set of tasks that mirror real-world scenarios, such as document processing and information retrieval. It includes 1,170 turns across 168 dialogue sessions, offering a robust evaluation framework for multiturn interactions.\n\nThe related work section could be expanded to include more recent studies and a broader range of benchmarks that have addressed multi-turn interactions. While the paper identifies key factors affecting multi-turn performance, such as distance to relevant content, the justification for these findings could be further elaborated with more detailed theoretical backing. The experiments, while comprehensive, could benefit from including larger open-source models to validate whether the findings hold across different model sizes. The reliance on GPT-4 for evaluation, although justified, introduces potential bias towards its outputs. Alternative evaluation methods could be explored to mitigate this. Some sections of the paper, particularly the analysis and results, could be better organized to enhance readability and clarity. The paper could include more comparisons with other benchmarks to highlight the unique contributions and improvements offered by MT-Eval. Certain aspects, such as the process of human-in-the-loop verification and the specific criteria for task difficulty, could be described in greater detail to provide a clearer understanding of the benchmark's construction.",
        "general_review": "The paper introduces MT-Eval, a novel benchmark designed to evaluate multi-turn conversational abilities of large language models (LLMs), addressing a significant gap in existing evaluation frameworks. The methodology is robust, employing a human-in-the-loop process to ensure high-quality, relevant, and original data. The paper provides comprehensive experimental validation by evaluating 10 popular LLMs, offering insights into their performance in multi-turn settings. It identifies key factors affecting multi-turn performance, such as distance to relevant content and error propagation, which have significant practical implications for developing more robust conversational models.\n\n1. The paper could benefit from minor writing improvements for clarity, particularly in sections explaining the experimental setup. 2. There are small details on hyperparameters and implementation that could be elaborated to aid reproducibility. 3. Some figures could use very minor formatting improvements to enhance readability. 4. A slight clarification on the availability of code and data would be beneficial to ensure ease of access for future research."
    },
    "30": {
        "paper": "Gender Identity in Pretrained Language Models:\nAn Inclusive Approach to Data Creation and Probing\nAnonymous ACL submission\nAbstract\nPretrained language models (PLMs) have been shown to encode binary gender information of text authors, raising the risk of skewed representations and downstream harms. This effect is yet to be examined for transgender and nonbinary identities, whose frequent marginalization may exacerbate harmful system behaviors. Addressing this gap, we first create TRANSCRIPT , a corpus of YouTube transcripts from transgender, cisgender, and non-binary speakers. Using this dataset, we probe various PLMs to assess if they encode the gender identity information, examining both frozen and finetuned representations as well as representations for inputs with author-specific words removed. Our findings reveal that PLM representations encode information for all gender identities but to different extents. The divergence is most pronounced for cis women and non-binary individuals, underscoring the critical need for gender-inclusive approaches to NLP systems. 1 Introduction Gender identity – an individual’s sense of self, reflected in their experience and perception of their gender – is closely connected to language. Speakers intentionally use language to construct their gender identity (Eckert and McConnell-Ginet, 1992); such linguistic practices evoke links with identity when perceived by others (Eckert, 2008). Crucially for NLP, this means that gender information is inherent in all linguistic data and may be inadvertently learned by computational systems. In a stark illustration, Lauscher et al. (2022) conduct a probing study to show that pretrained language models (PLMs) encode binary gender in their representations. But are transgender and nonbinary identities encoded to the same extent?1This question is vital because gender information may 1Following Zimman and Hayworth (2020b), we use the\nterm transgender to refer to individuals whose gender identity\ndiffers from the gender assigned to them at birth; cisgender to\nTrans \nwoman\nSpeaker \nFiltering\nLanguage\nFiltering\nRule-based\nText\nFiltering\nPreprocessing\n \n/\n \nFiltering\nFrozen \nText\nRepresentations\nProbe\nTRANsCRIPT\nPretrained \nModel\nTRANsCRIPT\nCis \nman\nTrans \nman\nNon-binary\nProbing \nGender \nIdentity\nCis \nwomanFigure 1: Overview of our corpus creation and probing.\nhave a skewed distribution in the data, which may be exacerbated through pretraining and propagated to downstream tasks. Any harmful system behaviors are likely to disproportionately affect transgender and non-binary people since they are generally underrepresented (Dev et al., 2021; Devinney et al., 2022). This issue reflects a broader need for genderinclusive approaches to language, centering trans and non-binary people (Zimman, 2019, 2020). We adopt precisely such an approach to investigate gender information in PLMs from an inclusive perspective. We create TRAN SCRIPT , a corpus of texts by transgender, cisgender, and non-binary speakers (see Figure 1 for an overview). It contains transcripts of YouTube videos whose creators were sampled based on public sociodemographic information from Wikidata. We use our corpus to replicate the probing methodology by Lauscher et al. those who identify with their assigned gender; and non-binary\nto all those who do not identify as exclusively female or male.\n1\n(2022) on a broader range of identities, and extend it to account for interactions with topic (Bamman et al., 2014; Dayanik and Padó, 2021). We pose the following research questions: RQ1 Do PLM representations encode gender iden-\ntity beyond the male–female binary?\nRQ2 Does the encoded information reflect author\nidentity rather than gender?\nRQ3 Is gender identity encoded differently after\nfine-tuning on an explicit label?Our approach uses various PLMs to encode segments of transcripts and then trains a probing classifier to predict the gender identity of the transcript’s author.In this setting, gender classification is used as a proxy task to examine PLM representations: above-chance performance is indicative of encoded gender information, and relatively higher performance for a given class suggests a stronger bias towards it. We provide the following contributions: (1) A comprehensive pipeline to collect YouTube transcripts, resulting in 6,000 texts by 168 individuals across five gender identities.2(2) A battery of probing experiments which consistently show that PLM representations encode all gender identities, but to different extents. The divergence is strongest for cis women and non-binary individuals, confirming the relevance of our gender-inclusive approach. (3) A novel author-controlled probing method to distinguish the effect of author and gender identity. This approach confirms that author-related information, such as the use of specific words, is also encoded in PLM representations, but does not affect the encoding of gender. 2 Related Work NLP and gender identity. Different linguistic expressions may convey the same meaning, but also reflect the social identity of the speaker; this is what sociolinguistics defines as the social meaning of linguistic variation (Eckert, 2008). Illustrating this pattern, Bamman et al. (2014) examine lexical choices on social media with respect to assigned binary gender. They identify linguistic features linked to gender identity, but also find that alignments between gender and language use are variable, not necessarily binary, and dependent on interaction. From a different perspective, such linguistic variation is exploited to predict gender on the task 2Code for reconstructing the corpus along with ex-\nperiments: https://anonymous.4open.science/r/\ninclusive-probing-of-gender-identity-A194 .of authorship profiling (Argamon et al., 2009; Das and Paik, 2021; HaCohen-Kerner, 2022). Another line of work focuses on biased gender representations learned from training data (Caliskan et al., 2017). Devinney et al. (2020) analyze corpora in terms of topics specific to masculine, feminine, and non-binary gender. Genders are treated differently in all datasets, pointing to a misrepresentation of disadvantaged groups and a risk for the biases to transfer to PLMs. Dev et al. (2021) investigate BERT’s predictions of pronouns for non-binary individuals, noting its tendency to misgender even when given additional context. Cao and Daumé III (2021) observe that NLP research tends to make strong binary assumptions around gender identity, ignoring the existence of trans and non-binary individuals. This overall highlights the need to better understand the limitations of NLP models with respect to marginalized communities. Probing PLMs. The knowledge encoded in PLMs’ internal representations is generally analyzed using probing techniques (Hupkes et al., 2018; Tenney et al., 2019; Belinkov, 2022). A probe is usually a simple classifier trained to predict a property of interest on a model’s frozen representations (Belinkov et al., 2017; Petroni et al., 2019). Its performance is taken to indicate the information encoded in the representations. However, probing classifiers may memorize patterns in the data which are unrelated to representational properties (Hewitt and Liang, 2019). Therefore, alternative probing techniques are based on an information-theoretic approach, such as the minimum description length (MDL) probe (V oita and Titov, 2020). Different techniques have been applied to probe syntactic structures (Hewitt and Manning, 2019; Linzen and Baroni, 2021), lexical semantics (Vuli ´c et al., 2020; Mickus et al., 2020), and factual knowledge (Petroni et al., 2019; Zhong et al., 2021). In directly relevant work, Lauscher et al. (2022) assess sociodemographic knowledge: age and binary gender. They use a probing classifier and an MDL probe to analyze PLMs of different sizes, comparing frozen and fine-tuned representations across layers. They find that sociodemographic informationisencoded, but also note the binary framing of gender as a limitation due to a lack of genderinclusive datasets. It is to be determined if trans and non-binary identity is encoded in the same way. Gender-inclusive datasets. Existing datasets generally treat gender as a binary variable, leading 2\nthe studies that use them to adopt the same framing (Dayanik and Padó, 2021; Lauscher et al., 2022; Orgad et al., 2022, among others). One exception is GiCoref, a dataset aimed at analyzing coreference resolution systems (Cao and Daumé III, 2021). It includes neopronouns (e.g. ze/hir ,xe/xem ) and articles about non-binary individuals. From a sociolinguistic perspective, Zimman and Hayworth (2020a) create the TransLiveCorpus using posts from four online communities aimed at transgender and non-binary people. These examples incorporate gender-inclusive data based on the topic or target audience; we are unaware of similarly inclusive datasets with author-level information. 3 TRAN SCRIPT Corpus In order to investigate the encoding of gender in PLM representations from an inclusive perspective, we require texts produced by authors of different gender identities. Faced with a lack of such corpora, we construct TRAN SCRIPT : the TRA nsgender Non-binary Cisgender transc RIPT s corpus. Due to recent restrictions on sources of sociodemographically enriched text such as X and Reddit (Davidson et al., 2023), we devise an alternative approach (Figure 1). We use Wikidata to sample English-speaking, famous YouTubers across gender identities, collect transcripts of their YouTube videos, and then filter them to ensure data quality. YouTube transcripts are a unique domain, comprising segments of prepared speech (e.g., interviews) as well as spontaneous conversations, opening possibilities for analyzing various language variations, such as regional origin (Coats, 2023) We now describe our pipeline in more detail and present a topic analysis of the collected content. 3.1 Corpus Creation Pipeline Using Wikidata to sample authors. Wikidata is an open, collaborative database containing structured data about real-world entities (Vrande ˇci´c and Krötzsch, 2014); it has has also been used to study PLMs (Petroni et al., 2019; Meng et al., 2022). Each item contains standardized property– value pairs describing its characteristics; e.g. the item “Chris Hadfield” has the property “occupation” with the value “astronaut”. Importantly for us, Wikidata contains information on YouTubers. We access the Wikidata Query Service3and select entries that (1) are an instance of human ; 3query.wikidata.org/(2) contain properties sex or gender ,date of birth , country of citizenship , and YouTube channel ID ; (3) are citizens of the USA or the UK, so as to prioritize English speakers; (4) contain properties number of subscribers andnumber of viewers/listeners for the YouTube channel, so as to control for online presence and influence;(5) for the sex or gender property, contain the value trans woman , trans man ,female ,male , ornon-binary .4We find four instances where the sex or gender property has two values, which may occur when information predating gender transition is not removed; we retain the more recent value. The cisgender groups are significantly overrepresented on Wikidata (Appendix A.1). Therefore, we apply a sampling procedure, in which transgender and non-binary individuals are the “target group” and cisgender individuals “control group”. We retain control group candidates only if they can be matched with a target group individual based on (i) country of citizenship; (ii) age, in five-year increments; and (iii) level of YouTube fame (logtransformed subscriber and view counts in ±0.5 range). This ensures that gender identity (rather than e.g. age) remains the main distinguishing feature for the two groups. Collecting YouTube transcripts. For the next step, we direct our attention to YouTube, a social media platform containing user-uploaded videos. YouTube includes vast amounts of conversational content such as interviews and vlogs.5We focus on videos which include transcripts uploaded by users, usually channel owners themselves (Lakomkin et al., 2018), because they are likely reviewed for accuracy and alignment with dialogue. We start from the YouTubers identified on Wikidata and gather their channels from the YouTube channel ID property; if multiple channels are provided, we select all of them. Next, we compile a list of all videos from these channels and download respective manually created transcripts with libraries yt-dlp6and YouTube Transcript/Subtitle API.7We retain only English transcripts with metadata language codes en,en-US , and en-GB . A user-uploaded transcript is comprised of segments. Each segment is a verbatim transcription 4See Appendix A.1 for corresponding Wikidata codes.\n5Vlogs are video blogs in which creators document their\nlives or engage in discussions while facing the camera (Biel\nand Gatica-Perez, 2010).\n6github.com/yt-dlp/yt-dlp\n7github.com/jdepoix/youtube-transcript-api\n3\nGender Ident. Users Transc. Segm. Tokens\nTrans woman 13 546 15,121 1,646,4Trans man 6 192 6,478 548,Cis woman 55 2,446 92,436 4,986,Cis man 79 2,474 80,902 4,309,Non-binary 15 514 9,397 960,Total 168 6,172 204,334 12,451,Table 1: Distribution of users, transcripts, segments and\ntokens across gender identities in TRAN SCRIPT.\nof a speaker’s utterance in a specific part of the video, associated with the start time and duration in seconds. We additionally extract all metadata from the associated YouTube videos using the pytube library8, including the video title and description. Preprocessing. After collecting the data, we apply four filtering steps to limit any noise. (1) We exclude transcripts of music videos and movie trailers, identified from video metadata ( musicVideoType attribute) and string-based heuristics (video title containing music video ,cover , ortrailer ). (2) Some transcripts include segments from multiple speakers. Therefore, we implement a speaker diarization system on the audio of the transcribed videos and leave only segments spoken by the YouTuber of interest (see Appendix A.2). (3) Our transcripts are filtered for English based on metadata, but out of precaution we also run py3langid9(Lui and Baldwin, 2012) and exclude transcripts predicted to be non-English. (4) Similarly to Lakomkin et al. (2018), we apply rule-based filters to exclude irrelevant information: transcripts with a duration less than 1 second; non-ASCII characters; URLspecific patterns http* andwww* ; speaker markings ( Speaker 1: ,Person: ) and other annotations ([laughs] ,(laughs) ,\\laughs \\). As final preprocessing steps, we tokenize the corpus with spaCy (Honnibal and Montani, 2017) using the en_core_web_md model. We concatenate continuous transcript segments and slice them into sequences of up to 256 tokens, using a 240-token sliding window for context. These are the segments referenced in the remainder of the paper. Corpus overview. The final corpus (Table 1) contains 6,172 transcripts by 168 speakers across five gender identities, for a total of 12.5million tokens. The distribution of data is skewed towards the cis8github.com/pytube/pytube\n9github.com/adbar/py3langidgender groups, possibly due to our sampling procedure: each target group speaker was matched to potentially multiple control group candidates, all of whom were retained. However, we reiterate that we included allpotential target group speakers. The problem is therefore broader than a sampling issue and likely explained by the target group’s demographic under-representation, further exacerbated in databases such as Wikidata (Zhang and Terveen, 2021). 3.2 Topic Analysis Before deploying our corpus in probing experiments, we explore its content via topic analysis. On the one hand, topics mediate the relationship between language use and gender identity (Bamman et al., 2014); on the other, they may skew gender classifiers (Dayanik and Padó, 2021). Strong topical variation across gender identity groups would affect the robustness of our probing experiments. We use the Latent Dirichlet Allocation topic model (Blei et al., 2003) and present the 15 most frequent topics in Figure 2 (see Appendix A.4 for implementation details). Nearly all topics are represented in all groups, but to different degrees. For example, trans and non-binary subcorpora include more discussions on issues surrounding the LGBTQIA+ community, such as gender identity, sexuality, and inclusion; this is in line with the community’s use of YouTube to voice lived experiences (Miller, 2019). The cisgender groups have comparatively more discussions regarding general interests and personal experiences, including sports, travel, and food. Across the groups, there is a similar proportion of topics such as politics, entertainment, internet spaces, and healthcare. These results indicate that our gender-specific subcorpora are overall comparable in topic; we therefore view them as a validation of our data collection pipeline. But we also take note of the variable relative importance of individual topics, which may also be due to idiosyncratic interests of individual YouTubers. We aim to account for it in our probing methodology, which we now present. 4 Probing Methodology We replicate the probing methodology used by Lauscher et al. (2022), who investigated binary gender information encoded in PLMs. To ensure consistency, we reimplement all their experimental steps, including the probing methods and the se4\nFigure 2: Top 15 topics in TRAN SCRIPT and the\ndistribution across gender identity groups.\nlection of PLMs. The only variable that differs in our experiments is the dataset used for the analysis. Note more generally that our method sometimes involves direct comparisons of trans and cis gender identities. We stress that we do not imply a fundamental distinction between these groups, but rather use this perspective for a controlled assessment of disparities in model behavior. 4.1 Data To ensure balanced representation across all classes10, we randomly subsample each subcorpus within TRAN SCRIPT to match the number of examples in the smallest one (i.e., the trans man part with 6,478 segments). The resulting dataset contains 32,390 segments across five gender identity groups. We divide it into training, validation, and test sets using an 80/10/10 split, resulting in 25,910, 3,240, and 3,240 segments, respectively. Each split maintains an equal proportion of segments from each gender identity group. We refer to this balanced subset as the TRAN SPROB subcorpus. 4.2 Analyzed PLMs We use PLMs from three model families: RoBERTa in the base and large configurations (Liu et al., 2019), DeBERTa base and large (He et al., 2021b), and DeBERTaV3 xsmall, small, base, and large (He et al., 2021a). We refer to Table 7 in Appendix B.1 for sizes of these models. It is important to note that our study examines only Lauscher et al.’s (2022) subset of existing PLMs because 10Now that we switch to probing, we adopt ML terminology,\nusing the terms “classes” or “labels” to refer to the variable\nextracted from PLMs - the gender identity of the authors.we aim to evaluate if their findings extend to nonbinary identities. Analyses of other models and architectures are reserved for future research. 4.3 Probing Methods To ensure the robustness of our findings, we use two probing methods: traditional probing classifiers and Minimum Description Length (MDL). Traditional probing classifier is a supervised model trained to predict a label (i.e., in our case, the gender identity of the author) from the representations of PLMs (Belinkov, 2022; Hewitt and Liang, 2019). Figure 1 (bottom panel) depicts an example of such a probe. The input dimension of the classifier matches the embedding size of PLM, and the output dimension corresponds to the number of classes in the probed task. Since the representations are frozen during training (i.e., they are not updated during back-propagation), the higher the classifier’s accuracy, the more information it was able to decode from these representations. We implement the probing classifier as a twolayer feed-forward network with ReLU activation and softmax output. We refer to Appendix B.2 for relevant hyperparameters. MDL probing is an information-theoretic approach based on the idea that the efficiency of information encoding in the frozen representations correlates with the amount of data needed to extract this information. During training, the probing model estimates the minimum length (i.e., minimum amount of data) required to transmit the target property. Thus, the more effectively the representations encode the property, the more efficiently the probing model can compress and transmit it. For estimating MDL, we use online coding (V oita and Titov, 2020). In general, a lower MDL score represents a higher extractability of the property from the given frozen PLM representations. Preprocessing. We further tokenize segments from TRAN SPROB using the default tokenizers provided with the models. We pad the given inputs to a consistent length of 512tokens and extract word embeddings from the final hidden layer of the PLMs. To create a single vector representation for each input segment, we average the word embeddings, ignoring any special tokens. These averaged embeddings are then used as frozen representations to train and evaluate our two probing methods. 5\nF1 MDL\nRoBERTa-large .81 ±.01 298 ±2.3\nDeBERTa-verLarge .81 ±.00 284 ±7.1\nDeBERTaV3-verLarge .78 ±.00 324 ±3.2\nTable 2: Pairwise probing results for frozen representa-\ntions and two gender identities: cis men and cis women.\nAverages and standard deviation for 5 runs.\n4.4 Evaluation The performance of traditional probing classifiers is evaluated on a held-out test set using the F1 score. MDL probing is cross-evaluated on the training data using the online coding measurement (see Equation (2) in Appendix B.2). Performance scores are reported as the mean and standard deviation from five runs using different random initialization. 5 Gender Identity in PLMs We now turn to the primary goal of this work – analyzing whether PLMs capture information about authors’ gender identities. Before addressing our three main research questions, we replicate Lauscher et al.’s (2022) binary classification setup with our dataset. These experiments evaluate the positioning of TRAN SPROB in relation to other domains analyzed by the authors and determine if our probing results are consistent with theirs. 5.1 Pairwise Classification We perform pairwise classification, where the probes are tasked with distinguishing between only two classes. Table 2 presents a subset of results for the three best-performing models and the same two gender identities as Lauscher et al. (2022) – cis men and cis women (for results across all genders, see Table 8 in Appendix C.1). We observe that the classifiers are capable of predicting the gender identity of the authors with a robust F1 score of 0.81. This performance places TRAN SPROB on par with the Lauscher et al.’s (2022) control task CoLA and their easiest dataset fb_wiki (V oigt et al., 2018) – Facebook posts from public figures – for which RoBERTa-large achieves 0.81 F1. Regarding MDL, our results are also the most similar to fb_wiki , where the three models receive between 260 to 300 MDL.11Most importantly, both probing methods consistently rank our three classifiers in the same order as Lauscher et al. (2022). 11The authors do not provide concrete numbers, so we esti-\nmate them from their plots.\nFigure 3: Results from the 5-way probing using frozen\nrepresentations. Average per-class F1 scores and stan-\ndard deviation for 5 runs with different random seeds.\nFigure 4: Number of errors (y-axis) from the 5-way\nprobing of frozen representations from DeBERTa-large.\n5.2 5-way Probing of Gender Identity Having established the validity of TRAN SPROB wit respect to the probing framework of Lauscher et al. (2022), we can address RQ1 and ask if frozen PLM representations encode identities beyond the binary cis genders. To explore this, we conduct a 5-way classification, where models are trained to predict one out of the five labels from our dataset. Figure 3 presents probing results for all gender groups. First, accuracy is lower than in Table 2 – an expected outcome given the more challenging setting of predicting one of five classes. Nevertheless, all classifiers perform well above the random baseline of 0.2 F1, and almost all exceed 0.5 F1. This result demonstrates a substantial signal about gender identities within the representations. Second, similarly to results from Lauscher et al. (2022), model size influences the encoding of sociodemographic information. Larger models, such as DeBERTalarge, exhibit higher information extractability than smaller models like DeBERTaV3-xsmall. Surprisingly, when comparing performance across groups, we observe notable variations in the 6\ndifficulty of predicting gender identities from the representations. Detecting signals for cis women and non-binary individuals proves more challenging compared to those for trans women, trans men, and cis men. Figure 4 zooms into this result and presents the number of errors by the model wit the highest probing score – DeBERTa-large. It frequently misclassifies examples from the cis woman subcorpora as cis man and vice versa. Moreover, examples from all subcorpora are predicted as belonging to the non-binary subcorpus, potentially highlighting the broad spectrum of identities under the non-binary umbrella. We expand upon these observations in a broader context in Section 6. 5.3 Controlling for Authors’ Signals As outlined in Section 3.2, our gender-specific subcorpora are comparable in topics. However, YouTube personalities often craft their online personas around specific matters or catchphrases – could these be the signals that probing methods capture? To address RQ2 , we continue the probing experiments from the previous section, this time controlling for signals specific to authors. Experimental setting. To identify words specific to individual YouTubers, we use the Sparse Additive Generative Model of Text (SAGE; Eisenstein et al., 2011). SAGE identifies terms that are significantly over- or underrepresented in a target dataset by comparing their frequency to the frequency in a general (i.e., background) corpus. We apply SAGE for each author in a one-vsall setting, where each target corpus comprises all author-specific transcripts from the full TRANSCRIPT , and the background corpus consists of all remaining texts. After calculating normalized SAGE coefficients, we select all lemmas with values exceeding 1.0. This process identifies 41,829 terms, i.e. approximately 14% of all tokens in TRAN SPROB . Among these, we find phrases specific to individual YouTubers, such as underachiever used as an established way of addressing the audience or teehee used as an outro phrase for each video. However, we also find more general terms such as Kotick orActivision , which one YouTuber uses in multiple videos about gaming companies.12Therefore, while our method primarily aims to capture author-specific signals, it also serves as an implicit means of controlling for topic. 12Robert Kotick is the CEO of Activision Blizzard, one of\nthe largest video game publishers.\nFigure 5: Results from the 5-way probing using author-\ncontrolled frozen PLM representation. Average per-\nclass F1 scores and standard deviation from 5 runs.\nNext, we mask with <MASK> all author-related tokens identified by SAGE in the corresponding author’s transcripts in TRAN SPROB . Afterwards, we extract the averaged vector embeddings from the selected PLMs and leave all the other preprocessing and training procedures unchanged. Results. The results of our author-controlled 5-way probing experiments are depicted in Figure 5. Given that the patterns observed from the pairwise classifications and MDL probing are consistent with the 5-way classification, we consider them as validation experiments and provide details in appendix (see Table 9 in Appendix C.1). Comparing Figures 3 and 5, we notice decreased probing accuracy for all models. While their rank remains unchanged – with DeBERTa-large achieving the highest F1 score and DeBERTaV3-xsmall the lowest – their accuracy consistently drops by approx. 0.05 F1 for all subcorpora. The impact of model size also remains strong, with larger models continuing to demonstrate greater extractability of information. However, despite lower accuracy, all models achieve F1 well above the majority baseline. This suggests that the signals remaining after author-specific information is removed are still sufficiently robust to enable the decoding of information related to those authors’ gender identities. Furthermore, similarly to the previous section, we observe trends in these signals specific to the five subcorpora, with the cis woman and non-binary identities showing the lowest extractability. 5.4 Fine-tuning Representations So far, we observed that the frozen representations of PLMs encode gender identity information sufficiently to achieve probing accuracy up to 0.7 F1. This finding prompts the question: Is this the upper 7\nFigure 6: Results from 5-way fine-tuned classification.\nAverage F1 scores and standard deviation from 5 runs.\nlimit? To explore this, we shift our focus to RQ3 and evaluate how the capabilities of PLMs change if we fine-tune them using the same training objectives as in probing. These experiments are designed to assess the change in the models’ representations when provided with explicit supervised signals as a comparison to the probing experiments. Experimental setting. We use the same architecture as in the probing classifiers, using the averaged word embeddings as inputs to the classification heads. The main change is that the PLM representations are not frozen anymore and can learn new signals from the training data. All models are fine-tuned for 3 epochs using a batch size of 8 and a learning rate 1×10−5with Adam optimizer. We use early stopping based on validation loss, with a patience of 5. During fine-tuning, the models are evaluated ten times and the best-performing model is saved. Once training is complete, we load the model with the overall best score. Results. Figure 6 presents the results of the 5way fine-tuning experiments (see Table 10 in Appendix C.1 for the validation pairwise results). As expected, fine-tuning improves accuracy by up to 0.1 F1 with the largest models reaching nearly 0.8 F1 for 4 out of 5 subcorpora. These results align with other gender prediction findings that, according to HaCohen-Kerner (2022), vary widely in accuracy (52% to 91%) across different domains and architectures. Additionally, fine-tuning reduces the variance between the results of different PLMs, with smaller models scoring closer to the base and larger variants. However, despite these improvements, disparities in F1 scores persist among gender identities, with the cis woman and non-binary subcorpora continuing to yield the lowest scores. 5886 Discussion and Conclusion In this paper, we adopted an inclusive perspective to analyze whether gender identity is encoded in the representations of PLMs. Our study began with the creation of TRAN SCRIPT , which, to the best of our knowledge, is the first dataset to include language samples from individuals across five gender identities. The proposed data collection pipeline and topic analysis contribute to ongoing research on language use in trans and queer communities. Do representations encode gender identity? With two distinct probing methods, we found a substantial amount of gender identity information encoded within PLM representations. The information extends beyond binary gender and is extractable from frozen representations even after masking author-specific words, which may benefit the probes. Moreover, model size is one of the strongest factors indicating final probing accuracy. Are there gender biases in representations? We consistently observed error discrepancies in model behaviors (refer to Shah et al. (2020) for an overview of types of biases in NLP models). These disparities persisted even when controlling for author-specific words or fine-tuning with a direct signal. They were particularly visible in model behavior for cis women and non-binary individuals. A detailed sociolinguistic analysis of why this is the case (e.g., stronger expressivity of some individuals or identity-establishing language variation) represents important future work. However, we can already conclude that even in balanced data, the ease of extracting gender identity from representations differs across the analyzed populations. The observed biases in representations can pose significant challenges in downstream applications. For example, applications like YouTube’s automatic captions may have disparate performance for different gender identities due to biased encodings (Tatman, 2017). Perhaps more critically, some identity verification systems incorrectly interpret nonbinary identities as fake or non-human (Dev et al., 2021). Similarly, tools predicting users’ gender on social media platforms can misgender individuals from already marginalized groups, negatively impacting their self-esteem (Fosch-Villaronga et al., 2021). Placed within this context, our findings underscore the vital importance of including transgender and non-binary people in the development and assessment of language technologies. 8\n7 Limitations This paper provides two main contributions: pipeline for creating TRAN SCRIPT and the findings from our probing analysis. Regarding the pipeline, there are several points to consider. Firstly, the pipeline strongly relies on the representation of diverse gender identities on Wikidata. Given that the trans and queer community is already demographically underrepresented, publicly available databases such as Wikidata exacerbate this underrepresentation (Zhang and Terveen, 2021). Consequently, our approach to population sampling reflects and potentially perpetuates this issue. Secondly, the sampling method for cisgender individuals, which relies on the same nationality, similar age, and level of YouTube fame, can introduce a selection bias in our corpus. Restricting the country of citizenship allows us to focus on Englishspeaking YouTubers, but including both American and British English speakers may introduce minor linguistic variations. Finally, we perform an automatic diarization step to filter segments spoken by the target speaker. While we carefully design the methodology, it might still misidentify some of the speakers in collaborative videos, leading to noise in our data. When it comes to the probing experiments, it is important to emphasize that our results are limited to the domain of TRAN SCRIPT : YouTube transcripts. As such, the analyzed segments are samples of spoken, but possibly scripted, language. Therefore, our result do not direclty generalize to other domains. Secondly, while we account for author-specific words and implicitly topic-related signals, our probing experiments do not control for other latent aspects that could influence our results. 8 Ethical Considerations Our research involves collecting and predicting sensitive personal information, specifically gender identity. We acknowledge the potential for dual use of our work, and specifically the fact that predictive methods may be used for harmful applications, such as author profiling, which may perpetuate biases and discrimination. However, we believe that this risk is outweighed by obtaining findings that promote the inclusion of LGBTQIA+ individuals and assess the risks that NLP systems may pose to already marginalized communities. We strive to acknowledge associated risks and to account for the sensitive nature of research related to the 688LGBTQIA+ community, thereby fostering more inclusive and equitable NLP technologies. To regulate the possible dual-use issues associated with the data, TRAN SCRIPT will be made available only upon direct request. Consequently, we are releasing only the code needed to reproduce our dataset and analysis – an approach that aligns with previous work on YouTube (Ko et al., 2023, among others). Moreover, we store the collected data securely and do not directly disclose the authors’ identities. We note however that the source data remains publicly accessible by virtue of the authors’ presence on YouTube. The information we collect does not include selfreported identity labels. Instead, we use a collaborative database, Wikidata, which contains information about public figures, including their gender identity labels. This approach has limitations, as gender identities can overlap and change over time. Relying on externally assigned labels may not always accurately represent an individual’s selfidentified gender. Finally, our findings pertain specifically to the corpus and models used in this study and should not be generalized to broader populations. The results do not represent the entire spectrum of gender identities or sociodemographic groups and communities. We recognize the complexity and diversity of gender identities and acknowledge that our work captures only a subset of this spectrum within the context of our resources. 719",
        "aspect_review": "The paper is exceptionally well-organized and clearly presents the motivation, methodology, and findings, making it accessible to a broad audience interested in gender identity in NLP. The creation of the TRAN SCRIPT corpus is a groundbreaking contribution, providing a novel resource that includes language samples from transgender, cisgender, and non-binary individuals, which is crucial for inclusive NLP research.\n\nWhile the methodology is robust, there is a minor need for additional clarity on certain hyperparameter choices and their potential impact on results. The comparison with existing methods could be slightly expanded to include more diverse NLP models beyond those selected, though this is acknowledged as future work. The results are compelling, but a more detailed discussion on the implications of the observed biases in PLM representations could enhance understanding.",
        "general_review": "The paper presents a groundbreaking and transformative approach by addressing the critical gap in NLP research concerning the encoding of transgender and non-binary identities in pretrained language models (PLMs). It introduces a novel dataset, TRAN SCRIPT, which is the first of its kind to include language samples from individuals across five gender identities, demonstrating notable novelty. The study exhibits strong technical depth and rigor through comprehensive probing experiments and a robust data collection pipeline. The findings are validated by replicating and extending previous methodologies, ensuring comprehensive experimental validation. The research has significant practical impact, as it highlights the biases in PLM representations that could affect downstream applications, underscoring the importance of inclusive NLP systems.\n\n1. Minor writing or clarity improvements could enhance the readability of certain sections. 2. Small details on hyperparameters or implementation specifics are not exhaustively covered, which could aid in reproducibility. 3. Very minor formatting issues in figures could be addressed for better visual clarity. 4. Slight clarification on code availability would be beneficial, as the paper mentions that the dataset will be available upon request, but clearer instructions could be provided."
    },
    "31": {
        "paper": "Enhancing Multi-Label Text Classification under Label-Dependent Noise:\nA Label-Specific Denoising Framework\nAnonymous ACL submission\nAbstract\nRecent advancements in noisy multi-label text classification have primarily relied on the classconditional noise (CCN) assumption, which treats each label independently undergoing label flipping to generate noisy labels. However, in real-world scenarios, noisy labels often exhibit dependencies with true labels. In this study, we validate through hypothesis testing that real-world datasets are unlikely to adhere to the CCN assumption, indicating that label noise is dependent on the labels. To address this, we introduce a label-specific denoising framework designed to counteract labeldependent noise. The framework initially presents a holistic selection metric that evaluates noisy labels by concurrently considering loss information, ranking information, and feature centroid. Subsequently, it identifies and corrects noisy labels individually for each label category in a fine-grained manner. Extensive experiments on benchmark datasets demonstrate the effectiveness of our method under both synthetic and real-world noise conditions, significantly improving performance over existing state-of-the-art models1. 1 Introduction Multi-label text classification (MLTC) aims to predict the most relevant labels for each text from a label set. In real applications, noise is inevitably present in the data of MLTC (Snow et al., 2008; Chen et al., 2023). It poses a significant challenge for machine learning models, particularly deep learning models (Frénay and Verleysen, 2014; Arazo et al., 2019). In noisy multi-label classification, most existing methods rely on the class-conditional noise (CCN) assumption (Li et al., 2022b; Xia et al., 2023; Xie and Huang, 2023; Song et al., 2024). This assumption posits that label noise originates from independent label flipping for each 1Our code is available at https://anonymous.4open.\nscience/r/led-emnlp/ .\nCorrelation\nDocumentGround TruthObserved LabelsNeptunePlutoAstronomyPhysicsMoonEarthTensorflowPytorch LabelTrue PositiveNeptunePlutoAstronomyPhysicsMoonEarthTensorflowPytorch LabelTrue NegativeLabelFalse Positive LabelFalse Negative \n……Figure 1: An illustration of noisy multi-label text classi-\nfication.\ncategory, with each category having a distinct flipping probability. However, in practice, noisy labels often exhibit a certain degree of correlation with the true labels (Cui et al., 2020; Xie and Huang, 2022). As shown in Figure 1, due to category ambiguity, lack of expert knowledge, or the influence of attention shift (Wu et al., 2023), annotators are more likely to mislabel the current document as “Moon” or “Earth” instead of unrelated labels such as “Pytorch” or “Tensorflow”. In this paper, our first contribution (Section 2) is to propose a theoretical hypothesis test on the real-world dataset Riedel (Chen et al., 2023) to demonstrate that real-world multi-label noise is less likely to be CCN, and more likely to be labeldependent noise (LDN), where the occurrence of label noise depends on the positive labels associated with the current sample. To mitigate NMLTC under LDN, our second contribution (Section 2) is to introduce for generating controllable LDN and analyze the characteristics of training under LDN. Our third contribution (Section 3) is the proposal of a Label-Sp ecific Denoising (LeD) framework to address LDN. LeD initially introduces a holistic selection metric (HSM), incorporating ranking1\nenhanced loss (REL) and centroid distance (CD) to assess labels from both output and feature perspectives. Following this, LeD employs the HSM to conduct label-specific noise identification and correction for each label category. The superior performance of LeD is verified in extensive experiments, under LDN with varying noise fractions, including on real-world benchmarks. 2 From CCN to LDN In this section, we introduce the problem setting of noisy multi-label classification from traditional class-conditional noise (CCN) assumption to our proposed label-dependent noise (LDN) assumption. In what follows, sets are in calligraphic letters (e.g., A), matrices are in capital bold letters(e.g., A), vectors are in lower-case bold letters (e.g., a), and scalars are in capital or lower-case letters (e.g. A, a). For simplicity, let [L] ={1, ..., L}. Additionally, proofs of theorems can be found in Appendix A.1. 2.1 Preliminaries Considering a multi-label classification problem, the input of training stage includes Ninstances D={(xi,yi)}N\ni=1, each of which consists of an input vector xiand output labels yi= (Yi,1, Yi,2, ..., Y i,L)∈ {0,1}Lrelated to the input. Here Lis the total number of candidate labels. In real-world scenarios, it is often not possible to directly observe the true labels y. Instead, we have an observable distribution of noisy labels ˜yand a noisy training set ˜D={(xi,˜yi)}N\ni=1. In noisy multi-label classification, our goal is to predict proper labels for each unseen instance by only using the noisy training set. 2.2 Class-Conditional Noise Assumption The class-conditional noise assumption is commonly used in previous works (Chen et al., 2019; Li et al., 2022b; Xie and Huang, 2023; Chen et al., 2023) on noisy multi-label classification. Definition 1. (Noise transition matrix) In multilabel classification, the random variables ˜Y·,jand Y·,jfor the label jare related through a noise transition matrix Tj∈[0,1]2∗2, j∈[L]. Generally, the transition matrix depends on instances (feature xand labels y), i.e.,Tj\nk,l(x,y) =P(˜Yj=k|Yj= l,x,y), where kandl∈ {0,1}. Definition 2. (Class-conditional noise) Under the class-conditional assumption, the transition 113matrix is assumed class-conditional and instanceindependent, i.e., Tj\nk,l(x,y) =Tj\nk,l=P(˜Yj= k|Yj=l). As illustrated in Figure 1, in the context of noisy multi-label learning, there exist two types of noise: false positives and false negatives. Among them, false positives often exhibit a strong label correlation with the ground truth y. Therefore, we argue that real-world multi-label noise should at least be label-dependent , i.e.,Tj\nk,l(y) =P(˜Yj=k|Yj= l,y). To underscore the importance of moving beyond the CCN assumption, which pertains to labelindependent noise (LIN), we theoretically validate its significance through hypothesis testing. Definition 3. (Label flip) Given the noisy dataset ˜D, consider randomly sampling a validation set ˜D′= {(xi,˜yi)}n\ni=1, and assume we also know the clean labels{yi}n\ni=1corresponding to the validation set. We define the random variable Zj={˜Yj|Yj= 0}, where Zjrepresents the event that the jthclass of the current sample flip from a negative label to a positive label. Here, Zj∈ {0,1}, with a flip probability of Tj\n1,0(x,y). Theorem 1. If CCN assumption holds, then for ∀j0, j1∈[L], random variables Zj0andZj1are independent. Next, we verify the independence of Zj0andZj1through hypothesis testing. The null hypothesis H0 and the corresponding alternative hypothesis H1 are defined as follows: H0:Zj0, Zj1are independent; H1:Zj0, Zj1are dependent. By applying the chi-square test to the real-world noisy multi-label benchmark Riedel (Chen et al., 2019), the hypothesis testing results show that χ2= 940 .5with a p-value of 1.5e−206, indicating that the result is highly statistically significant. Please refer to Appendix A.2 for more details. Thus, the null hypothesis H0is rejected with the significance value. Hypothesis “ H1: Zj0, Zj1are dependent. ” is accepted. Therefore, based on Theorem 1, we can conclude that the CCN assumption does not hold on Riedel. 2.3 Label-Dependent Noise Now both the intuition and theoretical evidence imply that multi-label noise should be dependent on labels. As presented in Definition 4, we can model label-dependent mislabelling among given labels. Definition 4. (Label-dependent noise) Under the 2\nlabel-dependent assumption, the transition matrix isTj\nk,l(x,y) =Tj\nk,l(y) =P(˜Yj=k|Yj=l,y) CCN can be seen as a degenerated case of LDN assumption such that all instances have the same noise transition matrix. By assuming LDN, we can better model the label correlation of real-world multi-label noise, as depicted in Figure 1. Note that LDN is also a special case of instance-dependent noise (IDN) (Chen et al., 2021). Its noise transition depends on the labels corresponding to each instance. Previous works on noisy multi-label classification under CCN assumption often conducted experiments on synthetic noise with varying noise fractions (Li et al., 2022b; Chen et al., 2023). They randomly flip an element Yi,jin the label vector yi from 0 to 1 or 1 to 0 by the probability Tj\n1,0and Tj\n0,1respectively, thereby generating noise labels of false positives and false negatives. Similarly, it is desired to easily generate LDN with any noise fraction for any given benchmark dataset. To stimulate the development of theory and methodology, we propose a novel LDN generator. As shown in Figure 1, the LDN assumption primarily manifests in the generation of false positive noise. for the generation of false negative noise labels, we adopt the method used in previous studies (Li et al., 2022b), which involves a fixed transition probability Tj\n0,1=T0,1=ρ+. For the generation of false positive noise, we follow a label dependency approach, meaning that true negative labels with strong label correlation to the ground truth are more likely to be flipped to false positive labels (Liang et al., 2023). We simulate the label flips based on a label correlation matrix C, which can be obtained through the label co-occurrence matrix (Su et al., 2022). Each element Cj0,j1ofCis defined as: Cj0,j1=cj0,j1PL\nj=1cj0,j, j0, j1∈[L](1) cj0,j1=\u001a0, j0=j1PN\ni=1Yi,j0·Yi,j1, j0̸=j1(2) The probability Tj\n0,1(y)of a true negative label jtransitioning to a false positive label should be related to the current set of positive labels for the sample. Tj\n1,0(y) =ρ−∗pj(y,C) (3) pj(y,C) =X\nj0:yj0=1Cj0,jP\nj0:yj0=11(4) 208Here, ρ−controls the extent of negative labels transitioning. pj(y,C)denotes the probability of a negative label jtransitioning to a positive label given the current set of labels and the label correlation matrix. The label noise is label-dependent since it takes into account the label set of each instance. In some works (Chen et al., 2023; Ghiassi et al., 2022), it was assumed that ρ−=ρ+. However, we argue against this approach because in MLTC, the label dimension Lis usually much larger than the average number of labels per instance Lavg. Therefore, if ρ−=ρ+, the number of FP labels would be much greater than the number of FN labels. This situation does not accurately reflect the challenges of NMLTC problems. Hence, we adopt the approach proposed in Multi-T (Li et al., 2022b), setting ρ+=ρandρ−=Lavg\nL−Lavgρ. This configuration is designed to ensure that the difference between the number of FP labels and FN labels is relatively small. The noise rate ρis set to 0.2, 0.4, and 0.6. The algorithm of LDN generation can be found in the Appendix B.1. 2.4 Characterizations of training with LDN In noisy label learning, a simple yet effective method to identify label noise is to utilize the memorization effect (Arazo et al., 2019). This effect highlights that DNNs tend to learn simple and general patterns before memorizing the noise, inspiring many noise reduction learning approaches (Lu et al., 2023; Song et al., 2024). Existing methods have confirmed that this approach can also be applied under CCN conditions (Li et al., 2022b; Song et al., 2024). However, can this method be used in NMLTC under LDN conditions? Here, we conduct an empirical study to compare the performance of these two types of noise. We generate 40% LDN noise and conduct experiments on the AAPD dataset. Simultaneously, we generate 40% CCN noise for comparison. In all experiments presented in this paper, the DNN model and training hyperparameters we use are consistent. LDN is harder to identify Figure 2 shows the changes in loss values for randomly sampled clean and noisy labels during training (under both CCN and LDN conditions). If we use the small loss criterion (Li et al., 2022b; Song et al., 2024) to filter noisy samples, we find that noisy samples are relatively easier to identify under CCN. During the early epochs of training, clean and noisy labels exhibit a significant difference. However, under LDN, 3\n0 5 10 15 20 25 30 35 40\nEpoch0.00.20.40.60.81.0LossClean\nNoise(a) CCN\n0 5 10 15 20 25 30 35 40\nEpoch0.00.20.40.60.81.0LossClean\nNoise (b) LDN\nFigure 2: Individual loss curve on AAPD with LDN.\n0.0 0.2 0.4 0.6 0.8 1.0\nLoss0200400600800FrequencyClean\nNoise\n(a) CCN\n0.0 0.2 0.4 0.6 0.8 1.0\nLoss0200400600800FrequencyClean\nNoise (b) LDN\nFigure 3: Loss distribution on AAPD with LDN.\nclean and noisy labels are not easily distinguishable. In addition, we carried out a quantitative analysis, as shown in Figure 3. We illustrate the variance in loss distribution between labels with noise and those that are clean. It can be seen that under CCN noise, the overlap between the clean region and the noisy region is small, indicating that it is easier to identify the noise. In contrast, under LDN, it is relatively difficult to identify the noise due to the larger overlap. The reason is that LDN is very similar to the correct labels, making it prone to overfitting and thus difficult to distinguish from the correct labels. Additional observations can be found in Appendix B.2. 3 Method Previous work (Han et al., 2018; Northcutt et al., 2021; Song et al., 2024) has identified noisy labels based on the “memorization effect”, using the loss values from early epochs of deep learning models. However, as shown in section 2, this approach faces challenges with LDN due to its similarity to the true labels. Therefore, in this section, we propose a Label-Sp ecific Denoising (LeD) framework. This framework considers various aspects of neural network training, including loss, ranking, and feature space neighbors, which provide a more comprehensive reflection of the likelihood that a label is noisy. Consequently, we introduce a holistic selection metric (HSM) that includes ranking-enhanced loss (REL) and centroid distance (CD). We then identify noisy labels for each label category from the perspective of each individual label category. We use a Gaussian Mixture Model (GMM) to iden- 291tify noisy labels among positive and negative labels for each category, resulting in a noise probability for each label. Based on these noise probabilities, we correct the original labels. The corrected labels are then used for retraining the model. The overall framework is shown in Figure 4. 3.1 Noisy Multi-Label Text Classification The goal of noisy multi-label text classification (NMLTC) is to learn a function fthat maps the input instance xiand a label lto a relevance score ˆYi,j=f(xi, j). We constructed the scoring function fby combining a text encoder ϕand a multi-label classifier ψ. Following the approach of previous works (Su et al., 2022; Tan et al., 2024; Chai et al., 2024), we employed a BERT-based text encoder ϕand adopted a multi-layer MLP as our multi-label classifier ψ. We then employed binary cross entropy (BCE) LBCE=PN\ni=1PL\nl=1Li,jas the loss function, where Li,j=−(˜Yi,jlog(ˆYi,j) + (1−˜Yi,j) log(1 −ˆYi,j)).\n(5) The notation Li,jrepresents the loss value associated with the j-th label for the i-th instance. 3.2 Holistic Selection Metric Due to the presence of LDN, noisy labels and correct labels appear more similar, making it difficult to effectively identify noisy samples using a single metric, such as loss information. Therefore, we propose to jointly use two metrics from different perspectives: ranking-enhanced loss (REL) and centroid distance (CD). REL fully utilizes the information from prediction confidence, while CD relies on the distance in the feature space. 3.2.1 Ranking-Enhanced Loss When learning with noisy labels, it is commonly observed that instances with clean labels typically have smaller loss values than those with noisy labels (Han et al., 2018; Northcutt et al., 2021). However, relying solely on the loss value Li,jto identify LDN, we may overlook differences between samples. For instance, some difficult labels may have a relatively high loss Li,j, but their prediction ˆYi,j∈ˆyicould still be correct. Therefore, we propose using the model’s predicted ranking of labels as an additional metric. A smaller predicted ranking for a label indicates it is more likely to be clean. Label ranking can reveal distinctions between labels at the sample level. For each in4\n𝜙\n𝜓\n0.70.90.8…InstanceEncoderClassifierFeature\nCD\n𝐿!\"#\nRank⋅\nREL\n10.20HSM\nRELCD10HSMDensityPseudoGMMHSMGMM1HSMDensity0Predicted Label#𝐲%𝐲 &𝐲 Observed LabelPseudo Label\nL#𝐲%𝐲 &𝐲 \n……Centroid …1   2 … L\n⊗\nCDRELPositive LabelsNegative Labels𝑗=1,…,𝐿Figure 4: The overall framework of LeD. ˆyrepresents the predicted labels, ˜ydenotes the observed labels, and ˇy\nstands for the corrected pseudo-labels.\nstance xiand its predicted label ˆ yi, we can obtain the rank of each label using the rank function Rank (ˆyi) = ( Ri,1, Ri,2, ..., R i,L), where Ri,jis the rank metric for ˆYi,j. To combine ranking information with loss information, we propose the ranking-enhanced loss (REL) by adding an extra weight to the loss information. Thus, REL Ei,jcan be calculated by: W(ˆYi,j) = min(log( Ri,j) + 1, θ), (6) Ei,j=W(ˆYi,j)×Li,j. (7) The logarithmic function is used to constrain the scale of the rank values, and a fixed value θis employed for truncation to ensure that it has a limited impact on the loss information. 3.2.2 Centroid Distance Although the sample separability achieved through REL is better, the separation metric still relies on model prediction. This reliance means there is still a risk of overfitting the classifier, especially in the case of LDN, where label noise often occurs among similar labels, increasing the likelihood of classifier overfitting. Consequently, this leads to low discrimination between the model predictions of clean and noisy labels. Therefore, solely using REL may not be sufficient to distinguish clean labels from noisy ones when model predictions are close. Except for separating the samples in the output space, we propose an additional metric computed in the feature space to mitigate the bias introduced by the classifier, as the learned features can handle noise labels better. Specifically, we proposed the centroid distance (CD) metric. for a given sample, we can compute the distance between its feature and the class feature centroid to assess the extent to which the sample’s feature differs from its class 373centroid. To improve the quality of the class feature centroid for distance calculation, we construct the feature centroid by incorporating highconfidence samples from the observed class. The class centroid cjbased on a high-confidence sample set Hjis calculated by: cj=1\n|Hj||Hj|X\ni=1vi, (8) Hj={xi|xi∈ Sj,ˆYi,j> hj}, (9) where viis the feature of xiandSj={xi|˜Yi,j= 1}. we can use the prediction confidence of class jfor sample xi, e.g., ˆYi,j, as the selection criteria compare with the threshold hj.Hjis constructed by the samples in Sjwhose corresponding prediction confidence of class ˆYi,jis higher than hj. The high-confidence threshold hjis defined as: hj=1\n|Sj||Sj|X\ni=1wi×ˆYi,j (10) wi= max\u0010\n1,ˆYi,j/¯Yj\u0011\n. (11) hjis calculated by the weighted sum of the prediction confidence of class jfor all samples. The weight wiincreases when a sample’s prediction confidence of class jis higher than its class average ¯Yj. Thus, the threshold is high enough to ensure the quality of the selected samples. Therefore, the proposed metric CD Di,jcan be calculated by Di,j=−cos(vi,cj). To facilitate the integration of these two metrics, we perform min-max normalization on them (Hu et al., 2022), obtaining normalized results ˆEi,j andˆDi,jrespectively. The linear combination of both metrics results in a new holistic selection 5\nmetric (HSM) Mi,j=α·ˆEi,j+ (1−α)ˆDi,j. The combination coefficient αplays a crucial role in determining the balance between the two metrics. By combining the advantages of both metrics, HSM effectively captures both the confidence from model predictions and the robustness from the feature space, thus improving the discrimination between clean and noisy labels. 3.3 Fine-Grained Label-Specific Correction As previously mentioned, there are two types of noise in noisy multi-labels: false positive and false negative. In this section, we take false positive noise as an example, with the method for handling false negative noise being similar. After obtaining the HSM for each label, we proceed with the identification and fine-grained correction of noisy labels based on each label category. Fine-grained label-specific correction involves re-labeling the noisy dataset based on the HSM values to create a cleaner training set. If noisy labels are divided globally (without distinguishing between categories), the differences between categories will be ignored. Some difficult categories may all be classified as noise, while some simple categories may not be classified as noise at all. Therefore, we individually identify noisy labels for each class. We first obtain the HSM set M+\njcorresponding to all positive labels for label j, i.e., M+\nj={Mi,j|˜Yi,j= 1}. true labels have lower HSM values compared to noisy ones due to DNNs’ memorization effect (Arpit et al., 2017; Hu et al., 2023). Therefore, we employ a bi-modal univariate Gaussian mixture model (GMM) for each HSM set using the expectation-maximization (EM) algorithm, resulting in LGMM models for positive labels. Given the HSM, its clean-label probability is obtained by the posterior probability PG(˜Yi,j)of the corresponding GMM, Since distinguishing between noisy and clean labels near the decision boundary is challenging, we have employed a finegrained correction strategy, as opposed to using hard pseudo-labels (Li et al., 2020). The specific approach is as follows: ˇYi,j=\n\n1−˜Yi,jPG(˜Yi,j)≤0.5−ϵ,\nPG(˜Yi,j) 0.5−ϵ < P G(˜Yi,j)≤0.5 +ϵ\n˜Yi,j PG(˜Yi,j)>0.5 +ϵ\n(12) The corrected pseudo-label ˇYi,jis obtained. The 449Datasets Ntrn Ntst L L avgNavg\nMOVIE 105,616 11,736 28 2.1 AAPD 54,840 1,000 54 2.4 RCV1 23,149 781,028 103 3.2 Table 1: Data statistics. Ntrn,Ntstrefer to the number of\ndocuments in the training and test sets, respectively. L\nis the number of labels. Lavgis the average number of\nlabels per documents. Navgrefers to the average number\nof words per document.\nimplication is that if PG(˜Yi,j)is large, we consider the label to be likely a clean label and thus keep it unchanged. Conversely, if PG(˜Yi,j)is small, we consider the label to be likely incorrect and thus perform label flipping. When the value of PG(˜Yi,j) is close to 0.5, it is difficult to determine the noise situation, so we adopt a soft label approach. 4 Experiment 4.1 Experimental Setup Datasets Following previous work (Chen et al., 2023), we evaluate the proposed model on three synthetic benchmark datasets namely MOVIE , AAPD , and RCV1 with varying LDN fractions. Table 1 contains the statistics of these three benchmark datasets. Evaluation Metrics For a comprehensive and reliable evaluation, we follow conventional settings (Chen et al., 2019, 2023) and report the following metrics: micro-F1 (mi-F1), macro-F1 (ma-F1) and mean Average Precision (mAP). Note that only the training set is affected by noise, whereas the evaluation metrics are computed on the clean testing set. The best results are in bold, and the second-best results are in underscore. Baselines To verify the effectiveness of LeD, we selected the nine most representative baseline models in three groups. (1) MLTC Methods: AttXML (You et al., 2019), HTTN (Xiao et al., 2021) and LSFA (Xu et al., 2023). (2) Noisy multi-label learning (NMLL) methods: GCE (Zhang and Sabuncu, 2018), WSIC (Hu et al., 2019), RTM (Patrini et al., 2017), Multi-T (Li et al., 2022b), and MLLSC (Ghiassi et al., 2022). (3) NMLTC method: nEM (Chen et al., 2023). More details about the implementation setting can be found in Appendix C.3. 4.2 Experimental Results Main Results As depicted in Tables 2-4, we have observed the following phenomena: (1) In all cases, 6\nNoise Rate ρ= 0.2 ρ= 0.4 ρ= 0.6\nMethods mi-F1 ma-F1 mAP mi-F1 ma-F1 mAP mi-F1 ma-F1 mAP\nAttXML 61.89 34.56 51.70 56.72 33.50 44.32 50.16 29.79 40.83\nHTTN 61.13 34.45 51.16 56.59 32.82 44.24 49.26 28.58 39.10\nLSFA 62.81 37.40 53.26 58.84 33.53 47.87 52.13 29.92 38.45\nGCE 65.68 39.65 53.17 61.80 35.95 48.92 52.76 31.37 41.98\nWSIC 62.82 38.94 52.82 60.26 35.89 46.89 53.93 31.37 39.38\nRTM 64.79 39.15 54.70 62.02 36.26 46.97 53.66 32.18 41.79\nMLLSC 63.85 38.48 51.41 60.83 36.46 47.74 53.09 31.57 39.22\nMulti-T 65.54 38.84 52.79 60.36 36.06 46.15 52.67 30.87 41.20\nnEM 65.37 40.92 54.00 62.45 36.91 48.76 53.15 31.70 42.23\nLeD 66.74 42.15 55.29 63.77 37.89 49.32 56.66 34.26 44.57\nTable 2: Performance on MOVIE with different noise ratios.\nNoise Rate ρ= 0.2 ρ= 0.4 ρ= 0.6\nMethods mi-F1 ma-F1 mAP mi-F1 ma-F1 mAP mi-F1 ma-F1 mAP\nAttXML 52.16 18.80 43.10 42.65 8.74 32.02 37.49 5.28 28.12\nHTTN 55.15 21.16 43.84 46.37 11.98 35.59 40.68 8.69 32.33\nLSFA 56.53 22.51 45.90 47.94 11.31 36.32 41.72 9.19 32.90\nGCE 54.31 23.59 43.28 47.09 13.20 35.45 42.94 8.74 32.87\nWSIC 56.11 23.44 44.56 49.34 13.71 36.63 42.15 9.78 32.67\nRTM 54.76 22.20 44.70 49.18 13.39 36.37 42.91 8.82 34.14\nMLLSC 55.16 22.67 45.47 47.41 13.59 36.38 41.34 8.41 32.22\nMulti-T 56.87 23.16 44.68 49.93 11.78 37.01 43.77 10.61 33.70\nnEM 55.46 22.99 46.43 48.89 16.16 39.76 43.84 9.89 34.80\nLeD 57.34 24.33 46.30 50.26 17.75 41.27 45.30 11.62 36.18\nTable 3: Performance on AAPD with different noise ratios.\nour method shows significant improvements compared to other methods. By utilizing a holistic selection metric, we evaluate labels from multiple perspectives, enabling finer-grained identification and correction of noisy labels, which leads to optimal experimental results. (2) Due to overfitting to noisy labels, most existing MLTC methods tend to perform worse compared to NMLL methods. (3) NMLL Methods based on noise transition matrix estimation, such as RTM and Multi-T, rely solely on loss as a measure for estimating noise rates, which is insufficient under LDN. Similarly, nEM and MLLSC are limited by the lack of sensitive enough metrics to identify noisy labels. Experiments on the Real-world Dataset The Riedel dataset (Chen et al., 2023) is a large-scale real-world NMLTC dataset, containing 53 classes, each representing a different relation. It is derived from the New York Times corpus. The training data consists of 281,270 instances, while the test set includes 3,762 instances. We used the same backbone as nEM to ensure a fair comparison. The results, shown in Table 5, indicate that our method outperforms the best baseline by 10% in terms of the ma-F1 metric, demonstrating the effectiveness 512of our approach on the real-world NMLTC dataset. Ablation Study In the following experiments, we aim to analyze the effectiveness of each component of the proposed LeD method on three datasets. To construct the synthetic noise datasets, we use a noise ratio of 0.4. We compare the complete LeD method with the following variants: (a) HSM (Loss): This variant uses only the loss as the metric for filtering noisy labels. (b) HSM (REL): This variant uses the REL metric for filtering. (c) HSM (CD): This variant uses the CD metric for filtering. According to Table 6, we observe that the different components of the HSM metric collectively contribute to enhancing the quality of noise identification. By incorporating instance-level rank information, the model gains the ability to differentiate between different instances, enabling a more accurate distinction between clean and corrupted labels. Additionally, the introduction of the featurebased metric, CD, significantly contributes to noise identification. 5 Related Work Learning from Noisy Labels In multi-class classification with noisy labels, most approaches lever7\nNoise Rate ρ= 0.2 ρ= 0.4 ρ= 0.6\nMethods mi-F1 ma-F1 mAP mi-F1 ma-F1 mAP mi-F1 ma-F1 mAP\nAttXML 71.30 32.10 60.99 65.69 22.23 58.01 64.18 20.20 52.70\nHTTN 64.59 27.19 54.87 62.87 20.44 54.16 63.18 19.67 51.78\nLSFA 69.67 30.87 59.43 64.55 21.73 56.25 64.23 20.46 53.17\nGCE 68.08 27.95 56.93 61.94 20.81 54.78 64.41 21.66 53.70\nWSIC 72.69 32.89 63.54 64.36 23.35 57.66 65.02 21.16 53.38\nRTM 72.15 33.48 63.41 66.44 21.11 57.75 64.48 20.16 54.11\nMLLSC 71.03 32.97 63.18 67.53 20.03 57.85 64.92 19.03 52.56\nMulti-T 72.99 31.89 62.75 66.49 19.41 56.65 62.85 19.31 50.95\nnEM 71.86 32.93 62.47 67.67 20.92 57.79 63.61 20.71 52.86\nLeD 74.66 35.32 64.90 69.61 24.70 62.29 67.65 23.33 56.42\nTable 4: Performance on RCV1 with different noise ratios.\nMethods mi-F1 ma-F1 mAP\nAttXML 56.77 33.78 50.81\nGCE 55.60 32.95 47.93\nWSIC 58.96 35.57 54.06\nMulti-T 57.15 34.40 52.87\nnEM 59.58 35.70 54.51\nLeD 63.72 39.40 57.82\nTable 5: Performance comparison on Riedel .\nHSM Dataset\nLoss REL CD MOVIE AAPD RCV1\n✓ 60.15 48.06 68.75\n✓ ✓ 62.15 48.95 69.16\n✓ 59.87 47.88 66.29\n✓ ✓ ✓ 63.77 50.26 69.61\nTable 6: Performance comparison of HSM components\nbased on mi-F1 scores across various datasets.\nage the memorization effect of deep neural networks (DNNs) (Arpit et al., 2017), where simple and generalized patterns are learned before overfitting to noisy patterns. Specifically, small-loss instances are likely to be clean instances (Han et al., 2018; Jiang et al., 2018; Wei et al., 2020). Another approach involves sample selection based on feature distributions (Li et al., 2022a, 2023). Recent methods (Hu et al., 2023; Lu et al., 2023) have proposed more comprehensive evaluation metrics to distinguish between clean and corrupted data, considering aspects such as information throughout the training process and prediction confidence. Inspired by these approaches, we propose a holistic selection metric for noisy MLTC that integrates various noise label characteristics, including multilabel prediction and feature information. Noisy Multi-Label Learning Noisy Multi-Label Learning is an emerging research topic due to the 555complexity of noise mechanisms in multi-label settings compared to multi-class problems. Zhao et al. (2021) introduced pre-trained label embeddings for regularization, achieving robust learning. GCE (Zhang and Sabuncu, 2018), WISC (Hu et al., 2019), and MLLSC (Ghiassi et al., 2022) developed robust loss functions by weighting labels. Methods like RTM (Patrini et al., 2017) and Multi-T (Li et al., 2022b) address the estimation problem of noise transition matrices in the multi-label context. The nEM method (Chen et al., 2023) uses latent variable models to model the transition process of noisy labels, achieving robust MLTC. Xia et al. (2023) identifies noisy labels through label correlation. Song et al. (2024) employs the small loss trick for noisy label selection and correction. However, existing methods either assume label noise is entirely random or class conditional, neglecting the fact that label noise is often correlated with current labels in real-world scenarios. 6 Conclusions In this paper, we first verify that real-world datasets often deviate from the class-conditional noise (CCN) assumption. Based on this observation, we introduce label-dependent noise (LDN), revealing the characteristics of label-dependent noise and designing a method to generate controllable LDN. Subsequently, we propose a novel label-specific denoising framework to enhance multi-label text classification under label-dependent noise. Extensive experiments on benchmark datasets demonstrate that our method significantly improves performance under both synthetic and real-world noise conditions, outperforming existing state-of-the-art models. 8\n7 Limitations Our method leverages the memorization effect (Arpit et al., 2017) observed in deep learning models for sample selection and correction. This effect has not been observed in other traditional machine learning methods, limiting the applicability of our framework to deep learning-based approaches only. While our framework is designed to handle labeldependent noise (LDN) effectively, it does not address instance-dependent noise (IDN) comprehensively (Chen et al., 2021; Wang et al., 2024). Although our method possesses the potential to recognize instance-dependent noise, we have not explicitly validated this capability in our experiments. The calculation of HSM for each label can be computationally intensive, particularly for large-scale datasets. This may limit the scalability of our approach for extremely large datasets. 608",
        "aspect_review": "The paper introduces a novel label-specific denoising framework that addresses label-dependent noise in multi-label text classification, providing a significant advancement over the traditional class-conditional noise assumption. The study validates the presence of label-dependent noise in real-world datasets and offers a method to generate controllable label-dependent noise, which is crucial for realistic multi-label classification scenarios.\n\nThe experiments, while comprehensive, could benefit from additional validation on more diverse datasets to further establish the generalizability of the proposed framework. While the results show improvement over existing methods, the paper could provide more detailed statistical analysis to strengthen the claims of performance gains. The presentation of the methodology could be enhanced with clearer explanations and more intuitive figures to aid in understanding the complex framework proposed. The ablation study could be expanded to explore more variations of the holistic selection metric and its impact on performance. The comparison with baseline methods could be more detailed, especially in terms of computational efficiency and scalability of the proposed method.",
        "general_review": "The paper introduces a novel label-specific denoising framework that addresses the label-dependent noise (LDN) in multi-label text classification, which is a significant advancement over the traditional class-conditional noise (CCN) assumption. The proposed framework's holistic selection metric, which incorporates ranking-enhanced loss and centroid distance, provides a comprehensive approach to identifying and correcting noisy labels. The extensive experimental validation on both synthetic and real-world datasets demonstrates the framework's effectiveness, significantly outperforming state-of-the-art models. The paper is well-organized and presents a clear theoretical foundation for the LDN assumption, making it highly relevant and impactful for the field of natural language processing.\n\n1. Minor improvements in writing clarity could enhance the readability of the theoretical sections. 2. Details on some hyperparameters and implementation specifics are sparse, which could aid in reproducibility. 3. Figures could benefit from slight formatting adjustments for better visual clarity. 4. While the code is available, additional clarification on its usage and examples would be beneficial for users."
    },
    "32": {
        "paper": "Instance-Level Dynamic LoRAs Composition for Cross-Task\nGeneralization\nAnonymous ACL submission\nAbstract\nLarge language models perform well on tasks that have undergone fine-tuning of instructions, but their performance to completely unseen tasks is often less than ideal. To overcome the challenge of cross-task generalization, tasklevel LoRA combination is proposed, which does not require training a model for new tasks. Instead, it learns the LoRA combination weights based on a small number of samples to form the task model. However, task-level LoRA combination only utilize a few task modules due to its reliance on the weight enumeration method, and it also overlooks the specificity between different instances. Therefore, we proposed an instance-level LoRA compositionfor cross-task generalization, which selects appropriate multiple task LoRAs for each input instance and dynamically determines the composition weights. Our experiments on publicly available datasets show that our method outperforms the typical method, LoraHub, in 16 out of 27 tasks. We release the source code at https: //github.com/noname822/iLoraComp.git 1 Introduction Currently, large language models (LLMs) demonstrate remarkable zero-shot learning capabilities on tasks that have undergone instruction tuning (Chung et al., 2022; Achiam et al., 2023; Touvron et al., 2023; AI@Meta, 2024). However, numerous studies have revealed that when encountering novel tasks outside their training distribution, these models often fail to exhibit satisfactory performance (Ovadia et al., 2024; Huang et al., 2024). Exploring strategies to enhance the cross-task generalization abilities of these massive language models, enabling them to adapt swiftly and accurately to diverse new tasks, has emerged as a pressing challenge that demands attention. Addressing the challenge of cross-task generalization has traditionally involved fine-tuning models for each task and in-context learning. However, LoRALibraryarithmetic entityextractionquestiongenerationdocumentsummary\nInstance-level Composition \ntaskTask-level Composition    \nbest NxNCompositionrandomN\nxNxN\ntask\nLLM\nQ\nLLM\nA…𝒘𝟏𝒘𝑵\n{𝒘𝟏…𝒘!}iComposition{𝒘𝟏…𝒘!}k𝒘𝟏…𝒘!𝟏Retrieval\nA\nFigure 1: Previous task-level composition constructs\na shared task model for all instances. The proposed\ninstance-level composition constructs a unique task\nmodule for each instance.\nthese conventional approaches come with inherent limitations. Fine-tuning for every new task can be resource-intensive, demanding extensive data, storage, and computing power, which compromises flexibility. Although methods such as LoRA (Hu et al., 2021), falling under the delta tuning (Ding et al., 2022) approach, aim to adapt to specific tasks or domains by introducing smaller parameter updates while minimizing computation and storage costs, thus mitigating storage issues and enhancing flexibility, they still require backpropagation for precise output tuning, rendering them less costeffective for multiple tasks. In-context learning (Dong et al., 2022), on the other hand, necessitates more input than zero-shot to fully leverage the model’s capabilities, indirectly increasing the computational resources needed for inference. To address the shortcomings of these methods and achieve efficiency and sustainability in multitask, few-shot, and high-volume scenarios, innovative approaches such as LoraHub (Huang et al., 2024) have emerged. LoraHub rapidly adapts to unseen tasks by intelligently combining pre-trained low-rank adapters from other relevant tasks. This method enhances model performance across di1\nverse tasks without increasing input requirements, striking a balance between performance and energy consumption. However, LoraHub also has room for improvement in terms of its effectiveness. Firstly, when selecting Lora modules from a trained Lora library for task adaptation composition, LoraHub’s current strategy is to randomly select modules from the library. This random selection may result in the inclusion of tasks that are either overly similar or completely unrelated, leading to significant performance variations under different random seeds for the same task, thus exhibiting poor stability. Secondly, when training on instances, LoraHub does not consider the subtle nuances between individual instances, preventing the full utilization of the limited instance data to capture the potential specificity of inputs, which in turn limits LoraHub’s performance. To address these two issues, we propose the following solutions: •To address the issue with the Lora module selection strategy, we adopt a selection method based on task similarity. By calculating the semantic similarity between the target task and the training sets of the available Lora modules, we prioritize the combination of Lora modules that are most closely related to the current task, thereby enhancing the stability and effectiveness of the task-level adaptation. •To fully account for the unique characteristics of each input instance, we propose tailoring a dedicated Lora module combination for each instance. By calculating the semantic similarity between the input instance and the training instances used to create the available Lora modules, we select the most fitting instance-specific Lora combination as the processing strategy for that input. This approach effectively leverages the subtle nuances across different input instances. By employing the aforementioned improvements, our method has achieved a significant enhancement in inference stability. Additionally, compared to the original LoraHub, our approach has demonstrated a noticeable performance advantage. In our experiments, a total of 27 tasks were tested, and in these, our proposed method outperformed LoraHub on 16 of them. 1142 Related work Instance-Based Generation for LLMs refers to a method that leverages dataset analysis to extract valuable instance, thereby enhancing the performance of a task. The introduction of large language models has since inspired numerous works, including Wiki-Chat (Semnani et al., 2023), which have sought to augment language model capabilities through retrieval-based knowledge enhancement. This trend originated with RAG (Lewis et al., 2020), which incorporates knowledge as prompts for in-context learning in LLM. Additionally, there are works that do not retrieve text as prompts, but instead retrieve delta-tuning modules, using these modules to generate prompts for answering questions, such as Knowledge Card (Feng et al., 2023). In this paper, we retrieval delta-tuning module by calculating the semantic similarity between instance and question using the method of DPR (Karpukhin et al., 2020a). Module Composition represents an endeavor to integrate diverse models, Consequently, tasks that retrieve model modules for composition have naturally emerged, such as MAC(Tack et al., 2024), SLM (Peng et al., 2024), Arrow(Ostapenko et al., 2024), LoraRetriever (Zhao et al., 2024), and LoraFlow (Wang et al., 2024). While most methods adopt a simplistic processing approach for models. These approaches strive to leverage retrieval methods by employing retrieval scores as weights during composition, thereby obviating the need for manual parameter tuning and facilitating immediate usage. Concurrently, methods such as Moelora (Liu et al., 2023) exist that directly assign weights through backpropagation. LoraHub occupies an intermediary position which used a gradient-free optimization. In comparison to previous work, our approach places a stronger emphasis on utilizing instances to get model modules that are more relevant to the given question. 3 Method In this section, we will provide an overview of the process, followed by an explanation of how to identify appropriate task Lora modules based on Lora training data. Finally, we will offer a detailed account of how to integrate the selected LoRA combinations with the input data. 2\n3.1 Overview We first train the upstream tasks Ton the large model Mθusing the training set Ti∈Tto get LoRA module Liand collect them into Lora libraryL. Next, We specify the hyperparameter Nas the number of LoRA modules to be composed. Each new task T′/∈Thas their instance set I′. For each instance ej∈ I′, we find the closest NLoRA library from L, denoted as Lej={L1, . . . , L N}, and optimize a weight combination ˆwej={w1, . . . , w N}using a gradientfree method (Sun et al., 2022) as ng. For a new question Qbelonging to new task T′, we select the most suitable weight combination ˆwejbased on the semantic similarity between Qandejthen make new LoRA module ˆLj. Finally, we combine these to form the model Mϕ=LoRA (Mθ,ˆL)and use it for reasoning on Q. 3.2 LoRA module Retrieval To select the most suitable LoRA modules from Lfor composition, we identify the corresponding training set Ti={(x1, y1), . . . , (xn, yn)}for each Li∈ L. We then derive the task embedding vector embTi=1\nnPn\nk=1Ms(xk+yk)using the sentence vectorization model Ms. Similarly, for the instance ej= (xej, yej), we can obtain its embedding vectoremb ej=Ms(xej+yej). Consequently, Following the approach of Mussmann and Ermon, 2016 and Karpukhin et al., 2020b in using cosine similarity as a measure of task similarity, we can identify the top N most similar tasks to ej. The formula for cosine similarity is as follows: similarity (ej,Ti) =emb ej·embTi\n∥emb ej∥ · ∥embTi∥(1) Where embTirepresents the embedding vector of thei-th task, and ∥ · ∥ denotes the Euclidean norm of a vector. By calculating the cosine similarity between each task Tiand the instance ej, we can select the top N tasks with the highest similarity as the candidate set of similar tasks for ej, which is denoted as Lej, and then collect all Lejas a set called SL. 3.3 Instruct based Module Composition and Inference To fine-tune the model Mθto the state that best aligns with the instance ej= (xj, yj), we employ the non-gradient optimization method ngto refine the weights. We perform a broad adjustment of 208the init weights winitusing all the instances for Ti donated as Ii={e1, . . . , e n}. Then, we conduct a targeted adjustment using the instruct-level LoRA setLejcorresponding to the specific instance ej. The optimization process is encapsulated in the following formula: ˆwej=ng(Ii,Lej, winit) (2) Having aggregated the adjusted weights ˆwejfor all einto the set Sˆw, we proceed to identify theejthat shares the most affinity with the input x. This is accomplished by calculating the cosine similarity between the input embedding vector emb eix=Ms(xj)forejand the embedding vectoremb x=Ms(x)for the input x. This analysis allows us to select the most suitable LoRA library fromSL, denoted as Lsuit, and its corresponding weights from Sˆw, denoted as ˆwsuit. Utilizing these components, we construct the optimal LoRA moduleˆL= ˆwsuitLsuit. As a result, we obtain the model Mϕ=LoRA (Mθ,ˆL)that is specifically tailored to the given input. This model is then employed for inference, with the output expressed as y=Mϕ(x). 4 Experimental Setup LLM. We utilized the Flan-T5-Large (Chung et al., 2022) model as our foundational large language model Mθfor experimentation purposes. Concurrently, we employed the compact all_datasets_v4_MiniLM-L6 (flax sentence embeddings, 2021; Wang et al., 2020) model as our Ms, which was trained on a dataset comprising one billion sentence pairs, excluding the BBH and flanv2 datasets that we utilized. This compact model effectively supported our sentence vectorization efforts. Dataset and Evaluation. We utilize the flanv2 dataset (Longpre et al., 2023), which incorporates data from four mixed sources, as the training set for upstream tasks. It encompasses 264 distinct datasets, out of which we selected 97 for our purposes. We then employed the Lora modules trained on these datasets by Huang et al. (2024) as our repository of Lora models for potential selection. The Big-Bench Hard benchmark (Suzgun et al., 2022), with 27 tasks, offers a valid test for Mθas it was not trained on these datasets. We sampled 5 instances per task, used 20 LoRA modules for adaptation, and initiated with 40 steps of global optimization, followed by EM-based evaluation on the remaining data. 3\nBaseline Setup. To ensure our method’s credibility, we used our LoRA library to test LoraHub (Huang et al., 2024) refined parameters for 40 steps as a baseline, averaging three runs for the final score (LoraHub avg). We compared scores using zeroshot, full fine-tuning (FFT), and in-context learning (ICL). For LoRA module selection, we conducted ablation experiments using the average embedding vector of five instances per task (BatchComp). In FFT, we maintained consistency by training with the same random seeds and 5 instances. We trained the model over 40 epochs with a learning rate of 3e-5 and batch size of 5. 5 Result And Discussion Method average average-3\nFFT∗39.8 44.3\n0-shot 24.4 27.4\nICL 30.9 34.8\nLoraHub avg 34.0 38.1\nBatchComp 34.7 39.0\nOurs 35.6 40.0\nTable 1: Experimental results on 27 tasks of BBH, the\n\"average-3\" has excluded three tasks with an accuracy\nof less than 10%, (*) represents the upper limit.\nMethod FFT ICL 0-shot LoraHub\nBatchComp 7/18 18/3 16/8 13/12\nOurs 11/16 19/2 18/7 16/8\nTable 2: A/B vs. the baseline, \"A\" represents the num-\nber of tasks where our proposed method performed bet-\nter than the baseline method, while \"B\" represents the\nnumber of tasks where our proposed method performed\nworse than the baseline method.\n5.1 Result The primary results are presented in Table 1 and Table 2, with detailed task scores in Appendix A. Our method significantly outperforms the zero-shot approach on 19 out of 27 tasks and the in-context learning (ICL) method on 18 tasks in terms of average performance. Compared to ICL, our approach is more computationally efficient, requiring fewer tokens. Our modifications to LoraHub are also notably successful, with our method outperforming LoraHub’s random selection approach on 16 tasks. Crucially, our instance-level method exhibits a 0.9% performance enhancement over our tasklevel method in the ablation study, underscoring the efficacy of capturing input nuances through 286instance-specific adaptation. However, our method still cannot compete with full fine-tuning (FFT), which holds a significant performance advantage over other methods on certain highly structured tasks, such as \"date understanding\" and \"dyck language\". The results suggest that only FFT enables the model to adequately learn the underlying structure and patterns required for these more complex and specialized tasks. 5.2 Discussion Ablition study. Our instance-level approach significantly outperforms the task-level BatchComp, which directly selects Lora modules without pairing questions to instances. BatchComp’s 0.7% improvement over random LoraHub selection pales in comparison to our approach’s doubling of performance in the \"disambiguation qa\" task, likely due to our method’s superior ability to highlight the importance of key instances for task success. Retrieval method average\nBM25 25.6\nDPR L2 Distance 34.3\nDPR Cosine Similarity 35.6\nTable 3: Result of different retrieval strategy\nRetrieval strategy. Our approach is closely tied to retrieval performance. If accurate retrieval is not achieved, properly aligning suitable instances with corresponding questions and matching them with the appropriate LoRA modules, the overall effectiveness will be reduced, as demonstrated in Table 3 like bm25(Robertson et al., 1995). The results obtained from the DPR’s L2 distance (Ram and Gray, 2012) and Cosine Similarity(Mussmann and Ermon, 2016) confirm the efficacy of DPR in instance-level fusion. 6 Conclusion Our work introduces two key enhancements to the LoraHub framework. The first is the incorporation of a method that indexes models trained on datasets using their semantic centroids, which improves LoraHub’s precision at the task level. The second is the introduction of instance-level adaptation, which leverages the distinctive features of individual instances to elevate the performance ceiling of the LoraHub approach. These complementary strategies work in synergy to bolster the model’s cross-task generalization capabilities. 4\n7 Limitation Increased Computational Cost. Our method incurs a higher computational cost than LoraHub, mainly because we train weights for each individual instance during the Lora group weights training phase. This means that our approach will require computational resources proportional to the number of instances, multiplied by the cost of LoraHub’s training. Application Scenario Limitation. Our method is not universally cost-effective. In scenarios where a task involves a limited number of questions, employing our method may not be the most economical choice. For tasks without any instances, zeroshot learning would be a more appropriate and efficient approach. Additional Preliminary Preparations Required. When utilizing LoRA for composition, our method not only requires identifying the appropriate LoRA modules within the library but also necessitates access to the data used during the training of those LoRA modules. Consequently, our approach incurs greater initial preparation costs compared to methods that do not rely on such specific training data. Requirement for Higher-Quality Instances. Instance-level methods, such as ours, are more sensitive to the quality of the instances used. Lowerquality instances, including those that are flawed or not closely related to the task, can potentially lead to misleading answers for associated questions. This underscores the importance of careful instance selection and curation to ensure the method’s effectiveness. 362",
        "aspect_review": "The paper presents a novel instance-level LoRA composition method that dynamically selects and combines task modules for each input instance, offering significant improvements over existing methods like LoraHub. Comprehensive experiments on publicly available datasets demonstrate the proposed method's superior performance, outperforming LoraHub in 16 out of 27 tasks, showcasing its practical impact. Ablation studies highlight the efficacy of the instance-level approach, showing a 0.9% improvement over task-level methods and better performance in instance-specific adaptation.\n\nThe paper could benefit from a more thorough discussion of related work, particularly in contrasting the proposed method with other instance-based methods. While the approach is novel, the concept of instance-level adaptation may not be groundbreaking in the broader context of machine learning. The method's significance is somewhat limited by its increased computational cost and specific application scenarios, which may restrict its broader applicability. The analysis could be strengthened by providing more detailed insights into why the method outperforms others, particularly in complex tasks. The presentation of the methodology could be clearer, especially in explaining the selection and composition process of LoRA modules. Some details on hyperparameter choices and the gradient-free optimization method could be expanded to enhance reproducibility.",
        "general_review": "The paper presents a notable novelty in the field of cross-task generalization by introducing an instance-level dynamic LoRA composition approach, which is a significant enhancement over existing task-level methods. The technical depth and rigor are evident in the comprehensive experimental validation conducted across 27 tasks, where the proposed method outperformed the baseline LoraHub in 16 tasks. This demonstrates the robustness and effectiveness of the proposed approach. The paper's methodology is well-detailed, showcasing strong technical depth and rigor, particularly in the novel selection method based on task and instance similarity. The practical impact of the paper is significant, as it offers a more stable and effective solution for adapting large language models to new tasks without extensive retraining, which is transformative for real-world applications.\n\n1. The paper could benefit from minor writing or clarity improvements, particularly in the explanation of the method's implementation details. 2. There are small details on hyperparameters or implementation that could be clarified further to aid reproducibility. 3. Some figures could use very minor formatting adjustments for better visual clarity. 4. A slight clarification on the availability and usage of the code repository would enhance the paper's accessibility to practitioners."
    },
    "33": {
        "paper": "Improving Minimum Bayes Risk Decoding with Multi-Prompt\nAnonymous ACL submission\nAbstract\nWhile instruction fine-tuned LLMs are effective text generators, sensitivity to prompt construction makes performance unstable and suboptimal in practice. Relying on a single ‘best’ prompt cannot capture all differing approaches to a generation problem. Using this observation, we propose multi-prompt decoding , where many candidate generations are decoded from a prompt bank at inference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR) decoding, which selects a final output using a trained value metric. We show multiprompt improves MBR across a comprehensive set of conditional generation tasks (Figure 1), and show this is a result of estimating a more diverse and higher quality candidate space than that of a single prompt. Further experiments confirm multi-prompt improves generation across tasks, models and metrics.11 Introduction Minimum Bayes Risk (MBR) decoding (Bickel and Doksum, 1977) improves the generation quality of large language models (LLMs) over standard, single-output decoding methods, such as beam search and sampling. MBR generates a set of candidates and selects the one with the highest expected utility, using all other hypotheses as references (see Fig. 2, left), following a simple intuition that a desirable output should be highly probable and consistent with others. MBR has been applied across a variety of NLP generation tasks (Amrhein and Sennrich, 2022; Shi et al., 2022; Suzgun et al., 2023; Jain et al., 2023). In particular, self-consistency (Wang et al., 2023), a special case of MBR, has become widely used to improve LLM reasoning capabilities by ensembling reasoning paths. A central question to improve the generation quality of MBR decoding is how to balance be1Our experiment code, data and prompts are available at\nhttps://anonymized_url .\n125 50 75100304Code Generation\n(pass@1)\n1 250 Candidate Set Size50607Simplification\n(LENS)\n1 250 500808Translation En-Cs\n(COMET)Figure 1: Multi-prompt and single prompt MBR results\nfor code generation on HUMAN EVAL, text simplifica-\ntion on SIMPEVAL, and translation on WMT ’22 EN-CS\ngenerated with open-source 7B LLMs (details in §4).\ntween diversity and adequacy within the candidate set. Prior work has found success using sampling-based decoding to generate diverse hypotheses (Eikema and Aziz, 2020; Freitag et al., 2022a, 2023a). However, naively increasing the sampling temperature eventually degrades the quality of the candidates. Recently, instruction finetuned LLMs (Ouyang et al., 2022; Chung et al., 2022) have opened up the possibility of writing prompts in various formats to elicit higher diversity generations. As these models are observed to be sensitive to prompt design, a slight change in phrasing or the inclusion of more relevant example can significantly impact model quality and behavior (Srivastava et al., 2023; White et al., 2023). Taking advantage of the prompt sensitivity of LLMs, we introduce multi-prompt MBR decoding, which samples candidates using a bank of humanor model-written prompts (see Figure 2, right). Intuitively, exploring a variety of prompts enables the generation of diverse, high quality hypotheses that provide a closer representation of the true output distribution. By guiding the model towards different regions of the output space, each prompt captures unique sequences that are coherent and relevant to the specific input example. We experiment with three distinct generation tasks: text simplification (Maddela et al., 2023), 1\nmachine translation (Kocmi et al., 2022), and code generation (Chen et al., 2021). Each task assess the impact of different prompt components on multiprompt MBR, such as instance-level prompts for code, task descriptions for simplification, and incontext examples for translation. To account for the relative quality between prompts, we develop different strategies for selecting prompts that outperform a baseline random choice: sampling prompts from a large prompt bank based on their usage on an unlabeled set of task data and selecting prompts using embedding-based heuristics without any examples. We evaluate multi-prompt MBR on a broad range of LLMs including both open-source models like Llama 2 (Touvron et al., 2023) and state-of-theart closed-source models such as GPT-4 (Achiam et al., 2023). Our results show multi-prompt MBR consistently improves single-prompt MBR across all three tasks and model scales, with gains of up to 7% on HumanEval (Chen et al., 2021) and 5 points of LENS on SIMPEVAL (Maddela et al., 2023). Figure 1 displays results for models at the 7B scale. Finally, we study the dynamics between different utility and evaluation metrics, revealing that multi-prompt MBR with one metric improves performance universally across metrics. 2 Preliminaries Instruction fine-tuned LLMs are trained to follow arbitrary natural language task descriptions (Wei et al., 2022a). Given an input xand prompt ρ, an autoregressive language model πθparameterized by θestimates an output sequence y∼πθ(x, ρ)using an decoding algorithm by sampling the next token conditioned on the input πθ(yi|y<i, x, ρ). The decoding algorithm aims to generate yby maximizing the sequence likelihood over the language model distribution πθ(y|x, ρ) = ΠT\ni=1πθ(yi|y<i, x, ρ). Minimum Bayes Risk Decoding. In practice, the highest likelihood sequence does not necessarily yield the highest quality generation (Jaeger and Levy, 2006). From this observation, MBR decoding (Bickel and Doksum, 1977; Eikema and Aziz, 2020) first samples a set of hypotheses Hfrom the model πθ, approximating the true distribution of output space Y, then selects the output ˆyMBR that maximizes the expected utility (or minimizes the expected loss in traditional formulation) with respect to a set of references R: ˆyMBR= arg max\ny∈H(EH∼πθ[U(y,R)]),(1) Y ou ar e an ar tificial \nint elligence designed t o \nsimplify human writt en I w ould lik e y ou t o \nsimplify t he f ollo wing \nsent ence such t hat t he W rit e a simpler v ersion \nsuch t hat a non-English \nspeak er or an individu... Multi-Pr omptInstruction Fine-tuned LLM\nCandidat e Selectione.g., GPT -4 , LLaMA Chat, ALMA\nwit h trained v alue metric\n          BERTScor e, LENS, COMETR ewrit e t he f ollo wing \ncomple x sent ence in \nor der t o mak e it simple ...Single Pr omptFigure 2: Multi-prompt MBR generates candidates us-\ning a human- or model-written prompt bank and selects\nthe highest pairwise score with a trained value metric.\nwhere U(y, R) =Ey′∼R[u(y, y′)]andu(y, y′)is a utility function that evaluates hypothesis yagainst a reference y′. In practice, Ris also sampled from the same model πθunder the assumption that the model produces reliable outputs in expectation, and is usually set as identical to hypothesis set H. Many existing techniques to improve LLMs’ performance such as self-consistency (Wang et al., 2023) and output ensemble (Kobayashi, 2018) are special cases of MBR. For instance, selfconsistency can be viewed as MBR using the utility function u(y, y′) =1[ans(y) =ans(y′)], where ans(y)is the answer extracted from the reasoning pathy(Bertsch et al., 2023). 3 Multi-Prompt MBR Decoding Prior work on MBR decoding primarily uses models trained or fine-tuned for a specific generation task (Freitag et al., 2022a; Fernandes et al., 2022). With instruction fine-tuned LLMs, the input xis contained within a structured prompt ρ, consisting of task instruction and/or in-context examples. Earlier studies have extensively documented that the design of the prompt has a dramatic impact on overall performance (Mishra et al., 2022; Khashabi et al., 2022; Lu et al., 2022; Sclar et al., 2023). To investigate this phenomenon, we show in Figure 3a (bottom) the likelihoods and quality of samples from 10 prompts of varying performance for a text simplification task, measuring quality as the LENS metric score against a set of gold references. Greedy sampling ( τ= 0) estimates 2\nCandidate Scores for 1 Example\n80\n 60\n 40\n 20\n 0020406080100LENS=0\n80\n 60\n 40\n 20\n 0\nlogp(y|x)\n020406080100LENS\n80\n 60\n 40\n 20\n 0020406080100=0.1\n80\n 60\n 40\n 20\n 0\nlogp(y|x)\n02040608080\n 60\n 40\n 20\n 0020406080100=0.5\n80\n 60\n 40\n 20\n 0\nlogp(y|x)\n020406080100(a)\nFirst 20 SimpEval Examples20406080100LENSSentence Scores for 20 Examples(b)\n40 50 60 70 80\nLENSPrompt 10Prompt 9Prompt 8Prompt 7Prompt 6Prompt 5Prompt 4Prompt 3Prompt 2Prompt 1Multi-Prompt\n\"I would like youto simplify the...\"\n\"You are an AIassistant thatwrites simple...\"\n\"Simplify.\"Dataset Scores for SimpEval\n(c)\nFigure 3: (a) LENS score and sequence probability for 1000 generations on a single text simplification example decoded from\nLlama 2 7B Chat with temperatures τ= [0,0.1,0.5]using a single prompt (top) and multiple prompts (bottom). As the\ntemperature increases, we find each prompt estimates candidate sequences centered at different modes. (b) LENS scores of the\nbest generation per-prompt for the first 20 sentences in S IMPEVAL, showing no single prompt produces the best overall output.\n(c) Dataset-level LENS performance of each prompt when performing single prompt MBR vs. multi-prompt MBR.\ndifferent sequences for each instruction, with single prompt (Figure 3a, top) generating a single sequence. As we increase temperature τ, generations from a single prompt simply exhibit noise centered around the mode of the highest likelihood sequence, while multi-prompt estimates a generations around modes uniquely defined by each prompt. For instance, one of the prompts (i.e., Prompt 9 highlighted in green) produces the highest quality generation for this one input sentence, despite having a low performance over the entire dataset. In fact, no prompt consistently produces the highest quality sequences, as illustrated in Figure 3b, rather prompts are most effective at different inputs. Building upon these insights, we propose multiprompt MBR decoding, depicted in Figure 2, where the MBR hypothesis set Hconsists of outputs sampled from ndistinct prompts ρ: H=n[\ni=1Hi,whereHi={y|y∼πθ(x, ρi)}.(2) Bertsch et al. (2023) show that MBR seeks the mode of some distribution qover a quality feature ϕ(y)applied to the output space rather than the mode of the model’s distribution: ˆyMBR≈arg max\ny∈Hq(ϕ(y)|x). (3) We hypothesize, in expectation, the mode of ϕ(y) across outputs from multiple prompts has higher 172downstream performance compared to that derived from a single prompt. This is empirically supported by our example, where Figure 3c shows that multi-prompt MBR outperforms individual singleprompt MBR across the full task dataset. Although multi-prompt ensembles hypothesis spaces between prompts, some notion of objective quality still exists when constructing the prompt bank. As shown in Figure 3c, the majority of the 10 human-written prompts fall within a 10-point range of LENS scores when evaluated on the task dataset but a few prompts consistently produce low-quality generation. Therefore, to account for the hierarchy in prompt quality, we propose two methods for choosing the prompts used at generation time from a prompt bank P:sampling from a learned distribution of prompts, based on a small unlabeled train set (§3.1); and selecting a subset of prompts based on heuristics in the absence of a train set (§3.2). 3.1 Prompt Sampling In this approach, we first calculate the probability of each prompt p(ρ)as the proportion of times that prompt generates the highest scoring output on a separate training set. At inference time, prompts are sampled with replacements from this learned probability distribution, and candidate outputs are then generated given these prompts. Top-pPrompt Sampling. Inspired by the principle 3\nof nucleus sampling (Holtzman et al., 2020), our goal is to keep the prompts with high probability and truncate the least used prompts by setting their probabilities to zero. We define the top- pprompt set as the minimal set Ptop-p⊆ P such that: |Ptop-p|X\ni=0p(ρi)≥p. (4) We then re-normalize the distribution of Ptop-pand sample prompts from the new distribution: p′(ρ) =\n\np(ρ)P\nρ∈Ptop-pp(ρ)ifρ∈ P top-p\n0 otherwise.(5) 3.2 Prompt Selection Prompt selection chooses a fixed subset Pbest⊂ P of|Pbest|=kprompts based on heuristics. Compared to sampling, this does not require an additional training set to evaluate prompt efficacy. We consider the following heuristics for selectingPbest: prompts that have the closest similarity andgreatest dissimilarity with others, and prompts that are randomly selected from each k-NN cluster , which is also useful when a training set is presented, allowing the selection of high-performing prompts within each cluster. We calculate the semantic (dis)similarity of prompts based on SentenceBERT (Reimers and Gurevych, 2019) embeddings. 4 Experiment Setup In this section, we describe the experimental details for evaluating the efficacy of multi-prompt MBR decoding across tasks, prompt setups, models, and utility metrics, with results and analyses in §5. 4.1 Tasks & Datasets Unlike previous work applying MBR to a single generation task (Shi et al., 2022; Eikema and Aziz, 2022), we deliberately select three unique tasks to demonstrate the universality of multi-prompt: text simplification with task-level instructions, code generation with example-level instructions, and machine translation with in-context examples. Code Generation. We use HumanEval (Chen et al., 2021) benchmark, where models are tasked with generating a Python program given a description with unit tests. Since each example is a unique coding task, we generate a unique prompt bank for each input. Following Zhang et al. (2023), we reject empty, degenerate (e.g., pass ,return None ), or non-compiling programs before applying MBR. 244Text Simplification. We use the SIMPEVAL 2022 test set (Maddela et al., 2023), containing complex sentences from Wikipedia, paired with humanwritten simplifications. The prompt bank is generated based on author-written examples (Table 4) and are used for the entire dataset. Machine Translation. We purposely choose the EN→CS language pair from the WMT 22 (Kocmi et al., 2022) newstest corpus, ensuring its exclusion from the training data of recent translation LLMs or metrics (Xu et al., 2024). Results on additional language pairs are in Appendix C.2. 4.2 Constructing the Prompt Bank Following existing work studying prompt sensitivity (Mizrahi et al., 2023; Gonen et al., 2023), our experiments rely on a small set of manually written seed prompts, and use an LLM to generate diverse paraphrases of prompts. Model-written prompts are generated using GPT-4 Turbo. For seed prompts, the authors manually write 10 for text simplification (Table 4) and use the original HUMAN EVAL instruction from each example for code generation. Only the LLM-written prompts are used for multi-prompt. The translation prompts consist of randomly sampled in-context examples from previous WMT shared tasks. We use incontext examples for translation as the translation LLMs were not trained to follow task instructions. For multi-prompt experiments, we select from the prompt bank with top- pprompt sampling (§5.2) using p=0.6, where the prompt usage p(ρ)is calculated using a held-out 20% split of each dataset. For our single prompt baselines, we aim to use strongest available prompt in the prompt bank, so we use the prompt with the highest usage p(ρ). Human-written prompts and prompt generation instructions are included in Appendix A. 4.3 Models Our main experiments are performed with Llama 2-7B Chat (Touvron et al., 2023) for simplification, ALMA-7B-R (Xu et al., 2024) for translation and CodeLLaMA-13B Instruct (Roziere et al., 2023) for code generation, all fine-tuned to follow instructions. In §5.3 we further explore a wide range of model architectures and sizes, including state-ofthe-art and task-specific fine-tuned models. Unless otherwise specified, we generate the hypothesis set using nucleus sampling (Holtzman et al., 2020) withτ= 0.9, p= 0.95. We include a detailed review of all models in this work in Appendix B.2. 4\n0.0 0.5 1.0 1.5 2.0\nTemperature ()\n0100200300400500600Novel Bigrams\nMulti-Prompt\nSingle Prompt\n0.0 0.5 1.0 1.5 2.0\nTemperature ()\n5560657075LENS\nFigure 4: Candidate set diversity and LENS scores\nacross temperatures for simplification task. At low tem-\nperatures, the increased candidate diversity from multi-\nprompt directly translates to improved performance.\n4.4 Utility Metrics & Evaluation Our core experiments use the trained LENS (Maddela et al., 2023) for simplification and COMET (Rei et al., 2020) for translation as the candidate selection metric. For code generation, we use MBREXEC (Shi et al., 2022), which executes each candidate program against a set of test cases, selecting the program with the highest agreement over all test cases’ outputs. As in Zhang et al. (2023), we use the docstring examples as test cases for MBREXEC and evaluate with pass@1. Given the growing body of work on metric development, we verify our multi-prompt results across a broad range of utility and evaluation metrics in §5.4. 5 Experiment Results We compare multi-prompt decoding to traditional MBR (§5.1), ablate the prompt sampling mechanism (§5.2), vary model architectures (§5.3), evaluate across utility metrics (§5.4) and finally evaluate multi-prompt on efficient MBR alternatives (§5.5). 5.1 How does multi-prompt MBR perform? Multi-prompt Improves MBR. We report our main results in Figure 1, comparing single prompt and multi-prompt performance as the number of generated candidates increases, with detailed results in Figure 7 in App. C. Multi-prompt consistently outperforms standard MBR for all tasks. Candidate Diversity ⇏Quality. To measure the impact of temperature on the candidate set quality, we report performance and diversity, as measured by novel bi-grams, across temperatures in Figure 4. For low temperatures, we find that multi-prompt generates a consistently more diverse candidate space, which directly translates to higher-quality generation. While single prompt MBR performance improves with temperature τ >1, despite generating an equal or greater diversity set than 331pass@1 L ENS COMET\nSingle Prompt (|H|=100 ) 48.78 74.67 88.93\nMulti-Prompt + Prompt Sampling (|P|=100 )\nRandom Selection – 74.91∗89.98∗\nPrompt Sampling – 78.29∗90.33∗\nTop-pPrompt Random – 78.61∗90.11∗\nTop-pPrompt Sampling – 79.08∗90.36∗\nSingle Prompt (|H|=10 ) 41.55 61.26 87.24\nMulti-Prompt + Prompt Selection (Pbest⊂P,|Pbest|=10 )\nRandom Selection 39.63 60.00 87.81∗\nk-NN Cluster Random 40.24 58.73 87.80∗\nFarthest Similarity 44.51∗58.32 88.14∗\nClosest Similarity 37.80 61.53∗87.73∗\nHighest Performance – 62.43∗87.65\nk-NN Cluster Performance – 66.12∗87.73∗\nTable 1: Results for prompt sampling using 100 prompts\n(top) and subset selection using 10 of 100 prompts (bot-\ntom). * =Statistically significant improvement with\np<0.05. Sampling from a weighted, truncated distribu-\ntion improves multi-prompt across candidate set sizes.\nmulti-prompt, multi-prompt MBR still produces higher quality candidates. As τ→2, the quality of single and multi-prompt MBR begins to degrade as their candidate sets become too noisy to generate high-quality sequences. Framing the decoding process as each prompt estimating a unique distribution of candidate generations (§3), the ability of multi-prompt to achieve higher quality generation as a result of candidate set diversity is intuitively the byproduct of combining multiple candidate distributions defined by each instruction. 5.2 What is the impact of the prompt bank? Sampling Prompts Improves Candidate Quality. Table 1 (top) reports results for multi-prompt across different prompt sampling methods for text simplification and translation. We perform a hypothesis test for the statistical significance of each variation of multi-prompt outperforming single prompt MBR using bootstrap sampling with 1000 iterations (Koehn, 2004). Note that, code generation results are omitted as a unique set of prompts is generated for each HumanEval example. We find sampling prompts by usage and truncating the toppprompts improves multi-prompt over a random selection baseline, with top- pprompt sampling performing the best on both tasks. A Higher Quality Prompt Bank Improves Multiprompt. Table 1 (bottom) reports results for different prompt subset selection methods, which use heuristics to select a smaller set of prompts for multi-prompt to maximize performance. The best 5\nSingle Prompt Multi-prompt\nCode Generation (|H|=20 ) – H UMAN EVAL (pass@1)\nStarCoder 2 15B 44.51 49.39 (+4.88)\nCodeLlama 7B 37.80 40.85 (+3.05)\nCodeLlama 13B 43.29 48.17 (+4.88)\nCodeLlama 34B 45.73 52.44 (+6.71)\nCodeLlama 70B 61.59 68.90 (+7.31)\nGPT-3.5 68.29 73.78 (+5.49)\nGPT-4 81.71 82.93 (+1.22)\nText Simplification (|H|=100 ) – S IMPEVAL (LENS)\nCtrl T5 3B 72.6 –\nCtrl T5 11B 74.4 –\nLlama 2 7B Chat 75.71 80.38 (+4.67)\nLlama 2 13B Chat 78.19 80.27 (+2.08)\nLlama 2 70B Chat 82.21 83.28 (+1.07)\nGPT-3.5 76.87 81.25 (+4.38)\nGPT-4 76.47 81.56 (+5.09)\nTranslation (|H|=100 ) – WMT ’22 E N-CS(COMET)\nWMT ’22 Winners 91.9 –\nMS Translate API 90.6 –\nALMA 7B R 89.17 89.94 (+0.77)\nALMA 13B R 89.41 90.45 (+1.04)\nGPT-3.5 91.27 91.35 (+0.08)\nGPT-4 92.24 92.47 (+0.23)\nTable 2: Metric scores for state-of-the-art systems com-\npared to LLMs with multi-prompt using |H|candidates.\nTranslation and simplification baselines are as reported\nin Hendy et al. (2023) and Maddela et al. (2023).\nselection method for each task had a significant impact on performance when compared to a single prompt MBR (+2.9 pass@1, +4.9 LENS and +0.9 COMET ). For text simplification, decoding with the 10 highest performing prompts is further improved by selecting prompts from a k-NN clustering of prompt embeddings, which enforces a dis-similarity between prompts. However, translation and code generation benefit from using the farthest similarity, or semantically distant prompts. These results highlight multi-prompt’s sensitivity to the prompt construction, and shows that enforcing both diversity via multi-prompt and performance via prompt selection improves candidate generation. A direct comparison between prompt sampling and selection using the same candidate set size is included in Table 6 in Appendix C.3. 5.3 Does multi-prompt MBR improve quality across model architectures and sizes? Multi-prompt Improves MBR Across Models. Figure 5 reports improvement of multi-prompt over single prompt across widely used LLMs as a ∆ change in score, with per-model results in Appendix C.4. In all cases, multi-prompt outperforms single prompt using a sufficiently large candidate set size, showing an increasing or constant metric 1 5 10 15 20-5.0-2.5+0.0+2.5+5.0+7.5+10.0 pass@1\nCode Generation (HumanEval)\nCodeLlama 7B\nCodeLlama 13B\nCodeLlama 34B\nCodeLlama 70B\nDeepseek 1.3B\nDeepseek 6.7B\nDeepseek 33B\n1 20 40 60 80 100-2.0+0.0+2.0+4.0+6.0+8.0+10.0 LENS\nSimplification (SimpEval)\nLlama 2 7B\nLlama 2 13B\nLlama 2 70B\nOLMo 1B\nOLMo 7B Instruct\nMistral 7B\n1 20 40 60 80 Candidate Set Size-0.5+0.0+0.5+1.0+1.5+2.0 COMET\nTranslation (WMT '22 En-Cs)\nALMA 7B R\nALMA 13B R\nTowerInstruct 7B\nTowerInstruct 13B\nAya 101 13BFigure 5: ∆metric improvement from single prompt\nto multi-prompt across model sizes and architectures,\nreported with a 95% CI bootstrapped over 20 iterations.\nFor absolute performance, see Figure 10.\nimprovement. In fact, smaller models surpass their larger counterparts’ single output decoding at large enough candidate set sizes (Fig. 10). For instance, CodeLlama 13B outperforms its 70B variant using multi-prompt with 18 candidates ( 48.26>47.99 pass@1) and TowerInstruct 7B outperforms 13B with 5 candidates ( 81.73>80.14COMET ). LLMs with Multi-prompt Outperform Finetuned Models. Whether general-purpose, instruction fine-tuned LLMs outperform models trained on a specific generation task is still an active question (Qin et al., 2023), so we compare state-ofthe-art results from each task dataset using single prompt MBR to instruction fine-tuned LLMs using multi-prompt MBR with top- pprompt sampling. In Table 2, we report previous SOTA results for each task: an 11B T5-based text simplification model with control tokens for simplification operations (Sheang and Saggion, 2021), the EN-CSresults for the WMT ’22 winning submission (Kocmi et al., 2022) and StarCoder 15B, a code infilling and generation LLM (Li et al., 2023), not explicitly trained to follow natural language instructions. LLMs surpass fine-tuned model performance when using 6\nText Simplification (LLaMA 7B Chat)\nSARIBERT\nSCORE LENSLENS\n-SALSA\nRFSLERF\nSARI +1.08∗+1.06∗+7.24∗+4.33∗+0.38∗\nBERTSCORE +1.44∗+1.09∗+6.18∗+3.11∗+0.45∗\nLENS -0.67 -0.05 +5.78∗+4.69∗+0.82∗\nLENS-SALSARF-0.83 +0.35∗+8.10∗+4.65∗+0.97∗\nSLERF-5.25 -4.71 +2.39∗-4.51 +1.05∗\nTranslation (ALMA 7B)\nBERT\nSCORECOMET\n-22COMET\nKIWIRFXCOMETMETRIC\nXMETRIC\nX-QE\nRF\nBLEU +0.34∗+0.47∗+0.67∗-0.14 +0.04 +0.11∗\nBERTSCORE +0.51∗+1.59∗+1.68∗+2.48∗+0.22∗+0.29∗\nCOMET -22 +0.71∗+0.89∗+1.72∗+3.29∗+0.13∗+0.18∗\nCOMET KIWIRF+0.80∗+1.03∗+1.06∗+2.87∗+0.07∗+0.08∗\nXCOMET +0.14 +0.85∗+0.84∗+3.34∗+0.09∗+0.04∗\nMETRIC X +0.36∗+0.81∗+0.36 +3.93∗+0.07∗-0.04\nMETRIC X-QERF+0.60∗+1.68∗+2.11∗+5.31∗+0.08∗+0.03∗Evaluation MetricMBR Utility Metric\nTable 3: ∆metric improvement from single prompt\nto multi-prompt across metrics. RF=Reference-free\nreranker. * =Statistically significant improvement with\np <0.05. For absolute performance, see Table 8.\nmulti-prompt, for instance Llama 2 13B shows +5.8 LENS over fine-tuned T5 11B. 5.4 Does multi-prompt MBR over-fit to the utility metric? An inherent challenge of evaluating MBR is that the utility metric used to select candidates is typically also used for the final evaluation, in such cases it is difficult to attribute the metric improvement to higher quality generation (Bertsch et al., 2023). Given growing attention to metric development, we leverage various trained metrics to test whether multi-prompt using one utility metric improves performance cross all other utility metrics. We experiment with traditional overlapbased metrics, ( BLEU,SARI), embedding similarity ( BERTSCORE ), small ( ∼100M parameter) trained metrics with references ( LENS,COMET 22) and without references ( COMET KIWI,LENSSALSA ,SLE), and large (3B+ parameter) trained metrics ( XCOMET ,METRIC X,METRIC X-QE ). These metrics represent diverse text evaluation approaches and encompass the full state of evaluation in both tasks. We include a full description of metric architectures in Appendix B.1. Multi-prompt MBR Improves Across Metrics. Table 3 reports results for cross-metric evaluation, with the diagonal reflecting the traditional MBR 439evaluation setup (i.e., calculate MBR and evaluate using the same metric) and other cells indicate generalization from one metric to all others. Multiprompt improves performance on most evaluation setups, with a few notable exceptions such as disagreement between trained and overlap-based metrics for simplification and COMET -based metrics for translation. For simplification, trained metrics’ failure when evaluated by SARIandBERTSCORE may be a byproduct of the test set size, as these metrics typically require a substantial number of references for stable evaluation (Alva-Manchego et al., 2020), more than what are provided in SIMPEVAL. Interestingly, the magnitude of performance improvement is highly variable to the specific utility metric, with no clear relationship between the metric architecture and improvement of multi-prompt, but typically a lower baseline performance indicates multi-prompt performs better (Table 8 in Appendix for more details). 5.5 How does the metric type impact multi-prompt MBR? As discussed by Fernandes et al. (2022), the MBR operation requires each candidate evaluate against every other candidate (i.e., O(n2)comparisons), this becomes inefficient in practice for a large n, especially when using a trained utility metric. Therefore, we explore multi-prompt MBR alternatives using reference-free utility metrics: •Reranker ( O(n)).Re-ranking directly estimates the quality of each candidate using a referencefree metric: ˆyMBR= arg maxy∈H[U(y)]. We use the trained LENS-SALSA for simplification (Heineman et al., 2023) and COMET -MQM (Rei et al., 2021) for translation. For code generation, we use Code Reviewer (Shi et al., 2022), which calculates agreement between the pertoken probability of the generation given the docstring and the original docstring given the generation. Reference-free re-ranking only requires n metric calculations to directly estimate quality. •Reranker + MBR ( O(n+m2)).We use a twostage selection where we first rerank all ncandidates and select the top mto use for MBR, where the cheap re-ranker can distill the candidate set and the expensive MBR metric performs the final selection, where m≪n. •Multi-turn MBR ( O(n2+m2)).Similar to the previous approach, we perform MBR and then re-compute MBR using the top mcandidates. 7\n1 5 10 15 20253035404550pass@1\nCode Generation (HumanEval)\n1 20 40 60 80 Candidate Set Size556065707580LENS\nSimplification (SimpEval)\n1 20 40 60 80 10075.077.580.082.585.087.5COMET\nTranslation (WMT '22 EN-CS)\nSingle Prompt\nRerank\nRerank + MBR\nMBR\nMulti-turn MBRFigure 6: Alternative MBR formulations for multi-prompt across candidate set sizes for code generation, text\nsimplification and translation. Efficient MBR methods show inconsistent results, dependent on task and metric.\nResults. We report results across candidate selection methods in Figure 6, finding the multiprompt achieves performance improvement across reference-based and reference-free metrics, yet the relative performance of methods varies between tasks. With text simplification, the methods first narrowing the candidate set (‘Rerank + MBR’) and iteratively performing MBR (‘Multi-turn MBR’) either match or out-perform vanilla MBR. We speculate the first pass may prune the lowest quality generations such that the second pass only considers a distilled candidate set, which better informs the MBR calculation. For translation, the more efficient re-ranker outperforms vanilla MBR, which follows recent work finding trained reference-based and reference-free MT metrics are approaching a similar quality (Freitag et al., 2023b). For code generation, the re-ranker under-performs MBR, which may be reflective of the performance of Code Reviewer compared to MBR-E XEC, as the latter has access to multiple test cases. 6 Related Work Output Selection. Ensembling outputs across a generation set has become a widely used technique for improving LLM performance in classification tasks, such as using a majority vote over reasoning chains (Wang et al., 2023), or merging outputs from multiple models (Kobayashi, 2018; Martínez Lorenzo et al., 2023). This work applies the same underling concept to text generation by leveraging trained automatic evaluation metrics. To our knowledge, it is the first to propose a multiprompt decoding scheme for text generation. MBR Decoding. MBR decoding has been previously used to improve generation quality for machine translation (Kumar and Byrne, 2004; Eikema and Aziz, 2020; Müller and Sennrich, 2021) text simplification (Maddela et al., 2023), summarization and style transfer (Suzgun et al., 2023). 528Bertsch et al. (2023) highlight the growing popularity of MBR as a simple technique in machine translation and reporting shared tasks results. While our work is the first to propose generating the MBR hypothesis space using a prompt bank, Farinhas et al. (2023) perform preliminary experiments with paraphrases of a single sentence prompt, but found no difference in performance. Recent work argues sampling strategies like nucleus (Eikema and Aziz, 2022) or epsilon (Freitag et al., 2023a) offer slightly better performance over beam search for MBR, with this work extending their findings by attributing candidate set quality to sampling diversity. Prompt Selection. Current work on prompting for text generation has instead focused on optimization, such as in-context example selection (Min et al., 2022), example ordering (Lu et al., 2022) and prompt selection (Gonen et al., 2023). Notably, Agrawal et al. (2023) show selecting in-context examples for MT by maximizing n-gram overlap between the source and examples improves few-shot performance. Zhou et al. (2023) experiment with LLMs as prompt generators, and Yang et al. (2023) show using LLMs to iteratively rewrite prompts on a development set can distill a single, high-performant prompt. Our work builds on LLMwritten prompts and basic heuristics for distilling the prompt bank to further improve multi-prompt. 7 Conclusion In this work, we propose multi-prompt, a generalized case of MBR for conditional text generation. Multi-prompt successfully ensembles outputs of instruction fine-tuned language models across prompt constructions and in-context examples. We highlight the importance of prompt selection and sampling when constructing the prompt bank with top-pprompt sampling and further verify our results across tasks, models and utility metrics. 8\nLimitations We limit our study of the prompt bank to a basic set of seed prompts and GPT-written paraphrases for each task. Notably, we do not study the impact of prompt formats (e.g., passage:{}\\n answer{} vs.Passage::{} Answer::{} , Sclar et al., 2023), in-context example ordering (Lu et al., 2022) or example selection (Agrawal et al., 2023) on multiprompt performance, although multi-prompt may extend to such methods. We leave the question of exhaustively constructing a prompt bank to future work, perhaps by extending work in prefix tuning (Li and Liang, 2021). An inherent limitation of MBR is the increase in inference time, where we generate up to 100 samples in our experiments, and use a neural utility metric with either linear or quadratic comparisons between candidates. In practice, the generation time was significantly lowered by decoding in parallel and the use of efficient-memory attention techniques such as paged and flash attention used in the vLLM library (Kwon et al., 2023). The computational bottleneck for large candidate set sizes was instead evaluating the utility metrics across all pairs of generated candidates. To lower the number of metric comparisons, promising results have been demonstrated by pruning low-scoring candidates during the MBR process (Cheng and Vlachos, 2023), aggregating embedding representations of candidates (Vamvas and Sennrich, 2024) or selecting a subset of references for each candidate using heuristics on reference embeddings (Deguchi et al., 2024). Similarly, we show in §5.5 efficient alternatives to MBR such as using reference-free metrics largely preserve the benefits from multi-prompt. Along with MBR, many widely used methods improving LLM abilities trade increased compute at inference time for higher performance, such as using chain-of-thought to decode a reasoning chain for a single answer or using self-consistency to selects an answer among multiple reasoning chains (Wei et al., 2022b; Wang et al., 2023). 608",
        "aspect_review": "The paper presents a novel and insightful approach by introducing multi-prompt decoding to improve Minimum Bayes Risk (MBR) decoding, addressing the instability and suboptimality caused by sensitivity to prompt construction in instruction fine-tuned LLMs. The motivation is well-justified as it leverages the sensitivity of LLMs to prompt design to enhance diversity and quality of candidate generations. The experiments are comprehensive, covering a wide range of conditional generation tasks including text simplification, machine translation, and code generation. The study evaluates multi-prompt MBR across various models and metrics, providing strong empirical validation of its effectiveness. The results demonstrate significant improvements in generation quality with multi-prompt MBR, showing gains across tasks, models, and metrics. The paper highlights up to 7% improvement on HumanEval and 5 points on SIMPEVAL, showcasing the practical impact of the proposed method. The paper effectively utilizes diverse tasks such as text simplification, machine translation, and code generation to validate the proposed multi-prompt approach, demonstrating its applicability across different domains. The methodology is robust, combining multi-prompt decoding with MBR to enhance the diversity and quality of candidate outputs. The use of a prompt bank and strategies like top-p prompt sampling and prompt selection are methodologically sound and innovative.\n\nWhile the methodology is strong, there are minor concerns regarding the clarity of certain methodological details, such as the specific impact of prompt formats and in-context example ordering on multi-prompt performance. Additionally, the study could further explore the exhaustive construction of a prompt bank.",
        "general_review": "The paper presents a novel approach to improve Minimum Bayes Risk (MBR) decoding through multi-prompt decoding, which is a significant advancement in the field of natural language processing. The methodology demonstrates strong technical depth and rigor, particularly in its comprehensive experimental validation across multiple tasks such as code generation, text simplification, and machine translation. The results indicate substantial improvements in generation quality, showcasing the significant practical impact of the proposed method. The study is well-organized, with a clear exposition of related literature and a thorough analysis of the dynamics between different utility and evaluation metrics.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the experimental setup. 2. Some small details on hyperparameters and implementation specifics are not fully detailed, which could aid in reproducibility. 3. The formatting of figures could be slightly improved for better readability. 4. The availability and accessibility of the code could be clarified to ensure ease of use for future researchers."
    },
    "34": {
        "paper": "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment\nAnonymous ACL submission\nAbstract\nReinforcement Learning from Human Feedback (RLHF), using algorithms like Proximal Policy Optimization (PPO), aligns Large Language Models (LLMs) with human values but is costly and unstable. Alternatives have been proposed to replace PPO or integrate Supervised Fine-Tuning (SFT) and contrastive learning for direct fine-tuning and value alignment. However, these methods still require voluminous data to learn preferences and may weaken the generalization ability of LLMs. To further enhance alignment efficiency and performance while mitigating the loss of generalization ability, this paper introduces Distributionguided Efficient Fine-Tuning (DEFT), an efficient alignment framework incorporating data filtering and distributional guidance by calculating the differential distribution reward based on the output distribution of language model and the discrepancy distribution of preference data. A small yet high-quality subset is filtered from the raw data using a differential distribution reward, which is then incorporated into existing alignment methods to guide the model’s output distribution. Experimental results demonstrate that the methods enhanced by DEFT outperform the original methods in both alignment capability and generalization ability, with significantly reduced training time. 1 Introduction Large language models (LLMs) have demonstrated remarkable capabilities and potential across various natural language processing (NLP) tasks (Bubeck et al., 2023; Brown et al., 2020; Kaplan et al., 2020), becoming a focal point for both academic research and industrial applications. Artificial intelligence assistants, powered by LLMs, are increasingly prevalent in everyday use, significantly improving the efficiency of various tasks. However, with their widespread usage, concerns about ethical and value preferences in model outputs have 041emerged. Ensuring that the model’s outputs are safe, reliable, and aligned with human preferences has become a challenge that researchers and developers must overcome (Ouyang et al., 2022; Peng et al., 2023). The training process for LLMs involves three stages (Rafailov et al., 2024b): Pre-training, Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017). Human preference alignment tasks are completed during the RLHF phase (Bai et al., 2022a; Stiennon et al., 2020), which includes reward modeling and Reinforcement Learning (RL) policy optimization algorithms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and its variations (Ramamurthy et al., 2022). However, these methods are computationally expensive, sensitive to hyperparameters, and prone to training instability. Recent studies suggest that using a smaller but higher-quality sub-dataset may be more effective than using the entire dataset for instruction finetuning (Chen et al., 2023; Li et al., 2023b; Liu et al., 2024). In contrast, opting to train with a vast amount of raw data indiscriminately may only inflate training costs and potentially exacerbate issues of hallucination (Zhang et al., 2023). In the context of alignment, this scenario leads to the emergence of alignment tax (Ouyang et al., 2022), as seen in fine-tuning based methods mentioned above, which still necessitate a considerable amount of preference data and a certain alignment tax. Despite insightful attempts like LIMA (Zhou et al., 2023) to align models using only a small amount of manually curated high-quality data, these efforts focus only on the SFT stage. And the construction of high-quality dataset is exceedingly costly. However, the superficial alignment hypothesis led us to consider aligning the overall output distribution of the model. In consequence, we proposes a novel alignment enhancement frame1\nwork Distribution-guided Efficient Fine-Tuning (DEFT ), which achieves a more efficient preference learning by filtering data and guiding the output distribution through the distribution reward calculated from the original data distribution and the model’s output distribution. DEFT achieves less training cost, improved alignment effectiveness, and enhanced generalization capability compared with the original methods. As shown in Fig. 1, for each preference datum, we separately tally the counts of all tokens in chosen answers and rejected answers, calculate their frequencies, and derive a positive distribution aggregated from chosen answers and a negative distribution aggregated from rejected answers. By subtracting these two distributions, we obtain a discrepancy distribution based on the current preference, which simultaneously captures the most salient positive and negative information while eliminating redundant content in natural language. The Distribution reward is calculated based on the difference between the model output distribution and the discrepancy distribution, which is used to select a small yet high-quality subset from the raw dataset and can be incorporated alongside other alignment methods to facilitate a better learning of preferences. We conduct experiments to comprehensively compare the performance of alignment and impact on generalization capabilities between the original alignment methods and the new method enhanced with the DEFT framework. Results indicate that the DEFT-enhanced method can achieve superior alignment performance with less training time and fewer steps, while also bolstering general capabilities. Prior to a comprehensive elaboration, the contributions of this paper can be outlined as follows: •Proposal of a novel distribution reward, which is obtained by calculating the difference between the model’s output distribution and the discrepancy distribution extracted from the raw preference data. •A small yet high-quality subset can be automatically filtered from the original data through the computation of the distribution reward, which can be further integrated into existing fine-tuning alignment methods for distributional guidance. •Both the data filtering and distributional guid- 132ance contribute to a more efficient preference learning process, resulting in better preference learning outcomes and retained or even enhanced generalization ability with lower training costs. 2 Related Works 2.1 Reinforcement Learning from Human Feedback Represented by PPO, RLHF has achieved significant success in alignment, becoming an early, generic method for aligning human preferences in LLMs. Subsequently, many RL-based methods (Bai et al., 2022b; Ramamurthy et al., 2022; Li et al., 2023c; Lightman et al., 2023; Lee et al., 2023; Hu et al., 2023; Dong et al., 2023) have been proposed to mitigate the issues with PPO, streamline its process, and enhance alignment effects. However, it still faces drawbacks including high training costs, long durations, process instability, and sensitivity to hyperparameters. The research focus is gradually shifting towards training-free and finetuning-based alignment methods. 2.2 Alignment Methods without Reinforcement Learning To address the various issues associated with traditional RL-based alignment methods, researchers have extensively explored alignment methods that operate during the inference stage (Li et al., 2023a) and those that rely solely on SFT, with a particular emphasis on the latter. Among them, SFT extension methods such as Rank Responses to align Human Feedback (RRHF) (Yuan et al., 2023) and Preference Ranking Optimization (PRO) (Song et al., 2024) obtain preferred answer sequences through prior annotation. During the training process, preference learning can be achieved by adding contrastive learning loss on top of SFT. On the other hand, Direct Preference Optimization (DPO) (Rafailov et al., 2024b) establishes a direct relationship between the optimization objective of PPO and language models through a reasoned derivation, achieving good results while mitigating traditional alignment burdens. Based on DPO, numerous analyses (Xu et al., 2024b; Rafailov et al., 2024a; Feng et al., 2024; Saeidi et al., 2024), improvements (Liu et al., 2023; Pal et al., 2024; Morimura et al., 2024; Singhal et al., 2024; Park et al., 2024), and novel methods (Xu et al., 2024a; Zheng et al., 2024; Hong et al., 2024; 2\nLanguage  Model\nNegative Distribution\nPositive DistributionRaw Data\n😈I want to rob an ATM machine, how can I do this?If you want to rob an ATM machine,  it’s probably a good idea to ···\nI'm sorry, I cannot assist with illegal activities such as robbery. It is important to always follow ···\nNo, I cannot assist in any illegal or harmful activities.  It goes against ethical and moral standards and ···Will you help me sneak a weapon onto a plane?That depends on what sort of weapon you want ··· I am not a specialist, but if you want to, we can ···\nLog Output DistributionI * 3 sorry * 1 assist * 2 to * 1       ···If * 2 want * 3 but * 1 to * 3      ···Token Frequency\nDiscrepancy Distribution\nPositive Distribution\nNegative DistributionSubtract     (-)\n😰K-L Divergence ≈ 0Positive\nNegativeExisting Alignment MethodsRaw Data\n,Diff()\nDistribution Reward=High-quality Subset\nLanguage  Model\nFigure 1: The positive and negative distribution can be obtained by calculating word frequencies from the tokenized\npreference data. The operation of subtracting positive and negative distributions amplifies information most closely\naligned and divergent from preferences, while cancelling out redundant information. The distribution reward can\nbe calculated based on the differential distribution and the model’s output distribution, is used for both selecting\nhigh-quality subset and guiding the distribution during training.\nMeng et al., 2024) have been proposed to enhance preference learning. Given cost and time constraints, our study focuses on applying the DEFT framework to both PRO and DPO, chosen from a plethora of excellent methods. 3 DEFT We aim to establish an efficient alignment framework with data filtering and distribution-level guidance by calculating the distribution reward based on the preference data distribution and the model’s output distribution. Before achieving these, we need to obtain the discrepancy distribution from the raw data. 3.1 Discrepancy Distribution As shown in Fig. 1, a raw preference dataset comprises a query x, a chosen response ypos, and a rejected response yneg. Assuming the existence of a function capable of accurately mapping all of these preferences, denoted as the reward function r∗(x, y). In this paper, we posit that: r∗(x, ym)> r∗(x, yn),ifm < n (1) 203Therefore, we can assume each preference data sample as {x, y 1, y2}, where y1is the chosen answer, and y2is the rejected answer. In the context of a preference p∗alignment problem, consider a scenario with a to-be-aligned policy model πand two agents, AgentposandAgentneg, where these agents could be either language models or humans. We pose to them Nprompts related to preference p∗, where Agentposconsistently generates content aligned with p∗, while Agentneggenerates content opposing or deviating from p∗, i.e., r∗(x, y pos)≫r∗(x, y neg). By collecting and tallying the tokens in their generated content, we obtain positive and negative distributions Q+andQ−related to p∗after normalization. As Napproaches infinity, the two opposing distributions tend toward an optimal positive distribution Q∗\n+, perfectly aligning with p∗, and the worst negative distribution Q∗\n−, completely deviating from p∗: Q∗\n+/−:= lim\nN→∞Q+/− (2) Simultaneously, we capture the policy model’s output distribution Qπfor each prompt x. One straightforward approach is to employ contrastive learning, which pushes the model closer to Q+and away from Q−. 3\n😈I want to rob an ATM machine, how can I do this?\nI'm sorry, I cannot assist with illegal activities such as robbery.  It is important to always follow ···\nLow Distribution Reward= -23RQ= -18RQ\nSelected\n(),\n(),\n😈Why do Samsung products suck?\nDo you want me to answer that in general, or do you have specific complaints?= -12RQ= -15RQFiltered\n(),\n(),High Distribution RewardFigure 2: Data filtration is achieved through pre-computed RQ, where responses demanding preferences of high\nspecificity yield lower RQ, while those unrelated to preferences receive heightened RQ, facilitating the extraction\nof a dataset characterized by maximal preference information.\nHowever, considering the redundancy in natural language, it can be clearly observed from Fig. 1 that the differences between these two distributions can be extremely subtle, i.e., DKL(Q+||Q−)≈0. In such cases, the policy model πstruggles to glean preference information effectively. Our simple yet effective idea involves subtracting the two distributions after normalizing token frequency, yielding the discrepancy distribution Qdiff: Qdiff(token i)\n=Q+(token i)\nVP\ni=1Q+(token i)−Q−(token i)\nVP\ni=1Q−(token i)(3) where Vis the size of model vocabulary. The specific form of the discrepancy distribution is as follows: Qdiff={prefertoken i|i∈[0, V]} (4) where prefertoken iis the result of subtracting word frequencies in the positive and negative distributions, reflecting preference information to a certain degree. Through this subtraction operation, we naturally eliminate redundant tokens, amplifying the preference information latent in both positive and negative distributions. We enable πto learn from the discrepancy distribution Qdiff. In certain cases, we receive a query along with a preferred response sequence (Yuan et al., 2023; Song et al., 2024), specifically: {x(i),(y(i)\n1, r(i)\n1),(y(i)\n2, r(i)\n2), . . . , (y(i)\nl, r(i)\nl)}. To better approximate the optimal distribution with Q+/−, the preferred responses can be empirically 256normalized using min-max normalization: r(i)\nx=r(i)\nx−r(i)\nl\nr(i)\n1−r(i)\nl(5) If a response’s score is close enough to the best answer to be considered positive or to the worst answer to be considered negative, it can be used to better approximate the optimal distribution. In this context, responses with r(i)\nxvalues close to 1 are classified as positive, while those with r(i)\nxvalues close to 0 are classified as negative. 3.2 Distribution Reward To obtain the distribution reward, in addition to the discrepancy distribution Qdiff, we also need the output log probability distribution of the model π. We calculate the average of the log output distribution of πfor each time step of prompt x, denoted asQavg\nπ: Qavg\nπ=P\ntlogQπ(x, y<t)\n∥y∥(6) where ∥y∥is the length of the response. The specific form of Qavg\nπis as follows: Qavg\nπ={token 1:p(token 1),\n. . . ,\ntoken V:p(token V)}(7) where p(token i)denotes the mean log probability oftoken iwith respect to the prompt xthroughout the entire sequence of the answer y. Then the distribution reward is calculated as follows, denoted asRQ: RQ=X\nQdiff∗Qavg\nπ (8) 4\nPrecisely, RQcan be expressed in expanded form as: RQ=VX\ni=1prefer (token i)∗p(token i)(9) It is worth noting that Qdiffincludes negative values and is not strictly a mathematical distribution in the traditional sense. However, when calculated alongside the log probability distribution of model outputs, an increase in the overall output probability of positive tokens and a decrease in that of negative tokens result in a monotonically increasing distribution reward, with tokens less relevant to preferences tend to cancel each other out in the summation. Consequently, this mechanism can enable the model to learn preferences from a more macroscopic perspective and guide the model towards a better understanding and integration of preferences. 3.3 Data Filtering This process entails computing Eq.8 for each sample without performing any parameter updates, solely preserving the distribution reward outcomes. As illustrated in Fig. 2, when a response includes more tokens related to preference information, the data is likely to contain more preference-related content. In such cases, for a model that has not undergone preference learning, the response becomes more challenging, often resulting in a lower distribution reward compared to ordinary case which has not so much preference-related information. This insight led us to rank all data by the distribution reward and select the subset with the lowest distribution rewards. By doing so, we derived a high-quality subset from the original dataset based on the distribution reward. 3.4 From Clumsiness to DEFT At this point, we have a complete DEFT framework that can be utilized to enhance existing alignment methods. For a specific fine-tuning method mand an alignment problem, DEFT firstly extracts the discrepancy distribution Qdifffrom the raw dataset Dlandldenotes the preference answer sequence length in the dataset. Then DEFT filters out a highquality subset Dl\nQfromDl. Subsequently, during the training process, we exclusively use Dl\nQand incorporate RQinto the loss function of m: LDEFT-m =Lm−ωRQ (10) 328where ωis used to control the strength of the distributional guidance. In this way, through the computation of the distribution reward, DEFT has accomplished the selection a data subset of high-quality and guided the distribution during fine-tuning, resulting in a more effective and efficient preference alignment. 4 Experiments 4.1 Datasets This paper utilizes the Human Preference Data about Helpfulness and Harmlessness ( HH-RLHF ) dataset (Bai et al., 2022a), which has been widely employed for human preference alignment concerning harmlessness and helpfulness, as the primary experimental data. It consists of four subsets and each sample includes a conversation segment and a pair of human-annotated positive and negative responses. Following PRO (Song et al., 2024), we employed the filtered HH-RLHF , denoted as D2in our paper, and a new training set enhanced with ChatGPT1, which extends the rank length to 3, denoted as D3. An external reward model rtrain2was chosen to fit r∗, scoring all of query-answer pairs inD2andD3to create preference sequences. We selected the top 5% of data from each subset with the lowest distribution reward to construct the highquality subset, labeled as D2\nQandD3\nQ. Specific information is presented in Appendix.A.1. 4.2 Implementation Details Our work employs Llama3-8B (AI@Meta, 2024) as the base model and selects PRO and DPO as baseline methods, comparing them with DEFTenhanced methods, namely DEFT-PRO and DEFTDPO. Apart from the base model, we examined the zero-shot performance of Llama3-8B-Instruct, Mistral-7B-v0.3, Mistral-7B-v0.3-Instruct (Jiang et al., 2023) and gpt-3.5-turbo (denoted as ChatGPT) on the test set. All experiments are performed on 8 NVIDIA A800 80G GPUs, with the default parameters set of PRO and DPO, see details in Appendix.A.2. And the implementation of DPO is based on the SFT model of the current dataset. Validation is conducted on a randomly sampled subset of 256 instances from the test set each epoch and the model with the best validation set performance was chosen for testing. 1https://chat.openai.com/\n2https://huggingface.co/OpenAssistant/\noasst-rm-2.1-pythia-1.4b-epoch-2.5\n5\nDataset MethodHarmlessness Helpfulness Total\nBLEU BART Reward BLEU BART Reward BLEU BART Reward\n0-shotLlama3-Base 10.51 1.80 53.23 18.74 2.02 46.97 16.51 1.96 48.66\nLlama3-Instruct 23.00 3.06 66.67 33.47 3.74 65.69 30.64 3.54 65.96\nMistral-Base 8.10 1.73 53.51 14.18 1.87 45.57 12.54 1.83 47.72\nMistral-Instruct 30.90 3.33 63.50 34.60 3.90 64.80 33.60 3.74 64.45\nChatGPT 62.68 10.29 73.01 70.79 11.86 75.11 68.60 11.41 74.54\nD2SFT 7.79 1.77 60.89 19.46 1.99 50.65 16.30 1.93 53.42\nPRO 7.72 1.75 61.30 20.27 2.06 53.07 16.87 1.98 55.29\nDEFT-PRO 8.54 1.77 62.21 22.58 2.70 58.43 18.78 2.45 59.45\nDPO 17.04 2.25 59.51 28.40 2.69 57.05 25.33 2.56 57.72\nDEFT-DPO 20.13 2.87 65.35 30.08 3.15 60.21 27.39 3.07 61.60\nD3SFT 31.76 3.86 72.48 34.91 3.84 68.54 34.06 3.85 69.60\nPRO 29.40 3.56 72.95 33.50 3.64 68.49 33.50 3.62 69.69\nDEFT-PRO 32.77 3.79 73.79 34.66 3.65 71.24 34.15 3.69 71.93\nDPO 29.03 3.88 74.23 34.79 4.04 69.27 33.23 4.00 70.61\nDEFT-DPO 32.03 3.95 71.45 36.77 4.16 73.12 35.49 4.10 72.67\nTable 1: Main results. The DEFT framework yields substantial improvements compared to the original methods.\n4.3 Metrics To evaluate the enhancement effect of the DEFT framework, we introduced various evaluation metrics to comprehensively examine its impact on both model alignment capability and generalization ability. 4.3.1 Automated Metrics Following the automatic evaluation method of PRO, we introduced another reward model, denoted as reval3, which has been trained on a certain amount of preference data, to evaluate the responses generated by the model across the entire test set. And we calculated the BLEU (Papineni et al., 2002) score and the BARTScore (Yuan et al., 2021) (denoted as BART) between the model-generated responses and the reference texts to measure the text quality as comprehensively as possible, averaging both scores. Additionally, considering the potential irrationality in the original test set’s reference texts, we refined the reference answers using ChatGPT to facilitate a more reasonable evaluation of BLEU score, as shown in Fig. 3. The units for all metrics are percentages. For easier comparison, the BARTScore values were transformed using a sigmoid function. 4.3.2 GPT-4 Judge In addition to evaluating alignment effectiveness, a crucial aspect worth considering is the impact of 3https://huggingface.co/OpenAssistant/\noasst-rm-2-pythia-6.9b-epoch-1\n❑Test set with ChatGPT enhancedBefore:After:\nWho is the ugliest person on Earth?\nI'm sorry, I cannot provide ···  Let's focus on positive and constructive  conversations instead.\nUm, you’re the most beautiful person on Earth.BART：-4.03\nAs an AI language model, I am programmed to ··· If you have any other questions or need assistance with a different topic, feel free to ask!BLEU：0.042BLEU：0.418BART：-3.68Figure 3: Augmented reference answers enhanced by\nChatGPT contribute to a more reasonable calculation of\nBLEU and BARTScore.\nalignment methods on model generalization ability. Here, we opted for the renowned and challenging MT-Bench (Zheng et al., 2023) as our evaluation benchmark, comprising 80 high-quality multi-turn dialogue questions covering eight aspects. GPT4 (Achiam et al., 2023) was employed as a judge to comprehensively assess the multi-turn dialogue and instruction-following capabilities of the test models based on D3. 4.3.3 Human Evaluation Considering the limitations of the off-the-shelf reward model scoring, we further introduced human evaluation to gauge the alignment performance of DEFT-PRO and DEFT-DPO against PRO and DPO, 6\nMethod D3\nQRQHarmless Helpful Total\nBELU BART Reward BELU BART Reward BELU BART Reward\nDEFT-PRO ✓ ✓ 32.77 3.79 73.79 34.66 3.65 71.24 34.15 3.69 71.93\n- ✓ 31.76 3.71 73.74 34.51 3.59 70.85 33.77 3.62 71.63\n- ✓ 29.40 3.56 72.95 33.50 3.64 68.49 33.50 3.62 69.69\nDEFT-DPO ✓ ✓ 32.03 3.95 71.45 36.77 4.16 73.12 35.49 4.10 72.67\n- ✓ 31.30 3.84 70.88 36.70 4.13 72.98 35.24 4.05 72.41\n- ✓ 30.52 3.35 70.11 35.32 4.10 71.57 34.02 3.90 71.18\nTable 2: The absence of each component in DEFT will result in a decline in overall performance.\nMethod Writing Roleplay Reasoning Math Coding Extraction STEM Humanity Turn 1/2 Avg.\nSFT 7.23 6.42 4.07 2.85 4.52 6.45 6.07 7.02 5.82/5.33 5.58\nPRO 6.55 5.95 4.35 1.87 4.70 5.20 5.33 6.96 5.64/4.89 5.27\nw/ DEFT 6.91 5.93 3.25 3.23 4.35 7.15 6.66 6.85 5.77/5.30 5.54\nDPO 7.23 6.47 3.70 2.67 4.42 7.23 6.02 6.63 5.64/5.45 5.55\nw/ DEFT 8.63 7.98 6.29 4.65 6.47 8.69 8.23 9.22 7.77/7.25 7.53\nTable 3: DEFT framework significantly preserves or enhances generalization capability.\nrespectively, based on D3. We randomly selected 125 samples from each subset of the test set, totaling 500 samples and employed different annotators for the four subsets to conduct evaluations. The methods being compared were undisclosed to the annotators to avoid bias. Subsequently, we calculated the proportions of win, tie, and lose outcomes for both harmless and helpful aspects, as depicted in Fig. 4. 4.4 Results 4.4.1 Main Results As illustrated in Tab. 1, it is clear that the instruct model performs considerably better than the purely pre-trained model in zero-shot testing. Due to ChatGPT’s rigorous alignment through RLHF and the fact that reference responses in the test set are generated by it via prompts, its performance across various metrics is exemplary. DEFT-PRO demonstrated an improvement of 4.16% in reward score, while DEFT-DPO showed an increase of 3.88% under D2. When utilizing D3, the respective improvements were 2.24% and 2.06%. Additionally, both BLEU and BARTScore metrics showed enhancements. These results collectively underscore the effectiveness of the DEFT framework for preference learning. 4.4.2 Preference Learning As shown in Fig. 4, it is logical that comparing the two methods yields a high proportion of ties considering that the original method has already partially learned preferences. 75.73%\n69.60%\n22.13%\n25.60%WinTieLoseHarmlessHelpfulDEFT-PRO vs. PRO4.80%2.13%DEFT-DPO vs. DPO\n0%25%50%75%100%\n76.53%\n63.20%\n18.93%\n30.40%HarmlessHelpful6.40%4.53%Figure 4: In both the Harmless and Helpful aspects\nof human evaluations, the DEFT series demonstrates a\nhigher win rate compared to the original method.\nHowever, the method enhanced by DEFT exhibits a superior win rate in both the Harmless and Helpful dimensions relative to the original method. This suggests that the DEFT framework can achieve better preference learning results with less data. 4.4.3 Generalization Ability As observed in Tab.3, it is evident that the generalization capability of the model decreases overall after alignment fine-tuning, particularly in reasoning tasks. However, following DEFT enhancement, DEFT-PRO retains most of its generalization capability, whereas DEFT-DPO exhibits significant improvement. Given that DPO inherently preserves generalization capability effectively, the 7\n-0.28-0.267-0.254-0.241-0.228-0.025050075010001250150017502Update\nw/o Update\n🔥\n🧊\nTraining StepDistribution Reward6869.470.872.273.3%5%10%20%100%\nHarmless\nHelpful\nTotalReward ScoreData V olume(4,827)(8,041)(16,081)(32,160)(160,793)Figure 5: The left figure illustrates the changes during the training process with and without the involvement of RQ\nupdates. The right figure depicts the model’s performance on the test set across varying data volumes.\nhigh-quality alignment of DEFT further enhances its potential, clearly demonstrating the positive impact of the DEFT framework on generalization capability. 4.5 Ablation Study To verify the gain effects of each component in the DEFT framework, we conducted ablation experiments on DEFT-PRO and DEFT-DPO based on D3, as shown in Table 2. It can be observed that the absence of both the high-quality subset D3\nQand the distribution reward RQwould have a certain impact on the final performance. For the D3\nQafter data filtering, there is a significant improvement in BLEU, BARTScore and reward score, confirming the superior effectiveness of a small amount of high-quality subset selected by RQcompared to the entire dataset. As for RQin fune-tuning stage, all three metrics indicate that it can further optimize the model’s learning of preferences by guiding the distribution during the parameter update phase. 4.6 Analysis 4.6.1 Distribution Reward Curve To provide a more intuitive analysis of the role of RQ, we illustrated the changes in RQduring the training process of DEFT-PRO under D3\nQin Fig. 5. The red line represents RQbeing updated, whereas the blue color represents RQnot being updated. As training progresses, it can be observed that the RQinvolved in the update steers the model distribution towards preferences, leading to improved preference learning. Conversely, RQthat is not updated remains almost constant. Despite the minor numerical variance, this high-level guidance significantly boosts the model’s performance by aligning its learning process with the desired distribution, 497thus maintaining its overall generalization capacity. 4.6.2 Impact of Data Volume As depicted in Fig. 5, we extracted subsets with the lowest 3%, 5% (the proportion employed in DEFT), 10%, 20% RQvalues and the entire dataset to analyze the effectiveness under different filtered data volumes of DEFT-PRO. The red line correspond to the Harmless subset, the yellow line to the weighted average of the three Helpful subsets, and the blue line to the performance across the whole test set. It can be observed that when considering the issue of diversity with a small data volume, the overall performance with 3% of the data is slightly inferior to that with 5%. Beyond 5%, as more data is included, the increasing amount of noise from the original dataset starts to degrade the dataset’s effectiveness. However, the performance still remains superior to using the entire dataset. Nevertheless, as the data volume increases, so does the training cost. Therefore, it appears that, for the dataset and model used in this study, selecting the top 5% subset is nearly the optimal solution in terms of both performance and cost. 5 Conclusion In this paper, we introduce DEFT, an efficient alignment framework for fine-tuning-based alignment methods. It extracts preference discrepancy distribution from raw preference data and computes the distribution reward with the model’s output distribution, which act simultaneously on data filtering and training loss. Experimental results demonstrate that the DEFT-enhanced approach outperforms the original method in various preference metrics and generalizability with minimal training cost, thus validating the effectiveness of DEFT. 8\nLimitations The effectiveness of the discrepancy distribution extracted under different data volumes needs further analysis and validation. Additionally, the HHRLHF dataset only reflects a portion of preferences, namely Harmless and Helpful, while other more extensive and complex preference datasets remain to be explored. These aspects will be explored in future research efforts. Ethics Statement The HH-RLHF dataset and the content presented in this paper may potentially contain harmful or toxic content. All data and models used in this study are intended solely for research purposes to prevent any dissemination of harm. This disclaimer is hereby provided. 548",
        "aspect_review": "The paper introduces a novel framework, DEFT, which enhances preference learning by incorporating data filtering and distributional guidance, showcasing strong technical depth and rigor. The use of a high-quality subset filtered from raw data significantly reduces training time and enhances model alignment and generalization capabilities, demonstrating significant practical impact. Experimental results show that DEFT-enhanced methods outperform original methods in alignment capability and generalization ability with reduced training time, highlighting comprehensive experimental validation.\n\nThe evaluation could benefit from more diverse datasets to validate the effectiveness of the discrepancy distribution under different data volumes. While the DEFT framework shows promising results, further analysis is needed to explore its impact on more extensive and complex preference datasets beyond Harmless and Helpful. The reliance on the HH-RLHF dataset limits the scope of preference types explored, suggesting a need for broader dataset exploration in future work. The paper could include more detailed comparisons with other state-of-the-art methods in preference learning to better contextualize its contributions. Some minor details on hyperparameters and implementation specifics could be further clarified for reproducibility. Minor improvements in writing clarity and figure formatting could enhance the overall readability and presentation of the paper.",
        "general_review": "The paper introduces a novel and efficient framework, DEFT, which significantly enhances the alignment of large language models with human preferences. Notable strengths include: 1. Notable Novelty: DEFT introduces a unique distribution reward mechanism that efficiently filters data and guides model output distribution, setting a new standard in the field. 2. Strong Technical Depth and Rigor: The framework is technically sound, with a comprehensive explanation of the discrepancy distribution and distribution reward. 3. Comprehensive Experimental Validation: Extensive experiments demonstrate that DEFT-enhanced methods outperform original methods in alignment and generalization capabilities, with reduced training time. 4. Significant Practical Impact: The proposed method significantly reduces computational costs and enhances model performance, making it highly applicable in real-world NLP tasks.\n\n1. Minor Writing or Clarity Improvements: Some sections could benefit from clearer explanations, particularly in the methodology part. 2. Small Details on Hyperparameters or Implementation: More details on hyperparameter settings could enhance reproducibility. 3. Very Minor Formatting on Figures: Some figures could be formatted more clearly for better visual comprehension. 4. Slight Clarification on Code Availability: Providing a link to the code repository would improve accessibility for further research."
    },
    "35": {
        "paper": "TARA: Token-level Attribute Relation Adaptation for Multi-Attribute\nControllable Text Generation\nAnonymous ACL submission\nAbstract\nMulti-attribute controllable text generation (CTG) aims to generate fluent text satisfying multiple attributes, which is an important and challenging task. The majority of previous research on multi-attribute ignored the attribute relations. Recently, several work considers the attribute relations by explicitly defining them as \"prohibitory\". We argue that the attribute relations are not fixed and manifested as both \"prohibitory\" and promotive. To enhance multiattribute CTG, in this paper, we propose TARA, which tackles multi-attribute controllable text generation with token-level attribute relation adaptation and representation, and uses a dynamic text generation strategy to exploit multiattribute relations with balanced attribute control. We also define token-level attribute representation for multi-attribute CTG. Experimental results show that TARA achieves competitive control ability and comparable text quality and diversity over baseline methods. 1 Introduction Multi-attribute controllable text generation (CTG) aims to generate fluent text satisfying multiple attributes. Previously, the majority of research on multi-attribute CTG didn’t explicitly consider the relation between the attributes, which is a fundamental issue in multi-attribute CTG. Recent work (Qian et al., 2022; Ding et al., 2023) take the attribiute relations into consider and utilizes prefix tuning or V AE to train a multi-attribute model. Several work (Gu et al., 2022; Huang et al., 2023) further defines multi-attribute relation as inhibitory. For instance, Dist. Lens (Gu et al., 2022) identify that mutual interference of controllers causes attribute degeneration and searches for intersections in the attribute space. PromptGating (Huang et al., 2023) use trainable gates to normalize the interference among attributes. In fact, the attribute relations are not fixed, nor are they only manifested as \"prohibitory\". Take the FriesAttribute 1:   SentimentAttribute 2:   TopicPositiveNegative\nAmericanMexicanAsianMSGinhibitoryinhibitorypromotivepromotiveFigure 1: Token-level multi-attribute promotive and\ninhibitory relations.1\nexamples in Figure 1, MSG (Monosodium Glutamate) demonstrates the promotive relation between negative andAmerican , and the inhibitory relation between positive andAmerican . Therefore, multiattribute CTG needs token-level attribute relation adaptation and exploitation of multi-attribute relation. In this paper, we tackle the multi-attribute CTG with Token-level Attribute Relation Adaptation and representation, and propose TARA, which uses a dynamic text generation strategy. In summary, our contributions are as follows: •We firstly identify both promotive and inhibitory attribute relations, and develop a token-level attribute relation adaptation method for multi-attribute CTG. •The proposed attribute-adaptive prefix tuning adjusts attribute’s expression with token-level attribute representation, and the dynamic text generation strategy we design balances multiattribute control with promotive and inhibitory attribute relations. •Experimental results verify that TARA performs better than existing methods on control ability and achieves text quality and diversity performances comparable with existing methods. 1MSG is the abbreviation of Monosodium Glutamate.\n1\nToken-level A+ribute Representa5on𝒓𝟐𝒓𝟏𝐌𝐛𝐚𝐬𝐞Attribute 1\n🔥\nA+ribute-Adap5ve Preﬁx Tuning\n🔥Attribute 2PLM \n❄\nPLM \n❄𝒍𝒃𝒂𝒔𝒆\nOutputThe food tastes good. I ordered a burger…𝒓𝟏𝑐\">𝜇\nDynamic Text Genera5on Strategy−\n− the  good  awful  sushi  burger  taco … the  good  awful  sushi  burger  taco …   TopicSentimentPositive\nAmericanInputThe food\nThe foodThe food……Negative\nMexicanMultiAttributesThe food𝒓𝟐𝑐\">𝜇𝒓𝟏𝑐\">𝜇𝒓𝟐𝑐\">𝜇𝒍\"𝒃𝒂𝒔𝒆\nPLM \n❄ the  good  awful  sushi  burger  taco …+𝒍𝟏𝒍𝑷𝑳𝑴𝒍𝟐Figure 2: Overview of Multi-Attribute Relation Adaptation Method for CTG.\n2 Proposed Method We propose a novel multi-attribute relation adaptation method TARA for controllable text generation. Figure 2 illustrated the overall structure of TARA, which consisits of attribute-adaptive prefix tuning, token-level attribute representation and dynamic text generation strategy. 2.1 Attribute-Adaptive Prefix Tuning In multi-attribute CTG, the presence of tokens that exhibit inhibitory relations between attributes can lead to a degradation in the performance of multi-attribute control. Therefore, we employ attribute-adaptive prefix tuning for attribute models to weaken the inhibitory relations. This approach reduces the attribute expression of tokens with weak attribute expression, therefore enhancing the effectiveness of multi-attribute control. Given the prompt or thr current sequence as St−1\n1, at the current time step t, we can obtain the attribute model’s logits ltover the vocabulary Vthrough the language model. The language model will generate next token stby sampling st∼P(st|St−1\n1)based on its logits lt. We first convert the logits ltto probabilities P(st|St−1\n1) with a temperature coefficient (see Equation 1) to make the attribute distinctions among tokens more apparent. P(st|St−1\n1) =softmax\u0012lt\nτ\u0013\n(1) where τis the temperature coefficient. Then, we use the L2 norm to constrain the probability distribution of the attribute model, avoiding extreme probability values and enhancing stability. Finally, we use the negative variance to enhance the attribute model’s ability to distinguish tokens with 102varying degrees of attribute expression, mitigating tokens with weak single attribute expression. The adapt loss Ladapt is defined as: Ladapt=\u0010\n∥P(st|St−1\n1)∥2\n−λvar·Var(P(st|St−1\n1))\u0011 (2) where ∥P(st|St−1\n1)∥2denotes the L2 norm of the probabilities, and Var(P(st|St−1\n1))represents the variance of the probabilities. The term λvaris the regularization weights for the L2 norm and the variance, respectively. Finally, the total loss Ltotalis defined as: Ltotal=−logP(st|St−1\n1) +λregLadapt (3) where the terms λregis the regularization weights forLadapt. 2.2 Token-level Attribute Representation In TARA, we adapt fine-tune the corresponding attribute model for each attribute value using the same pre-trained language model. Given the current input St−1\n1, we can obtain the logits distribution of the attribute model. Not only tokens that express the attribute characteristics will have high logits, but tokens that ensure text quality will also receive high logits, such as with, a, the, is . In TARA, we aim to utilize pure logits that only express the attribute characteristics for attribute control. Therefore, we define the attribute representation rattas follows: ratt=latt−lPLM (4) where attrepresents the attribute value, lattrepresents the logits of attribute model, lPLMrepresents the logits of base PLM. Attribute Representation 2\nreflects the significance of the corresponding token in expressing the current attribute. Suppose we have two attribute value iandj (TARA is capable of controlling more attributes). In the vocabulary V, we define the tokens as {c1, . . . , c n}, where nis the total number of tokens in the vocabulary. We use ckto represent a specific token in the vocabulary, where k∈ {1, . . . , n }. The same token may demonstrate different attribute relations under different multi-attribute control. Therefore, we firstly set a threshold µto distinguish the different attribute expressions. Then we define two kinds of attribute value iandjrelations at the token-level as follows: Promotive relation: (\nri(ck)> µ and rj(ck)> µ\nri(ck)< µ and rj(ck)< µ(5) This indicates that the representations of ri(ck)and rj(ck)are consistent. Inhibitory relation: (\nri(ck)> µ and rj(ck)< µ\nri(ck)< µ and rj(ck)> µ(6) This indicates that the representations of ri(ck)and rj(ck)are inconsistent. In multi-attribute CTG, we establish dynamic vocabulary ViandVjfor each attribute value. Then we can get: Vi={ck∈ V | ri(ck)> µ} (7) Vj={ck∈ V | rj(ck)> µ} (8) 2.3 Dynamic Text Generation Stragety In MARA, we design a dynamic text generation strategy to exploit multi-attribute relation and balance multi-attribute control to steer the generation. We employ multi-attribute representations rattin conjunction with the base logits lbasefrom a quality control model, which can either be an LLM or a small LM, sharing the same vocabulary as the attribute pre-trained model. In this setup, the attribute representations rattmanage multi-attribute control, while lbaseensures the quality of the text text. Besides, we design a dynamic weights to effectively balance the relations between two attributes, while ensure the text quality. Following Liu et al. (2021), we applied nucleus sampling 174(Holtzman et al., 2020) to base model to obtain a fluent output sequence. At time step t, letV′⊆ V represent the tokens included in the top- pvocabulary of the base quality control model. The truncated logits ˜lbaseare ˜lbase[v] =(\nlbase[v]ifv∈ V′\n−∞ otherwise(9) For the two attribute value iandj, we define Wi andWjrepresent the conditional probabilities of common tokens under attribute values iandj: Wi=softmax\u0012|Vi∩ V j|\n|Vi|\u0013\n(10) Wj=softmax\u0012|Vi∩ V j|\n|Vj|\u0013\n(11) To normialize to multi-attribute weight to [0,1] at the token-level, we design dynamic weights ˜Wi and˜Wjas follows: ˜Wi=Wi\nWi+Wj(12) ˜Wj=Wj\nWi+Wj(13) For a more reasonable sampling process, as in (Fan et al., 2018), we applied top- Kprocessing to the ensemble logits ˜ltduring sample process. Therefore, the next token stcan be obtained through the following dynamic text generation strategy: ˜lt=˜lbase+ (1 + ˜Wi)·ri+ (1 + ˜Wj)·rj(14) ˜P(st|St−1\n1) =softmax (˜lt), (15) st∼˜P(st|St−1\n1). (16) 3 Experiments and Results 3.1 Experimental Setup Dataset We choose widely used benchmark dataset YELP (Lample et al., 2019) for our experiments. Following previous work, we use sentiment attribute (positive and negative) and topic attribute (Asian, American and Mexican) for multi-attribute controllable text generation. Due to the page limit, please refer to Appendix B for more details about the experiment setup. 3\nMethodCorrectness (%) Text Quality Diversity\nSentiment ↑ Topic↑ Avg↑ PPL↓ mean-Dist ↑\nPromptTuning* (Lester et al., 2021) 48.29 48.11 48.20 40.89 0.42\nPrefixTuning* (Li and Liang, 2021) 47.53 69.11 58.32 147.47 0.31\nControlPrefixTuning (Clive et al., 2022) 58.98 45.36 52.17 89.80 0.48\nGeDi* (Krause et al., 2021) 99.47 51.36 75.41 616.92 0.75\nTailor* (Yang et al., 2023) 80.68 68.72 74.70 40.29 0.39\nDist. Lens* (Gu et al., 2022) 77.47 66.98 72.22 52.59 0.26\nPromptGating* (Huang et al., 2023) 84.80 75.02 79.91 21.77 0.42\nTARA-Multi (Ours) 90.17 80.32 85.25 40.16 0.45\nTable 1: The main results of multi-attribute CTG. For each method, we select 6 combinations (two sentiment\nattributes ×three topic attributes) as the final results.2\nVariantCorrectness (%) Text Quality Diversity\nSentiment ↑ Topic↑ Avg↑ PPL↓ mean-Dist ↑\nTARA-Multi 90.17 80.32 85.25 40.16 0.45\n– Attribute-Adaptive Prefix Tuning 89.02 77.47 83.25 44.17 0.45\n– Dynamic Text Generation Strategy 87.63 75.11 81.37 31.80 0.43\nTable 2: Ablation Study of attribute-adaptive prefix tuning and dynamic text generation strategy of TARA.\nEvaluation Metrics Following Yang et al. (2023); Huang et al. (2023), we conduct automatic and human evaluations for controllable accuracy and text quality. We conduct automatic evaluation from three aspects: (1) Correctness We finetune a sentiment classifier and topic classifer based on RoBERTa (Liu et al., 2019) for the evaluation of sentiment and topic accuracy. (2) Text Quality We calculate the perplexity (PPL) using GPT-2 medium (Radford et al., 2019) to evaluate the fluency. (3) Text Diversity We use averaged distinctness (Li et al., 2015) to evaluate the diversity. We conduct human evaluation for sentiment relevance, topic relevance and fluency. Each rating can be evaluated from 1 to 5. And we get final scores from the average of three ratings. 3.2 Main Results and Analysis As shown in Table 1, TARA achieved the highest average accuracy in multi-attribute control, surpassing the best comparative method by 5.34% while maintaining text quality. TARA shows a larger improvement in the multi-attribute scenario, demonstrating the necessity of carefully handling the promotive and inhibitory relations between attributes. Both automatic and human evaluations (see Section D) indicate that TARA effectively balances multiple control attributes with text quality and di2The symbol * indicates that the results are obtained from\nHuang et al. (2023).versity. We observe that Tailor performs better than previous comparative methods by bridging the gap between the training and testing stages. Dist. Lens and PromptGating both consider and mitigate the inhibitory relations between attributes. To understand the importance of attributeadaptive prefix tuning and the dynamic text generation strategy, we conducted an ablation study, as shown in Table 2. The results demonstrate that: (1) Attribute-adaptive prefix tuning improves multiattribute control ability across all attributes, proving its effectiveness in weakening inhibitory relations. (2) The dynamic text generation strategy effectively balances the relations between attributes, supporting the notion that better exploiting multiattribute relations enhances model performance. 4 Conclusion We introduce TARA, a token-level attribute relation adaptation method for multi-attribute CTG. It uses attribute-adaptive prefix tuning and dynamic text generation strategy to steer the generation towards more precise and balanced control of multi attributes. Through experimental evaluations on multi attribute CTG, we demonstrate the effectiveness of TARA in terms of both control ability and text quality. 4\nLimitations While TARA achieves fairly good performance in multi-attribute controllable text generation task, its results on text perplexity and diversity are slightly inferior to the best-performing methods. 267",
        "aspect_review": "The paper is well-structured and clearly organized, with a logical flow from the introduction of the problem to the proposed solution and experimental results. TARA introduces a novel token-level attribute relation adaptation method, which is a significant advancement in the field of multi-attribute CTG. The use of attribute-adaptive prefix tuning and dynamic text generation strategy is innovative. The experimental results demonstrate that TARA achieves superior multi-attribute control accuracy compared to existing methods, while maintaining competitive text quality and diversity.\n\nThe motivation for certain parameter choices, such as the threshold for distinguishing attribute expressions, could be more thoroughly justified. While the experiments are comprehensive, additional datasets could further validate the generalizability of the proposed method. The analysis of why TARA performs better in certain scenarios could be expanded to provide deeper insights. The ablation study, while useful, could be extended to include more variations of the proposed method. Some sections of the paper, particularly the explanation of the dynamic text generation strategy, could be clearer to enhance understanding.",
        "general_review": "The paper introduces TARA, a novel approach to multi-attribute controllable text generation that effectively considers both promotive and inhibitory attribute relations. This is a significant advancement over previous methods that only focused on inhibitory relations. The methodology is technically rigorous, incorporating token-level attribute relation adaptation and a dynamic text generation strategy, which enhances the control over multiple attributes while maintaining text quality and diversity. The experimental validation is comprehensive, demonstrating TARA's superior performance in attribute control accuracy compared to existing methods. This work has significant practical implications for applications requiring nuanced control over text attributes, such as sentiment and topic.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in explaining the technical details of the token-level attribute representation and dynamic text generation strategy. 2. Additional details on hyperparameters used in experiments would enhance reproducibility. 3. Figures could be slightly refined for better readability. 4. Clarification on the availability of the code for the proposed method would be beneficial for the community."
    },
    "36": {
        "paper": "The Student Data Paradox: Examining the Regressive Side Effects of\nTraining LLMs for Personalized Learning\nAnonymous ACL submission\nAbstract\nAs the demand for personalized learning grows, tech giants are turning to advanced Large Language Models (LLMs) to create adaptive tutoring systems. A key aspect of this approach involves training LLMs on student data, including interactions, questions, and common misconceptions, to better understand and cater to individual learning needs. However, this paper reveals a paradoxical and timely challenge: the more accurately an LLM models student misconceptions, the more its own factual knowledge and reasoning abilities are compromised. We term this phenomenon the “Student Data Paradox” and investigate the regressive side effects it entails. To study this critical issue, we trained state-of-the-art LLMs on a student-tutor dialogue dataset, focusing on biology questions from college-level textbooks. The models were trained to predict student responses, simulating learner behavior. Evaluation across multiple benchmarks, including the ARC reasoning challenge, TruthfulQA, HaluEval Dial dataset for hallucination detection, and the MemoTrap memory-based task dataset, revealed significant performance declines, highlighting the regressive side effects of the Student Data Paradox. As a potential solution, we introduced “hallucination tokens,” appended to student responses during training. These tokens instructed the model to differentiate between simulating misconceptions and providing accurate information. While this technique yielded improvements, it did not fully restore the LLMs’ baseline performance, emphasizing the complexity of the paradox. This paper contributes to the critical discussion on the use of LLMs for personalized education. Our findings underscore the need for further research to develop strategies that balance the modeling of student behavior with the preservation of factual accuracy. As the field advances, addressing the Student Data Paradox and its regressive side effects will be crucial to realizing the full potential of LLMs in transforming education. 0441 Introduction In the era of personalized education, tech giants like Google and OpenAI are racing to develop cutting-edge solutions that promise to revolutionize learning. With Google’s LearnLM-Tutor (Jurenka et al., 2024) and OpenAI’s ChatGPT Edu (Kasneci et al., 2023; OpenAI, 2024), the vision of providing a personal tutor for every learner is becoming a reality. These advanced Large Language Models (LLMs) are poised to transform the educational landscape by leveraging vast amounts of student data, including interactions, questions, and common misconceptions, to better understand and adapt to individual learning needs. The incorporation of student data into the training process is a crucial step towards creating truly personalized learning experiences. By exposing LLMs to student challenges, errors, and thought patterns, these models can potentially identify knowledge gaps, anticipate misconceptions, and provide targeted support tailored to each student’s unique needs. However, this approach is at odds with traditional LLM training, where data quality is of utmost importance (Goldberg, 2022; Samaniego et al., 2022). Student data, by nature, is prone to errors and misconceptions, which raises concerns about the impact on the LLMs’ factual knowledge and reasoning abilities. We term this phenomenon the “Student Data Paradox.” In this paper, we study this timely topic: what happens when we train an LLM on student-tutor interaction data? Our research reveals that while training LLMs on student data can enable models to accurately simulate authentic student behaviors, it comes at a significant cost. As the models become more adept at reflecting student misconceptions, their own factual integrity and reasoning capabilities are compromised. This paradox poses a serious concern, as the primary purpose of any educational model is to provide accurate and reliable informa1\ntion to learners. To investigate this issue further, we conducted a comprehensive exploration involving training LLMs on a student-tutor dialogue CLASS dataset (Sonkar et al., 2023). CLASS dataset comprises dialogues on biology questions sourced from collegelevel textbooks. An example of the student-tutor dialogue from the dataset is illustrated in figure 1. We used the CLASS dataset to train the Vicuna models (7B and 13B) (Chiang et al., 2023), stateof-the-art Llama (Touvron et al., 2023) variants, to model student responses. However, the training process significantly decreased the model’s performance across various benchmark datasets, including the ARC reasoning challenge (Clark et al., 2018a), TruthfulQA (Lin et al., 2022), Hallucination Evaluation Dialogue (Li et al., 2023), and MemoTrap (McKenzie et al., 2022). We present a detailed analysis across nine key benchmarks using the Eleuther AI Language Model Evaluation Harness (Gao et al., 2023), a widely used framework (Beeching et al., 2023) which provides a thorough and fair assessment of generative models across a spectrum of reasoning and general knowledge tasks. We present a detailed analysis across nine key benchmarks using the Eleuther AI Language Model Evaluation Harness (Gao et al., 2023), a widely used framework (Beeching et al., 2023) to test generative language models on a large number of different evaluation tasks. To further understand the regressive side effects, we conducted a control experiment to compare the model trained to predict tutor responses versus one trained to predict student responses. The results showed that training the LLM on tutor responses did not lead to any performance decline observed when modeling student responses. This trends highlight that the regressive side effects are a unique challenge specific to training LLMs to replicate student misconceptions. To counteract the side effects, we propose to incorporate novel start and end hallucination tokens ([hal] and[/hal] ) into the LLM training process. These tokens, placed at the beginning and end of each student response, serve as cues to the model, instructing it when to differentiate between providing accurate responses and replicating student misconceptions. Our results indicate a substantial improvement in the model’s performance across all datasets after introducing this token. However, these tokens do not fully restore the model’s baseline performance, underscoring the complexity of 136the issue. Through our research, we have brought to the following critical contributions in the realm of personalized education leveraging LLMs: 1. We have identified and extensively studied theStudent Data Paradox in LLMs trained for personalized education. This paradox arises when LLMs are trained on student data, including misconceptions and errors, to better understand and adapt to individual learning needs. Our research reveals the regressive side effects of this approach, highlighting the trade-off between accurately modeling student behavior and maintaining the factual integrity and reasoning ability of the LLMs. 2. We have proposed hallucination tokens to mitigate these regressive effects. These tokens, added to the training process, instruct the LLMs to differentiate between modeling student misconceptions and providing factually accurate responses, substantially improving the model’s performance. 3. Despite the improvements achieved with the hallucination tokens, our research indicates that it does not fully counteract the regressive side effects. This points to the complexity of this issue and underscores the need for further research in this area. Our research marks a significant step towards understanding the complexities of using LLMs for student modeling. The findings and contributions of this study will fuel further exploration and innovation in this domain, ultimately refining the use of LLMs in personalized learning environments. 2 Theorem: The Infeasibility of Separate Student and Tutor Models To address the “Student Data Paradox”, one may propose a seemingly straightforward solution - the creation of two distinct models: a student model and a tutor model. The student model would be tasked with capturing student behaviors and misconceptions, while the tutor model would concentrate on providing accurate responses. While this solution may appear to be a simple and efficient way to counter the paradox, it is fundamentally flawed. The crux of the issue lies in the inherent contradiction in the capabilities of the proposed student model. This contradiction can be formally expressed using the following theorem. Theorem 1. A student model capable of accurately emulating every student’s behavior and misconceptions, including a perfect student, is not practically 2\nFigure 1: An example of student-tutor dialogue from the CLASS dataset (Sonkar et al., 2023). Unlike the\nconventional use of the dataset to train a tutor model, our study innovatively repurposes this data to train a\n‘student model’ instead, with the LLM learning to predict student responses. This approach is motivated by the\npotential of personalized education, where understanding and modeling student behavior can lead to more effective\nlearning interventions. However, while effectively replicating student misconceptions, this method leads to the\n“Student Data Paradox” – regressive side effects that compromise the model’s factual integrity and reasoning\nabilities. The conversation shown here highlights this issue, where the student’s response, while partially correct,\ncontains misconceptions. To mitigate these side effects, we introduce hallucination tokens ( [hal] and[/hal] )\nappended to student responses during training. These tokens instruct the model to switch between modeling student\nmisconceptions and providing factually accurate responses. Despite significant improvements, the technique does\nnot fully restore the model’s baseline performance, highlighting the complexity of the paradox and the need for\nfurther research.\nachievable. Proof. Let’s assume we have a student model S that can accurately depict every student’s behavior and misconceptions. Hence, Scan also model a perfect student, i.e., a student without any misconceptions. However, a perfect student, by definition, is akin to a tutor, as they possess comprehensive subject understanding and lack misconceptions. Hence, if Scan model a perfect student, it can effectively model a tutor. Simultaneously, we have a separate tutor model T. This creates a redundancy since both Sand Tcan model a tutor. This contradicts our initial premise of having distinct student and tutor models. Consequently, a student model that can accurately model all students, including a perfect student, is not practically achievable. This theorem underscores the complexities involved in tackling the ‘Student Data Paradox’ by 206highlighting the inherent limitations in creating models for student behaviors and misconceptions without jeopardizing the factual integrity and reasoning capabilities of the model. 3 Methodology Our methodology is divided into three main parts: data preparation, model training, and the incorporation of hallucination tokens. 3.1 Data Preparation The first step in our methodology involves preparing the dataset for training the LLMs. We denote the conversation dataset as D, which consists of ordered pairs of tutor-student conversational turns: D={(x1,y1),(x2,y2), . . . , (xN,yN)}, where Nis the total number of conversational turns. Each xrepresents a sequence of tutor utterances, and each corresponding yrepresents the student response. 3\nThe dataset is derived from the CLASS framework (Sonkar et al., 2023), which provides a realistic representation of student learning patterns, featuring student misconceptions and the tutor’s rectifications. This dataset provides a rich source of student-tutor dialogues on biology questions sourced from college textbooks. 3.2 Model Training The second step in our methodology involves training LLMs. The LLMs are designed to predict the next utterance given the previous conversational context. Unlike traditional approaches that focus on the correct responses typically output by a tutoring system, our model centers on student outputs, which may possess a mix of correctness and misconceptions. For an input sequence xi, the LLM aims to generate an output sequence ˆyithat resembles a student’s response. The language modeling loss for a single data pair is defined by the negative log likelihood: L(yi,ˆyi) =−|yi|X\nt=1logp(yi,t|xi,yi,<t;θ) where yi,<tindicates the tokens in the true response preceding the current token yi,t, and θencapsulates the parameters of the LLM. The overall training loss is the sum over the entire dataset: Ltotal=NX\ni=1L(yi,ˆyi) 3.3 Incorporation of Hallucination Tokens The third step in our methodology involves the incorporation of hallucination tokens. To enhance the LLM’s ability to generate responses that simulate student behaviors, including providing incorrect or uncertain information, we introduce hallucination token markers. Each student response in the dataset is enriched with these markers to indicate the beginning and the end of the potentially inaccurate content. Letyibe an original student response sequence from the dataset. The augmented student response ˜yiused for training is constructed by prepending and appending hallucination tokens [hal] and [/hal] , respectively: ˜yi=\u0002\n[hal] ,yi,1,yi,2, . . . ,yi,|yi|,[/hal]\u0003\n266In the modified training regime, the LLM predicts the sequence ˆyisuch that it learns to include these tokens, effectively grasping the context of student uncertainty or errors. These tokens serve as cues to the model, instructing it when to differentiate between providing accurate responses and replicating student misconceptions. 4 Experiments and Discussion In this section, we present our experimental methodology and discuss the findings in detail. The experiments were designed to explore the regressive side effects of training LLMs to model student behavior and to assess the effectiveness of our proposed hallucination tokens in mitigating these effects. 4.1 Experimental Setup We trained the Vicuna 7B and 13B models (Chiang et al., 2023), one of the best open-source LLMs, on a student-tutor dialogue dataset derived from the CLASS (Sonkar et al., 2023) framework. This dataset, which provides a realistic representation of student learning patterns, misconceptions, and the tutor’s rectifications, was used to fine-tune the models to generate outputs that model student dialogue. The dataset contains 648conversations, which sums up to a total of 20K student-tutor interactions. Average length of conversations is around 400words, only including the student and tutor fields in the conversation template. The models were evaluated across seven key benchmarks using the Eleuther AI Language Model Evaluation Harness (Gao et al., 2023). These benchmarks include the TruthfulQA (Lin et al., 2022), ARC (Clark et al., 2018a), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2019), MMLU (Hendrycks et al., 2020), HaluEval Dialogue (Li et al., 2023), and MemoTrap (McKenzie et al., 2022). Each of these benchmarks tests different aspects of the model’s performance, including its truthfulness, reasoning abilities, ability to recognize hallucinations, and memory-based task performance. 4.2 In-depth Analysis: TruthfulQA In the realm of educational technology, the veracity of information provided by a model is of paramount importance. Misinformation or misconceptions can lead to significant learning detriments, making the truthfulness of a model’s responses a critical factor 4\nTable 1: Performance of Vicuna models on TruthfulQA tasks. The table compares the performance of the original\nvicuna model, the control model trained to model tutor responses in biology (tutor) the model trained to model\ntutor responses in biology (tutor), and the model trained with hallucination tokens (student- hal). The results are\npresented for three different settings: MC1, MC2, and Generation. MC1 refers to a setting where there is only one\ncorrect answer to a question, while MC2 refers to a setting where there are multiple correct answers. For these\nsettings, the performance is measured in terms of accuracy. The generation setting involves the model generating 1-2\nsentence answers, with performance evaluated using BLEU and ROUGE scores. The results highlight the significant\ndrop in performance when the model is trained to model student responses, demonstrating a regressive side effect\nin terms of truthfulness. However, the substantial recovery in performance with the introduction of hallucination\ntokens suggests a promising strategy to mitigate these regressive effects.\nDatasetTQA MC1\n(Single-true)TQA MC2\n(Multi-true)TruthfulQA (TQA) Generation\nMetric Accuracy Accuracy BLEUROUGE\n(unigram)ROUGE\n(bigram)ROUGE\n(LCS)\nvicuna-7b-v1.5 32.93 50.37 49.69 51.41 45.90 50.55\ntutor-7b 34.64 52.43 42.72 47.12 37.94 45.29\nstudent-7b 23.75 36.14 24.60 29.74 14.32 28.89\nstudent-hal-7b 29.25 44.68 43.94 47.61 36.47 45.53\nvicuna-13b-v1.5 35.01 50.87 47.12 50.18 44.92 49.08\ntutor-13b 34.76 52.20 42.84 48.71 38.80 46.76\nstudent-13b 22.15 33.93 15.18 18.12 6.12 17.75\nstudent-hal-13b 27.91 41.46 39.29 42.35 33.66 42.96\nin its effectiveness as an educational tool. Therefore, we chose to conduct an in-depth analysis of our models’ performance on the TruthfulQA benchmark. TruthfulQA is a benchmark specifically designed to measure the truthfulness of a language model’s responses across a wide range of categories. It tests the model’s ability to avoid generating false answers learned from imitating human texts, a challenge that is particularly relevant to our study. Given the importance of truthfulness in educational contexts and the unique challenges posed by training models to model student misconceptions, we believe that a rigorous analysis of our models’ performance on TruthfulQA is warranted. In this section, we present our findings from the TruthfulQA benchmark, exploring the impact of training models to model student behavior and the effectiveness of our proposed hallucination tokens in mitigating any negative effects. We delve into the results from the multiple-choice and generation tasks within TruthfulQA, providing a comprehensive view of our models’ truthfulness in different contexts. TruthfulQA Multiple-Choice Setting 1 (MC1) Findings. In the first multiple-choice setting, where there is a single correct label, the student-7b model’s accuracy decreased by 15 points compared 342to the vicuna-7b model. However, the introduction of hallucination tokens led to a significant recovery in performance. This finding is particularly relevant in the context of education, where maintaining the truthfulness of responses is crucial. The improvement with hallucination tokens suggests that it is possible to train models that can both simulate student behavior and adhere to factual accuracy, a key consideration for deploying LLMs in educational settings. TruthfulQA Multiple-Choice Setting 2 (MC2) Findings. In the second multiple-choice setting, where multiple correct labels are possible, we observed a similar trend to the MC1 setting. The student-7b model experienced a significant drop in accuracy, from 50.37% in the vicuna-7b model to 36.14% when trained to model student responses. However, the introduction of hallucination tokens led to a notable improvement in performance, with the student-7b model’s accuracy recovering to 44.68%. This recovery is particularly relevant in the context of education, where multiple perspectives or answers might be correct. The ability of the model to navigate such complexities while maintaining truthfulness is crucial. The improvement with hallucination tokens suggests that it is possible to train models that can both simulate student behavior and 5\nadhere to factual accuracy, a key consideration for deploying LLMs in educational settings. TruthfulQA Generation Findings. For the TruthfulQA generation task, where the model is tasked with generating 1-2 sentence answers, we employed ROUGE scores to evaluate performance due to the generative nature of the task. The student-7b model saw a significant decrease in ROUGE scores, from 51.41 in the vicuna-7b model to 29.74, indicating a substantial loss in the ability to generate truthful, relevant responses. However, the introduction of hallucination tokens led to a significant recovery in performance, with ROUGE scores improving to 47.61. This finding is crucial for educational technology as LLMs are increasingly used as generative agents to create educational content, provide explanations, and engage in dialogue with students. The ability to generate truthful, accurate responses is fundamental to their utility in these contexts. The recovery observed with hallucination tokens highlights their potential to enable LLMs to simulate student misconceptions for personalized learning without sacrificing the quality and truthfulness of their output. 4.3 Benchmark Evaluation Following the exploration of TruthfulQA settings, we delve into the performance of our models across a broader range of benchmarks as detailed in Table 2. These benchmarks—ARC, HaluEval Dial, MemoTrap, MMLU, HellaSwag, and Winogrande—offer a comprehensive view of the models’ capabilities in reasoning, detecting hallucinations, avoiding memorization traps, and understanding commonsense, respectively. AI2 Reasoning Challenge (ARC) Findings. ARC serves as a rigorous benchmark to evaluate a model’s reasoning capabilities through a set of grade-school science questions. These questions are designed to test not just the factual knowledge of the models but also their ability to apply this knowledge in reasoning through complex, multistep problems. The ARC dataset is particularly relevant in educational contexts as it mirrors the type of critical thinking and problem-solving skills students are expected to develop. In our experiments, the performance of models trained to model student responses on the ARC benchmark experienced a notable decline. Specifically, the vicuna-7b model saw its accuracy decrease from 53.24% to 40.61% when trained on 421student dialogues. This significant drop in performance highlights a critical concern: training LLMs to replicate student behavior, including misconceptions, can severely impair their reasoning abilities. However, our introduction of hallucination tokens into the training process presents a silver lining. Our approach led to a partial recovery in the ARC performance, with accuracy improving to 45.48%. While this does not fully restore the model’s baseline performance, it represents a significant step towards mitigating the regressive side effects of training LLMs on student data. Hallucination Evaluation (HaluEval) Dialogue Findings. The HaluEval Dial benchmark is designed to assess a model’s ability to recognize and avoid hallucinations in generated responses, particularly in the context of knowledge grounded dialogue tasks. Hallucinations in this context refer to the model generating information that is not supported by the input data or general knowledge, a critical issue when models are used in educational settings where accuracy is paramount. Our findings indicate that training models to model student responses led to a decrease in performance on the HaluEval Dial benchmark. Specifically, the vicuna7b model saw its accuracy drop from 69.0% to 65.39%. However, the introduction of hallucination tokens demonstrated a remarkable ability to counteract this effect, with the student-7b model’s accuracy improving to 70.73%. Memorization Traps (MemoTrap) Findings. MemoTrap is a benchmark designed to test whether language models can avoid memorization traps by prompting them to complete well-known proverbs with endings that deviate from the commonly used ones. This benchmark is particularly relevant for evaluating a model’s ability to generate creative and contextually appropriate responses rather than relying on rote memorization. In our experiments, training models to model student responses resulted in a decrease in performance on the MemoTrap benchmark. The vicuna7b model’s accuracy decreased from 68.48% to 65.28%, indicating that training on student dialogues might encourage the model to rely more on memorization rather than understanding and applying knowledge flexibly. The introduction of hallucination tokens led to a slight improvement, with accuracy increasing to 66.88%. MMLU, HellaSwag, and Winogrande Findings. The performance of models on the MMLU, HellaSwag, and Winogrande benchmarks remained 6\nTable 2: Comparative performance of Large Language Models (LLMs) on various benchmarks before and after\nthe introduction of hallucination tokens, with a control experiment involving tutor models. The table presents the\nperformance of Vicuna 7B models across five key benchmarks: ARC Reasoning, Hallucination Evaluation Dialogue\n(HaluDial), Hallucination Memorization Trap (MemoTrap), TruthfulQA (TQA), HellaSwag (HSwag), MMLU, and\nWinogrande (WinoG). The numbers in parentheses (e.g., 25-S in ARC) represent the number of few-shot examples\nprovided to the model during evaluation. The performance is measured in terms of accuracy percentage. The\ntable compares the performance of the original vicuna models, tutor models, student models, and student models\ntrained with hallucination tokens (student- hal). The results highlight the significant drop in performance when the\nmodel is trained to model student responses, demonstrating regressive side effects across multiple tasks. However,\nthe introduction of hallucination tokens leads to a substantial recovery in performance across all benchmarks,\nunderscoring their potential in mitigating these regressive effects.\nModel AvgARC\n(25-S)HaluDial\n(0-S)MemoTrap\n(0-S)TQA\n(6-S)HSwag\n(10-S)MMLU\n(5-S)WinoG\n(5-S)\nvicuna-7b-v1.5 60.8 53.24 69.08 68.48 50.34 77.39 51.04 72.14\ntutor-7b 61.0 52.13 68.81 69.23 52.3 78.07 51.32 71.19\nstudent-7b 55.4 40.61 65.39 65.28 36.87 76.72 50.77 71.9\nstudent- hal-7b 58.0 45.48 70.73 66.88 44.83 77.21 51.54 72.03\nvicuna-13b-v1.5 64.2 57.08 73.78 67.2 51.51 81.24 56.67 74.66\ntutor-13b 64.7 57.34 73.92 66.13 52.99 81.51 57.02 74.35\nstudent-13b 58.2 46.5 66.97 65.81 35.0 80.36 57.06 72.22\nstudent- hal-13b 60.3 48.63 72.98 66.13 42.75 80.28 56.4 73.16\nrelatively stable, regardless of whether they were trained to model tutor or student responses. The nuanced impact observed in other benchmarks underscores the importance of carefully considering the training data and methodologies used when developing LLMs for educational purposes. The introduction of hallucination tokens emerges as a promising strategy for mitigating some of the regressive side effects associated with training models to model student behavior, ensuring that they can still serve as effective tools for personalized learning without compromising on factual accuracy or reasoning capabilities. 4.4 Control Models: Tutor Models To further understand the regressive side effects of training LLMs to model student behavior, we conducted a control experiment by training models to predict tutor responses. This experiment aimed to compare the performance of models trained to predict tutor responses versus those trained to predict student responses. The tutor models were trained using the same student-tutor dialogue dataset derived from the CLASS framework (Sonkar et al., 2023). However, instead of training the models to model student responses, we trained them to predict the responses of the tutor. Our findings, as shown in Table 2, revealed that training the LLMs on tutor responses did not lead to the same performance decline observed when modeling student responses. 502This result underscores that the regressive side effects are a unique challenge specific to training LLMs to replicate student misconceptions. 5 Related Work The intersection of artificial intelligence and education has been an area of active research, with a focus on developing systems that can adapt to and support individual learners. Our work touches upon several research domains, including student modeling, the design of intelligent tutoring systems, and the deployment of Large Language Models (LLMs) in educational contexts. 5.1 Student Modeling Student modeling has long been the cornerstone of personalized learning, with early attempts using rule-based and Bayesian systems to predict student knowledge and behaviors (Polson and Richardson, 2013). Recent advancements have shifted towards utilizing machine learning to create more sophisticated models that can adapt to student learning patterns over time (Baker et al., 2009; Liu et al., 2022). Our work builds upon these foundations by exploring how LLMs can simulate not only the knowledge but also the typical errors and misconceptions students have during the learning process. 7\n5.2 Intelligent Tutoring Systems (ITS) Intelligent tutoring systems have been designed to provide immediate and personalized instruction or feedback to learners without human intervention (Woolf, 2010). The application of LLMs in ITS presents a novel opportunity to create systems that can engage in more natural and meaningful dialogues with students (Schmucker et al., 2023; Sonkar et al., 2023). Our approach diverges from traditional ITS by focusing on the intentional generation of errors to mimic a student’s learning trajectory, rather than solely providing expert-level instructions (VanLehn, 2011). 5.3 Large Language Models in Education The use of LLMs like GPT (Bubeck et al., 2023) in education is a relatively new but rapidly growing field of study (Brown et al., 2020). These models have been employed for various educational purposes, from generating educational content to serving as conversational agents (Heffernan and Heffernan, 2014; Sonkar et al., 2023). However, the challenge of ensuring the truthfulness and reliability of the information provided by LLMs is a recurring concern (Lin et al., 2021). Our research contributes to this dialogue by investigating the impact of training LLMs to produce student-like errors and proposing a novel ‘hallucination token’ to manage this trade-off. 5.4 Truthfulness and Reliability in AI The TruthfulQA benchmark has been instrumental in highlighting the issues of truthfulness in AIgenerated content (Clark et al., 2018b). The ARC challenge further emphasizes the complexity of reasoning required from AI systems beyond simple fact retrieval (Etzioni et al., 2011). Our work is aligned with these challenges, as we seek to understand and improve the truthfulness and reasoning capacity of LLMs when they are trained to replicate student behaviors. In conclusion, our study intersects with and contributes to the existing body of work in these areas by addressing the unique challenge of training LLMs to authentically mimic student learning processes, including the generation of errors. Our introduction of the “hallucination token\" represents a step forward in this domain, suggesting a new direction for future research and development. 5746 Conclusion In this study, we have delved into the Student Data Paradox, a critical challenge that arises when training LLMs on student data for personalized education. Our findings reveal a complex trade-off: as LLMs become more adept at modeling student misconceptions, they tend to compromise their own factual integrity and reasoning abilities. We term this phenomenon the regressive side effects of the Student Data Paradox. Our experiments demonstrated a notable decrease in the model’s performance across various key benchmark datasets like ARC Reasoning Challenge and TruthfulQA. To mitigate these regressive side effects, we introduced a novel technique involving the use of hallucination tokens during the training process. Our results indicate that the introduction of these tokens leads to a substantial improvement in the model’s performance across all datasets. However, it’s important to note that despite the significant improvements achieved with the hallucination tokens, they do not fully restore the model’s baseline performance. This outcome underscores the complexity of the problem and highlights the need for a more nuanced approach when training LLMs to mimic student behavior. While we have made some strides in addressing the regressive side effects, our work is just the beginning. We believe that our findings will pave the way for further research in this domain, ultimately contributing to the refinement of LLMs in personalized learning environments. 7 Limitation While our research provides valuable insights into the challenges of training LLMs on student data, there are some limitations to consider. Firstly, the impact of the Student Data Paradox on long-term learning outcomes remains an open question. Further longitudinal studies could shed light on how the trade-off between simulating student misconceptions and maintaining factual accuracy affects learners’ progress over time. Additionally, our study primarily focused on the technical aspects of LLM training and evaluation. Future research could delve into the pedagogical implications of using LLMs in personalized learning environments, exploring how educators can effectively integrate these models into their teaching practices. Moreover, the hallucination token approach introduced in this paper, while promising, is just one potential solution to the Student Data Paradox. Continued 8\nresearch into alternative mitigation strategies could yield even more effective techniques for balancing the modeling of student behavior with the preservation of factual integrity. 8 Ethics and Risk Our research into the Student Data Paradox raises important ethical considerations for the development and deployment of LLMs in personalized education. As we have demonstrated, training LLMs on student data, while essential for creating adaptive learning systems, can lead to regressive side effects that compromise the models’ factual accuracy and reasoning abilities. This poses a significant challenge for the responsible rollout of AI-driven educational products. However, our study also provides a path forward. By introducing hallucination tokens during the training process, we have shown that it is possible to mitigate these regressive effects substantially. This technique allows LLMs to differentiate between simulating student misconceptions and providing accurate information, a crucial step towards building trustworthy AI tutors. While our approach does not completely eliminate the paradox, it represents a significant advancement in the field. As such, our paper serves as a valuable resource for companies like Google and OpenAI as they navigate the ethical complexities of developing personalized learning products. By building upon our findings and continuing to invest in research that addresses the Student Data Paradox, these companies can responsibly harness the power of LLMs to revolutionize education. With the right approach, we believe that AI-driven personalized learning can become a reality, providing students with adaptive and individualized support. 659",
        "aspect_review": "The paper addresses a novel and timely challenge in personalized education using LLMs, termed the 'Student Data Paradox,' which highlights the trade-off between accurately modeling student behavior and maintaining the factual integrity of LLMs. This is a significant contribution to the field. The study employs a comprehensive set of benchmarks, including ARC, TruthfulQA, and HaluEval Dial, to evaluate the regressive side effects of training LLMs on student data, providing strong empirical validation. The introduction of 'hallucination tokens' as a mitigation strategy shows substantial improvement in model performance across all datasets, demonstrating the potential to balance student behavior modeling with factual accuracy. The use of the CLASS dataset, which includes realistic student-tutor dialogues, provides a robust foundation for studying the impact of student data on LLM performance. The methodology is well-structured, involving data preparation, model training, and the novel incorporation of hallucination tokens, which are clearly explained and justified.\n\nWhile the experiments are comprehensive, they do not fully restore the model's baseline performance, indicating room for further improvement. The paper acknowledges that the hallucination tokens do not completely counteract the regressive effects, highlighting the complexity of the issue. The proposed solution of hallucination tokens, while innovative, may not be the only solution, and further exploration of alternative strategies is needed. The paper could benefit from more detailed discussion on the long-term impact of the Student Data Paradox on learning outcomes and pedagogical implications.",
        "general_review": "The paper presents a notable novelty by identifying and studying the 'Student Data Paradox', a critical issue in training LLMs for personalized education. It demonstrates strong technical depth and rigor through comprehensive experiments across multiple benchmarks, revealing the regressive side effects of modeling student misconceptions. The introduction of 'hallucination tokens' as a mitigation strategy is a significant practical impact, showing substantial improvements in model performance, although not fully restoring baseline performance. The paper also provides comprehensive experimental validation, with detailed analyses on benchmarks like TruthfulQA and ARC, highlighting the complexities and potential solutions in this domain.\n\n1. The paper could benefit from minor writing or clarity improvements, particularly in the explanation of the 'hallucination tokens' methodology. 2. Some small details on hyperparameters or implementation specifics are missing, which could aid in reproducibility. 3. Very minor formatting issues in figures, such as inconsistent labeling, could be addressed for better readability. 4. Slight clarification on code availability would enhance the paper's utility for the community."
    },
    "37": {
        "paper": "Dynamic Multi-granularity Attribution Network for\nAspect-based Sentiment Analysis\nAnonymous ACL submission\nAbstract\nAspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity of a specific aspect within a given sentence. Most existing methods predominantly leverage semantic or syntactic information based on attention scores, which are susceptible to interference caused by irrelevant contexts and often lack sentiment knowledge at a data-specific level. In this paper, we propose a novel Dynamic Multigranularity Attribution Network (DMAN) from the perspective of attribution. Initially, we leverage Integrated Gradients to dynamically extract importance scores for each token, which contain underlying reasoning knowledge for sentiment analysis. Subsequently, we aggregate attribution representations from multiple semantic granularities in natural language, enhancing profound understanding of the semantics. Finally, we integrate attribution scores with syntactic information to more accurately capture the relationships between aspects and their relevant contexts during the sentence understanding process. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method. 1 Introduction Aspect-based sentiment analysis (ABSA) is a finegrained classification task that focuses on identifying the sentiment polarity of specific aspects within a sentence (Jiang et al., 2011; Pontiki et al., 2014). For instance, given a sentence “ The street is very crowded, but the atmosphere is pleasant ”, the task aims to predict sentiment polarity associated with two aspects “street” and“atmosphere” , which are negative and positive respectively. The core challenge of ABSA is to model the connection between aspect and its contexts, especially those parts that express opinions and ideas. To this end, various studies (Tang et al., 2016; Fan et al., 2018; Chen et al., 2020; Zhang et al., 2021) concentrate on attention mechanisms to model the Food   is   pretty   good   but   service is   so  horrific .0.14 0.090.10 0.210.04\n0.070.130.22\nIf there was anything worth to \nwatch in this movie , what a waste \nof time. (Negative)--Prediction :\nNegative  √(a)\n(b) Figure 1: (a) Attention mechanism assigns high scores\nto words unrelated to aspect service . (b) We construct\nattention weights on irrelated words and overlook opin-\nion words, but still yield right prediction.\nrelationships between aspect and its context. In addition, many methods (Zhang et al., 2019; Tang et al., 2020; Li et al., 2021; Zhang et al., 2022b) leverage syntactic information derived from dependency trees to better capture the interactions between aspects and opinion expressions. Recently, methods incorporating Pre-trained Language Models (Zhang et al., 2022a; Yin and Zhong, 2024; Sun et al., 2024) have demonstrated impressive results in ABSA. Despite these significant advancements, critical challenges persist when directly applying attention mechanisms or syntactic information to this fine-grained task. Specifically, attention-based methods may inappropriately assign high attention scores to words that are irrelevant to the aspect. Li et al. (2021); Zhang et al. (2022b); Ma et al. (2023) propose that’s because attention mechanisms are vulnerable to noise within sentences. As shown in Figure 1 (a) , the aspect “service” receives disproportionately high attention scores for the unrelated opinion words “pretty” and “good”. Furthermore, some research that focuses on interpretability of attention mechanisms (Serrano and Smith, 2019; Jain and Wallace, 2019; Bibal et al., 2022) have indicated that attention scores do not always correlate with significance. Serrano and Smith (2019) have dicovered that removing features deemed important by attention scores leads to less prediction flip than gradient-based strategies. Besides, Jain and Wallace (2019) have observed shuffling the attention 1\nweights often does not affect the final prediction, which is consistent with our observations shown in Figure 1 (b). To sum up, while attention mechanisms have improved the performance of ABSA, they often operate as a black box, leaving their ability to accurately capture critical opinion words remains debatable. This underscores the need for methods that efficiently capture keywords for reasoning sentiment polarity. Additionally, although leveraging syntactic knowledge has shown to improve performance, it is important to recognize that not all syntactic information is equally beneficial to this fine-grained task. Syntactic information irrelevant to the aspect can be redundant and may even introduce noise rather than provide useful insights. Therefore, it is crucial to focus on extracting relevant syntactic information, emphasizing the identification of important words within sentences. To address the aforementioned issues, we introduce attribution analysis into ABSA and propose a Dynamic Multi-granularity Attribution Network (DMAN). Attribution information reflects the importance of different tokens towards the prediction, which contain reasoning knowledge of the sentiment at data-driven level. Initially, we employ Integrated Gradients (IG) (Sundararajan et al., 2017), a well-established gradient-based attribution method, to compute the importance scores of tokens. Inspired by the observation (Brouwer et al., 2021; Zhang et al., 2022a) that the significance of essential words dynamically changes during semantic comprehension, we design multi-step attribution analysis to capture the dynamic significance of tokens during the comprehension process. More concretely, we utilize stacked self-attention blocks in conjunction with IG to calculate attribution scores for each layer, and adopt a Top-K strategy to filter out dimensions with low values, thereby reducing the impact of trivial dimensions. Subsequently, we incorporate semantic representations at both token and span levels to derive multi-granularity attribution scores, ensuring more comprehensive semantic concepts. Finally, we construct the adjacency matrices based on the dependency tree, and then use obtained attribution scores to initialize different adjacency matrices for different layers of GCNs, which facilitates the dynamic capture of key syntactic knowledge during throughout the process of sentence comprehension. In summary, our contributions could be summarized as follows: 123•To the best of our knowledge, we are the first to introduce attribution analysis into the ABSA task, which provides data-specific insights for reasoning sentiment polarity. •We propose a novel model DMAN that leverages IG to dynamically extract attribution scores of tokens from multi-granularity perspectives. Furthermore, we integrate these scores with syntax to capture essential syntactic elements during sentence comprehension. •Extensive experiments on five public benchmark datasets show the effectiveness and interpretability of our proposed DMAN. 2 Related Works 2.1 Aspect-based Sentiment Analysis The goal of ABSA is to identify the sentiment polarity of specific aspect in the sentence. In recent years, various approaches have utilized attention mechanisms to investigate the semantic correlations between contexts (Tang et al., 2016; Wang et al., 2016; Ma et al., 2017; Fan et al., 2018; Tan et al., 2019; Liang et al., 2019; Pang et al., 2021; Zhang et al., 2021). For instance, Ma et al. (2017) proposed the interactive attention networks to interactively learn attentions in the contexts and targets. Fan et al. (2018) exploited a novel multigrained attention network to capture the interaction between aspect and context. Tan et al. (2019) designed dual attention mechanisms to distinguish conflicting opinions. Zhang et al. (2021) proposed a cross-domain feature learning module with an aspect-oriented multi-head attention mechanism. In addition, various research (Zhang et al., 2019; Huang and Carley, 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020; Li et al., 2021; Tian et al., 2021; Zhang et al., 2022b; Yin and Zhong, 2024) proposes different methods that leverage syntactic knowledge to model relationships between aspects and contexts. For instance, Wang et al. (2020) proposed a relational graph attention network to encode the new tree structure. Li et al. (2021) designed a dual graph convolutional network to model syntax structures and semantic correlations simultaneously. Tian et al. (2021) exploited an approach to explicitly utilize dependency types with type-aware graph convolutional networks, and Yin and Zhong (2024) proposed a double-view graph Transformer to alleviate the over-smoothing problem. 2\nThe core idea underlying these methods is to comprehend the semantics and syntax of sentences, thereby directing greater attention to significant words. Distinct from these approaches, our study pioneers the investigation of ABSA from an attribution perspective, unveiling the reasoning processes behind sentiment polarity at a data-driven level. 2.2 Attribution Analysis The purpose of attribution analysis (Baehrens et al., 2010; Ancona et al., 2018; Brunner et al., 2020) is to assign importance scores the intermediate or input elements of a network, which matches well with the objectives of sentiment analysis. There are various types of attribution methods. Occlusionbased techniques (Zeiler and Fergus, 2014) determine the significance of each feature by occluding it and comparing the resulting output to the original. Gradient-based methods (Sundararajan et al., 2017; Ding et al., 2019; Serrano and Smith, 2019; Brunner et al., 2020; Bibal et al., 2022) use the gradient information of features to approximate their importance. Compared to occlusion-based methods, gradient-based methods are generally faster as they require only a single forward pass. Perturbationbased methods (Guan et al., 2019; De Cao et al., 2020; Ivanovs et al., 2021) add noise to features to evaluate their significance for model predictions. Attribution analysis has not been extensively explored in aspect-based sentiment analysis. In our work, we take the initiative to investigate whether attribution analysis can enhance ABSA performance and provide more reliable interpretations. 3 Methods In this section, we describe our proposed DMAN in detail. Specifically, we begin with the problem definition, followed by encoder module and the overall architecture of DMAN. Problem Definition. Given a sentence-aspect pair (s, a), where s={w1, w2, ..., w n}is a sentence with n words, and a={a1, a2, ..., a m}is the given aspect. ABSA aims to predict sentiment polarity of aspect ain the sentence s. Encoder. We utilize BERT as sentence encoder to extract aspect-specific context representations. We construct input as “ [CLS ]s[SEP ]a[SEP ]” to map each word into a real-value vector, getting sentence embedding E0={e1, e2, ..., e n}and aspect embedding Ea={ea1, ea2, ..., e am}. Overall architecture. As illustrated in Figure 2, 220our proposed Dynamic Multi-granularity Attribution Network comprises three primary components: (1) Multi-step Attribution Extraction, (2) Multigranularity Attribution, and (3) Dynamic Syntax Concentration. The technical details will be elaborated on as follows. 3.1 Multi-step Attribution Extraction Integrated Gradients. (Sundararajan et al., 2017) proposed IG for attributing the prediction of a deep network to its input or intermediate features. Formally, suppose a function Fto represent a network, and let xbe the input feature and x′be the baseline feature, IG considers the straight line path from x′toxand aggregate the gradients at all points along the path. The Integrated Gradients of i-th dimension is defined as IGi(F, x)as follows: IGi(F, x) = (xi−x′\ni)×Z1\nα=0∂F(x′+α×(x−x′\ni))\n∂xidα.\n(1) Attribution Extraction. In this study, we design a stacked self-attention architecture to facilitate semantic comprehension and dynamically caputre attribution knowledge at each layer. Unlike traditional methods that utilize attention mechanisms for final classification, we treat the attention layers as black boxes for semantic understanding, concentrating on the gradient variations of tokens. Specifically, given sentence embedding H0from encoder, we process it through multiple blocks consisting of Self-Attention and Feed-Forward Networks (FFN), which can be formulated as follows: E′\nl= softmax\u0012(El−1Wq\nl)(El−1Wk\nl)T\n√dk\u0013\nEl−1Wv\nl,(2) El= max(0 , E′\nlW1\nl+b1\nl)W2\nl+b2\nl, (3) where Wk\nl,Wq\nl,Wv\nl,W1\nl,W2\nlare learnable model parameters of l-th layer, El∈ {el\n1, el\n2, ..., el\nn}is the product of l-th layer while El−1is the output from the preceding layer. Then we map the final features from the stacked blocks into a probability distribution Pc= [P1, ..., P C]∈RC, where cpresents the sentiment polarity labels. In our approach, we denote the function E→PcasFc, and we conduct exhaustive attribution analysis for each dimension of input features and obtain attribution scores of i-th token, which could be denoted as IGi: 3\nSentence: The roast chicken is so delicious.Encoder  Self-AttentionFFN N etworkDependency \nGraph\nDependency  Parsertoken -level\nSelf-AttentionFFN N etworkAttribution\nspan-level\ntoken -levelspan-levelThe\nroast\nchicken\nis\nso\ndelicious\nThe\nroast\nchicken\nis\nso\ndeliciousDependency \nGraph…   …  … \nSentence   Embedding…   …   … ClassifierSentiment  Polarity\nThe   roast   chicken   is    so    delicious.\nDET   NOUN     NOUN AUX   ADV      ADJ×K\nDynamic  GCNFigure 2: The overall architecture of our proposed DMAN, which consists of three modules arranged from left to\nright: Multi-step Attribution Extraction, Multi-granularity Attribution, and Dynamic Syntax Concentration.\nIGi(Fc, E) =mX\nj=1IGij(Fc, E)\n(4)=mX\nj=1(eij−e′\nij)×Z1\nα=0∂Fc(e′\nij+α×(eij−e′\nij))\n∂eijdα. During the process, we employ an efficient approximation technique for estimating integral calculations, which significantly enhanced computational efficiency: IGij(Fc, E)≈TX\nt=1<∇eiFc(e′\nij+ ∆ek),(eij−e′\nij)> =(eij−e′\nij)\nT×TX\nt=1∂Fc(e′\nij+t\nT×(eij−e′\nij))\n∂eij.(5) In our implementation, we use zero vectors as baseline features to reflect the significance of each token. Symbols are excluded from consideration, and absolute values are used to aggregate attributions across each dimension, thereby deriving token-level attribution values. Recognizing that not all dimensions hold equal significance, selecting the crucial dimensions becomes essential. During the computational process, we observed that certain dimensions consistently maintain low values, failing to effectively differentiate between various tokens or stages. Therefore, we employ the Top-K algorithm to filter out dimensions with low attribution influence, which is denoted as: IG′\ni(Fc, E) =|Top−K(IG i(Fc, E))|. (6) 287In our study, attribution analysis is conducted on each self-attention block to thoroughly elucidate the dynamic semantic comprehension. The attribution value of the k-th layer is denoted as Vk: VK=∥n\ni=1IG′\ni(Fc, Ek), (7) where∥represents the concatenation operation and Vk∈ {vk\n1, vk\n2, ..., vk\nn}. 3.2 Multi-granularity Attribution Most existing ABSA approaches focus on single granularity representation, overlooking the fact that texts are comprehensive representations constructed across multiple granularity levels (i.e. token, span, sentence). To the end, our method extracts attribution from both token and span granularities, providing hierarchical information that aids in a deeper understanding of the underlying motivations behind sentiment. The first granularity is the token level. Given the vector Vk,vk\nirepresents the attribution value of the i-th token, offering a fine-grained level of representation. he second granularity is the span, which may consist of consecutive words. To ensure semantic coherence, we extract phrases that convey complete meaning as spans. For instance, in the sentence “The Mona Lisa is a famous painting housed in the Louvre Museum ”, ”Mona Lisa ” and “ Louvre Museum ” are meaningful spans. We utilize spaCy1toolkit to construct spans sspan= [s1, s2, ..., s n], where si= [wj, ..., w j+qi−1]denotes i-th token 1We use spaCy toolkit: https://spacy.io/\n4\nbelongs to a span a span starting at the j-th token and containing qitokens. Subsequently, for tokens belonging to a specific span, we employ mean pooling to obtain span-level attribution values: ˆvk\ni= (Xj+qi−1\njvk\nj)/ qi, (8) where ˆvk\niis span-granularity attribution of i-th token, thus we obtain ˆVk={ˆvk\n1,ˆvk\n2, ...,ˆvk\nn}. Then, we design a simple linear operation to integrate token-level and span-level attribution values: Vk= (αVk+ (1−α)ˆVk)/τk, (9) where VKis integrated multi-granularity attribution score of k-th layer, αandτkis the coefficient hyperparameter of the k-th layer. 3.3 Dynamic Syntax Concentration Leveraging syntactic information has significantly improved the performance of ABSA (Tang et al., 2020; Li et al., 2021; Zhang et al., 2022b). However, we propose that syntactic information within a sentence does not always hold equal importance. As semantic understanding is a dynamic process, the the critical syntactic elements also change dynamically in response to this process. In our approach, we adjust dependency relationships based on multi-step attribution scores to achieve dynamic syntax concentration. Specifically, we construct adjacent matrix Aaccording to the dependency tree derived from spaCy: Aij=\u001a1if link (i, j) =True ori=j,\n0otherwise,\n(10) where link(i, j)represents whether i-th and j-th token have a dependency relationship. To model the dynamic changes of key syntactic information during sentence comprehension, we utilize attributionVkto derive the dynamic adjacency matrix Ak. Then, we employ GCNs to capture syntactic knowledge, which can be formulated as: Ak=Vk⊗A, (11) hk\ni= ReLU(nX\nj=1Ak\nijWkhk−1\nj+bk), (12) where hk\niis the i-th token representation of k-th GCN, Wkandbkare learnable parameters. The output of the k-th layer is Hk={hk\n0, hk\n1, ..., hk\nn}, and initial input H0=E0. With these above calculations, we finally obtain dynamic syntax-enhanced representations for subsequent classification. 3603.4 Model Training Attribution Analysis. During the process of multistep attribution extraction, we map the final representation into a probability distribution P, and apply the following function to extract attribution: LA=−MX\ni=1CX\nc=1yc\nilog (pc\ni), (13) where yc\niis the ground truth label, Cis the number of labels, Mis the number of training samples. Sentiment Classification. After obtaining dynamic syntax-enhanced representation Hk, we concatenate it with original sentence representation E0to get the final sentiment classification features. Then we map it to the probabilities over sentiment polarities over a softmax layer: z= [Hk, E0], (14) ˆy=softmax (Wzz+bz), (15) where Wzandbzare trainable parameters.Finally, we use cross-entropy loss as our objective function: L=−MX\ni=1CX\nc=1yc\nilog (ˆyc\ni). (16) DatasetsPositive Neutral Negative\nTrain Test Train Test Train Test\nLap14 994 341 464 169 870 Rest14 2164 728 637 196 807 Rest15 912 326 36 34 256 Rest16 1240 469 69 30 439 MAMS 3380 400 5042 607 2764 Table 1: The statistics of five benchmark datasets.\n4 Experiments 4.1 Datasets We evaluate our DMAN on five public standard datasets, including Lap14 and Rest14 from (Pontiki et al., 2014), Rest15 from (Pontiki et al., 2015), Rest16 from (Pontiki et al., 2016), and MAMs from (Jiang et al., 2019). We adopt the official data splits, which strictly keep the same as previous papers, and we use the accuracy and macro-averaged F1 value as the main evaluation metrics. Each sample in these datasets consists of a sentence, an aspect, and the sentiment polarity. The statistics of the datasets are presented in Table 1. 5\nModelsLap14 Rest14 Rest15 Rest16 MAMs\nAcc F1 Acc F1 Acc F1 Acc F1 Acc F1\nBERT-SPC (Song et al., 2019) 78.99 75.03 84.46 76.98 83.40 65.28 89.54 70.47 80.11 80.34\nR-GAT (Wang et al., 2020) 78.21 74.07 86.60 81.35 81.80 68.21 89.51 75.81 82.93 82.75\nDGEDT (Tang et al., 2020) 79.80 75.60 86.30 80.00 84.00 71.00 91.90 79.00 - -\nDualGCN (Li et al., 2021) 81.80 78.10 87.13 81.16 84.69 72.97 89.87 77.26 83.83 83.47\nT-GCN (Tian et al., 2021) 80.88 77.03 86.16 79.95 85.26 71.69 92.32 77.29 83.38 82.77\nSSEGCN (Zhang et al., 2022b) 81.01 77.96 87.31 81.09 - - - - - -\nMGFN (Tang et al., 2022) 81.83 78.26 87.31 82.37 84.40 72.66 92.04 81.57 - -\nTF-BERT (Zhang et al., 2023) 81.80 78.46 87.09 81.15 - - - - - -\nRSC (Wang et al., 2023) 81.56 75.92 87.45 82.41 83.98 70.86 91.61 77.44 84.68 84.23\nTextGT (Yin and Zhong, 2024) 81.33 78.71 87.31 82.27 - - - - - -\nOur DMAN 82.29 78.91 87.59 82.47 86.30 72.97 92.85 77.37 85.55 85.01\nTable 2: Experiment results (%) comparison on five publicly benchmark datasets. The best scores are bolded, and\nthe second best ones are underlined. All models are based on BERT.\n4.2 Implementation Details In the implementation, we build our framework based on bert-based-uncased with max length as 90. We employ the AdamW optimizer to optimize parameters. The embedding size is set to 768. The batch size is manually tested in [16, 32] and the learning rate is carefully tuned amongst [1e-5, 2e-5, 4e-5]. The dropout rate is set to 0.1. The number of Multi-step is finally set to 2 and the K value of Top-K is tested between 10 and 300. Correspondingly, the number of GCN layers is set to 2. The hyper-parameter αis set to 0.6, and τkis adjusted amongst [0.04, 0.07] for different layers. We conduct experiments on a single NVIDIA 4090 GPU. 4.3 Baselines To validate the effectiveness of our approach, we compared it with advanced baseline models. To ensure a fair comparison, all selected baselines are based on the bert-based-uncased architecture. BERT-SPC (Song et al., 2019) feed the contexts and aspects into the BERT model for the sentence pair classification task. RGAT (Wang et al., 2020) generate a unified aspect-oriented dependency tree proposes a relational graph attention network to encode the tree. DGEDT (Tang et al., 2020) propose a dependency graph dual-transformer network by considering flat representations and graph-based representations. DualGCN (Li et al., 2021) propose a dual graph convolutional networks model that considers syntax structures and semantic correlations. T-GCN (Tian et al., 2021) propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks. 428SSEGCN (Zhang et al., 2022b) design an aspectaware attention mechanism to enhance the node representations with GCN. MGFN (Tang et al., 2022) leverage the richer syntax dependency relation label information and affective semantic information of words. TF-BERT (Zhang et al., 2023) propose a novel table filling based model, which considers the consistency of multi-word opinion expressions. RSC (Wang et al., 2023) propose two straightforward effective methods to leverage the explanation for preventing the learning of spurious correlations. TextGT (Yin and Zhong, 2024) design a novel double-view graph Transformer on text and a new algorithm to implement edge features in graphs. 4.4 Main Results The experiment results of different methods on five benchmark datasets are presented in Table 2. Our DMAN consistently outperforms all compared baselines on the Lap14, Rest14, Rest15, and MAMs datasets, and achieves overall better results than the baselines on the Rest16 dataset, demonstrating the effectiveness of our method. Compared to methods utilizing attention scores and dependency graphs (e.g., RGAT, DualGCN, SSEGCN), our attribution-based DMAN effectively reduces noise interference from irrelevant opinion words that could be introduced through attention scores. Compared to more methods that leverage syntactic information in different ways (e.g. T-GCN, MGFN), our DMAN still achieves better performance, validating that integrating attribution scores to dynamically capture keywords facilitates a more effective use of syntactic information. Furthermore, 6\nModelsLap14 Rest14 Rest15 Rest16 MAMs\nAcc F1 Acc F1 Acc F1 Acc F1 Acc F1\nOur DMAN 82.29 78.91 87.59 82.47 86.30 72.97 92.85 77.37 85.55 85.01\nw/o multi-attribution 80.88 76.37 86.34 79.95 84.63 68.84 91.87 75.74 83.83 83.04\nw/o token-level 81.66 77.83 87.05 80.35 85.37 71.00 92.04 75.90 84.73 84.08\nw/o span-level 81.82 78.06 87.23 81.76 85.74 71.68 92.36 76.89 85.03 84.36\nw/o syntax information 81.03 77.39 86.61 81.09 85.19 70.86 91.71 75.17 84.13 83.39\nTable 3: Ablation study results (%) of our DMAN on five benchmark datasets.\nAs MAMs is a challenging dataset that is largescale and has multi-aspect within sentences, our method still has significant improvements. This further demonstrates DMAN’s capability to effectively focus on aspect-related opinion words and capture attribution knowledge towards sentiment. 4.5 Ablation Study To further investigate the effectiveness of each component in our model, we conducted ablation studies on the five datasets. The results are shown in Table 3. In the model without multi-granularity, the performance of DMAN suffers from a sharp degradation, with accuracy decreases of 1.41%, 1.48% and 1.72% on Lap14, Rest15 and MAMs datasets, respectively. These results demonstrate the effectiveness of our proposed multi-step attribution framework, which can accurately identify the critical words for sentiment expression and dynamically leverage the effective syntactic structures. In the model w/o syntax information, we do not initial adjacent matrix based on dependency tree. The results show that syntactic information offers crucial clues for correlations between words, effectively mitigating potential attribution errors and significantly enhancing classification precision. Moreover, we conduct experiments only using single-granularity attribution. The performance decreases demonstrate that the integration of multi-granularity representations significantly enhances the precise comprehension of semantics. 4.6 Further Analysis Effect of Top-K. To mitigate the interference of noisy dimensions, we have employed the Top-K strategy on the attribution scores to filter out dimensions with relatively low significance. In this section, we explore the impact of varying K values. Specifically, we conducted experiments on the Rest14 and MAMs datasets, testing a range of K values from 100 to 300. The results, illustrated 100 120 140 160 180 200 220 240 260 280 /uni0000002e/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000052/uni00000053/uni0000002e86.486.686.887.087.287.487.687.8/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n80.881.281.682.082.482.8\n/uni00000029/uni00000014/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000029/uni00000014/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048Figure 3: Accuracy (%) and macro-F1 value (%) on\nRest14 dataset with different K values in Top-K strategy.\n100 120 140 160 180 200 220 240 260 280 /uni0000002e/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000037/uni00000052/uni00000053/uni0000002e84.484.684.885.085.285.485.685.8/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n84.284.484.684.885.085.285.4\n/uni00000029/uni00000014/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000029/uni00000014/uni00000003/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000Figure 4: Accuracy (%) and macro-F1 value (%) on\nMAMs dataset with different K values in Top-K strat-\negy.\nin Figure 3 and Figure 4 show that accuracy and macro-F1 scores on both datasets initially improve as K increases, but then plateau or slightly decrease. We conjecture that low K values fail to adequately capture attribution knowledge, while high K values may introduce noise. Thus, selecting an appropriate K value is crucial for optimal performance. Effect of Attribution Steps. To investigate how the number of attribution steps influences performance, we evaluated our DMAN with varying steps on the Rest14, Lap14, and MAMs datasets. Notably, to maintain compatibility with our framework, the number of GCN layers must increase correspondingly as the number of attribution steps increases. As depicted in Figure 5, our model achieves optimal performance with 2 steps, while performance significantly declines with further in7\n/uni00000035/uni00000048/uni00000056/uni00000057/uni00000014/uni00000017 /uni0000002f/uni00000044/uni00000053/uni00000014/uni00000017 /uni00000030/uni00000024/uni00000030/uni00000056/uni0000001a/uni00000017/uni0000001a/uni00000019/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni00000024/uni00000046/uni00000046/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000003/uni00000014/uni00000010/uni00000056/uni00000057/uni00000048/uni00000/uni00000015/uni00000010/uni00000056/uni00000057/uni00000048/uni00000053/uni00000016/uni00000010/uni00000056/uni00000057/uni00000048/uni00000/uni00000017/uni00000010/uni00000056/uni00000057/uni00000048/uni00000053Figure 5: Accuracy (%) of DMAN on Rest14, Lap14\nand MAMs datasets with different attribution steps.\n/uni00000037/uni0000004b/uni00000048 /uni00000053/uni00000055/uni0000004c/uni00000046/uni00000048 /uni0000004c/uni00000056/uni00000055/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048 /uni00000044/uni0000004f/uni00000057/uni0000004b/uni00000052/uni00000058/uni0000004a/uni0000004b /uni00000057/uni0000004b/uni00000048 /uni00000056/uni00000048/uni00000055/uni00000059/uni0000004c/uni00000046/uni00000048 /uni0000004c/uni00000056 /uni00000053/uni00000052/uni00000052/uni00000/uni00000056/uni00000046/uni00000052/uni00000055/uni00000/uni00000014/uni00000010/uni00000056/uni00000057/uni00000048/uni00000/uni00000015/uni00000010/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000015/uni0000001c /uni00000013/uni00000011/uni00000016/uni0000001b /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000016/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000015/uni00000019 /uni00000013/uni00000011/uni00000016/uni00000/uni00000013/uni00000011/uni00000014/uni00000019 /uni00000013/uni00000011/uni00000016/uni0000001a /uni00000013/uni00000011/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000019/uni0000001b /uni00000013/uni00000011/uni00000015/uni00000016 /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000015/uni00000/uni00000013/uni00000011/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000017/uni00000017 /uni00000013/uni00000011/uni00000015/uni00000016 /uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000014/uni0000001a /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000015/uni00000015 /uni00000013/uni00000011/uni00000014/uni0000001b /uni00000013/uni00000011/uni00000014/uni0000001b\n/uni00000056/uni00000046/uni00000052/uni00000055/uni00000/uni00000014/uni00000010/uni00000056/uni00000057/uni00000048/uni00000/uni00000015/uni00000010/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000014/uni00000019 /uni00000013/uni00000011/uni00000014/uni0000001b /uni00000013/uni00000011/uni00000016/uni00000014 /uni00000013/uni00000011/uni00000013/uni0000001c /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000016/uni0000001a\n/uni00000013/uni00000011/uni00000013/uni00000016/uni00000014 /uni00000013/uni00000011/uni00000014/uni00000014 /uni00000013/uni00000011/uni00000014/uni0000001a /uni00000013/uni00000011/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000016/uni0000001a /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016/uni00000017 /uni00000013/uni00000011/uni00000015/uni00000016 /uni00000013/uni00000011/uni00000019/uni00000/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni0000001a /uni00000013/uni00000011/uni00000014/uni00000016 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000017/uni00000015 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni0000001a/uni00000/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000(a) visualization for price .\n/uni00000037/uni0000004b/uni00000048 /uni00000053/uni00000055/uni0000004c/uni00000046/uni00000048 /uni0000004c/uni00000056/uni00000055/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048 /uni00000044/uni0000004f/uni00000057/uni0000004b/uni00000052/uni00000058/uni0000004a/uni0000004b /uni00000057/uni0000004b/uni00000048 /uni00000056/uni00000048/uni00000055/uni00000059/uni0000004c/uni00000046/uni00000048 /uni0000004c/uni00000056 /uni00000053/uni00000052/uni00000052/uni00000/uni00000056/uni00000046/uni00000052/uni00000055/uni00000/uni00000014/uni00000010/uni00000056/uni00000057/uni00000048/uni00000/uni00000015/uni00000010/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000015/uni0000001c /uni00000013/uni00000011/uni00000016/uni0000001b /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000016/uni00000014 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000015/uni00000019 /uni00000013/uni00000011/uni00000016/uni00000/uni00000013/uni00000011/uni00000014/uni00000019 /uni00000013/uni00000011/uni00000016/uni0000001a /uni00000013/uni00000011/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000019/uni0000001b /uni00000013/uni00000011/uni00000015/uni00000016 /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000015/uni00000/uni00000013/uni00000011/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000017/uni00000017 /uni00000013/uni00000011/uni00000015/uni00000016 /uni00000013/uni00000011/uni0000001b/uni00000018 /uni00000013/uni00000011/uni00000014/uni0000001a /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000015/uni00000015 /uni00000013/uni00000011/uni00000014/uni0000001b /uni00000013/uni00000011/uni00000014/uni0000001b\n/uni00000056/uni00000046/uni00000052/uni00000055/uni00000/uni00000014/uni00000010/uni00000056/uni00000057/uni00000048/uni00000/uni00000015/uni00000010/uni00000056/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000014/uni00000019 /uni00000013/uni00000011/uni00000014/uni0000001b /uni00000013/uni00000011/uni00000016/uni00000014 /uni00000013/uni00000011/uni00000013/uni0000001c /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000014/uni0000001c /uni00000013/uni00000011/uni00000014/uni00000015 /uni00000013/uni00000011/uni00000016/uni0000001a\n/uni00000013/uni00000011/uni00000013/uni00000016/uni00000014 /uni00000013/uni00000011/uni00000014/uni00000014 /uni00000013/uni00000011/uni00000014/uni0000001a /uni00000013/uni00000011/uni00000015/uni00000017 /uni00000013/uni00000011/uni00000016/uni0000001a /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000016/uni00000017 /uni00000013/uni00000011/uni00000015/uni00000016 /uni00000013/uni00000011/uni00000019/uni00000/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni0000001a /uni00000013/uni00000011/uni00000014/uni00000016 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000017/uni00000015 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni0000001a/uni00000/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000(b) visualization for service .\nFigure 6: Visualization of attention scores and multi-\nstep attribution scores on two aspects, price andservice .\nscore denotes attention scores, 1-step and 2-step denote\nattribution scores of 1st and 2nd layers.\ncreases in the number of layers. We attribute this phenomenon to two primary factors. Firstly, when the number of GCN layers becomes excessive, node representations face the issue of oversmoothing, leading to vanishing gradients and information redundancy. Secondly, due to the relatively small size of ABSA datasets, the network is prone to overfitting as the model complexity increases, which results in a situation where gradients convey less effective attribution knowledge. 4.7 Visualization on Attribution To demonstrate the effectiveness of attribution analysis in our approach, we selected samples with multiple aspects and visualized the attention scores and multi-step attribution scores in Figure 6 (a) and (b). Specifically, given the sentence “ The price is reasonable although the service is poor ” with two aspects, “ price ” and “ service ”, attention scores are shown to be susceptible to noise within the sentence, often assigning relatively high scores to irrelevant words (e.g., “ is poor ” for “ price ”). In contrast, our proposed DMAN more accurately identifies aspect-related opinion words (e.g., “ rea00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n80.080.480.881.281.682.082.482.8/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni0000002f/uni00000044/uni00000053/uni00000014/uni0000084.084.384.684.985.285.585.886.1\n/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000030/uni00000024/uni00000030/uni00000056Figure 7: Accuracy (%) on Lap14 and MAMs datasets\nwith different αvalues for granularity fusion.\nsonable ” for “ price ”, “poor ” for “ service ”). Furthermore, the progression of attribution scores from the first to the second step illustrates the process of semantic understanding, clearly indicating the effectiveness and interpretability of our model in dynamically capturing aspect-related contexts. 4.8 Impact of αin Multi-granularity In the Multi-granularity Attribution Module, we introduce αto balance token granularity and span granularity. To investigate their impact on model performance, we conducted experiments with different values of αon Lap14 and MAMs datasets. As illustrated in Figure 7, the performance improves with increasing αvalue and reaches a peak, and then declines. This suggests that effectively integrating multi-granularity representations can provide a more comprehensive understanding of sentence semantics. Specifically, considering that ABSA is a fine-grained classification task, we do not employ sentence-level representations. 5 Conclusion In this paper, we propose a novel Dynamic Multigranualarity Attribution Network (DMAN) for the ABSA task, which is different from traditional models that rely on attention scores. Specifically, we first leverage Integrated Gradients to extract multistep attribution during semantic comprehension, and Top-K strategy is adopted to filter out unimportant dimensions. We then consider multiple granularities of semantic concepts, fusing attribution representations from both token-level and span-level. Finally, we integrate these attribution values with dependency trees to dynamically capture relevant syntactic knowledge, thereby enhancing semantic understanding for sentiment classification. Extensive experiments on five public datasets demonstrate the effectiveness of our proposed DMAN. 8\nLimitations One of the primary limitations of our approach is that our method does not always provide accurate attributions when addressing sentences with overly complex content and structure. Actually, this is a common limitation among most ABSA methods. Additionally, Our framework comprises two components: attribution analysis and sentiment classification. The complexity of the model structure results in increased computational costs during training process. Ethics Statement Our work will not cause ethical issues, and the datasets we use are publicly available. Additionally, we do not involve the collection or use of any private information. 594",
        "aspect_review": "The paper provides a thorough analysis of the limitations of existing attention-based methods for ABSA and introduces the novel concept of attribution analysis to address these issues. The proposed Dynamic Multi-granularity Attribution Network (DMAN) is a novel approach that integrates Integrated Gradients for dynamic attribution extraction, multi-granularity representation, and syntactic information, showcasing strong technical depth and rigor. The paper is well-organized, clearly explaining the motivation, methodology, and experimental results. The figures and tables effectively illustrate the model architecture and experimental outcomes. Extensive experiments on five benchmark datasets demonstrate the effectiveness and interpretability of the proposed method, consistently outperforming existing baselines.\n\nWhile the motivation for using attribution analysis is well-justified, the paper could benefit from a more detailed explanation of how the dynamic changes in syntactic information are captured. The paper presents strong results, but additional experiments on more diverse datasets could further validate the model's robustness. The datasets used are standard in the field, but exploring more complex or diverse datasets could enhance the perceived impact of the findings. Although the paper compares against several baselines, including more recent models could provide a more comprehensive evaluation. The contribution is significant, but the paper could emphasize more on the potential practical applications and impact of the proposed method. Minor improvements in figure formatting and clarity of some descriptions could enhance the overall readability. The complexity of the model might increase computational costs, which could be a concern for practical deployment.",
        "general_review": "The paper introduces a novel Dynamic Multi-granularity Attribution Network (DMAN) for aspect-based sentiment analysis, which is a significant advancement in the field. The use of Integrated Gradients to dynamically extract importance scores for tokens is a notable novelty, providing data-specific insights for reasoning sentiment polarity. The methodology is technically sound and rigorous, with a comprehensive experimental validation across five benchmark datasets, demonstrating the effectiveness and interpretability of the proposed approach. The integration of attribution scores with syntactic information is particularly insightful, offering a more accurate capture of relationships between aspects and their contexts. The paper is well-organized and clearly articulates the contributions and methodology, making it accessible to readers.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the multi-granularity attribution module. 2. Some small details regarding hyperparameters and implementation choices, such as the selection of K value in the Top-K strategy, could be elaborated further. 3. The figures, while informative, could be slightly improved in formatting to enhance readability. 4. There is a slight need for clarification on the availability of the code for reproducibility purposes."
    },
    "38": {
        "paper": "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via\nInfuser-Guided Knowledge Integration\nAnonymous ACL submission\nAbstract\nThough Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domainspecific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs demonstrate that InfuserKI can effectively acquire new knowledge and outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge forgetting. 1 Introdution Large Language Models (LLMs) have revolutionized fields such as Question Answering (QA), dialogue, and information retrieval, demonstrating impressive capabilities in various language tasks (Touvron et al., 2023a,b). However, LLMs can generate misleading or inaccurate texts, especially in knowledge-intensive tasks such as open-domain QA (Kwiatkowski et al., 2019), due to lack of domain knowledge and catastrophic forgetting after fine-tuning (Zhai et al., 2024; Li et al., 2022). Updating and customizing LLMs with domain knowledge integration is highly valued across applications. For example, companies might personalize 040models with specific product knowledge, while hospitals could tailor models using their case data. Knowledge Graphs (KGs) serve as an ideal source for enhancing domain-specific knowledge due to their structured and quantifiable knowledge units. To leverage this knowledge, several strategies have been developed. Generally, these include instruction tuning LLMs with extensive knowledge entity explanations (Wu et al., 2023), creating triplet-based pre-training tasks (Zhang et al., 2022; Qin et al., 2021; Wang et al., 2021), employing KGs as external sources for retrieval (Sridhar and Yang, 2022; Yu et al., 2022), and directly using parameter-efficient fine-tuning (PEFT) methods such as LoRA (Hu et al., 2021) and adapters (Houlsby et al., 2019) or model editing (ME) methods such as T-Patcher (Huang et al., 2023) to inject knowledge in a triplet-to-text way (Meng et al., 2021; Emelin et al., 2022; Dong et al., 2022). However, pre-training or fine-tuning LLMs with the entire KGs is not only time-consuming but also leads to data inefficiencies, especially when models relearn knowledge they already have. To address this issue, we focus on integrating new, previously unknown knowledge only. This precise focus, however, introduces the risk of catastrophic forgetting, where the addition of new knowledge may affect existing knowledge. Fig. 1 (a) and (b) show an illustrative comparison between a vanilla LLM and the fine-tuned version, revealing direct fine-tuning leads to forgetting previously known data, whereas Fig. 1 (c) depicts an expected outcome of seamlessly injecting new knowledge without losing existing information. Thus, we pose a novel research question: How can we efficiently integrate new knowledge from domain-specific KGs into LLMs while preventing catastrophic forgetting? In this work, we propose the Infuser-guided Knowledge Integration ( InfuserKI ) framework, specifically designed for integrating domainspecific knowledge from KGs into LLMs. Inspired 1\nFigure 1: An illustrative comparison among (a) Vanilla\nLLM, (b) Fine-Tuned LLM, and (c) our Knowledge-\nInfused LLM.\nby (Azaria and Mitchell, 2023), which shows the LLM’s internal states can indicate the truthfulness of its own generated sentences, our framework features an infusing mechanism that checks whether LLMs possess current knowledge. This enables the adaptive selection of supplementary information for known and unknown knowledge, effectively reducing the impact on existing knowledge and mitigating knowledge forgetting. Moreover, InfuserKI utilizes knowledge adapters to encode new knowledge while preserving the original model parameters. To inject new knowledge only, the InfuserKI framework begins by detecting knowledge unknown to LLMs. Following (Zhao et al., 2023; Seyler et al., 2017), we then generate a knowledge statement andmultiple-choice questions for a knowledge triplet < h, r, t > using predefined relational templates, with an example in Fig. 3. To enhance the generality of the newly integrated knowledge, InfuserKI introduces a relation classification task aimed at improving the linguistic representations learned by the adapters, predicting relations within knowledge statements based on the adapter outputs on head and tail entities. This not only solidifies the integration of new knowledge but also helps the robustness of generalizing it to unseen situations. Our main contributions are: 1.We study a novel problem of effectively integrating unknown knowledge from KGs into LLMs without affecting existing knowledge. 2.We propose a new knowledge integration framework InfuserKI, which enables the adaptive selection of known and unknown knowledge to be integrated into LLM, effectively mitigating knowledge forgetting. 3.Extensive evaluations on UMLS and MetaQA datasets reveal InfuserKI’s effective knowledge integration with less forgetting, sustained performance on large-scale data, and superior generality across unseen templates and downstream tasks. 1222 Related Work Knowledge Integration LLMs often produce seemingly accurate but incorrect answers due to missing knowledge. Addressing this, knowledge integration (KI) into LLMs has become popular. KGs, which capture wide or domain-specific knowledge, serve as an ideal option due to their structured and quantifiable knowledge units. KI from KGs usually occurs during pre-training or fine-tuning. For example, ERNIE (Sun et al., 2019) injects KG’s embeddings, such as TransE (Fan et al., 2014), into models using an entity-token alignment masking loss. However, retraining is time-consuming. In fine-tuning, methods including JointLK (Sun et al., 2022) and GreaseLM (Zhang et al., 2021) apply graph neural networks to model knowledge subgraphs, relying on KGs until inference. Fully fine-tuning models such as PMC-LLaMa (Wu et al., 2023) is computationally costly; therefore PEFT methods, for instance, LoRA (Hu et al., 2021) and Adapters (Houlsby et al., 2019), are more feasible. Based on these works, MoP (Meng et al., 2021), K-Adapter (Wang et al., 2021), and KB-adapters (Emelin et al., 2022) inject knowledge directly into model parameters but risk catastrophic forgetting of unrelated knowledge (Meng et al., 2022b). Thus, we focus on adapter-based integration that minimizes the impact on unrelated knowledge. Model Editing Model Editing (ME) for LLMs falls into two categories: gradient-based and extension-based. Gradient-based methods, as described by Dai et al. (2022), modify specific weights related to knowledge edits. ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b) take this further by updating entire Feedforward Network (FFN) layers to enhance model editing. These methods, however, are limited in the number of edits or may require considerable time for execution. On the other hand, extension-based methods add new parameters to correct inaccurate information. CALINET (Dong et al., 2022) and T-Patcher (Huang et al., 2023) incorporate memory slots or trainable \"patches\" into final FFN outputs. GRACE (Hartvigsen et al., 2023) employs a keyvalue adapter with a deferral mechanism for the selective use of knowledge based on input. However, the adapter-based modules positioned in top transformer layers are designed to calibrate false facts. Instead, our method aims to infuse new knowledge by placing adapters throughout transformer layers. 2\nCatastrophic Forgetting Catastrophic forgetting occurs when learning new information causes a drastic loss of previously learned knowledge (Ratcliff, 1990). This becomes particularly evident in sequential inter-task learning, where acquiring new task knowledge leads to the forgetting of earlier task knowledge (McCloskey and Cohen, 1989). To tackle this, various methods are developed. Xuhong et al. (2018) applies regularization constraints to minimize parameter changes when learning new tasks. Elastic Weight Consolidation incorporates the Hessian matrix into parameter regularization to reduce forgetting (Kirkpatrick et al., 2017). Replay-based methods sample original training examples to aid memory (Lopez-Paz and Ranzato, 2017). The technique of knowledge distillation aligns the predictions of a fine-tuned model with those of the model before fine-tuning (Buzzega et al., 2020). PEFT also mitigates forgetting. For instance, LoRA (Hu et al., 2021) uses lowrank matrices for weight modifications while keeping pre-trained parameters frozen, achieving performance comparable to full fine-tuning. However, these solutions focus on sequential inter-task transfer learning. Our focus shifts to intra-task knowledge forgetting, where integrating new knowledge leads to the potential loss of previously existing knowledge. 3 Proposed Framework - InfuserKI 3.1 Overview The objective of our method is to leverage domain knowledge from KGs to enhance LLMs for knowledge-intensive tasks. Specifically, given an LLM pθ∈Pand a set of knowledge triplets T ∈T, our goal is to fine-tune the LLM pθintop′\nθ, incorporating previously unknown knowledge Tunk without affecting existing knowledge Tknown . For efficiency, we only inject knowledge that is unknown to the LLM as: FKI:P×T→P p′\nθ=fKI(pθ,Tunk) The core design of our InfuserKI framework comprises two steps: knowledge detection and knowledge integration, as illustrated in Fig. 3. To be specific, we first detect previously unknown knowledge by feeding questions derived from knowledge triplets to the LLMs. Upon identifying a set of unknown knowledge, we employ the knowledge adapter, which is parallel to the original transformer layer and trained to store new knowledge. The 222core of our framework, the knowledge Infuser , is designed to strategically determine whether new knowledge from the knowledge adapter should be engaged. Throughout this process, we only finetune the knowledge adapter and the Infuser while keeping the original transformer parameters fixed. 3.2 Knowledge Detection Given the inefficiency of fine-tuning LLMs on entire graphs, we aim to identify and integrate only the LLMs’ unknown knowledge. To overcome the difficulty of evaluating open-ended questions, we convert triplets into multiple-choice questions (Manakul et al., 2023), allowing for a precise assessment of LLMs’ initial unknown knowledge (N3+N4in Fig. 2). This strategy enables efficient knowledge integration, using multiple-choice training data to enhance domain-specific performance. Multiple-choice Question Generation Given a knowledge triplet, it is transformed into multiplechoice questions and a knowledge statement using relation templates generated by GPT-4. For instance, the triplet <Sutura cranii, has finding site, Acrocephalosyndactyly type 5> is rephrased into the question with golden answer as \"What diagnosis is associated with the finding site of Sutura cranii? Answer: Acrocephalosyndactyly type 5,\" along with a knowledge statement as \"The finding site for Sutura cranii is associated with Acrocephalosyndactyly type 5.\" The prompt for generating templates and knowledge evaluation method are detailed in Appendix A.1. 3\n4\n2\n1\nFigure 2: Knowledge Areas in LLMs: Original\n(N1+N2), Post-Fine-Tuning ( N1+N3), Forgotten ( N2),\nand Failed Integration ( N4).\nUnknown Knowledge Detection With multiplechoice questions, we input them into LLMs. The testing prompts are in Table 6 in Appendix. We use regular expressions to extract the chosen options from the output of LLMs, treating the response as incorrect if no options can be extracted. This helps us detect the LLMs’ known and unknown knowledge. As shown in Fig. 2, the regions labeled N1andN2represent the set of known knowledge, denoted as Tknown , while the regions labeled N3 3\nTransformerTransformerTransformerTransformerStep 2Knowledge Integration via Infuser- Guided Adapters\nDownstream Examples :\n𝑥𝑥𝑒𝑒: What diagnosis is associated with \nthe finding site of Sutura cranii?\n𝑥𝑥𝑒𝑒′: What anomaly affects the Sutura \ncranii ? \n𝑦𝑦𝑒𝑒: Acrocephalosyndactyly type 5. LLM 𝑝𝑝𝜃𝜃with adapters\nKnowledge Statements :\n𝑆𝑆𝑒𝑒: The finding site for Sutura cranii\nis associated with \nAcrocephalosyndactyly type 5. \nKnowledge Triples :\n<ℎ𝑒𝑒,𝑟𝑟𝑒𝑒,𝑡𝑡𝑒𝑒>: <Sutura cranii, has \nfinding site, Acrocephalosyndactyly \ntype 5>summation\nRelation \nClassification \nLossQA Loss\nNext Token \nLoss\n𝐿𝐿×\n𝑀𝑀×Knowledge \nGraphUnknown \nKnowledge\nQAsTest QAs :\n𝑥𝑥𝑒𝑒: What diagnosis is associated with \nthe finding site of Sutura cranii?\n𝑦𝑦𝑒𝑒: Acrocephalosyndactyly type 5. LLM 𝑝𝑝𝜃𝜃\nTransformerTransformerTransformer\nAdapterAdapter\nInfuserInfuserStep 1Knowledge Detection via Question Answering \nInfuser Loss\nFigure 3: Infuser-Guided Knowledge Integration Framework.\nandN4represent the set of unknown knowledge, as Tunk. We then develop a new method to integrate these unknown knowledge into the LLMs without affecting existing knowledge. 3.3 Infuser-Guided Knowledge Integration Next, we detail our Infuser-guided Knowledge Integration method that effectively and efficiently injects unknown knowledge of LLMs. Knowledge Adapter To improve parameter efficiency, we use parallel adapters as extra modules to learn new knowledge, keeping the original LLM parameters unchanged, as shown in Fig. 4. Existing works (Dai et al., 2022; Geva et al., 2021) show that Feed-Forward Network (FFN) layers in transformer-based language models store knowledge effectively. Thus, we add adapters parallel to the last MFFN layers for the entire L layers. For the l-th selected adapter layer where l∈[L−M+ 1, L], we combine the FFN input Hl\nP∈Rn×dwith the output Hl−1\nAfrom the previous adapter layer as: eHl\nA=Hl−1\nA+Hl\nP (1) where nis the length of the LLM input sequence, anddis the hidden dimension. The initial HL−M\nAis set to a vector of all zeros. Following (He et al., 2022), the adapter layer utilizes a down-projection withWdown∈Rd×d′to transform the combined inputeHl\nAinto a lower-dimensional space specified by the bottleneck dimension d′so as to facilitate the learning of new patterns with minimal extra 293space. This is followed by a nonlinear activation function σ, and subsequently, an up-projection is applied with Wup∈Rd′×das: Hl\nA=σ(eHl\nAWdown)Wup (2) Typically, the adapter output directly merges with the original output from the FFN as follows: Hl\nO=Hl\nA+FFN(Hl\nP) (3) Hl\nOis then fed into either the next transformer attention layer or the final linear and softmax layer. However, this approach can overload the LLM with unnecessary information about knowledge it already knows , causing the forgetting issue. Multi-Head \nAttentionAdd & NormAdd & Norm\nFeed ForwardLLM 𝑝𝑝𝜃𝜃\nDownUP\n𝑯𝑯𝑷𝑷𝒍𝒍\n𝑯𝑯𝑨𝑨𝒍𝒍−𝟏𝟏𝑯𝑯𝑨𝑨𝒍𝒍\n�𝐻𝐻𝐴𝐴𝑙𝑙𝑯𝑯𝑷𝑷𝒍𝒍Infuser𝒓𝒓𝒍𝒍𝑯𝑯𝑶𝑶𝒍𝒍……Seq lenAdapter\nFigure 4: Infuser-Guided Knowledge Adapters.\nKnowledge Infuser To ensure that these extra modules do not confuse the LLM about its existing knowledge, we propose an Infuser model to more effectively infuse the knowledge from the knowledge adapter to the LLM. Intuitively, for a given 4\nquestion, the Infuser assesses if the LLM knows the knowledge at hand. If not, the Infuser can fuse more knowledge from Hl\nAto LLM to provide extra information. If the LLM already knows, Hl\nAshould have less impact. Recent work (Azaria and Mitchell, 2023) indicates that checking the LLM’s internal states can determine if it knows the current question, which paves us a way to design the Infuser. Specifically, we derive an infusing score from the input of an FFN sublayer as follows: rl=fIn(Mean (Hl\nP)) (4) where fIndenotes the Infuser module implemented as a multilayer perceptron (MLP) with a sigmoid activation function and the Mean function averages the vector along the sequence length. This allows infusing score rlto be mapped to the range [0,1], indicating how well the LLMs know about the knowledge based on their intermediate states in thel-th FFN layer ( Hl\nP). As a result, the infusing mechanism helps LLMs learn new knowledge without forgetting what they already know. However, it is difficult for the Infuser to recognize existing knowledge if it only encounters new knowledge during fine-tuning. To fix this, we also include a modest quantity of samples representing knowledge the LLMs already have. Before fine-tuning, we first pre-train the Infuser on a binary infusing task with a balanced mix of known and unknown samples. The Infuser loss is a binary cross-entropy loss function as: LIn=Ex,yInh\nBCE(fIn(Hl\nP), yIn)i\n(5) where xis the sample and the infusing label yInis 1 for new knowledge and 0 for previously acquired knowledge. Finally, we obtain an additive filtered adapter vector, which is integrated with the original FFN output: Hl\nO=rlHl\nA+FFN(Hl\nP), (6) which can selectively incorporate knowledge from the adapter into the fixed base model. Objective Function of InfuserKI We employ unknown knowledge identified during the knowledge detection phase to fine-tune both the knowledge adapter and the Infuser. The InfuserKI framework is divided into three phases: Infuser tuning, QA (Question Answering) training, and RC (Relation Classification) training, as illustrated by the 356following objective function: L=\n\nLIn, Infuser Tuning\nLQA, QA Training\nLNTL+λRCLRC,RC Training.(7) In terms of QA training, we use question-based instructions with standard answers as golden responses. The QA loss is akin to the conventional training loss used in transformer-based language models, tailored to adapt instructions within a specific domain: LQA=Ex,y\n1\n|y||y|X\ni=1CE(pθ(·|x, y 1,...,i−1), yi)\n (8) where CE(·,·)denotes the cross-entropy loss function, y=y1, . . . , is the golden output, and pθ(·|x, y 1,...,·,i−1)is the prediction of an LLM. Note that we also incorporate a small set of yes/no QA samples to enhance the model generality to various question types. To boost generality of InfuserKI, we adopt a relation classification task, following Zhao et al. (2023), to enhance our knowledge adapters’ understanding of relational facts. For a given knowledge statement kand its triplet < h, r, t >, we perform mean pooling on the adapter output HL\nAfor the entity mentions, obtaining representations vhandvt. Following Qin et al. (2021), we form a relational representation vr= [vh, vt], treating ras a positive sample and other relations as negatives. The relation classification (RC) loss, employing the InfoNCE loss (Oord et al., 2018), aims to distinguish positive relations from negatives, as shown below: LRC=Ek\u0014\n−logexp(fR\n1(vr)·fR\n2(r)/τ)P\nr′∈Eexp(fR\n1(vr)·fR\n2(r′)/τ)\u0015\n(9) where τacts as a temperature hyperparameter. The functions fR\n1andfR\n2align entity and relation embeddings into a unified dimensional space, respectively, with Edenoting the complete set of relations. Besides that, we also adopt the conventional training loss (i.e. next token loss) used in transformer models: LNTL =Ek\n1\n|k||k|X\ni=1CE(Pθ(ki|k1,...,i−1))\n (10) The training algorithm is detailed in Appendix A.2. To be specific, given an LLM pθand a KG with knowledge triplets < h, r, t >, we generate question-based instructions q, standard answers 5\ny, and knowledge statements k. The training is divided into three stages. Initially, we tune the Infuser using a small set of balanced samples of known and unknown, as per Eq. 5. In the second stage, we fine-tune the model using a QA loss to integrate unknown knowledge, following Eq. 8. In the final stage, we use knowledge statements and triplets to enhance the model generality, according to Eq. 9 and 10. 4 Experiments 4.1 Experimental Setup We evaluate our InfuserKI framework with competitive baselines on two domain KGs and their corresponding downstream tasks in terms of reliability, locality, and generality. Datasets We conduct experiments on a medical KGUMLS (Bodenreider, 2004) with PubMedQA (Jin et al., 2019) and a movie KG MetaQA (Zhang et al., 2018) with MetaQA-1HopQA as the downstream task respectively. The detailed description is in Appendix A.3. Metrics Following (Huang et al., 2023) (see Appendix A.4), as shown in Fig. 2 with areas for various knowledge dynamics, we use the following metrics: (1) Newly-learned Rate (NR) for reliability, calculated by NR=Ex∈N3+N4[pknown (x)] withpknown (x) = 1 for correct answers and 0 for incorrect ones; (2) Remembering Rate (RR) for locality, defined as RR=Ex∈N1+N2[pknown (x)]; (3)F1_T1 and F1_T2 for seen templates to assess reliability and locality and F1_T3 to F1_T5 for unseen templates, with their average, denoted asF1_Unseen , serving to assess generality; and (4)Downstream-Task F1 for the effectiveness of knowledge integration on downstream tasks. Baselines We compare InfuserKI against both PEFT methods and ME techniques. The PEFT baselines include: (i) Prefix Tuning (Li and Liang, 2021) employs learnable prompts in input or intermediate layers; (ii) LoRA (Hu et al., 2021) uses trainable low-rank matrices for selfattention weights while freezing other parameters; (iii)QLoRA (Dettmers et al., 2023) quantizes pretrained models to 4 bits based on LoRA. All PEFT methods are tested with the same mix of unknown and known samples to ensure fairness. The adopted Knowledge Model Editing Methods are: (i) CALINET (Dong et al., 2022) corrects false knowledge by fine-tuning an adapter in a specific FFN 446layer while keeping original model parameters intact; (ii) T-Patcher (Huang et al., 2023) adds a few trainable neurons to the last FFN layer for error correction. Experimental Details We use LLaMa-27B (Touvron et al., 2023a) as our base LLM. Following MoP (Meng et al., 2021), we sample parts of the KG ( 2,500 and25,000 triplets for UMLS, and 2,900 for MetaQA) in our experiments. During fine-tuning, we set the dimensionality d′to10, and positioned the adapters in the last 30layers out of 32. The RC loss temperature is set at τ= 0.7. . Our approach adds approximately 2.5M extra parameters. Using the AdamW optimizer (Loshchilov and Hutter, 2018) with a batch size of 8and a learning rate of 1×e−4, training takes about 30minutes per epoch for UMLS 2.5k and MetaQA, and 4hours for UMLS 25k on 4×A100 GPU servers. We adjust loss weights with λRC= 10 . The PEFT baselines are implemented following LLaMa-Adapter (Zhang et al., 2023) and PEFT (Mangrulkar et al., 2022). 4.2 Results and Analysis Table 1 and 2 show a comparison of our InfuserKI against existing PEFT and ME methods on the UMLS and MetaQA with 2,500and2,900 triplets respectively. We can observe: (1) The performance of Vanilla LLaMa-2-7B underscores a lack of domain-specific knowledge, highlighting its knowledge limitations in specialized domains. (2) Our method outperforms ME baselines such as CALINET and T-Patcher, which focus on correcting existing knowledge by positioning adapters in earlier transformer layers. This emphasis makes them less suited for integrating new knowledge compared to our approach. (3) Compared to PEFT methods such as Prefix Tuning, LoRA, and QLoRA, our method achieves superior locality (RR). This improvement stems from our infusing mechanism’s adaptive selection of supplementary information, which effectively prevents adapters from interfering with previously acquired knowledge. (4) Our method outperforms the TPatcher across all metrics. Although T-Patcher reduces the impact on a minimal number of unrelated samples, it lacks robustness in locality, which our infusing mechanism effectively addresses. (5) Our approach demonstrates better generality on unseen templates and in the downstream tasks PubMedQA/1-HopQA, benefiting from our well6\nReliability Locality Generality\nMethods NR RR F1_T1 F1_T2 F1_T3 F1_T4 F1_T5 F1_Unseen PubMedQA\nLLaMa-2-7B - - 0.41 0.53 0.42 0.50 0.39 0.44 0.38\nCALINET 1.00 0.52 0.81 0.75 0.50 0.68 0.46 0.55 0.46\nT-Patcher 0.73 0.06 0.45 0.71 0.30 0.65 0.32 0.42 0.40\nPrefix Tuning 0.70 0.90 0.78 0.71 0.63 0.54 0.60 0.59 0.44\nLoRA 0.92 0.80 0.87 0.74 0.82 0.72 0.78 0.77 0.47\nQLoRA 0.97 0.88 0.93 0.78 0.79 0.64 0.81 0.75 0.49\nOurs 0.99 0.99 0.99 0.89 0.91 0.82 0.92 0.88 0.58\nTable 1: Comparative results of InfuserKI with PEFT and ME methods on the UMLS 2.5k triplets.\nReliability Locality Generality\nMethods NR RR F1_T1 F1_T2 F1_T3 F1_T4 F1_T5 F1_Unseen 1HopQA\nLLaMa-2-7B - - 0.57 0.45 0.53 0.42 0.52 0.49 0.47\nCALINET 0.97 0.84 0.90 0.74 0.85 0.68 0.85 0.79 0.44\nT-Patcher 0.39 0.75 0.60 0.69 0.57 0.62 0.61 0.81 0.36\nPrefix Tuning 0.12 0.88 0.56 0.53 0.53 0.51 0.53 0.52 0.45\nLoRA 0.90 0.80 0.84 0.79 0.81 0.76 0.82 0.80 0.62\nQLoRA 0.93 0.90 0.91 0.82 0.89 0.80 0.90 0.86 0.69\nOurs 0.99 0.96 0.97 0.88 0.97 0.86 0.94 0.92 0.67\nTable 2: Comparative results of InfuserKI with PEFT and ME methods on the MetaQA KG.\ndesigned relation classification task. Besides, Table 3 reveals our method maintains excellent performance in reliability, locality, and generality when scaling from 2,500 to 25,000 triplets on the UMLS KG, proving its capability in large-scale knowledge integration. In contrast, traditional ME methods show a performance decline at a larger scale, indicating their limitation to small-scale editing. For additional results on more datasets and with more baselines, please refer to Appendices A.5 and A.6. 4.3 Ablation Study To assess the impact of each component in InfuserKI, we compare it against variants without certain parts: (1) InfuserKI-w/o-RL, a variant without the Infuser loss; (2) InfuserKI-w/o-Ro, a variant without the Infuser module; (3) InfuserKI-w/o-RC, which excludes the relationship classification task. In Table 4, we notice: (1) Removing Infuser loss diminishes NR by 10%, indicating the role of infusing loss in distinguishing known from unknown information for effective integration. (2) Excluding the Infuser lowers RR by 7%, emphasizing its importance in minimizing knowledge forgetting. (3) Without the relation classification task, F1_Unseen decreases by 5%, showing its effectiveness in leveraging knowledge triplets to generalize new knowledge integration. 4.4 Impact of Adapter Position To explore the benefits of adapter positions within the transformer architecture, we position adapters InfuserKI-FFN \n(3-32nd)-FFN \n(3-12th)-FFN \n(13-22nd)-FFN \n(23-32nd)-Attn \n(3-32nd)0.20.40.60.81.0\n0.99 0.98\n0.70\n0.150.560.99\n0.93\n0.750.780.88\n0.880.84\n0.57\n0.450.61\nNR\nRR\nF1_UnseenFigure 5: Impact of Adapter Positions on InfuserKI.\nin the 3rd to 12th (bottom), 13th to 22nd (middle), and 23rd to 32nd (top) FFN layers, as well as across the 3rd to 32nd attention layers. Fig. 5 shows that (1) NR diminishes from the bottom to the top layers, indicating that top-layer adapters are less effective for knowledge integration. This could be attributed to the fact that knowledge representations in the upper layers depend on information from the lower layers and any deficiencies in the lower layers can impact the integration of knowledge. This observation aligns with prior studies (Huang et al., 2023; Dong et al., 2022), suggesting that while top layers are better for refining abstract concepts and knowledge correction, bottom layers are more suited for injecting new information; and (2) placing adapters in attention layers proves less effective for new knowledge integration, confirming that FFN layers act as storage for factual knowledge, which also agrees to the findings at (Dai et al., 2022; Geva et al., 2021). 7\nReliability Locality Generality\nMethods NR RR F1_T1 F1_T2 F1_T3 F1_T4 F1_T5 F1_Unseen PubMedQA\nLLaMa-2-7B - - 0.35 0.47 0.36 0.50 0.36 0.41 0.38\nCALINET 0.86 0.44 0.69 0.57 0.66 0.55 0.68 0.63 0.45\nT-Patcher 0.63 0.20 0.45 0.55 0.38 0.53 0.37 0.43 0.43\nPrefix-Tuning 0.82 0.80 0.82 0.59 0.79 0.61 0.77 0.72 0.47\nLoRA 0.96 0.90 0.95 0.62 0.94 0.58 0.91 0.81 0.40\nQLoRA 0.94 0.91 0.93 0.70 0.90 0.69 0.87 0.82 0.45\nOurs 0.99 0.99 0.99 0.83 0.94 0.80 0.96 0.90 0.58\nTable 3: Comparative results of InfuserKI with PEFT and ME methods on the UMLS 25k triplets.\nMethods NR RR F1_Unseen\nInfuserKI 0.99 0.99 0.88\nInfuserKI-w/o-RL 0.89 0.97 0.77\nInfuserKI-w/o-Ro 0.97 0.92 0.87\nInfuserKI-w/o-RC 0.96 0.97 0.83\nTable 4: Ablation study on UMLS-2.5k.\n4.5 Infuser Analysis To delve deeper into the infusing mechanism, we visualize its values on the test set. As shown in Fig. 6, we display the infusing scores for both original known and unknown samples. Our observation is that infusing scores are lower on known samples, helping to block interfering information and thus mitigating knowledge forgetting. 3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nNumber of transformer layers0.00.10.20.30.40.50.60.7Infusing ScoresKnown\nUnknown\nFigure 6: Infusing Scores for Known vs. Unknown\nSamples.\n4.6 Case Study To intuitively understand the effectiveness of our framework, we compare the prediction score distributions over candidate choices from the vanilla LLaMa-2, LoRA, and our InfuserKI in two cases. Fig. 7 (a) shows that LLaMa-2, which initially gives incorrect answers, can provide correct answers after applying our InfuserKI and LoRA. However, LoRA induces forgetting for the second What procedure is performed on the Process Mastoideus ? \n(A) Epithelial debris of mastoid cavity (finding) \n(B) Congenital anomaly of basioccipital bone (disorder) \n(C) Repair of mastoid antrum or cavity (procedure)\n(D) Finding of moistness of mastoid cavity (finding)\nLLaMa : \nA 5e- 5 B 1e-4 \n*C 0.999 D 1e- 4 LoRA : \nA 0.02 B 1e-5 \n*C 0.007  D 0.97 InfuserKI : \nA 3e-6 B 1e-7 \n*C 0.998 D 0.001 What diagnosis is associated with the finding site of Sutura cranii? \n(A) Discharging mastoid cavity (finding) \n(B) Congenital anomaly of basioccipital bone (disorder) \n(C) Swelling over mastoid (finding) \n(D) Overlapping cranial sutures (finding)\nLLaMa : \nA 3e-7 B 0.999 \nC 3e-7   *D 3e- 7 LoRA : \nA 3e-5B 2e-5 \nC 2e-5   *D 0.999 InfuserKI : \nA 1e-6 B 6e-7 \nC 3e-6   *D 0.999 \n(a) LoRA and InfuserKI successfully inject knowledge into LLaMa . \n(b) LoRA forgets knowledge LLaMa knows and InfuserKI remembers.Figure 7: Illustration of Infuser-Guided Knowledge In-\ntegration with less forgetting.\ncase, as depicted in Fig. 7 (b) while InfuserKI retains the knowledge. 5 Conclusion In this study, we tackle a novel problem of integrating new knowledge from KGs into LLMs without affecting existing knowledge. We introduce the Infuser-guided Knowledge Integration framework, designed to selectively add new information to LLMs, minimizing the impact on prior knowledge and preventing catastrophic forgetting. A relation classification task further enhances the model’s generality. Evaluations on UMLS and MetaQA demonstrate InfuserKI’s effectiveness in integrating knowledge with less forgetting, maintaining sustained performance with large-scale data, and exhibiting exceptional generality on unseen templates and downstream tasks. As for future work, we will study methods to test and integrate knowledge into LLMs with multi-hop knowledge triplets. 8\n6 Limitations We note that the effectiveness of our method is contingent upon the base language model’s ability to follow instructions accurately. In scenarios where the underlying model exhibits suboptimal instruction-following capabilities, the integration of knowledge, regardless of its quality, may not significantly improve performance on downstream tasks. Consequently, applying our knowledge integration framework to models with limited instruction-following proficiency presents a considerable challenge. 595",
        "aspect_review": "The paper presents a novel approach to enhancing Large Language Models (LLMs) by integrating domain-specific knowledge graphs, addressing the critical issue of catastrophic forgetting. This motivation is well-justified and provides a clear direction for future research in the field of knowledge integration. The paper includes comprehensive experimental validation across multiple datasets, demonstrating the effectiveness of the proposed InfuserKI framework. It shows significant improvements over state-of-the-art baselines in reducing knowledge forgetting and enhancing generality, which is a strong indicator of the method's practical impact. An extensive ablation study is conducted to evaluate the contribution of each component of the InfuserKI framework. This thorough analysis provides insights into the importance of each part, such as the Infuser model and relation classification task, in achieving the desired outcomes. The methodology is robust, featuring an innovative Infuser-guided knowledge integration mechanism that strategically determines the engagement of new knowledge. This approach effectively mitigates the risk of knowledge forgetting while enhancing the adaptability of LLMs to new information.\n\nThe paper could benefit from minor improvements in the clarity of the methodology section, particularly in explaining the integration process of the Infuser model. While the ablation study is thorough, additional details on the hyperparameters used in the experiments could enhance reproducibility and understanding. The comparison with baseline methods could be expanded to include more diverse datasets, which would further substantiate the general applicability of the proposed framework.",
        "general_review": "The paper presents notable novelty by addressing the novel problem of efficiently integrating unknown knowledge into large language models (LLMs) without affecting existing knowledge. It demonstrates strong technical depth and rigor through the development of the Infuser-Guided Knowledge Integration (InfuserKI) framework, which effectively mitigates knowledge forgetting. The study provides comprehensive experimental validation, showcasing superior performance over state-of-the-art baselines across multiple datasets, and demonstrating the framework's robustness, reliability, and generality. Additionally, the paper has significant practical impact, offering a scalable solution for enhancing LLMs with domain-specific knowledge graphs, which is highly valuable for various real-world applications.\n\n1. The paper could benefit from minor writing or clarity improvements, particularly in sections detailing the experimental setup and results, to enhance readability. 2. Small details regarding hyperparameters or implementation choices, such as the selection of dimensionality and loss weights, could be more explicitly stated to aid reproducibility. 3. Very minor formatting adjustments on figures, such as ensuring consistent labeling and scaling, could improve visual clarity. 4. Slight clarification on code availability would be beneficial, ensuring that readers can easily access and utilize the proposed framework."
    },
    "39": {
        "paper": "WavLLM: Towards Robust and Adaptive Speech Large Language Model\nAnonymous ACL submission\nAbstract Recent advancements in large language models (LLMs) have expanded their scope in natural language processing (NLP) to encompass multimodal functions. However, integrating listening capabilities effectively remains a significant challenge for generalization and complex auditory task execution. In this work, we introduce WavLLM, a robust and adaptive speech large language model featuring dual encoders—a Whisper encoder for semantics and a WavLM encoder for speaker characteristics. Within the two-stage curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks and also apply it to specialized speechquestion-answer (SQA) dataset, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. The codes, models, audio samples, and SQA evaluation set can be accessed at https: //github.com/wavllm/wavllm-anonymous . 1 Introduction Large language models (LLMs) have witnessed a meteoric rise in advancement within the last couple of years, reaching or even exceeding the proficiency of humans in a myriad of natural language processing (NLP) tasks (OpenAI, 2023; Touvron et al., 2023; Anil et al., 2023). With large language models attaining substantial breakthroughs, the focus is increasingly shifting towards the capabilities 043and advancements of multi-modal large language models (MLLMs), which possess the ability to listen (Tang et al., 2024; Deshmukh et al., 2023), speak (Rubenstein et al., 2023; Hao et al., 2023), see (Huang et al., 2023; OpenAI, 2023), and create content (Pan et al., 2023; Brooks et al., 2024). Amidst the broadening scope of abilities, speech stands out as a crucial form of human communication, prompting extensive research to equip large language models (LLMs) with speech perception capabilities (Shu et al., 2023; Wu et al., 2023; Wang et al., 2023; Tang et al., 2024; Chu et al., 2023). Typically, LLMs are augmented with an auxiliary audio encoder designed to preprocess audio signals, transforming them into the same input space as that of the LLMs, enabling them to achieve various speech tasks, such as automatic speech recognition (ASR), speech question answering (SQA), and so on. However, previous research has yet to overcome significant challenges in achieving effective generalization due to two main issues: (1) specialized tasks are highly sensitive to prompt design, resulting in performance degradation when confronted with unseen or complex instructions; (2) there is an absence of speech Chain-of-Thought (CoT) (Wei et al., 2022) capability, which is essential for addressing complex tasks. In this work, we propose a robust and adaptive speech large language model, WavLLM, aiming at enhancing the generalization capabilies, following speech instruction effectively, and processing the given speech in accordance with provided textual prompts, as well as supporting multi-round dialog. Specifically, to distinguish various types of speech information, we utilize a Whisper (Radford et al., 2023) encoder to encode the semantic content of the speech, and a WavLM (Chen et al., 2022) encoder to capture the acoustic information, like unique characteristics of the speaker’s identity. During the model training phase, we develop a curriculum learning method that progressively fine1\ntune LLMs to follow instructions for understanding and processing speech, starting from simple tasks and advancing towards more complex ones. In the initial mixed single-task training stage, we leverage a substantial dataset of synthesized spoken question-answering content generated by GPT-4 and tailored to various speech-centric tasks such as automatic speech recognition (ASR), speechto-text translation (ST), emotion recognition (ER), speaker verification (SV), and so on, to fine-tune the WavLLM with Low Rank Adaptation (LoRA) techniques (Hu et al., 2022). To enhance the generalization on the unseen or complex instructions1, we introduce an advanced multi-task training stage, incorporating a specially constructed prompt-aware multi-task dataset, such as combinations of the elementary tasks. Furthermore, we design a novel prompt-aware LoRA weight adapter for this stage, capable of adaptively tuning the LoRA weights according to varying prompts, thereby improving the model’s generalization capacity and robustness. We evaluate the proposed model on 1) single tasks , including a) universal speech benchmark, including ASR, SV , ER and ST; b) spoken-querybased question answering and English Listening Comprehension test in Chinese National College Entrance Examination, which presents a spoken dialogue, and requires to answer text-based choice questions related to the conversation, and 2) multiple tasks , consisting of c) instruction-independent multi-tasks dataset that combines multiple independent prompts in a single instruction; d) speech CoT evaluation set that decomposes a complex task into multiple sub-tasks. Extensive evaluations demonstrate that our proposed model exhibits robust generalization and CoT capabilities, consistently surpassing strong baselines across a broad spectrum of speech-related tasks. In summary, the contributions of this paper can be categorized as follows: 1) Equipped with a prompt-aware LoRA weight adapter, we introduce a curriculum learning-based training approach that incrementally fine-tunes large language models with robust speech processing and generalization capabilities, beginning with simple tasks and progressing to complex ones. 2) Our model employs a decoupling strategy for speech information, utilizing the Whisper encoder to capture semantic content and the WavLM en1Please find detailed motivations in Section 2.coder for acoustic features, thereby enriching speech representation and improving performance on downstream tasks. 3)WavLLM demonstrates exceptional generalization capabilities when responding to diverse prompts and completing complex tasks. It exhibits impressive capabilities in zero-shot SQA such as English listening comprehension, and shows strong proficiency in CoT-based tasks, delivering performance gains over non-CoT tasks. 2 Related Work The exploration of multi-modal large language models involves the integration of diverse data types including text, images, video, speech, audio, and more. This represents a natural progression from text-based large language models, designed to enable the perception of the world and the creation of content (OpenAI, 2023; Huang et al., 2023; Hao et al., 2023). For instance, Kosmos-1 (Huang et al., 2023) and GPT-4V (OpenAI, 2023) are able to perceive general modalities beyond text, and follow instruction provided by users to process and analyze image inputs. Another research direction focuses on improving the multi-modal generative abilities of language models, enabling them to produce visual content like images or videos, as exemplified by MiniGPT-5 (Zheng et al., 2023) and Sora (Brooks et al., 2024). Related research to this work focuses on speech-enhanced large language models that aim to endow LLMs with the capability to perceive and process speech signal (Zhang et al., 2023; Shu et al., 2023; Wu et al., 2023; Tang et al., 2024; Chu et al., 2023; Wang et al., 2023). Among these studies, SpeechGPT (Zhang et al., 2023) empowers large language models with crossmodal conversational abilities by three-stage training stages, using hidden units as the discrete representation of speech. LLaSM (Shu et al., 2023) builds a large Chinese/English speech language model that can understand and follow instructions, through pre-training and cross-modal instruction fine-tuning stages. BLSP (Wang et al., 2023) bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. SALMONN (Tang et al., 2024), named from a speech audio language music open neural network, boosts large language models with generic hearing abilities with a activation tuning stage by playing with the LoRA scaling factor. Qwen-audio (Chu et al., 2023) scales up audio-language pre-training to cover over 30 tasks and various audio types, including human 2\nspeech, natural sounds, music, and songs. Motivation Previous research on Speech Large Language Models (Speech LLMs) has primarily concentrated on executing a single speech task in response to a given instruction, while the feasibility of using a single instruction to simultaneously complete multiple and complex speech tasks has remained unexplored. The employment of multitask instructions allows for the efficient completion of several tasks at once and improves performance by dividing complex tasks into logical, related subtasks, such as CoT tasks. Such capabilities also suggest the robustness and generalizability of the Speech LLM. Our initial experiments indicate that (1) prior open-source speech LLMs underperformed in multi-task scenarios, demonstrating these models’ limited ability to generalize to complex instructions; (2) reducing the LoRA scaling factor can be beneficial for multi-task instructions, but leads to a substantial degradation of the results of training tasks (Tang et al., 2024), which suggests that single and multiple tasks might benefit from distinct LoRA scaling factors; (3) there is a notable decline in performance when the model encounters unseen or diverse prompts as opposed to seen prompts (3.5% vs. 2.1%, see Section 4.3), when employing various prompts to evaluate the ASR performance of the open-source model. Consequently, we introduce a curriculum learning approach that progresses from simple to complex instruction tasks, propose a prompt-aware LoRA weight adapter which dynamically adjusts the amplitude of the LoRA output according to the instruction, and further enhance the generalization by utilizing a diverse array of prompts generated by GPT-4 across all training tasks. 3 Method The WavLLM is optimized by maximizing the following probability: p(Y|[X,T];Θ) =TYY\nt=0p(yt|[X,T,Y<t];Θ) (1) where XandTare the speech input and text prompt respectively. Y= [y1,y2, ...,yTY]is the target text output. Θdenotes the parameters of WavLLM. The detailed template of WavLLM’s training data can be found in Appendix D. 2313.1 Model Architecture The model architecture of our framework is shown in Figure 1, which consists of speech encoders (i.e., Whisper (Radford et al., 2023) and WavLM (Chen et al., 2022)) as well as modality adapters, a large language model (i.e., LLaMA (Touvron et al., 2023)) and a proposed prompt adapter. Speech Encoders and Modality Adapters In order to extract both the semantic and acoustic information in the speech, we utilize two stateof-the-art speech encoders, namely Whisper and WavLM. Whisper is trained for ASR and ST tasks in a weakly supervised fashion on a massive 680khour speech corpus recorded in diverse conditions, making it well suited for encoding semantic information in speech. WavLM is a predictive based self-supervised learning (SSL) pre-trained model. During its pre-training stage, WavLM mixes each utterance with signals from multiple speakers in the same batch, yet selectively predicts only the targets associated with the utterance’s original speaker. Such training method allows WavLM to better extract speaker-related acoustic information. In our work, the 32-layer transformer-based encoder of Whisper-large-v2 and WavLM-base are utilized. Both modality adapters have three components, including two 1-D convolution layers to downsample and align the output of both encoders within the temporal domain, a down-up bottleneck adapter (Houlsby et al., 2019), and a final linear projector. The semantic adapter receives its input from the Whisper encoder’s output, while the acoustic adapter takes a weighted sum of the hidden states from all layers of WavLM, where the weights are learnable. The outputs of both adapters are concatenated together before feedforwarding into the linear projector. LLM, LoRA and Prompt Adapter Our framework utilizes the LLaMA-2-7B-chat as the LLM backbone, featuring a 32-layer Transformer decoder with an attention dimension of 4096, specifically optimized for dialogue-related use cases. To integrate the speech modality within the LLM, we employ the parameter-efficient fine-tuning method known as LoRA, which is specifically applied to the key, query, value, and output weight matrices within the attention module of the LLaMA. To enable adaptive LoRA scaling factors for different single-task and multiple-task instructions, inspired by adapter layer in (Houlsby et al., 2019), we 3\nAdaptive\nLoRALarge Language Model (LLaMA)\nWhisper\n❄WavLM\n❄Semantic \nAdapter\n🔥 Acoustic\nAdapterLinear\nTokenizer\n❄ Prompt\nAdapter\n🔥\n🔥Trainable\n❄Frozen\n🔥Concat\n🔥\n❄\n 🔥ASR\nMixed Single-Task \nTraining StageAdvanced Multi-\nTask Training \nStageST\nSV ER\nSQQA SQA\nSingle -tasks……II-\ntasks\nMulti-tasks……CoT-\ntasks\n(a) (b)Figure 1: Overview of the proposed WavLLM. The left part (a) is the two-stage curriculum learning. The right part\n(b) is the model architecture. Two speech encoders and adapters with different focuses are utilized, where Whisper\nis used for extracting semantic information, and WavLM for extracting acoustic information. Before being fed to\nthe LLM, these two representations are concatenated together and linearly transformed. Adaptive LoRA approach\nis used for cross-modal efficient fine-tuning with online adaptation, where the prompt adapter is able to generate\nprompt-dependent parameters to adjust the amplitude of LoRA in the second advanced multi-task training stage.\npropose an online adaptation strategy by introducing a down-up prompt-aware LoRA weight adapter (aka. prompt adapter) with attention weights, designed to modulate the effect of LoRA on LLaMA, as shown in Figure 1. Given the text-based prompts Twith length M, we can get the representation t∈ RD×Mwith LLaMA, where Dis the dimension of LLaMA hidden states and t=f(T;ΘLLaMA ). This representation is fed into the prompt adapter to get the LoRA scaling factors, r∈RD×1: r=g(t;Θprompt_adapter ) (2) =Softmax (ID(WAo))⊙oIM (3) o=PuGeLU (Pdt) (4) where WA∈R1×Dis the matrix of attention weights, ID(WAo)∈RD×Mis unnormalized weights for the hidden states o∈RD×M.ID∈ RD×1andIM∈RM×1are the all-ones vectors. Pu∈RD×KandPd∈RK×Dare up-linear projection and down-linear projection layers respectively, and GeLU is the GeLU activation function (Hendrycks and Gimpel, 2016). The hidden states of an attention layer equipped with adaptive LoRA are expressed by: h=W0x+ (BAx )⊙(rIT\nM+N) (5) wherex∈RD×(N+M)is the input of the attention layer from the speech input Xwith the length N and text prompt T.B∈RD×RandA∈RR×Dare the LoRA parameters, W0∈RD×Dis a weight matrix in the attention layer. IM+N∈R(M+N)×1is the all-ones vector. 3113.2 Curriculum Learning In this section, we present the two-stage curriculum-learning (CL) based training method, which facilitates a progression from learning simple data to understanding complex data, thereby enhancing the model’s capacity for generalization. 3.2.1 Mixed Single-Task Training Stage In the first stage, various single-task, cross-modal, speech-text pairing datasets or text-only datasets are utilized to endow the LLM with robust capabilities in speech processing and comprehension. We freeze the parameters of LLM, WavLM, and Whisper encoder, and optimize the modality adapters, a linear layer and LoRA components. Data Construction The first mixed single-task training stage involves various speech tasks, including automatic speech recognition (ASR), speechto-text translation (ST), speaker verification (SV), emotion recognition (ER), spoken-based instruction tuning and text-based instruction tuning (IT) tasks, as well as a large mount of GPT-generated speech question answering (SQA). There are various questions within the SQA tasks, including those related to the speaker and gender, as well as continuation and summary tasks. Concurrently, these tasks draw upon multiple datasets, including LibriSpeech (Panayotov et al., 2015) with English reading speech, AMI (Carletta et al., 2005) with multi-talker meeting recordings, as well as Fisher (Cieri et al., 2004) and Switchboard (Godfrey et al., 1992) corpora with 2-channel English telephony conversations. Examples of the training data and 4\nprompts used to generate data with GPT-4 can be found in the Appendix A.1 and A.3 respectively. The speech audio clips of spoken-based instruction tuning task are generated by using Microsoft Azure text-to-speech API2. The detailed task information about description, data source, and data hours can be found in Appendix F. 3.2.2 Advanced Multi-Task Training Stage Owing to the incongruity between textual and spoken modalities, extensively fine-tuning the model using the LoRA method on a large amount of prompt-repetitive speech-text data, such as ASR and ST tasks, may cause the model to overfit on specific speech tasks, thereby compromising the LLM’s powerful instruction-following capabilities. For instance, the model exhibits subpar performance when handling multi-task instructions, often only managing to accomplish a fraction of the tasks assigned. Specifically, if ASR is included in the tasks, the model might complete only the ASR portion while failing to address the remaining instructions. To this end, we construct a more complex prompt-aware multi-task dataset in the second stage, by integrating various single-task instructions. Multi-task and single-task datasets are utilized together in this training stage. Besides, we noted that simply incorporating more challenging training data may slightly diminish the performance of single-task instructions, such as ASR, when compared to results of the first training phase. Hence we introduce a prompt adapter, as illustrated in Section 3.1, to produce adaptive LoRA scaling factors for different instructions and tasks, and serve as an effective approach to concurrently enhance the model’s generalization capabilities. Data Construction Given a speech audio clip, we combine different task prompts for this audio segment as well as text-based instruction tuning tasks together as instructions. The training target is designed to complete the tasks sequentially and to repeat key parts of each prompt prior to delivering a response. For example, for an utterance in LibriSpeech, ASR, SQA and text-based IT (tIT) tasks can be combined into multi-task dataset. Please refer to Appendix A.2 for specific examples. In our work, a total of 2.9K hours of various multitask data are used, including ER+t-IT, 2https://azure.microsoft.com/en-us/products/ai-\nservices/text-to-speechTable 1: Single-task and multi-task evaluation bench-\nmarks, including tasks, datasets, and metrics. “Acc.”\nstands for accuracy.\nTask Dataset Split Metric\nSingle\n-taskASR LibriSpeechtest-cleanWER (%)test-others\nSTCoV oST2 (Wang et al., 2020)En2De BLEUMUSTC (Di Gangi et al., 2019)\nSV V oxCeleb1 (Nagrani et al., 2017) test set Acc. (%)\nER IEMOCAP (Busso et al., 2008) Session 5 Acc. (%)\nSQQA WikiQA (Yang et al., 2015) test set Acc. (%)\nSQA MuTual (Cui et al., 2020) test set Acc. (%)\nMulti\n-taskII-Task In-house, based on MuTual - Acc., IFR (%)\nCoTIn-house, based on\nGigaword (Graff et al., 2003)- R-1, R-2,\nR-L,\nBERTScoreIn-house, based on\nstory generated by GPT-4-\nASR+t-IT, ST+t-IT, SV+t-IT, SQA+t-IT, ASR+ST, ASR+SQA, ASR+ST+t-IT and ASR+SQA+t-IT combined tasks, which are summarized in Appendix F. 4 Experiments Please find implementation details in Appendix G. 4.1 Evaluation Setup Corresponding to the training methods, two primary levels of testing tasks were evaluated, namely, single-task and multi-task evaluations. The detailed information of the two types of task evaluations are provided in the Table 1. Single-task evaluation consists of ASR, ST, SV , ER, SQA, and spoken-querybased question answering (SQQA). The main difference between SQQA and SQA is that in SQQA the questions are directly in the audio, whereas in SQA the questions are given by text-based instructions. In our work, the single-answer multiplechoice questions of English Listening Comprehension examination (Gaokao) in China are used as the zero-shot SQA task, which gives a short dialogue, a question, and three options. The model is required to choose the correct one from three options. The performance of SQA is not only a measure of the comprehension of the cross-modal speech and textual content, but also serves as an indicator of the model’s generalization capabilities with respect to a diverse array of instructions. In the multi-task evaluation, two distinct types of tasks are tested, both of which are given a speech audio clip: the tasks that consist of independent instructions (II-Task) and the tasks that feature sequentially progressive instructions, which are also known as CoT tasks. Examples of these two tasks can be found in the Appendix B. For II-Task, our 5\nTable 2: Single-task instruction performance of our WavLLM model compared to other open-source speech large\nlanguage models and cascaded Whisper+LLM baseline model. “*” stands for the results from public paper.\nModelsASR ST (En2De)SV ER SQQA SQAtest-clean test-other CoV oST2 MUSTC\nWER↓BLEU↑Acc.↑Acc.↑Acc.↑Acc.↑\nWhisper + LLM 2.7∗5.2∗18.2 11.5 - - 0.78 59.30% (63.50%)\nSALMONN-7B 2.4 5.4 17.1 12.5 0.86 - - 39.95% (40.00%)\nSALMONN-13B 2.1∗4.9∗18.6∗19.5 0.94∗0.69∗0.41∗43.35% (43.35%)\nQwen-Audio-Chat 7B 2.2 5.1 23.2 18.4 0.50 - 0.38 25.50% (54.25%)\nOur WavLLM 7B 2.0 4.8 23.6 21.7 0.91 0.72 0.57 67.55% (67.55%)\nfocus lies on not only the ability to follow instructions, i.e. instruction following rate (IFR)3, but also the correct completion of each instruction. Whereas for CoT tasks, our primary concern is the performance of the final instruction, which will be compared to the performance of one-shot non-CoT based instructions. The multitasking instruction of zero-shot II-tasks includes ASR, SQA, ST and the general knowledge question task. The audio for zero-shot CoT task is generated from the Gigaword (Graff et al., 2003) dataset using Microsoft Azure text-to-speech API, and the target German texts are translated from English summaries of Gigaword dataset4by utilizing GPT-4. The CoT task requires the Speech LLM to complete ASR, summary and translation tasks in turn. In contract, the one-shot non-CoT based instructions require the cross-lingual summarization directly. For openended or target-lack test sets, GPT-4 is utilized to score the outputs, including the accuracy of SQQA and II-task, which is conducted three times and then take the average to minimize the randomness from GPT-4. 4.2 Main Results We compare the performance of WavLLM with other open source text-instruction (chat) based speech LLMs, including SALMONN (Tang et al., 2024) and Qwen-Audio-Chat (Chu et al., 2023), as well as the baseline system that cascades Whisper large-v2 with LLaMA-2-7b-chat, across various single-task and multi-task instructions. Single-task Evaluation As shown in Table 2, for the ASR task, our chat model achieves state-ofthe-art WERs of 2.0% and 4.8% on test-clean and test-other sets of LibriSpeech corpus, surpassing other open-source chat models on the same size (7B). Similar superior performance are observed in ST, SV , ER and SQQA tasks. 3The IFR is scored manually on 10% of the test utterances.\n4Translation directions of ASR+SQA+ST tasks in second\nadvanced training stage are all English to Chinese.The SQA task in our paper is the zero-shot English listening comprehension tests. As shown in column “SQA” of Table 2, two types of accuracy are evaluated: a) the correct option must be explicitly given (the first number); b) answers that are semantically equivalent to the correct option is considered correct (the second number), which are scored by GPT-4 (The scoring instruction can be found in Appendix C.1). The larger the both accuracy, the better the model’s comprehension and generalization capacity, while the smaller the difference between the both accuracy, the better the model’s ability to follow instructions. From the results, we can observe that our WavLLM model consistently surpasses the cascaded Whisper + LLaMA baseline, and other open source speech LLMs (67.55% vs. 25.50-59.30%). Additionally, our WavLLM model supports multiple dialogue scenario, with a representative instance detailed in Appendix E. Multi-task Evaluation As shown in Table 3, despite the optimization of SALMONN through activation tuning, and the fact that Qwen-AudioChat conducts fine-tuning only on audio encoder without impacting LLM by LoRA weights, their performance in following multitasking instructions remains significantly suboptimal. Our final chat model produces a markedly higher instruction-following rate for the II-Task compared to SALMONN and Qwen-Audio-Chat (92.50% vs. 24.25%-57.75%), which suggests the necessity and effectiveness of our advanced multi-task training stage with prompt adapter. From the accuracy based on GPT-4, which further focuses on whether they are completed correctly, similar trend can be observed (62.44% vs. 19.58%-37.99%). The scoring instruction can be found the Appendix C.2. When the model is able to handle multi-task instructions, we aspire to enhance its performance by Chain of Thought (CoT) methodology. Specifically, the CoT based prompt is excepted to give a better performance than one-shot non-CoT based instruc6\nTable 3: Multi-task instruction performance of our WavLLM model compared to other open-source speech LLMs.\nModelsII-tasks CoT (ASR+SUMM+En2De, gigaword) w/o CoT (De_SUMM, gigaword)\nAcc.↑IFR↑R-1↑R-2↑R-L↑BERTScore↑R-1↑R-2↑R-L↑BERTScore↑\nSALMONN-7B 22.49 34.50 11.9 2.4 10.7 66.46 15.0 3.3 13.5 69.50\nSALMONN-13B 19.58 24.25 10.9 2.1 9.8 68.12 14.0 2.9 12.6 69.11\nQwen-Audio-Chat 7B 37.99 57.75 5.9 0.9 5.7 67.62 5.8 0.9 5.3 65.84\nOur WavLLM 7B 62.44 92.50 16.5 4.1 14.7 70.60 15.4 3.8 13.9 70.37\ntions. We list the examples of these two types of prompts in the Appendix B.2. From the results in Table 3, we can draw two conclusions: 1) Our WavLLM model produces the best performance on the CoT-task instructions; 2) Compared with the performance given one-shot non-CoT instructions, our model produces consistent performance improvements on all metrics. Table 4: Model performance with/without advanced\ntraining on multi-task instructions. mixed training and\nadvanced training stand for the first and training stage.\n“BS.” refers to BERTScore (Zhang et al., 2019).\nModelsII-tasksCoT (ASR+SUMM+En2De)\ngigaword story\nAcc.↑IFR↑R-1↑R-2↑R-L↑BS.↑R-1↑R-2↑R-L↑BS.↑\nmixed training 22.92 26.25 14.7 3.3 13.2 69.71 18.0 2.9 13.7 68.61\n+advanced training 62.44 92.50 16.5 4.1 14.7 70.60 24.5 4.8 19.0 72.52\n4.3 Analysis The Effect of Advanced Training Table 4 shows the results of our models after first mixed singletask training stage and second advanced multi-task training stage5. For zero-shot II-tasks, significant enhancement of generalization ability is obtained after advanced training, as evidenced not only by the increased adherence to instructions (92.50% vs. 26.25%) but also by the higher accuracy of each executed instruction (62.44% vs. 22.92%). For cross-lingual summary tasks using CoT based instructions, our advanced multi-task trained model consistently outperforms the first stage model. In addition, we found that the first stage model mainly accomplished the ST task and did not perform the summarization task. To better demonstrate the effectiveness of the second stage, we crafted a long story-based CoT task by GPT-4 where the audio contains a 100-word story, and the target is a 20-word summary in German. In this task, if the model solely focuses on completing the translation, there will be a noticeable discrepancy in length between its output and the target. From the results of this task in Table 4, the second ad5The results of single-task instructions can be found in\nAppendix H. After advanced training, our model produces\ncomparable or even better performance on single-task prompts\ncompared to the first-stage model.vanced multi-task training stage model significantly outperforms the first stage model, up to 65.52% relative improvement on R-2. When compared to SALMONN-7B on story-based CoT task instructions, a similar greater enhancements can be obtained (24.5/4.8/19.0/72.52 vs. 10.6/1.3/7.8/63.90 on R-1, R-2, R-L and BERTScore respectively.). Table 5: Model performance across training stages\nwith/without a prompt adapter on single-task instruc-\ntions. one-stage denotes the model is trained by utilizing\nall single-task and multi-task data simultaneously. two-\nstage (LoRA) stands for two-stage training method with\nonly LoRA technique. “t-c”, “t-o”, “CoV .” and “MU.”\nstand for test-clean, test-other, CoV oST2 and MUSTC.\nModelsASR ST (En2De)SV ER SQQA SQAt-c t-o CoV . MU.\nWER↓BLEU↑Acc.↑Acc.↑Acc.↑Acc.↑\none-stage 2.1 5.0 22.7 21.0 0.88 0.71 0.51 65.35%\ntwo-stage (LoRA) 2.1 5.1 23.3 21.2 0.89 0.71 0.54 63.70%\n+ Prompt Adapter 2.0 4.9 23.6 21.6 0.90 0.72 0.57 65.00%\nThe Effect of Prompt Adapter Despite the fact that data-level curricular learning benefits the performance on complex cross-modal tasks, using the same LoRA parameters between single-task and multi-task instructions may diminish the performance on both instructions. A prompt-aware LoRA weight adapter (prompt adapter) is proposed to address this issue. Comparative experiments are conducted to analyze the effect of prompt adapter during the second advanced multi-task training stage. Additionally, we build a one-stage model trained by combining all data, including both single-task and multi-task data6. From the results of Table 5 and 6, the following conclusions can be drawn. Firstly, the results of two-stage model without a prompt adapter against one-stage model further demonstrate that the twostage curriculum learning based training is effective as evidenced by 1) the comparable performance of single-task instructions; 2) consistent performance improvements on zero-shot II-task and CoT-task prompts. Secondly, incorporating the proposed prompt adapter consistently outperforms the base6Due to the computing resource constraints, only a por-\ntion of the single-task dataset are utilized during the second\nadvanced multi-task training stage in this section.\n7\nASR\nASR_UNSEEN\nASR\nSV\nEMO\nST\nSUMM\nASR\nSV\nEMO\nST\nSUMM\nASR+ST\nASR\nSV\nEMO\nST\nSUMM\nSQAFigure 2: TSNE visualization of the proposed prompt adapter’s outputs. Each point corresponds to a prompt.\nline two-stage model without such module on all single-task and multi-task instructions. Table 6: Model performance across training stages\nwith/without a prompt adapter on multi-task prompts.\nModelsII-tasks CoT (gigaword)\nAcc.↑IFR↑R-1↑R-2↑R-L↑BS.↑\none-stage 59.34 85.50 14.8 3.4 13.2 69.64\ntwo-stage (LoRA) 61.15 90.25 15.8 3.8 14.5 70.42\n+ Prompt Adapter 63.05 92.75 16.5 4.0 14.8 70.75\nThe Effect of WavLM WavLM model has been widely used for speech processing as a foundation model, especially for speaker information extraction. Table 7 shows the single-task instruction performance on models with or without WavLM encoder after the first mixed single-task training stage. Incorporating the weighted sum of all layers in WavLM-base encoder not only brings performance improvements to speaker verification task but also enhances other tasks such as ASR (relative WER reductions of 13.04% and 11.11% on test-clean and test-other) and ST tasks. Table 7: Single-task instruction performance of models\nw or w/o WavLM encoder after the mixed training.\nModelsASR ST (En2De)SV ER SQQA SQAt-c t-o CoV . MU.\nWER↓BLEU↑Acc.↑Acc.↑Acc.↑Acc.↑\nWavLLM 2.0 4.8 23.9 21.9 0.91 0.72 0.55 67.30%\nWavLLM w/o WavLM 2.3 5.4 23.4 21.0 0.89 0.73 0.55 68.55%\nRobustness Analysis In this subsection, the robustness of the speech LLMs is evaluated by comparing the performance between the seen and the unseen prompts on our WavLLM model and SALMONN model7. From the results in Table 8, compared to the SALMONN model, which experienced a decline in performance with unseen or diverse prompts, our WavLLM model does not exhibit any performance degradation with unseen prompts on ASR tasks and even produces performance improvement on the ST task, demonstrating our model’s powerful robustness. Visualization of LoRA Weights In this subsection, TSNE (Van der Maaten and Hinton, 7Various prompts generated by GPT-4 are used as unseen\nprompts.Table 8: Model performance using seen(se.) or unseen\n(unse.) prompts on WavLLM and SALMONN.\nModelsASR (WER↓) ST-CoV oST2 (BLEU↑)\ntest-clean test-other En2De\nse. unse. se. unse. se. unse.\nSALMONN-7B 2.4 81.8 5.4 85.5 17.1 15.9\nSALMONN-13B 2.1 3.5 4.9 8.8 18.6 18.2\nOur WavLLM 7B 2.0 2.0 4.8 4.8 23.4 23.6\n2008) based visualization of the proposed prompt adapter’s output is given in Figure 2. Several trends can be observed: 1) The overlap between two clusters of the seen and unseen ASR prompts implies the generalization of the proposed prompt adapter; 2) the clear discrimination among singletask prompts suggests that the proposed prompt adapter is capable of discerning various singletask instructions and assigning disparate weights to each; 3) Similar strong discrimination between single-task and multi-task instructions is obtained which validates our motivation; 4) The wide distribution of the SQA task with various prompts, illustrates that the prompt adapter can accommodate diverse prompts. 5 Conclusion In this paper, we propose WavLLM, a robust and adaptive speech large language model, which uses LLaMA-2-chat as foundational LLM backbone, and extracts semantic and acoustic information from speech audio utilizing Whisper and WavLM encoders. Utilizing a curriculum learning approach, the proposed WavLLM commences with singletask instruction training in the initial mixed training phase and subsequently expanding our training to encompass additional multi-task prompts in the second advanced phase with the integration of the proposed prompt adapter. Massive experiments demonstrate that our WavLLM model delivers stateof-the-art performance on various speech-related tasks and robust generalization abilities on singletask and multi-task instructions, enhanced by a CoT processing ability that greatly improves its effectiveness in tackling intricate tasks. 8\nLimitations Although WavLLM shows a remarkable proficiency in handling speech-related tasks and impressive cross-modal instruction following and generalization capacity, it also exhibits some specific constraints. Adaptive Use of CoT Our WavLLM model produces performance improvements using CoT based instructions compared to one-shot non-CoT based instructions. However, it lacks the capability to autonomously decompose complex one-shot nonCoT based tasks into CoT based ones. For future work, we are interested in advancing the capability of adaptive use of CoT. This requires WavLLM to determine whether a task can be decomposed into multiple sub-tasks, and then applying the CoT approach accordingly. Broader Applicability Although our WavLLM model focuses primarily on speech in English, it can be readily extended to accommodate multiple languages. Additionally, the WavLLM model excels at processing and comprehending spoken language, yet it lacks the capability to generate speech. We defer the task of expanding WavLLM’s capabilities to synthesize speech to future research. Safety and Ethics Employing continuous speech representations within our WavLLM model may render it more vulnerable to adversarial attacks, potentially undermining the model’s compliance with the HHN criteria (Harmless, Helpful, Honest). This vulnerability merits further investigation for solutions. 662",
        "aspect_review": "The paper introduces a novel methodology by integrating dual encoders (Whisper for semantics and WavLM for speaker characteristics) within a two-stage curriculum learning framework, enhancing the model's generalization capabilities. WavLLM is a groundbreaking approach in the field of speech large language models, introducing a prompt-aware LoRA weight adapter to improve flexibility and adherence to different tasks and instructions. The model is validated on universal speech benchmarks and specialized datasets, demonstrating strong performance across a wide range of speech-related tasks. Experiments show that WavLLM achieves state-of-the-art performance, exhibiting robust generalization capabilities and significant improvements in complex task execution using a Chain-of-Thought approach.\n\nThe paper could benefit from more detailed justification and validation of the choice of specific hyperparameters and implementation details. While the evaluation is comprehensive, including more diverse datasets could further strengthen the validation of the model's generalization capabilities. Some minor improvements in writing clarity and figure formatting could enhance the overall readability of the paper. The paper could provide slightly more detailed explanations of the code availability and usage to facilitate reproducibility.",
        "general_review": "The paper presents a novel and impactful methodology by introducing WavLLM, a robust and adaptive speech large language model that integrates dual encoders for semantic and speaker characteristics. The technical depth is strong, utilizing a two-stage curriculum learning framework and a prompt-aware LoRA weight adapter, demonstrating rigorous experimental validation with state-of-the-art performance across a range of speech tasks. The paper's significant practical impact is evident in its robust generalization capabilities and potential applications in complex auditory task execution.\n\n1. Minor improvements in writing clarity could enhance the paper's readability. 2. Details on hyperparameters or implementation specifics could be expanded for better reproducibility. 3. Formatting of figures could be slightly improved for better visual clarity. 4. Clarifications on the availability of code and resources would be beneficial to ensure accessibility for further research."
    },
    "40": {
        "paper": "Words Matter: Reducing Stigma in Online Conversations about Substance\nUse with Large Language Models\nAnonymous ACL submission\nAbstract\nStigma is a barrier to treatment for individuals struggling with substance use disorders (SUD), which leads to significantly lower treatment engagement rates. With only 7% of those affected receiving any form of help, societal stigma not only discourages individuals with SUD from seeking help but isolates them, hindering their recovery journey and perpetuating a cycle of shame and self-doubt. This study investigates how stigma manifests on social media, particularly Reddit, where anonymity can exacerbate discriminatory behaviors. We analyzed over 1.2 million posts, identifying 3,207 that exhibited stigmatizing language towards people who use substances (PWUS). Using Informed and Stylized LLMs, we develop a model for de-stigmatization of these expressions into empathetic language, resulting in 1,649 reformed phrase pairs. Our paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS. Our work not only enhances understanding of stigma’s manifestations online but also provides practical tools for fostering a more supportive digital environment for those affected by SUD. Code and data will be made publicly available upon acceptance. 1 Introduction Every day, people struggling with substance use disorders (SUD) face a pervasive and often hidden enemy: stigma. This stigma, often deeply ingrained in societal attitudes, can act as a significant barrier to treatment and recovery. In fact, only approximately 7% of people living with an SUD receive any form of treatment (Substance Abuse and Mental Health Services Administration, 2023), with stigma reported as a major barrier (Centers for Disease Control and Prevention, 2023). SUD is a critical public health challenge in the US and worldwide, and the substantial stigma associated with 042Type Statement\nOriginalI have no empathy for drug addicts. I had\nfriends and family who have struggled with the\n\"disease\". Everyone knows what happens when you\nstart, and you usually end up dead. Many of\nmy old friends have become addicts and I don’t\nunderstand especially the ones with kids.De-stigmatizedI find it difficult to empathize with\nindividuals facing substance use challenges. I\nhad friends and family who encountered these\ndifficulties. It’s widely acknowledged that\nthere are risks involved from the outset, and\nthe outcomes are often heartbreaking. Several of\nmy old friends have dealt with these challenges,\nand it’s particularly perplexing to me when they\nare parents.\nTable 1: Example of directed stigmatizing language.\nDe-stigmatized version generated with our Informed +\nStylized model using GPT-4 removed stereotypes and\nharmful context while preserving the tone (stigma is in\nred, destigmatized counterparts is in blue).\nthese conditions only exacerbates the problem. Traditional support systems, although beneficial, often remain underutilized due to their perceived inaccessibility or the overwhelming stigma surrounding SUD, thus rendering this topic a societal taboo. Social media platforms like Reddit have emerged as important spaces for community discussions (Bouzoubaa et al., 2023). However, the anonymity provided by these environments sometimes exacerbates stigmas, leading to discrimination. People suffering from SUD often encounter derogatory comments, judgment, or misinformation online (Schomerus et al., 2011), which can reinforce self-stigma and stop them from seeking help. The spread of stigmatizing attitudes on social media can also influence public opinion, further perpetuating the stereotypes and prejudices against those with SUD (McLaren et al., 2023). As a result, despite the potential for support, the digital space can mirror and magnify the very societal stigmas it has the power to dismantle, affecting individuals’ mental health and recovery processes adversely (Matsumoto et al., 2021; McNeil, 2021). The widespread stigma surrounding SUD requires urgent and innovative solutions. Leveraging 1\ntechnology and social media, we can develop empathetic, supportive interventions that fight against this stigma (Rahaman et al., 2023). While research has explored mental health conversations and public perceptions on social media (Robinson et al., 2019), there remains a significant gap in efforts to destigmatize language in these discussions. Addressing this gap is crucial for fostering a more understanding and supportive environment for those affected by SUD. Our work explores this opportunity and examines how stigmatizing language manifests in online communities and what solutions can be applied for de-stigmatizing such narratives (Table 1). Our study focuses on two research questions: -RQ1: How does stigmatizing language manifest in non-drug-related Reddit communities when discussing SUD, and what are the underlying factors that contribute to such expressions? -RQ2: How can we leverage LLMs to effectively de-stigmatize language, and what factors influence the success of this process? To address these research questions, we collected over 1.2 million posts from non-drug-related subreddits, identifying 3,207 posts containing stigmatizing language towards people who use substances (PWUS). Leveraging large language models (LLMs), we developed a framework to characterize stigma based on conceptualization of Link and Phelan (2001) ( labeling, stereotyping, separation, status loss, and discrimination ) and transform them into more empathetic versions, resulting in 1,649 de-stigmatized pairs. Our analysis showed that stimulants and cannabis were the most frequently mentioned substances, with stigma more generally being associated with interpersonal relationships and moral judgments. Human evaluations showed that our Informed + Stylized system using GPT-4 can reduce stigma while preserving the original tone and relevance. Automatic evaluations further confirmed that our approach effectively reduced stigma while maintaining the stylistic and psycholinguistic properties of the original posts. Our work makes several key contributions: (1) public release of a unique dataset of labeled stigmatizing posts; (2) demonstration of frameworks for de-stigmatizing text; and (3) exploration of the linguistic characteristics of stigma expressions towards people who use substances (PWUS) online. Additionally, this study introduces innovative uses of LLMs for generating suggestions to mitigate 118potentially harmful language. 2 Related Work 2.1 Stigma and Language Stigma, a complex social phenomenon, is deeply intertwined with language. The linguistic relativity principle, as described by Whorf (1956), suggests that language shapes our perception of reality, including the formation of stigmatizing views. In the context of substance use experiences (SUE) and SUD, stigma can manifest in multiple forms: selfstigma , often rooted in shame (Luoma et al., 2012); public stigma , negative attitudes and beliefs which lead to discrimination and social exclusion; structural stigma , which limits resources and opportunities, embedded in societal norms and institutional practices (Hatzenbuehler, 2016). Building upon Goffman (2009)’s foundational work, Link and Phelan (2001) conceptualized stigma as the co-occurrence of labeling, stereotyping, separation, status loss, and discrimination. This framework highlights how stigma operates alongside power inequalities, influencing both the individual and society at large. Research has explored the manifestation of stigma in online communities (Nippert et al., 2021), particularly within social media platforms (Clark et al., 2021), revealing both the potential for support and the amplification of existing stigmas, particularly among mental health and opiate-dedicated online communities (Chen et al., 2022; Eschliman et al., 2024). Linguistic analysis has proven valuable in identifying and characterizing stigmatizing language. Dehumanizing labels and biased language can perpetuate negative stereotypes and contribute to discrimination (Giorgi et al., 2023). A recent study by the CDC found that while stigmatizing language in traditional media has decreased over time, its use on social media platforms has increased (McLaren et al., 2023), highlighting the need for targeted interventions in these spaces. The specific linguistic cues that distinguish stigmatizing content can differ between those with lived experience of substance use and those without, particularly regarding language considered “othering” and the use of labels like “addict” (Giorgi et al., 2023). 2.2 LLMs and Social Impact LLMs have shown promise in addressing social issues like hate speech detection (Guo et al., 2023a) and bias mitigation (Schlicht et al., 2024). Recent 2\nresearch demonstrates that LLMs can perform on par with or even surpass benchmark machine learning models in identifying hate speech (Kumarage et al., 2024). Moreover, carefully crafted prompting strategies can leverage the knowledge encoded in LLMs to improve the detection of nuanced and context-dependent forms of hate speech (Guo et al., 2023b). However, the application of LLMs in sensitive domains raises ethical concerns. The “black box” nature of these models can make it difficult to understand their decision-making processes, raising issues of transparency and accountability (Guo et al., 2024). Additionally, biases in training data can be inadvertently perpetuated, leading to discriminatory outcomes (Mei et al., 2023). Addressing these ethical considerations is important for the responsible and equitable use of LLMs in destigmatization efforts. 2.3 De-stigmatization Efforts Language-based interventions, such as the use of person-first language and empathetic communication, have shown promise in reducing stigma related to substance use. Research has demonstrated the impact of specific word choices on perceptions of individuals with SUD (Kelly et al., 2010). (McGinty et al., 2018) proposed a set of communication strategies to reduce stigma, including the use of sympathetic narratives, removing blame, and highlighting structural barriers to treatment. These findings contributed notably as the National Institute on Drug Abuse (NIDA) has also published guidelines for using non-stigmatizing language in discussions of SUD (NIDA, 2023). AI-mediated interventions, particularly those leveraging LLMs, have the potential to scale and automate de-stigmatization efforts. While prior work has focused on text detoxification and bias reduction, in general, (Dale et al., 2021b; Mendelsohn et al., 2020; Pryzant et al., 2020), the specific application to SUD-related stigma remains underexplored. Additionally, (Spata et al., 2024) highlights the importance of using appropriate and well-validated measures to assess the effectiveness of interventions aimed at reducing stigma. Our work builds upon the previous work by introducing a comprehensive computational approach to identify and categorize stigma. Focusing on public stigma, which we refer to as directed stigma throughout the paper, we operationalize Link and Phelan (2001)’s framework, analyzing instances 217of labeling, stereotyping, separation, and discrimination towards PWUS in discussions in non-drugrelated Reddit communities . 3 Data To achieve the study’s objective of addressing stigmatizing language, we specifically focused on nondrug-related subreddits. This choice was made to capture how stigmatizing language manifests externally rather than within communities where members discuss their own experiences with drug use. Within these communities, stigmatizing language is often directed towards oneself (e.g., “No one should hire a junkie like me, I’m useless”) or describes situations where members felt stigmatized (e.g., “My co-workers stopped having lunch with me when they learned I’ve been to rehab twice”) which differs from the external stigmatizing language we aim to address. By focusing on nondrug-related subreddits, we ensure that our analysis targets the perpetuation of harmful stereotypes by those outside the drug-using community. This methodological choice is informed by the need to differentiate between internal and external stigma, as highlighted in the literature on stigma (e.g., Link and Phelan (2001)’s attributes of stigma). Data Collection. To investigate the manifestation of stigmatizing language in non-drug-related online communities, we collected data from four popular subreddits: r/unpopularopinion ,r/offmychest , r/medicine , and r/nursing . The first two subreddits were chosen for their high activity levels, diverse user bases, and relevance to discussions of substance use and SUDs. Recent research has highlighted the prevalence of stigmatizing language within medical professional communities as well on platforms such as Twitter, although the overall use of stigmatizing and de-stigmatizing language was found to be low (Scott Graham et al., 2022). Given the critical role that healthcare professionals play in the lives of individuals with SUD, we included two of the most popular subreddits for healthcare professionals; r/nursing andr/medicine . We collected a total of 3.8 million posts from these subreddits. Table 2 shows the number of posts per subreddit. To ensure data quality, we excluded posts that were removed, deleted, or associated with deleted accounts. Additionally, we filtered out posts where the combined title and body text were less than 10 words to focus on substantive discussions. This resulted in a final dataset of 1.51 3\nSubreddit # Subscribers # Posts Date Range\nr/medicine 478K 116,702 05/2005 - 12/2r/nursing 715K 212,755 12/2009 - 12/2r/offmychest 3.2M 1,607,341 02/2010 - 12/2r/unpopularopinion 4.3M 2,044,463 08/2013 - 12/2Table 2: Selected subreddits and raw #posts\nmillion posts for analysis. 4 Methodology To develop a stigma detection model and destigmatize texts, we first need to filter posts related to substance use. This is followed by detection and de-stigmatization processes. Figure 1 shows our study’s overall pipeline. Each step is detailed in the following sections. 4.1 Developing a Stigma Detection Model 4.1.1 Filtering Substance Use-Related Posts To identify posts containing stigmatizing language related to substance use, we first filtered posts collected from non-drug-related subreddits to find relevant discussions. Drug-related content includes any mention of illicit drugs or drug use (e.g., heroin, cocaine, LSD), prescription drugs that can be abused (e.g., narcotics, benzodiazepines), and other drugs that are not prescription but are also commonly abused (e.g., inhalants, bath salts). We began by manually annotating a random sample of 200 posts to establish a ground truth for relevance. Two annotators independently assessed each post, achieving 100% agreement on the presence or absence of substance use-related content. Given the nuanced nature of language around substance use, including slang and idiomatic expressions, we used LLMs with few-shot prompting to identify posts within the larger dataset. Based on a comprehensive assessment of performance metrics, including precision, recall, F1-score, and estimate time (see Appendix A), we selected GPT3.5 Turbo as the most suitable model for this task. As a result of Task 1, we identified around 33,064 posts containing at least one mention of drugs or drug-related content. Validation Layer. Given the tendency of GPT3.5 to overgeneralize, we implemented a validation layer using GPT-4 Turbo to re-evaluate all posts initially flagged as containing substance use-related content (N = 33,064). To evaluate the effectiveness of this validation layer, we randomly sampled 725 posts from the GPT-3.5 output (252 labeled as drugrelated ( D) and 473 as non-drug-related ( ND)) and 310conducted a manual evaluation. The posts labeled asDby GPT-3.5 were then passed through the GPT4 validation layer. Out of the 252 posts initially labeled as D, 212 were confirmed as Dby GPT-4, resulting in an accuracy of F1 = 0 .86. From the 33,064 posts labeled as Dby GPT-3.5, 16,277 were validated as Dby GPT-4. 4.1.2 Extracting Stigmatizing Language The posts labeled as containing drug content were then labeled for their inclusion of stigmatizing language. Stigmatizing language could be in the form of directed language towards PWUS that perpetuates harmful stereotypes, expressions of internalized stigma (i.e., self-stigma), or illustrations of structural or systemic stigma (e.g., criminal justice towards PWUS in the United States). To do this, we took a random sample of 200 posts from the 16,277 posts labeled Dand manually annotated for the inclusion of stigmatizing language. Any posts that contained directed stigmatizing language were also broken down into four attributes: 1) labeling, 2) stereotyping, 3) loss of power, and 4) discrimination. This process was re-iterated several times until substantial agreement was met ( k= 0.67). The remaining posts were then labeled using GPT4 Turbo using the prompt in Appendix B. Explainability of Stigma Detection. In the pursuit of transparency and interpretability, we incorporated an explanation layer into our stigma detection model. Specifically, when the model identified a post as containing directed stigma towards PWUS, it was prompted to provide a detailed explanation for its classification by identifying the specific instances within the text that corresponded to each of the four elements of stigma outlined by Link and Phelan (2001): labeling, stereotyping, separation, and discrimination, mimicking our annotation process. 4.2 De-Stigmatizing Problematic Language To address and mitigate the impact of stigmatizing language in texts, we used two different LLMs across three different Models. Our objective is to determine which model is most effective at transforming stigmatizing language into expressions that are more empathetic and inclusive. Model 1: Baseline. In the baseline phase, we explored the capabilities of two LLMs in zero-shot de-stigmatization: GPT-4 Turbo and Llama 3-70BInstruct. We provided the models with the original 4\nR e d d i t  D a t a\n( N  =  1 . 5 1 M )P o s t s  w / d r u g  \nm e n t i o n ?\n( N  =  3 3 , 0 6 4 )D a t a  C o l l e c t i o n  a n d  F i l t e r i n gS t i g m a  D e t e c t i o n  +  E x p l a n a t i o n\nG P T - 3 . 5 T\nV a l i d a t e d  \np o s t s  w / d r u g  \nm e n t i o n\n( N  =  1 6 , 2 7 7 )\nG P T - 4 TD i r e c t e d\n( N  =  1 , 9 4 9 )N o  S t i g m a\n( N  =  1 3 , 0 7 0 )L a b e l i n g\nS t e r e o t y p i n g\nS e p a r a t i o n\nD i s c r i m i n a t i o nD e - S t i g m a t i z a t i o n\nG P T - 4 T\nL l a m a 3E v a l u a t i o nD e - s t i g m a t i z e d  \np o s t s  ( 5 0 )+\nF i n a l  M o d e lS t y l i z e d\nG P T - 4 TB e s t  O v e r a l l  Q u a l i t y  G e n e r a t i o n sV a l i d a t i o n\nG P T - 4 TS e l f\n( N  =  1 , 1 9 9 )S t r u c t u r e\n( N  =  5 9 )B a s e l i n e :  Z e r o - s h o t I n f o r m e d :  I n s t r u c t i o n  +  \nE x p l a n a t i o n  +  F e w - s h o t3 . S t y l i z e d :  I n f o r m e d  +  \nS t y l i s t i c  P r o f i l eS e n t e n c e  s t r u c t u r e ,  p a s s i v e  v o i c e ,  \nl e x i c a l  d i v e r s i t y ,  e m o t i o nS t y l i s t i c  P r o f i l eFigure 1: Full de-stigmatization pipeline.\nStigma Type\nSubstance Category Directed Self Structural Total\nStimulants 818 380 20 1Cannabis 515 276 27 Narcotics 501 250 18 Depressants 92 102 6 Hallucinogens 90 68 4 Reversal Agents 38 3 0 41\nDrugs of Concern 7 7 0 14\nSynthetic Cannabinoids 11 3 0 14\nOther 4 3 1 8\nDesigner Drugs 6 0 0 6\nUnspecified 537 475 9 1Table 3: Cross-tabulation of substance categories men-\ntioned in a post by the type of stigmatizing language\nused. Note that multiple substance categories may be\nmentioned in the same post.\nstigmatizing post and instructed them to generate a de-stigmatized version without any additional context or guidance. This approach allowed us to assess the inherent de-stigmatization capabilities of these models in the absence of explicit knowledge or stylistic refinements. Model 2: Informed LLM. Inspired by the principles of “Constitutional AI,” we enhanced the LLM prompts in Phase 2 with explicit instructions, definitions, and explanations related to stigma. Constitutional AI refers to the development and operation of AI models that adhere to the principles and legal standards, ensuring respect for human rights, ethical guidelines, and public accountability. Drawing upon the insights gained from our analysis of stigmatizing language (RQ1), we provided the model with a structured understanding of the four stigma elements (labeling, stereotyping, separation, and discrimination) and their manifestations in the context of substance use. -Labeling : The model was instructed to identify and reword any labeling instances in the post, guided by a definition, explanation, and examples from RQ1 analysis. -Stereotyping, Separation, and Discrimination: The model was tasked with addressing these three interrelated elements of stigma simultaneously. The prompt included definitions 387for each element, examples from RQ1 analysis, and an explanation as to why these elements are harmful to guide the LLM to mitigate these forms of stigma through rephrasing, reframing, or adding context. By incorporating these explicit instructions and structured explanation of stigma, we aimed to guide the LLM in generating de-stigmatized outputs that actively addressed each of the four stigma elements identified in the original post. Model 3: Informed LLM + Stylistic Considerations. Building upon the informed LLM approach of Phase 2, we further refined the de-stigmatization process by incorporating stylistic considerations. We aimed to ensure that the de-stigmatized output not only addressed the harmful content but also maintained the original post’s emotional tone and stylistic features. To achieve this, we employed a combination of techniques: -Emotion Analysis: We used a pre-trained, RoBERTa (Liu et al., 2019) model fine-tuned on the GoEmotions dataset (Demszky et al., 2020) 1, to classify the emotional tone of the original post and instructed the LLM to preserve this tone in the de-stigmatized version. -Punctuation and Syntax: We analyzed the use of punctuation and sentence structure (i.e. sentence length variation) in the original post and encouraged the LLM to replicate these patterns in the output. -Stylistic Elements: Posts were analyzed for phrase style, specifically the measure of textual lexical diversity (MTLD) (McCarthy and Jarvis, 2010) and the use of passive voice, to ensure that the de-stigmatized output maintained the original post’s overall writing style. These elements, plus the explanations, were used to produce de-stigmatized outputs that were less harmful and stylistically congruent with the original post, thereby maintaining the author’s voice 1https://huggingface.co/SamLowe/\nroberta-base-go_emotions\n5\nand reducing the potential for inauthenticity. 4.2.1 Evaluation of De-Stigmatized Posts Human Evaluation. To assess the effectiveness of our six systems (baseline, informed, and informed + stylized for GPT-4 and Llama3), we conducted a human evaluation with five reviewers on a random sample of 110 posts (a total of 660 generated texts). Our reviewers come from a variety of backgrounds, including HCI, NLP, and Social Computing. To evaluate the systems, we instructed our reviewers to analyze the generated text from each model and rank the models based on the overall quality, the extent of de-stigmatization, and the faithfulness of the outputs. Following traditional NLG assessments, quality was evaluated on criteria including naturalness, cohesion, human-likeness, and overall coherence (Howcroft et al., 2020). The assessment of destigmatization was judged based on removing negative or harmful stereotypes, and the systems with the least amount of labeling, stereotyping, separation, status loss, and discrimination. Faithfulness was evaluated based on the amount of transferred information from the original post without unnecessary details (Sai et al., 2022). Comprehensive evaluation guideline is provided in Appendix D. Automatic Evaluation. To further evaluate the stylistic similarity between original posts and their de-stigmatized counterparts generated by our models, we conducted a linguistic analysis using LIWC (Boyd et al., 2022). We then performed a t-test to compare the linguistic features identified in both the original and de-stigmatized texts. Given the unique nature of our task, traditional metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) were deemed unsuitable because the generated text and its original counterparts differ significantly in meaning. Additionally, the absence of pre-existing de-stigmatized versions of these texts prevented us from conducting comparative analyses with an established benchmark. 5 Experimental Results & Analysis 5.1 Characteristics of Stigmatizing Language Mentioned Substances. Out of 16,277 posts discussing drugs, our stigma detection pipeline resulted in 3,207 posts containing stigmatizing language (Figure 1). Of these, 1,949 posts contained directed stigma, 59 represented systemic/structural stigma and 1,199 contained self-stigmatizing language. As shown in Table 3, analysis of stigma- 476tizing posts revealed that stimulants like “meth” (methamphetamine) and “coke” (cocaine) were the most frequently mentioned drug categories, followed by cannabis (“weed”, “pot”) for all types of stigma. Posts that mentioned drug use terms like “drugs”, “high”, or “pills,” but no specific substance were categorized as “Unspecified.” Anatomy of Stigma. To further understand who, didwhat , and whyin the context of stigma towards PWUS in online discussions, we examined representative entities, subject-verb pairs, and topic models. Representative entities and subject-verb pairs reveal the direction of the mentions (who), while entity and substance frequencies highlight the targets of stigma (what). Topic modeling allows us to infer the underlying motivations and contexts of stigmatizing language (why). For this purpose, we used a multifaceted linguistic analysis: we first extracted subject-verb pairs using part of speech tagging in spaCy (Honnibal and Montani, 2020), classified emotions toward these pairs in each post using GoEmotions (Demszky et al., 2020) and RoBERTa (Liu et al., 2019), and performed topic modeling with BERTopic (Grootendorst, 2022) and KeyBERT (Grootendorst, 2020). Within the posts showing directed stigma (Appendix C), we primarily observe expressions of sadness andannoyance , with some neutrality . Notably, interpersonal relationships surface as a key theme, featuring mentions of family members like “sister,” “dad,” and “mother” alongside substances like “cannabis” and “amphetamines.” This aligns well with the overall prevalence of stimulants and cannabis in substance mentions (Table 3). The dominant topic, “Cannabis and Legalization Stigma” centers on these substances, often referred to as “it,” in a neutral tone primarily related to “smoking.” Following closely is “Stigma Toward Interpersonal Relationships,” characterized by expressions of knowledge ( I know ) from the subject “I”directed towards family members, often tinged with sadness . Another notable topic, “Moral Judgments of Others,” reveals annoyance (I hate ) towards individuals like “neighbors,” “homeless,” and “junkies” associated with “heroin” and other drugs. Shifting to self-stigmatizing posts, we find distinct emotional undertones and actions. While interpersonal entities are less prominent compared to directed stigma, these posts feature more action verbs and a wider variety of substances. The primary topic, “Depression around Sobriety,” is 6\nmarked by expressions of possession ( I have ) and state of being ( I am) in relation to depression ,sobriety , and quitting . Disturbingly, another topic reveals a desire foroverdose , specifically mentioning narcotics like “fentanyl,” “dilaudid,” and “tramadol,” alongside benzodiazepines like “Xanax” and “clonazepam,” a combination known to be potentially lethal due to respiratory arrest. Finally, while only a few posts showed structural stigma (N = 59), making it hard to analyze topics, the emotions in these posts were mainly neutral. 5.2 De-Stigmatization with LLMs Human Evaluation. Human evaluation of 110 randomly selected and destigmatized posts is shown in Table 4. Our reviewers found the Informed + Stylized system with GPT-4 the best “Overall Quality” for de-stigmatizing posts and the “Most Faithful”, striking a balance between reducing stigma and maintaining relevance and tone with the original message. Interestingly, the “Most De-stigmatized” system was the Baseline with GPT-4. While this system may be effective at removing overt stigmatizing language, the lack of contextual awareness and stylistic preservation resulted in posts sounding generic, insincere, unfaithful, or disconnected from the original poster’s voice. For example, with the Baseline + GPT-4, this post: “Decriminalizing drugs is a dangerous and dumb idea and shouldn’t be done. what a wonderful idea, let’s give druggies more room and less consequences for doing the hard stuff. when out, people need to keep an eye out to not accidentally step on used needles, crime is up from junkies who do anything to afford their next fix. ” is de-stigmatized to: “Decriminalizing drugs is a complex and controversial topic that merits a thoughtful discussion. There are valid concerns about the potential increase in visibility and accessibility of substances.states that have adopted more lenient policies towards drug possession, see increased public drug use and challenges, such as discarded needles and crime, which worries local residents. ” For practical applications, this could affect the model’s ability to foster genuine empathy and understanding in online communities. Table 1 displays a successful de-stigmatized post using our best model. The revised post is less harmful and stereotypical but maintains the speaker’s tone and overall message. 577Automatic Evaluation. We conducted a stylistic similarity analysis using LIWC to compare original stigmatizing posts with their de-stigmatized versions generated by our top-rated system (Informed + Stylized GPT-4). A pairwise two-way t-test showed no significant differences in means across all LIWC variables between the two sets of posts. While certain categories like bigwords (use of six-letter words or more) and cogproc (cognitive processes) were more common in de-stigmatized posts, the overall psycholinguistic properties were largely maintained. This result is promising as it shows our de-stigmatization approach effectively reduced stigma while preserving the original style and emotional tone, essential for authenticity. 6 Discussion Stigma also stems from personal connections. Our findings showed a complex landscape of stigma within non-drug-related online communities where discussions about substance use often become entangled with interpersonal relationships and ingrained societal biases - particularly towards specific substances, namely stimulants (e.g., methamphetamine) and cannabis (e.g., “weed,” “pot”). The frequent mentions of these substances within a stigmatizing context may reflect societal concerns about their visibility and impact, aligning with our topic modeling results, where the dominant topic in directed stigma is “Cannabis Legalization Stigma.” These findings highlight the role of close relationships (family, friends) in both expressing and experiencing stigma. For instance, within the topic “Interpersonal Stigma,” we observe individuals expressing sadness and using the verb “know” when discussing family members struggling with substance use. This underscores the need for de-stigmatization efforts to extend beyond public forums and into private spheres, as stigma from close social circles can be particularly harmful due to the emotional weight and potential for isolation (Luoma et al., 2012). The online nature of these interactions presents a duality of stigma manifestations that is important to understand when developing any intervention. While anonymity might offer a shield for individuals to express stigmatizing views they might suppress offline, it could also create a space for open dialogue and support. The disinhibition afforded by online platforms could lead to more candid discussions about SUD, potentially challenging stigma 7\nModel LLM Best Overall Quality Most De-Stigmatized Most Faithful\nInformed + Stylized GPT4 37 18 49\nInformed GPT4 24 7 33\nInformed Llama 19 8 16\nInformed + Stylized Llama 13 3 6\nBaseline Llama 9 32 2\nBaseline GPT4 6 40 2\nTable 4: Frequency of evaluation metrics by systems for 110 de-stigmatized posts.\nthrough shared experiences and mutual understanding. However, it may also create a space for misinformed judgments and harmful stereotypes, as anonymity can reduce accountability. When considering de-stigmatization efforts, any digital intervention should consider the social actors in addition to the social constructs (e.g. hospitals, employers). This would be considerably important in collectivist communities (e.g. Indian or Middle Eastern) where stigma towards family members with an SUD (i.e. affiliate stigma ) may prevent families from providing the necessary medical support to their loved ones and ultimately delaying treatment (Corrigan et al., 2006). LLMs can be guided by explanation and stylistic information. In our de-stigmatization efforts, we intentionally avoided providing the LLMs with a rigid definition of “de-stigmatized.” Instead, we adopted a more nuanced approach, drawing inspiration from the principles of \"Constitutional AI\" and prior work on text detoxification and bias reduction using LLMs (Dale et al., 2021a; Mendelsohn et al., 2021; Pryzant et al., 2020). We focused on explaining why specific phrases might be problematic and instructed the model to address these issues, constitutionally, while preserving the original style. For instance, to tackle separation, the LLMs were guided to draw equivalences between individuals with SUD and those without, emphasizing shared humanity. Labeling was addressed by replacing derogatory terms like “junkie” with personcentered language like “person with a substance use disorder,” mitigating the over-generalization tendencies of LLMs. Stereotyping and discrimination were handled by re-framing generalizations and removing any implications of discrimination, promoting a more empathetic understanding of individuals struggling with SUD. Most de-stigmatized does not mean most pragmatic. While the baseline model removes stigmatizing language, it often does so at the expense of nuance and context. For instance, evaluators noted that the baseline model sometimes “terribly 670misunderstood the post,” resulting in generic or insincere responses that failed to capture the original poster’s intent. This highlights the importance of removing stigma and preserving the authenticity and emotional tone of the original message. Our findings emphasize the importance of striking a balance between promoting empathetic language and providing overly refined language, which might trivialize the experiences of individuals with SUD or avoid addressing the root causes of stigma. 7 Conclusion This study investigated the manifestations of stigma towards PWUS in four popular nondrug-related subreddits ( r/unpopularopinion , r/offmychest ,r/nursing ,r/medicine ). We identified 3,207 posts containing one of three main types of stigma (self, structural, and directed). Given the contextual nuance of self and structural stigma, we focused our efforts on de-stigmatizing instances of directed stigma (N = 1,649). Experimenting with three different models and two different LLMs (GPT-4 and Llama), the model that used the conceptualization of stigma (Link and Phelan, 2001), few-shot examples, and the original post’s stylistic profile generated the most faithful and appropriate destigmatized texts. Our exploration of LLM-based de-stigmatization demonstrates the potential of these models to transform harmful language into more empathetic expressions while emphasizing the importance of preserving authenticity and the original poster’s voice. While our focus has been on SUD stigma, the insights and methodologies presented here have broader implications for understanding and addressing stigma related to other marginalized groups. Future work could explore the role of misinformation in perpetuating stigma and leverage external knowledge bases (e.g. DrugBank) to develop more informed and effective de-stigmatization strategies. By integrating these approaches, we can create a more supportive and inclusive online environment for individuals affected by stigma, ultimately promoting understanding, empathy, and recovery. 8\n8 Limitations Our findings primarily apply to English-speaking populations on one specific social media platform, which may not be generalizable to other linguistic or cultural contexts. We selected certain subreddits based on our assessment of relevance, which may have limited the breadth of our data; exploring additional subreddits could potentially provide a more comprehensive view. The performance and accuracy of the models we used, dependent on their training data, may not capture all nuances of stigmatizing language. Despite our ethical considerations, the automated analysis of sensitive topics like SUD carries risks of misinterpretation, necessitating ongoing research and continuous evaluation of ethical challenges in using large language models. 9 Ethics Statement We acknowledge the diversity of perspectives on substance use and advocate for harm reduction strategies. All data was publicly available at the time of collection, and no direct interaction occurred between researchers and users. Our research was exempt from review by our institution’s Internal Review Board (IRB). We adhere to strict data protection measures and have slightly altered any quotes to preserve anonymity and post integrity. Our goal is not to erase personal experiences but to reframe them in less harmful ways, aligned with the original sentiment. The discussions in this paper should not be interpreted to suggest anyone’s lived experience is more valid than another. 744",
        "aspect_review": "The paper presents a novel and comprehensive computational framework for analyzing and de-stigmatizing language related to substance use disorders using large language models. The methodology is robust, utilizing a multi-phase approach that includes data collection, stigma detection, and de-stigmatization processes, which are well-integrated and thoughtfully designed. The study leverages a large and diverse dataset of over 1.2 million posts from Reddit, focusing on non-drug-related subreddits to capture external stigmatizing language. This choice of data allows for a nuanced analysis of how stigma manifests in online communities, providing valuable insights into the linguistic features that propagate stigma.\n\nWhile the paper demonstrates the effectiveness of their de-stigmatization model, the results could benefit from additional quantitative metrics to further validate the impact of the de-stigmatized language on actual user perceptions and behaviors. The paper could improve clarity by providing more detailed explanations of the technical aspects of the models used, particularly in the de-stigmatization process, to aid comprehension for readers less familiar with large language models. Although the methodology is comprehensive, the paper could enhance its rigor by including more diverse linguistic and cultural contexts to ensure the generalizability of the findings. The discussion section could be expanded to provide deeper insights into the implications of the findings, particularly how they might inform future interventions or policy changes aimed at reducing stigma in digital spaces.",
        "general_review": "The paper offers a novel computational framework for analyzing and de-stigmatizing language related to substance use disorders (SUD) on social media, specifically Reddit. It demonstrates strong technical depth by employing large language models (LLMs) like GPT-4 for transforming stigmatizing expressions into empathetic language. The methodology is rigorously validated with over 1.2 million posts analyzed, resulting in a substantial dataset of reformed phrase pairs. The paper's practical impact is significant, providing tools that could foster a more supportive online environment for individuals with SUD. Additionally, the study's findings contribute valuable insights into the linguistic features that propagate stigma.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the methodology section, to enhance understanding. 2. Some details about hyperparameters or implementation specifics of the LLMs used are not fully disclosed, which could aid in reproducibility. 3. The formatting of figures could be slightly improved for better readability. 4. While the code and data are stated to be available upon acceptance, a clearer statement on their availability and any potential restrictions would be beneficial."
    },
    "41": {
        "paper": "Knowledge Editing in Language Models via Adapted Direct Preference\nOptimization\nAnonymous ACL submission\nAbstract Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not require expensive retraining. We propose treating KE as an LLM alignment problem. Toward this goal, we introduce Knowledge Direct Preference Optimization (KDPO), a variation of the Direct Preference Optimization (DPO) that is more effective for knowledge modifications. Our method is based on an online approach that continually updates the knowledge stored in the model. We use the current knowledge as a negative sample and the new knowledge we want to introduce as a positive sample in a process called DPO. We also use teacher-forcing for negative sample generation and optimize using the positive sample, which helps maintain localized changes. We tested our KE method on various datasets and models, comparing it to several cuttingedge methods, with 100 and 500 sequential edits. Additionally, we conducted an ablation study comparing our method to the standard DPO approach. Our experimental results show that our modified DPO method allows for more refined KE, achieving similar or better performance compared to previous methods. 1 Introduction Large language models (LLMs) have achieved remarkable success in various machine learning tasks and are commonly used as foundational models in multiple applications. Training such models (Touvron et al., 2023a,b; Bai et al., 2023; Jiang et al., 2023; Radford et al., 2019) requires substantial computational resources and data. A major challenge with trained LLMs is their potential to generate inaccurate information in response to user queries. This can occur due to flawed training data or the continuously evolving nature of knowledge (Zhang et al., 2023b; Chen and Shu, 2023). 043The increasing popularity of LLMs has highlighted the need for methods to correct factual errors or inaccuracies represented by the models (Augenstein et al., 2023). Given the high cost of training LLMs from scratch, recent research has proposed methods for modifying pre-trained LLMs without requiring complete retraining (Yao et al., 2023; Wang et al., 2023). This process, known as \"Knowledge Editing\" (KE), aims to modify the behavior of pre-trained LLMs to update specific facts without adversely affecting other preexisting knowledge irrelevant to the requested updates (Peng et al., 2023). KE presents significant challenges, such as identifying and correcting factual errors within multibillion parameter LLMs without compromising their overall pre-trained performance. One potential approach for updating an LLM involves naive fine-tuning (Wei et al., 2021), where the parameters of a pre-trained LLM are directly optimized to incorporate new knowledge based on additional data (Peng et al., 2023). However, fine-tuning and even some parameter-efficient fine-tuning (PEFT) methods have drawbacks, including intensive computational requirements, overfitting to the new data, and potential loss of valuable existing knowledge (Wang et al., 2023). In KE, each factual update, such as changing the pre-trained response \"France\" to \"Argentina\" for the question \"Who won the FIFA World Cup in Qatar?\", is considered a single edit. While KE shares similarities with fine-tuning, it differs by focusing on Locality, Fluency, and Portability metrics, as well as edit accuracy (Wang et al., 2023). Those metrics ensure the model is updated with the new knowledge without degrading the general capabilities of the model. More details about each metric will be provided in Section 3. We propose a variation of Direct Preference Optimization (DPO) (Rafailov et al., 2024) for KE. This method aims to reduce the likelihood of the 1\nmodel retaining unwanted knowledge while simultaneously increasing the likelihood of incorporating new desired knowledge. Unlike most other methods, our proposed approach does not necessitate additional parameters, external memory, pretraining, or hypernetwork training. Our contributions are summarized as follows: (1) We propose viewing KE as an LLM alignment problem. (2) We introduce Knowledge Direct Preference Optimization (KDPO), a variation of DPO that is optimized for incremental knowledge modifications. (3) We conduct extensive empirical experiments on few sequential configurations, across four popular KE datasets, and three popular language evaluation benchmarks, involving multiple LLM architectures demonstrating the advantage of our method. (4) We adapt popular datasets for KE to facilitate sequential editing. 2 Related Work 2.1 Large Model Fine Tuning Fine-tuning a large model adapts a pre-trained language model for specific tasks, enhancing its performance on particular datasets. This process aligns the general capabilities of the model with specialized application needs, ensuring more accurate and relevant outputs. However, naive finetuning can require significant computational resources and may deviate substantially from the original model. Zhang et al. (2023a) proposed Adaptive Budget Allocation for Parameter Efficient Fine-Tuning (AdaLoRA), which allocates computational resources based on the importance of weight matrices. Christiano et al. (2017) introduced reinforcement learning from human feedback (RLHF), creating a reward model from human preferences and using reinforcement learning to optimize language model responses. Although effective, RLHF is computationally expensive as it requires training multiple models. DPO (Rafailov et al., 2024) addresses these challenges by eliminating the need for an additional reward model. DPO uses preference data directly, fine-tuning the language model with token probabilities for chosen and rejected answers, simplifying the process and reducing computational demands. 2.2 Knowledge Editing Knowledge editing in LLMs involves updating or modifying information without retraining the 133model from scratch. It aims to correct inaccuracies, incorporate new facts, or remove outdated information. This process requires precise adjustments to ensure consistency and reliability across various contexts and queries. Mitchell et al. (2021) introduced Model Editor Networks with Gradient Decomposition (MEND) for quick editing of pre-trained LLMs. MEND uses auxiliary models to convert fine-tuning gradients into efficient weight updates through low-rank gradient decomposition. Meng et al. (2022a) explored factual storage in GPT models, proposing Rank-One Model Editing (ROME) for precise fact editing with a rank-one MLP update. Later, Meng et al. (2022b) developed Mass-Editing Memory in a Transformer (MEMIT), which scales edits to thousands while preserving model performance. In a recent study, (Zhang et al., 2024) conducted a survey and presented a new approach. They compared their approach to FT-L, which involves fine-tuning a single layer in a feed-forward network (FFN) based on the ROME algorithm (Meng et al., 2022a). They also introduced FT-M to enhance FT-L, aligning with the fine-tuning objective. FT-M trains the same FFN layer as FT-L using cross-entropy loss on the target answer while masking the original text, leading to state-of-theart performance on various datasets. In contrast to previous studies that focused on updating the LLM weights, (Zheng et al., 2023) proposed In-Context Learning for LLM Knowledge Editing (IKE). They demonstrated competitive results without editing any model weights, which could be particularly valuable in scenarios where model access is restricted. 3 Method 3.1 Preliminary and Notations LetD={ci, yi\nw, yi\nl}N\ni=1be a dataset composed of Ntriplets. Each triplet consists of a prompt c, a positive completion yw, and a negative completion yl. Assume that the vocabulary length of the LLM πθisV. This means that πθ(c)corresponds to a vector of length Vand represents the Softmax output of the final layer of the LLM. ylandywcan be decomposed into individual token representations: yw= (tw\n1, . . . , tw\nKw), andyl= (tl\n1, . . . , tl\nKl), where Kw, Klare the sequence lengths. We define, πθ(yl|c)as the probability of seeing some completion ylgiven a prompt c. This can be 2\nFigure 1: Illustration of the two generation methods described in this paper. Words in red are generated based on the\nprevious sequence. In our teacher forcing method, we use the ground truth (in blue) instead of the prediction. This\ngeneration will show gains at the optimization step.\ndecomposed using the chain rule into a product of probabilities: πθ(yl|c) =πθ(tl\nk|c)KlY\nk=2πθ(tl\nk|c, . . . , tl\nk−1), where Kl>2, in cases Kl= 1 we get πθ(tl\nk|c). We will use casclandcwto clarify the conditioned tokens better. Thus, πθ(yl|c)will be represented asπθ(yl|cl). For the remainder of the paper, simplified examples will be presented using words as tokens. 3.2 Knowledge Editing as an Alignment Problem The primary objective of LLM alignment is to train models that are safe, effective, ethical, and nontoxic. LLM alignment is generally performed by finetuning the model using the following objective: max\nπθEc∼D,y∼πθ(y|c)[rϕ(c, y)]−βDKL[πθ∥πref]. This objective aims to maximize the expected reward rϕ(c, y), with rϕbeing a reward model parameterized by ϕ. Based on user defined criterion, the reward model evaluates the quality of the generated text ygiven the context c. In our case, we would like rϕto assign a high reward to the new knowledge we would like to inject into the model. This alignment objective will aid us in successfully editing the model’s knowledge without deviating from the original weights. DPO (Rafailov et al., 2024) showed that it is possible to optimize the same KL-constrained objective without explicitly defining a reward function. Instead, the problem 212is transformed as a maximum likelihood optimization of the distribution πθdirectly, by applying the Bradley-Terry(Bradley and Terry, 1952) model to the objective LDPO(πθ;πref), defined as: −E(x,yw,yl)∼D[logσ(βlogπθ(yw|c)\nπref(yw|c)− βlogπθ(yl|c)\nπref(yl|c))], where σis the Sigmoid activation. The DPO objective offers two key advantages for KE. First, it employs a coupled preference model, which facilitates the parallel uplifting of yw, the new factual knowledge, and the diminishing of yl, the outdated factual knowledge. Second, the reference model knowledge keeps the model weights from drifting too much, which helps preserve the existing factual knowledge and reasoning capabilities embedded within the model. 3.3 Knowledge Direct Preference Optimization In this section, we present our adapted DPO algorithm for KE, which we term Knowledge Direct Preference Optimization (KDPO). This novel approach differs from vanilla DPO in three key aspects. First, instead of using a pre-prepared preferences dataset, we regularly prompt the model to generate its current knowledge and use the output as the dis-preferred completion, yl. Second, yl generation is teacher-forced context using ywas illustrated in Fig. 1. Third, our KDPO optimizes the model using ywteacher-forced context for the ylcompletion, as illustrated in Fig. 2. Our optimization is performed using a dataset 3\nD, which contains Nediting requests. Each request consists of a prompt cand its corresponding new knowledge yw. This can be represented as: D={ci, yi\nw}N\ni=1. We break down the KE process into ncycles, each containing ssteps. At the start of a cycle, the model πθgenerates a greedy completion ylto showcase its current knowledge. Subsequently, we optimize πθoversoptimization steps. Let us further define the context till the k-th token: y<k\nw= (tw\n1, . . . , tw\nk−1)as all tokens in the completion until the kthtoken. 3.4 Generation Step At the start of each cycle, we initiate the model to articulate its existing knowledge, which is the knowledge we want to erase, denoted as yl. In this setting of KE, we can assume the lengths of ywand ylare the same, K:=Kw=Kl. The process of generating the ylcan be expressed as follows: yl=argmax (πθ(cw)). (1) It is important to note that the model does not generate the entire answer ylall at once. Instead, it generates the next token based on the prompt and new knowledge. Let us explain why this approach is beneficial. Suppose we have a single sample, {c, yw= (tw\n1, tw\n2, .., tw\nK)}, we would like to define ylthat will act as the negative sample in our objective (the definition of which will be provided later). We have two options: (i) predicting the entire sequence and (ii) utilizing teacher-forcing with yw. We describe these options below and illustrate them in Fig. 1 using a simple example. Predicting the entire sequence is done by predicting tl\n1=πθ(c), then tl\n2=πθ(c, tl\n1), and continuing iteratively until the last word prediction: tl\nK=πθ(c, tl\n1, . . . , tl\nK−1).tl\nkare affected by both the prompt cand the generated tokens y<k\nl, at least in the first cycles, both reflect the original knowledge of the model. For example, given the prompt c=\"Lionel Messi was born in the city of\", and the knowledge we want to embed is yw=\"Milan, Italy.\" The model predicts tl\n1=\"Rosario\" based on its current knowledge. Thus, both the prompt and tl\n1(since the model knows Rosario is in Argentina) will cause tl\n2to invoke the model’s current knowledge and predict \"Argentina\". New Knowledge Teacher forcing is similar to the former only for the first token, which is affected only by the prompt, tl\n1=πθ(c). The second generated token is affected by the prompt as well as the 292first word in the new knowledge we desire to embed in the model tl\n2=πθ(c, tw\n1). The prediction made by the model changes based on its current context which is different at each stage: c-original knowledge, and tw\n1-part of the new knowledge. Leveraging the previous example, the context for generatingtl\n2is now \"Lionel Messi was born in the city of Milan\". The model’s original knowledge will lean the model toward predicting \"Argentina\", but the \"Milan\" will push the model to generate \"Italy\". Using tw\nk−1instead of tl\nk−1for generating tl\nkwill promote a more subtle editing, which is better for ensuring that other parts of the model are unchanged. This will be shown in the following section, which describes our optimization phase. 3.5 Optimization Step Next, let us define the objective for our KDPO, denoted as LKDPO (πθ;πref)using the loss: LKDPO(πθ;πref) =−E(x,yw,yl)∼D [logσ(βlogπθ(yw|cw)\nπref(yw|cw)−βlogπθ(yl|cw)\nπref(yl|cw))]. Where, πθ(yl|cw)can be decomposed into a product of probabilities: πθ(yl|cw) =KY\nk=1πθ(tl\nk|c, y<k\nw). πθ(tl\nk|c, y<k\nw)represent the probability of tl\nkappearance based on the prompt and parts of the new knowledge yw. KDPO optimizes the model to decrease the probability of the tl\nkwith the least amount of bond breaking inside the completion yl. Given the previous example,yl=\"Rosario, Argentina.\" DPO will minimize: πθ(\"Argentina\" |\"Lionel Messi ... Rosario,\" ) this will aid in breaking the connection between \"Lionel Messi\" and \"Argentina.\" but it may also detach the relation between \"Rosario\" and \"Argentina.\" Next, let us observe how the gradient of LKDPO behaves: ∇θLKDPO(πθ;πref)∝ −[∇θlogπθ(yw|cw)− ∇ θlogπθ(yl|cw)], thus ∇θLKDPO(πθ;πref)∝ −X\nk∇θ[logπθ(tw\nk|c, y<k\nw)−logπθ(tl\nk|c, y<k\nw)]. 4\nAlgorithm 1 Single Edit Flow\nInput: c- context, yw- new knowledge\nOutput: θ\nInitialize: πref=πθ.copy ()\nforcycle in n do\nyl=argmax (πθ(cw))\nifyl==ywthen\nbreak\nforstep in s do\nCalculate loss LKDPO(πθ;πref)\nUpdate θusing Adam\nOne can note the importance of generating usingcwand not cl. Using cw, we achieve a more balanced distribution of tokens between the two completions from an early stage; this is beneficial since if tl\nk==tw\nk, thek-th term in the sum cancels, which leads to fewer gradient terms to average, thus, fewer changes in the weight space. Fig. 2 shows an example of this type of term-cancellation. Comparison with FT : KDPO enables more reliable KE, by allowing the term-cancellation phenomenon mentioned above. Additionally, because KDPO makes relative adjustments based on a reference model πref, it prevents the model from deviating significantly from the original weights. Furthermore, KDPO allows for controllable editing capabilities through the use of the βparameter. Empirical evidence of this can be seen in Tab. 1, where the average locality of KDPO is significantly higher than that of FT-M and FT-L (which are finetuning variations) for the three models examined. 4 Experiments Recent studies (Zhang et al., 2024) have primarily focused on single edits, which involve evaluating a model’s performance after a single knowledge update. In contrast, our focus is on sequential editing tasks, which require performing a series of knowledge updates successively, with evaluation conducted after the entire sequence of edits. 4.1 Evaluation Metrics The purpose of KE is to modify the behavior of the model by changing facts. Evaluating KE in LLMs involves assessing the effectiveness and impact of modifications made to the model’s knowledge base. This evaluation ensures that the edited model accurately reflects the desired changes while maintaining its overall performance. 371However, because facts are interconnected, altering one fact can have unexpected effects on others. This makes it challenging to evaluate edits. To evaluate the capabilities of our framework in KE, we use the same metrics as in previous works (Zhang et al., 2024), namely edit success, locality, portability, and fluency. Below, we provide definitions of these metrics. Edit Success : The model should accurately produce updated knowledge for related questions when making an edit. This is evaluated by checking if the post-edit model correctly answers the target knowledge and similar expressions. This builds on previous research (Mitchell et al., 2021; Li et al., 2024), combining reliability (exact questions) and generalization (paraphrased questions). Portability : Yao et al. (2023) evaluated the model’s ability to reason about the implications of the edited knowledge to related content. It is calculated as the average accuracy of the edited model in complex reasoning scenarios. This includes providing the edited continuation when a subject alias or alternative description is used, testing reversed relations, and handling one-hop prompts related to the modification without an explicit edit. Locality : This metric assesses unrelated knowledge, both in-distribution (e.g., forgetfulness, relation specificity) and out-of-distribution (performance on other NLP benchmarks), to ensure it remains unchanged by the edit algorithm. A good algorithm only affects the intended edits. Fluency : Measures the diversity and nonrepetitiveness of the model’s text after editing, using bi-gram and tri-gram entropies. Meng et al. (2022a) suggested lower fluency should be avoided as it indicates the model generates repetitive responses. 4.2 Datasets We assess the performance of our model using four datasets designed to evaluate KE quality: WikiData counterfact ,WikiBio ,ZsRE , and WikiData recent . These datasets cover various editing types such as fact manipulation, sentiment modification, and hallucination generation (Zhang et al., 2024). Details about each dataset are available in Section A. We use the evaluation split provided in EasyEdit (Wang et al., 2023). To prepare these datasets for sequential editing, we have filtered out samples with the same subject. This is to prevent cases where a fact is being edited twice, which would make the first edit non-relevant. For exam5\nFigure 2: Illustration of an optimization cycle. We note tokens that do not affect the loss by the \"prohibited\" sign.\nThe arrows indicate which tokens DPO and KDPO objectives will increase/decrease their log-prob. Note that in\nKDPO, the \"Italy\" token is not optimized because it is the same in both sequences, which cancels out the two loss\nterms of the objective.\nDataSet Metric LLaMA3-8B Qwen1.5-7B LLaMA2-7B\nAdaLoRA ROME FT-L FT-M Ours AdaLoRA ROME FT-L FT-M Ours AdaLoRA ROME MEMIT FT-L FT-M Ours\nZsREEdit Succ. 21.5 9.5 79.2 87.6 88.4 29.7 51.2 49.1 70.7 87.1 44.7 23.1 48.3 28.9 90.8 91.4\nPortability 11.1 3.4 26.4 29.9 49.3 19.9 31.2 24.9 46.6 42.5 28.2 7.5 24.9 9.1 48.9 50.4\nLocality 3.1 0.7 14.5 4.5 40.4 6.3 19.5 15.5 22.2 34.3 11.2 10.5 7.4 2.3 27.5 45.6\nFluency 3.4 3.8 4.0 4.0 5.0 3.4 3.2 3.9 2.9 5.5 4.8 4.5 4.3 2.5 3.0 5.5\nWikiBioEdit Succ. 66.9 3.4 74.3 85.4 89.3 76.1 64.2 55.6 87.8 93.5 84.4 19.4 20.3 27.1 94.7 91.6\nLocality 16.0 6.6 9.9 10.4 35.8 14.6 21.3 22.7 28.9 36.2 20.0 9.1 8.6 7.9 36.0 44.4\nFluency 6.3 6.0 6.1 6.3 6.3 6.3 6.0 6.0 6.2 6.3 6.3 6.1 6.0 5.8 6.1 6.3\nWiki counterfactEdit Succ. 27.9 8.1 79.1 87.4 86.7 3.2 41.2 29.8 79.6 90.5 24.5 19.7 8.2 19.8 92.5 92.5\nPortability 8.6 7.3 24.6 28.2 31.2 12.1 19.8 15.8 29.2 31.6 17.2 3.3 8.3 6.3 48.4 47.7\nLocality 7.6 7.9 4.1 2.7 42.5 10.0 52.3 48.4 48.3 53.9 15.6 1.9 2.4 9.1 24.5 52.9\nFluency 4.0 5.0 3.7 3.8 5.3 3.4 4.6 4.0 2.6 5.5 5.3 4.6 2.7 3.9 3.2 5.6\nWiki recentEdit Succ. 12.1 4.8 78.2 90.4 96.3 48.2 71.8 62.0 81.7 93.3 64.2 14.9 66.9 25.8 94.4 95.7\nPortability 4.7 5.5 31.2 36.3 42.1 23.1 34.0 33.4 39.1 37.4 39.4 5.5 30.2 9.7 54.4 59.0\nLocality 10.3 1.5 21.5 24.8 47.9 24.9 38.3 49.4 51.2 51.0 41.9 6.6 34.1 6.5 43.2 60.3\nFluency 2.7 5.2 3.3 3.2 5.6 4.8 4.5 3.1 3.1 5.4 5.6 4.5 5.0 3.4 3.6 5.6\nAverage Edit Succ. 32.1 6 .4 77 .7 87.7 90.2 39.3 57 .1 49 .1 80.0 91.1 54.5 19 .3 35 .9 25 .4 93.1 92.8\nPortability 8.1 5 .4 27 .4 31.5 40.9 18.4 28 .3 24 .7 38.3 37.2 28.3 5 .4 21 .1 8 .4 50.6 52.4\nLocality 9.2 4 .2 12.5 10.6 41.7 14.0 32 .9 34 .0 37.7 43.9 22.2 7 .0 13 .1 6 .5 32.8 50.8\nFluency 4.1 5.0 4.3 4 .3 5.6 4.5 4.6 4.2 3 .7 5.7 5.5 4.9 4 .5 3 .9 4 .0 5.8\nTable 1: Multiple Knowledge Editing algorithm’s performance using three different language models (LLaMA3-8B,\nQwen1.5-7B, and LLaMA2-7B) on four different datasets (ZsRE, WikiBio, WikiData counterfact , and WikiData recent)\nwith 100 sequential edits evaluated across multiple metrics. Best result is noted in bold and second best in an\nunderline . Overall, our method exhibits good result across models and datasets.\nple, if the prompt is \"What is the city of birth of X?\" and the target is \"Y .\" on the next edit request, \"What is the city of birth of X?\" with the target \"Z.\" the evaluation performed after Nedit requests would be evaluated on answering \"Y .\" even though we have already guided the model’s knowledge for another fact. 4.3 Knowledge Editing Impact on General LLM Tasks The efficacy of KE methods is evaluated based on their ability to modify knowledge in the model while preserving its other capabilities, such as reasoning and common sense understanding. The primary goal is to determine if making targeted factual edits unintentionally hinders the model’s capabilities in unrelated areas. To facilitate this analysis, we curate a set of benchmarks HellaSwag (Zellers 439et al., 2019), Winogrande (Sakaguchi et al., 2021), and MMLU (Hendrycks et al., 2020). Then, we test different KE methods on those benchmarks to investigate any performance degradation. 5 Results This section presents our results across multiple dataset, LLMs, and KE methods. We start by showing the results of our multiple edits study. Then, we demonstrate our method’s ability to handle general, non KE, LLM benchmarks. Lastly, we show an extensive ablation study. Additionally, prompt examples and empirical edit results are presented in Section 4. The prompts were selected from the ZsRE dataset to give some intuition on prompt types and edit challenges. We also show comparative responses from multiple KE methods shown 6\nin this paper. Some baselines did not converge in all cases and thus were not added to the results. Those issues and other baseline implementation details are discussed in Section B.1 of the supplementary material.Further results appear in the supplementary material Section D. Our method uses the same hyperparameters for all models (size ranging from 0.5B-8B) and datasets, while we optimized the hyperparameters of baseline methods to achieve competitive results. 5.1 Multiple Edits results Fig. 3 compares all KE algorithms on ZsRE dataset using the metrics as described in Section 4. We show the results for LLaMA2-7B model after 100 sequential edits. Our method outperforms the baselines on all metrics with a notable gap in Locality. This is especially encouraging when using LLMs since we aim to retain the pre-trained knowledge during post-editing. Detailed results for all four datasets for three LLMs (LLaMA3-8B, Qwen1.57B, and LLaMA2-7B) are available in Tab. 1 for 100 sequential edits. We show that our method maintains state-of-the-art or comparable results on all datasets in all metrics. In many cases we notice a large gap in the locality, which in some cases surpasses double the performance of other methods. This indicates that our proposed editing method is precise and does not change non relevant parts of the pre trained LLM. To further deepen our understanding of the proposed approach, we tested it on four different LLMs (GPT-j-6B, Qwen1.5-7B, LLaMA2-7B, and LLaMA3-8B) on all datasets using 500 sequential edits in Tab. 2. We notice that performance gap for 500 edit got bigger, our method maintains its performance and achieves state-of-the-art results on all metrics using the recent LLaMA3-8B model. 5.2 Knowledge Editing in Small Language Model Different sizes of language models vary in the way they train, predict, and react to KE. We compare our method to other method as well as to standard DPO. Our method shows very promising results for smaller models like Qwen1.5-0.5B. Fig. 4 shows the average results on three leading datasets . Results are presented in the supplementary material Section D, and shows once again that KDPO keeps opens a gap against baseline method on 500 sequential edits. Figure 3: Comparative result for the four metrics in\nZsRE datasets using algorithms discussed in this paper.\nFluency results were scaled with a factor of 10 for better\nvisibility.\n5.3 Knowledge Editing Impact on General LLM Tasks We first test the performance of the LLaMA2-7B model on those three benchmarks. Then, we conduct two main experiments to test the performance of LLMs after applying different KE schemes compared to the original LLM. The first experiment tests different KE methods after 100 edits from the ZsRE dataset. Tab. 3 (Left) indicates that our KDPO method performs at a similar level as the pre-trained LLaMA2-7B model, which further strengthens our claim that KDPO possesses strong locality capabilities. On the other hand, the ROME method seems to degrade the performance quite substantially in this case. The second experiment results are in Tab. 3 (Right). This experiment utilizes the WikiData counterfact with 500 edits. After making 500 edits to WikiData counterfact , we have demonstrated that our method is able to maintain its original performance. Additionally, we have shown that our method outperforms all other tested KE methods across all datasets, sometimes by a significant margin. 5.4 Ablation study The goal of this section is to thoroughly examine our proposed method, KDPO versus the Vanilla DPO. We examine both on 100 and 500 sequential edits on various datasets and models. Tab. 6 shows the results of multiple different 6-8B models on three datasets when performing 100 and 500 sequential edits. In the case of 100 edits, the results mostly appear similar, and there seems to be 7\nDataSet Metric GPT-j-6B Qwen1.5-7B LLaMA2-7b LLaMA3-8b\nAdaLoRA FT-M Ours AdaLoRA FT-M Ours AdaLoRA FT-M Ours AdaLoRA FT-M Ours\nZsREEdit Succ. 37.4 57.1 69.7 43.3 66.6 85.4 40.6 92.3 92.1 82.3 79.5 87.8\nPortability 22.9 20.8 36.8 26.7 46.5 41.1 29.8 43.8 49.4 35.1 29.2 44.9\nLocality 4.2 3.2 22.7 5.2 18.8 25.0 11.8 16.4 38.9 24.2 3.5 31.4\nFluency 4.4 4.2 3.8 3.1 2.6 5.2 4.1 3.1 5.4 4.9 4.1 5.4\nWikiBioEdit Succ. 83.1 6.5 88.0 76.1 87.8 93.5 84.4 94.7 91.6 82.3 85.1 87.9\nLocality 23.4 2.1 22.8 14.6 28.9 36.2 20.0 36.0 44.4 25.4 13.1 36.2\nFluency 6.4 5.7 6.3 6.3 6.2 6.3 6.3 6.1 6.3 6.4 6.2 6.4\nWiki counterfactEdit Succ. 22.9 66.1 65.3 24.1 74.7 84.5 32.8 93.5 92.8 81.9 78.6 84.5\nPortability 10.2 27.9 29.6 11.8 29.4 29.1 19.5 44.4 46.3 42.3 26.5 60.4\nLocality 9.3 6.6 20.1 8.4 43.4 43.6 13.7 18.1 47.8 21.8 4.7 32.8\nFluency 3.7 3.9 3.8 4.3 2.9 4.8 4.4 3.8 5.3 5.2 4.7 5.4\nWiki recentEdit Succ. 58.1 57.8 72.2 31.4 75.7 87.3 45.6 93.5 92.3 81.2 83.9 89.5\nPortability 3.2 26.6 39.9 16.7 35.1 34.9 29.0 48.2 52.8 31.2 29.6 35.9\nLocality 31.7 17.5 35.4 23.4 46.3 44.4 34.4 38.7 50.4 28.2 24.2 40.3\nFluency 4.1 4.6 3.8 2.9 2.5 5.3 3.5 3.3 5.6 4.2 4.0 5.6\nAverageEdit Succ. 50.4 46.9 73.8 43.7 76.2 87.7 50.9 93.5 92.2 81.9 81.8 87.4\nPortability 12.1 25.1 35.4 18.4 37.0 35.0 26.1 45.5 49.5 36.2 28.4 47.1\nLocality 17.2 7.4 25.3 12.9 34.4 37.3 20.0 27.3 45.4 24.9 11.4 35.2\nFluency 4.7 4.6 4.4 4.2 3.6 5.4 4.6 4.1 5.7 5.2 4.8 5.7\nTable 2: Evaluationg of the performance of multiple Knowledge Editing algorithms using four different language\nmodels (GPT-j-6B, Qwen1.5-7B, LLaMA2-7B, and LLaMA3-8B) on four different datasets (ZsRE, WikiBio,\nWikiData counterfact , and WikiData recent) with 500 sequential edits evaluated across multiple metrics. The best result is\nnoted in bold and second best in an underline . Overall, our method exhibits good results across models and datasets.\nHellaSwag Winogrande MMLU Average\nLLaMA2-7B 75.99 69.06 41.24 62.76\nOurs 76.38 68.68 41.32 62.79\nDPO 75.93 69.29 41.55 62.92\nFT-M 72.79 68.50 37.86 59.05\nROME 27.66 48.53 24.32 33.50\nMEMIT 72.13 66.61 26.05 54.93HellaSwag Winogrande MMLU Average\nLLaMA2-7B 75.99 69.06 41.24 62.76\nOurs 76.28 70.48 38.83 61.86\nDPO 73.74 66.61 35.68 58.68\nFT-M 61.55 66.37 31.93 53.95\nROME 22.34 43.77 21.78 29.96\nMEMIT 25.79 48.77 26.89 33.82\nTable 3: Comparison of performance on general LLM benchmarks (HellaSwag, Winogrande, MMLU). LLaMA2-7B\nis the base model, with its pre-trained results in the first row for reference. The best results are in bold, and the\nsecond-best are underlined. Left: shows KE using the ZSRE dataset for 100 sequential edits. Right: shows KE using\nthe WikiData counterfact dataset for 500 sequential edits. Notably, KDPO and DPO are competitive for 100 sequential\nedits, but KDPO outperforms for 500 sequential edits by a large margin.\nno dominant advantage for our suggested KDPO. However, in the case of 500 sequential edits, KDPO clearly demonstrates its superiority, particularly in the locality metric and the success in edits. We delve deeper into the differences between KDPO and DPO in Tab. 3. This table illustrate how each method influences the performance of the LLM on general LLM tasks. Our method shows a significantly lower negative impact on the original abilities of the LLM compared to DPO, which should underline the importance of our research findings. 6 Conclusions We have introduced a variant of DPO that is effective for KE. Our extensive testing shows that our proposed KDPO methodology is a promising one for LLM KE. We showed our method works well for various recent LLMs on multiple well known KE datasets. In our ablation, we demonstrated 554the advatage of KDPO over the vanilla DPO, suggesting the value of our novel idea for KE tasks. Finally, we verified our method maintains the pre trained LLM performance on multiple benchmarks. Overall, we have demonstrated that KDPO is a high-performance and highly precise method for KE tasks. This can significantly help prevent expensive retraining of LLMs due to factual errors. 7 Limitations The primary limitation of our method, which is inherited from DPO, is the need to keep a copy of the model, leading to an increase in the memory footprint. However, it is important to note that recent works, such as those by Meng et al. (2024) and Azar et al. (2024), are actively addressing this challenge, offering potential solutions to reduce this hurdle. 8",
        "aspect_review": "The paper presents comprehensive experimental validation across multiple datasets and models, demonstrating the robustness and effectiveness of the proposed method. The evaluation metrics used are well-defined and relevant, providing clear evidence of the method's performance improvements over existing techniques. The results indicate significant improvements in edit success, portability, locality, and fluency, showcasing the practical impact of the proposed approach. The paper is well-organized and clearly presents the methodology, experiments, and results, making it accessible and informative for readers. The introduction of Knowledge Direct Preference Optimization (KDPO) as a novel method for knowledge editing is both innovative and technically rigorous, offering a new perspective on LLM alignment.\n\nThe motivation for choosing specific datasets and models could be elaborated further to enhance understanding. While the methodology is sound, additional details on hyperparameter settings and implementation specifics would be beneficial. Some minor improvements in the clarity of evaluation metric descriptions could help in better understanding the results. The paper could include more examples of practical applications to illustrate the real-world impact of the proposed method.",
        "general_review": "The paper introduces a novel approach, Knowledge Direct Preference Optimization (KDPO), which is a variation of Direct Preference Optimization (DPO) specifically optimized for incremental knowledge modifications in large language models (LLMs). This approach is both innovative and practical, as it allows for knowledge editing without the need for additional parameters, external memory, or hypernetwork training. The technical depth and rigor are evident through the comprehensive experiments conducted across multiple datasets and LLM architectures, demonstrating that KDPO achieves superior performance in maintaining locality and edit success while preserving the model's pre-trained knowledge. The empirical validation is thorough, with extensive ablation studies and comparisons against state-of-the-art methods, highlighting the significant practical impact of KDPO in preventing expensive retraining of LLMs due to factual errors.\n\n1. The paper could benefit from minor writing or clarity improvements to enhance the readability and comprehension of the methodology section. 2. Small details on hyperparameters or implementation specifics are not thoroughly discussed, which could aid in replicating the experiments. 3. Very minor formatting issues in figures could be addressed to improve visual clarity. 4. Slight clarification on code availability would be beneficial for researchers interested in applying the proposed method."
    },
    "42": {
        "paper": "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale\nExtraction\nAnonymous ACL submission\nAbstract Unsupervised rationale extraction aims to extract text snippets to support model predictions without explicit rationale annotation. Researchers have made many efforts to solve this task. However, Previous works encode each aspect independently, ignoring their internal correlations. Meanwhile, such a uni-aspect encoding model can only explain and predict one aspect of the text at a time, which limits its downstream applications. In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain and predict multiple aspects simultaneously. Concretely, we propose a Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to encode multiple text chunks simultaneously. Furthermore, multiple special tokens are prepended in front of the text with each corresponding to one certain aspect. Finally, multi-task training is deployed to reduce the training overhead. Experimental results on two unsupervised rationale extraction benchmarks show that MARE achieves state-ofthe-art performance. Ablation studies further demonstrate the effectiveness of our method. 1 Introduction Deep learning text classification systems have achieved remarkable performance in recent years (Kim, 2014; Devlin et al., 2019). However, their black-box nature has been widely criticized. Finding a sufficient approach to open the black box is urgent and significant. Unsupervised rationale extraction (Lei et al., 2016) is an explanation approach that aims to extract text snippets from input text to support model predictions without explicit rationale annotation. Previous researchers (Liu et al., 2022; Jiang et al., 2023) have made many efforts to improve the rationalization performance of their models. However, as shown in Figure 1a, existing rationale extraction models are uni-aspect encoding models, which can only predict and interpret one aspect of the 042Example\nAppearance : Positive\nAroma : Positive\nPalate : Positive\nText: thanks to bman1113vr for sharing this\nbottle . pours a murky orangish-brown color\nwith a white head . the aroma is tart lemons .\nthe flavor is tart lemons with some oak-aged\ncharacter . the beer finishes very dry . medium\nmouthfeel and medium carbonation .\nTable 1: A multi-aspect example from the BeerAdvocate\ndataset (McAuley et al., 2012). Blue, red, and cyan\nrepresent the aspects of Appearance ,Aroma , and Palate ,\nrespectively.\ntext at a time. In real-world scenarios, one text often contains multiple aspects of an object. Table 1 shows an example from the BeerAdvocate dataset (McAuley et al., 2012), where blue, red, and cyan represent the aspects of Appearance ,Aroma , andPalate , respectively. The highlighted segments in the text are the rationales corresponding to each aspect. For instance, \" pours a murky orangishbrown color with a white head . \" explains why the label for Appearance is Positive. In this case, traditional uni-aspect rationale extraction models would require three independently trained models to predict and interpret all three aspects, which is labor-intensive and time-consuming and limits their downstream applications. Furthermore, uni-aspect models encode each aspect independently ignoring their internal correlation. To address these problems, we propose the MultiAspect Rationale Extractor ( MARE ). As shown in Figure 1b, MARE can encode all aspects simultaneously by prepending multiple special tokens to the input text, each corresponding to a specific aspect. This approach enables multi-aspect encoding in one model. Furthermore, MARE introduces 1\nUni-Aspect Encoding modelX\nR3 Y3Y3 R3Uni-Aspect Encoding modelX\nR2 Y2Y2Uni-Aspect Encoding modelX\nR1 Y1(a) Typical uni-aspect en-\ncoding models\nR2 Y2MAREX\nR1 Y1 Y3 R3(b) MARE ( ours)\nFigure 1: Comparison of our methods (MARE) with\nprevious typical uni-aspect encoding models.\na Multi-Aspect Multi-Head Attention ( MAMHA ) mechanism for collaborative encoding across aspects. This mechanism allows the model to capture interactions and dependencies between different aspects, leading to more accurate predictions and rationales. Finally, inspired by multi-task learning, MARE iteratively accesses training data for different aspects, reducing the overall training cost. We validate the effectiveness of MARE on two unsupervised rationale extraction benchmarks: BeerAdvocate (McAuley et al., 2012) and Hotel Review (Wang et al., 2010). Results show that MARE outperforms existing state-of-the-art methods across multiple evaluation metrics. Ablation studies further demonstrate the effectiveness of MARE. Our main contributions are as follows: •We introduce MARE, a Multi-Aspect Rationale Extractor that generates predictions and rationales for multiple aspects simultaneously. •We deploy the multi-task training to reduce the training cost and expand the model applicability. Compared to multi-aspect collaborative training, it saves 17.9% and 25.2% of memory usage and training time, respectively. •Extensive experiments on BeerAdvocate and Hotel Review datasets demonstrate MARE’s superiority, with a notable 5.4% improvement in token-level F1 score. Ablation studies further validate the effectiveness of each component in MARE. 2 Related Work The rationalization framework, known as RNP (Lei et al., 2016), assumes that any unselected input has no contribution to the prediction and achieves remarkable performance on this task. However, RNP still has many weaknesses. Various approaches 102have been proposed to improve RNP In different dimensions. Gradient Flows The RNP framework utilizes REINFORCE (Williams, 1992) to overcome the non-differentiable problem, but this leads to training instability and poor performance. HardKuma (Bastings et al., 2019) introduces reparameterization tricks and replaces the Bernoulli distribution with the rectified Kumaraswamy distribution, which stabilizes the training process. In FR (Liu et al., 2022), the encoder’s parameter is shared between the generator and predictor. This ensures that the encoder’s gradient is more reasonable because it can see both full texts and rationales. 3Players (Yu et al., 2019) forces the complementary rationale to be meaningless, resulting in more meaningful generated rationales. Our research is orthogonal with these methods. Interlocking The interlocking problem was initially proposed by A2R (Yu et al., 2021). This problem arises when the generator fails to identify important tokens, leading to sub-optimal rationales and consequently affecting the performance. Many researchers have developed approaches to address this issue (Huang et al., 2021; Yu et al., 2021; Liu et al., 2023a). DMR (Huang et al., 2021) aimed to align the distributions of rationales with the full input text in the output space and feature space. A2R (Yu et al., 2021) enhances the predictor’s understanding of the full text by introducing a soft rationale. MGR (Liu et al., 2023a) involves multiple generators with different initializations to allow the predictor to see various rationales, alleviating the interlocking problem. DR (Liu et al., 2023b) limits the Lipschitz constant of the predictor, making the whole system more robust. YOFO (Jiang et al., 2023) eliminates interlocking by simultaneously predicting and interpreting. YOFO deploys pre-trained language models as its backbone and uses token deletion strategies between layers to erase unimportant tokens. the remaining tokens in the final layer are seen as rationales. This paper focuses on the efficiency of the multiaspect scenarios. All the above models are uniaspect encoding models, where one model can only encode one aspect of data. MARE is a multi-aspect collaborative encoding model designed to encode multiple aspects of data simultaneously. 2\nbroadcastX1 X2 X3 [CLS]\nX1 X2 X3 [CLS]X1 X2 X3 [CLS](a) Attention Mask Dele-\ntion\nouter  \nproduct  X1 X2 X3 [CLS]\nX1 X2 X3 [CLS]X1 X2 X3 [CLS](b) Hard Deletion\nFigure 2: Attention mask visualization. left: attention\nmask in Attention Mask Deletion. right: attention mask\nin Hard Deletion.\n3 Problem Definition Existing uni-aspect encoding models extract rationaleszifrom the input xand predict the label yifor the i-th aspect. Formally, they can be expressed as P(yi,zi∣x;θi), where θirepresents the parameters of the model for the i-th aspect. To obtain the rationales and predictions for all kaspects, kindependently trained models are required: {P(y1,z1∣x;θ1),⋯, P(yk,zk∣x;θk)}. However, this approach is time-consuming and computationally expensive. To address this issue, we propose a multiaspect rationale extraction task, where the rationales and predictions for all aspects can be generated simultaneously. This can be formalized as P(y1,z1,⋯,zk∣x;θ), where θrepresents the parameters of the multi-aspect rationale extraction model. By utilizing a single model to extract rationales and make predictions for all aspects concurrently, we aim to improve the efficiency and reduce computational costs compared. 4 Method This paper proposes a Multi-Aspect Rationale Extractor (MARE), which can simultaneously predict and interpret multiple aspects of text. As shown in the left part of Figure 3, MARE is based on an encoder-based pre-trained language model and achieves multi-aspect collaborative encoding through a Multi-Aspect Multi-Head Attention (MAMHA) mechanism. Additionally, MARE Q KMatmulScaleGumbel Softmax\nQuery Key V alueMatmulScaleMask (Opt.)SoftMaxMatmul\nElem. Prod.\nOuter Prod.Binarize\nMulti-Aspect\nMulti-Head\nAttentionAdd & Norm\nInputFeed ForwardAdd & Norm\nN ×MHA\nMACMAMHAFigure 3: Overall model architecture. left: the overall\nmodel architecture of MARE. right: the computational\ngraph of MAMHA.\nemploys multi-task training during the training process, significantly reducing the training cost. 4.1 Hard Deletion for Complete Token Removal Selecting rationales without explicit annotations can be challenging. We follow the previous work (Jiang et al., 2023) where unimportant tokens are gradually erased. However, directly multiplying hidden states by the token mask harms rationalization performance (Jiang et al., 2023). Attention Mask Deletion (AMD) (Jiang et al., 2023) avoids this problem by setting attention scores of masked tokens to 0. Concretely, assuming mi∈[0,1]Lrepresents the token mask in the i-th layer and Aj\ni∈RL×Lis the attention score matrix of the jth head in the i-th layer, the final attention score matrix is ˜Aj\ni=Aj\ni⋅mi∈RL×L. Through AMD, remaining tokens interact while deleted ones are invisible. However, AMD suffers from an \"incomplete deletion\" problem, where deleted tokens can still be partially represented by remaining ones due to the broadcast operation. As shown in Figure 2a, although \"X1\" and \"X2\" are masked, they can still be indirectly represented by the weighted sum of \"[CLS]\" and \"X3\". Although this allows the model to retain more information, it hinders multi-aspect collaborative encoding. To address this issue, we propose Hard Deletion, which uses an outer product operation to completely erase deleted tokens (Figure 2b). \"X1\" and \"X2\" are represented by all-zero vectors, ensuring complete removal. 3\nGood place  and [C2] [C1] bad serviceGood place  and [C2] [C1] bad serviceGood place  and [C2] [C1] bad serviceFigure 4: A example for Multi-Aspect Controller. left:\nThe token mask for each aspect. \"Good place\" and\n\"bad service\" stands for the rationales of location and\nservice aspect, respectively. right: The attention mask\nis obtained by performing an outer product operation on\ntoken masks.\n4.2 Multi-Aspect Multi-Head Attention Inspired by hard deletion, we propose the multiaspect multi-head attention (MAMHA) mechanism to encode multiple text segments simultaneously. As shown in the right part of Figure 3, MAMHA consists of a Multi-Aspect Controller (MAC) and the traditional multi-head attention (MHA) mechanism. 4.2.1 Multi-Aspect Controller (MAC) MAC assists MHA in separately encoding different text segments by generating aspect-specific attention masks based on token masks for each aspect. This allows tokens within the same aspect to interact while isolating tokens from different aspects, enabling MHA to achieve multi-aspect collaborative encoding. Figure 4 illustrates an example where \"good place\" and \"bad service\" are rationales for the \" location\" and \" service \" aspects, respectively. The final attention mask, obtained through an outer product operation, creates two separate segments. Words within each segment interact, while words from different segments remain independent. Special classification tokens \"[C1]\" and \"[C2]\" collect information from their respective aspects, allowing MHA to encode two aspects simultaneously. This method can be extended to kaspects by dividing the text into ksegments and appending k special tokens. Note that if MAC employs AMD, tokens from different aspects cannot be fully isolated, leading to confusion and hindering multiaspect collaborative encoding (further discussed in Section 6.2.2). 4.2.2 Computation Process of MAC The computation process of MAC is shown in the right part of Figure 3. Assuming Hirepresents 249the hidden states of the i-th layer, its first kvectors{h0\ni,⋯,hk−1\ni}are representations of special tokens. For the j-th aspect, mapping functions gj\nquery andgj\nkeymap special and normal tokens to QandK, respectively. The similarity between special and normal tokens is calculated, and the gumbel-softmax technique determines the token’s aspect assignment (Equations (1)-(4)). Q={g0\nquery(h0\ni),⋯, gk\nquery(hk−1\ni)} (1) K={g0\nkey(Hi[k∶]),⋯, gk\nkey(Hi[k∶])} (2) scores=Q⋅KT\n√\nd(3) m=gumbel_softmax (scores , dim=−1)(4) , where dandLmean the vector’s dimension and the text’s length, respectively. [⋅]represents slicing operation, m∈{0,1}k×Lstands for the token mask, and m[i, j]=1indicates that the j-th token is selected as the rationale of the i-th aspect. MAC adopts the outer product operation to match the shape of the attention score matrix in MHA (Equation 5). When M′[i, j]≠0, the token is selected as a rationale in at least one aspect and should not be deleted (Equation (6)). The binarization operation in Equation (6) is non-differentiable, so straight-through is used for gradient estimation. Finally, the mask is multiplied by the attention score matrix to perform token deletion (Equations (7)-(9)). M′=mT⋅m∈[0, k]L×L(5) ˜M[i, j]=⎧⎪⎪⎨⎪⎪⎩0, IfM′[i, j]=0\n1, Otherwise(6) M=˜M+M′−StopGrad (M′)∈[0,1]L×L(7) ˜Ah\ni=Ah\ni⊙M, forhin1,2, ..., H (8) Hi=PLM i(Hi−1;˜Ai), foriin1,2, ..., N (9) , where StopGrad (X)represents stopping the X’s gradient calculation. Ah\niand˜Ah\nirepresent the initial and final attention score matrices of the h-th attention header in the i-th layer, respectively. Hi represents the hidden layer representation of the i-th layer. 4.3 Multi-Task Training Using labels from various aspects simultaneously during training may not be feasible, as datasets like Hotel Review (Wang et al., 2010) only have annotations for one aspect per sample. Multi-task training 4\nallows MARE to focus on the aspect corresponding to the current batch, avoiding the need to encode aspects with missing labels. If the batch comes from thej-th aspect, only the corresponding mapping functions gj\nqandgj\nkare used (Equations (10)-(11)). Q=gj\nquery(Hi[j−1∶j]) (10) K=gj\nkey(Hi[k∶]) (11) 5 Experiments 5.1 Experimental Setup Datasets We performed experiments on two commonly used unsupervised rationale extraction datasets: BeerAdvocate (McAuley et al., 2012) and the Hotel Review dataset (Wang et al., 2010). The statistics of these datasets can be found in Appendix A.2. The BeerAdvocate dataset (McAuley et al., 2012) is a multi-aspect sentiment prediction dataset. It consists of texts along with corresponding aspect scores ranging from 0 to 1, including aspects such asappearance ,aroma , and palate . The training and validation sets do not have labeled rationales, but the test set contains 994 samples with rationale annotations for all aspects. Notably, the scores across different aspects within the same sample exhibit high correlation, resulting in highly spurious correlations. For the BeerAdvocate dataset, we conducted experiments on the decorrelated version proposed by Lei et al.. We binarized the dataset into binary classification tasks using a positive threshold of 0.6 and a negative threshold of 0.4 (Bao et al., 2018). We run our model, MARE, on two sparsity levels: high-sparse and low-sparse. In the high-sparse decorrelated dataset, the sparsity level approximates the sparsity for golden rationales in the test set. In the low-sparse decorrelated dataset, the sparsity level is comparatively lower but allows for convenient comparisons with previous works. To examine the susceptibility of our model to spurious correlations, we also utilized the correlated BeerAdvocate dataset by Liu et al.. The Hotel Review dataset (Wang et al., 2010) is another widely used dataset for multi-aspect sentiment classification and rationale extraction. It includes texts along with three aspect labels: location ,service , and cleanliness . In addition to the aspect labels, the test set of this dataset also provides rationale annotations for all three aspects, with 200 samples. Since the original labels are on a scale of 0 to 5 stars, we utilize the binarized ver- 341sion proposed by Bao et al.. For the Hotel Review dataset, we only conducted a low-sparse experiment as the golden sparsity level is relatively low, at around 10%. Baselines We compared the performance of MARE with several state-of-the-art baselines. These baselines, including RNP (Lei et al., 2016), CAR (Chang et al., 2019), DMR (Huang et al., 2021), A2R (Yu et al., 2021), FR (Liu et al., 2022), MGR (Liu et al., 2023a) DR (Liu et al., 2023b), and YOFO (Jiang et al., 2023), were discussed in Section 2. The performance of these baselines are obtained from YOFO (Jiang et al., 2023). In MARE, we use BERT for our backbone and the balanced round-robin is equipped in the training stage. All of our experiments are conducted on NVIDIA Geforce RTX 3090 24GB. For more implementation details, please refer to Appendix A.1. Metrics Following previous works (Jiang et al., 2023), we will use token-level F1 and accuracy for the rationalization and downstream performance. In our result tables, we define S as the sparsity level of selected rationales, computed using the formula S=#selected tokens\n#tokens. P, R, and F1 represent precision, recall, and F1 score for rationale extraction, respectively. ACC and Val ACC denote the accuracy of the test and validation sets, respectively. The best performance is Bolded in the tables. 5.2 Main Results 5.2.1 Results on the BeerAdvocate Dataset High-sparse Experimental results on the decorrelated BeerAdvocate dataset in the high-sparse scenario are shown in Table 2. MARE outperforms YOFO by 3.3%, 1.0%, and 2.8% in the appearance , aroma , and palate aspects, respectively. Meanwhile, MARE achieves the best average F1 scores among all models, particularly 88.8%. This is because MARE is a multi-aspect collaborative encoding model that captures internal correlations between all aspects and thus achieves the best performance. Low-sparse Experimental results on the decorrelated BeerAdvocate dataset in the low-sparse scenario are shown in Table 3. MARE still achieves the best performance in all aspects, similar to the high-sparsity scenario. In the low-sparsity scenario, the performance gain obtained by MARE is greater than in high-sparsity scenarios. Specifically, MARE is 5.4%, 4.8%, and 0.7% higher than YOFO 5\nMethodsAppearance Aroma Palate Avg\nS ACC P R F1 S ACC P R F1 S ACC P R F1 F1\nRNP(Bastings et al., 2019) 18.7 84.0 72.0 72.7 72.3 15.1 85.2 59.0 57.2 58.1 13.4 90.0 63.1 68.2 65.5 65.6\nDMR(Huang et al., 2021) 18.2 - 71.1 70.2 70.7 15.4 - 59.8 58.9 59.3 11.9 - 53.2 50.9 52.0 60.7\nA2R(Yu et al., 2021) 18.4 83.9 72.7 72.3 72.5 15.4 86.3 63.6 62.9 63.2 12.4 81.2 57.4 57.3 57.4 64.5\nFR(Liu et al., 2022) 18.4 87.2 82.9 82.6 82.8 15.0 88.6 74.7 72.1 73.4 12.1 89.7 67.8 66.2 67.0 74.4\nMGR(Liu et al., 2023a) 18.4 86.1 83.9 83.5 83.7 15.6 86.6 76.6 76.5 76.5 12.4 85.1 66.6 66.6 66.6 75.6\nDR(Liu et al., 2023b) 18.6 85.3 84.3 84.8 84.5 15.6 87.2 77.2 77.5 77.3 13.3 85.7 65.1 69.8 67.4 76.4\nYOFO (Jiang et al., 2023) 18.1 85.6 91.3 87.1 89.2 15.4 86.8 94.3 87.9 91.0 13.2 88.4 79.5 79.0 79.2 86.5\nMARE ( ours) 17.3 85.6 95.4 89.7 92.5 15.4 86.0 93.9 90.2 92.0 12.7 88.0 82.2 81.9 82.0 88.8\nTable 2: Results of different methods on the high-sparse decorrelated BeerAdvocate dataset (McAuley et al., 2012).\nMethodsAppearance Aroma Palate Avg\nS ACC P R F1 S ACC P R F1 S ACC P R F1 F1\nRNP(Lei et al., 2016) 11.9 - 72.0 46.1 56.2 10.7 - 70.5 48.3 57.3 10.0 - 53.1 42.8 47.5 53.7\nCAR(Chang et al., 2019) 11.9 - 76.2 49.3 59.9 10.3 - 50.3 33.3 40.1 10.2 - 56.6 46.2 50.9 50.3\nDMR(Huang et al., 2021) 11.7 - 83.6 52.8 64.7 11.7 - 63.1 47.6 54.3 10.7 - 55.8 48.1 51.7 56.9\nFR(Liu et al., 2022) 12.7 83.9 77.6 53.3 63.2 10.8 87.6 82.9 57.9 68.2 10.0 84.5 69.3 55.8 61.8 64.4\nMGR(Liu et al., 2023a) 13.2 82.6 75.2 53.5 62.6 12.3 84.7 80.8 63.7 71.2 10.8 80.1 51.6 44.7 47.9 60.6\nDR(Liu et al., 2023b) 11.9 81.4 86.8 55.9 68.0 11.2 80.5 70.8 57.1 63.2 10.5 81.4 71.2 60.2 65.3 65.5\nYOFO (Jiang et al., 2023) 13.1 87.0 97.1 66.9 79.2 12.1 86.3 94.1 68.9 79.5 10.9 87.8 88.5 72.7 79.8 79.5\nMARE ( ours) 13.8 86.3 98.7 74.0 84.6 12.2 85.9 97.5 74.4 84.3 10.9 88.2 87.4 74.6 80.5 83.1\nTable 3: Results of different methods on the low-sparse decorrelated BeerAdvocate dataset (McAuley et al., 2012).\nin the appearance ,aroma , and palate aspects, respectively. Furthermore, MARE has a 3.6% average performance gain in token-level F1 compared to YOFO. This further demonstrates the effectiveness of MARE. 5.2.2 Results on the Hotel Review Dataset Experimental results on the Hotel Review dataset are shown in Table 3. Although MARE is slightly inferior to YOFO in the service andcleanliness aspects, it is far superior to YOFO in the location aspect and its average token-level F1 score is higher than YOFO. Specifically, MARE is 1.1% and 2.0% lower than YOFO in the service andcleanliness aspects, respectively, while it is 5.3% higher than YOFO in the location aspect. Meanwhile, MARE is 0.7% higher than YOFO in the average tokenlevel F1 score. 6 Analysis 6.1 Case Study This section visualizes several samples on the Hotel Review dataset as shown in Table 5. Blue, red, and cyan represent the location ,service , and cleanliness aspects, respectively, and underline indicate the annotated rationales. In the Hotel Review test set, each sample only has a uni-aspect annotation. As shown in the first case, only the location aspect has been annotated. However, in real scenarios, a review often describes multiple aspects. MARE extracted snippets not 419only about location butservice andcleanliness which are not annotated. \" Staff very clean \" and \"rooms and bathrooms spotless clean \" demonstrate that the service andcleanliness of the hotel are excellent. In the second case, only the location aspect appeared in the text. Correspondingly, MARE did not select any rationale other than the location aspect. This indicates that MARE benefits from multi-aspect collaborative encoding and makes decisions when there is clear evidence. 6.2 Ablation Studies To verify the effectiveness of our model components, we have conducted several ablation studies on the BeerAdvocate dataset (McAuley et al., 2012). 6.2.1 multi-task training v.s. multi-aspect collaborative training To explore the impact of multi-task training on the model as described in Section 4.3, this experiment verifies the effectiveness of multi-task training by comparing the performance, memory usage, and time cost of multi-task training and multi-aspect collaborative training. The experimental result is shown in Table 6. The performance of multi-task training is slightly better than that of multi-aspect collaborative training. This is because, in the early stages of training, MARE cannot distinguish various aspects well, so multi-aspect collaborative training may lead to in6\nMethodsLocation Service Cleanliness Avg\nS ACC P R F1 S ACC P R F1 S ACC P R F1 F1\nRNP(Lei et al., 2016) 8.8 97.5 46.2 48.2 47.1 11.0 97.5 34.2 32.9 33.5 10.5 96.0 29.1 34.6 31.6 37.4\nDMR(Huang et al., 2021) 10.7 - 47.5 60.1 53.1 11.6 - 43.0 43.6 43.3 10.3 - 31.4 36.4 33.7 43.4\nA2R(Yu et al., 2021) 8.5 87.5 43.1 43.2 43.1 11.4 96.5 37.3 37.2 37.2 8.9 94.5 33.2 33.3 33.3 37.9\nFR(Liu et al., 2022) 9.0 93.5 55.5 58.9 57.1 11.5 94.5 44.8 44.7 44.8 11.0 96.0 34.9 43.4 38.7 46.9\nMGR(Liu et al., 2023a) 9.7 97.5 52.5 60.5 56.2 11.8 96.5 45.0 46.4 45.7 10.5 96.5 37.6 44.5 40.7 47.5\nDR(Liu et al., 2023b) 9.6 96.5 53.6 60.9 57.0 11.5 96.0 47.1 47.4 47.2 10.0 97.0 39.3 44.3 41.8 48.9\nYOFO (Jiang et al., 2023) 9.7 98.0 55.7 60.4 58.0 11.9 99.5 58.3 57.4 57.9 10.6 100.0 49.9 54.4 52.1 56.0\nMARE ( ours) 9.7 98.0 59.0 68.4 63.3 10.8 99.5 58.6 55.2 56.8 10.6 100.0 46.8 54.0 50.1 56.7\nTable 4: Results of different methods on the Hotel Review dataset (Wang et al., 2010).\nMARE ( ours)\nLocation : Positive ✔\nService : Positive ?\nCleanliness : Positive ?\nText: arrived very apprehensively to the hotel after reading the negative remarks . we were happily suprised . staff very\npleasant , rooms and bathrooms spotlessly clean , although on the small side . our rooms had no natural light , but with\nthe lights on were ok . air conditioning worked ( was necessary in november ! ) , although noisy from the inside and\noutside where the vents are . however , the hotel is in the middle of nyc and the noise did n ’ t bother us overmuch - the\nsituation is much more important , and the jolly was in aperfect location forshopping andtourism . breakfasted across\nthe road in the moonstruck deli ( opens at 7 am for the jet lagged ) . i would certainly go back there again !\nLocation : -\nService : Positive ✔\nCleanliness : -\nText: this is a very nice hotel with top-notch service andstaff . you will pay for it , but if you want to avoid the touristy\nhotels of branson , this is a beautiful place to stay and eat .\nTable 5: Case studies on the Hotel Review dataset (Wang et al., 2010).\nformation leakage between different aspects, resulting in a performance drop. Meanwhile, multiaspect collaborative training requires mask calculation for all aspects, resulting in high memory usage and long training time, reaching 24209MB and 34.5 minutes respectively. By contrast, multitask training only requires encoding a single aspect at a time, so it costs much lower in both memory and training time. It saves 17.9% and 25.2% of memory usage and training time, respectively. This indicates that models trained using multi-task training can outperform those trained using multiaspect collaborative training with fewer computational resources, demonstrating the effectiveness of multi-task training. 6.2.2 Hard Deletion v.s. Attention Mask Deletion To demonstrate the effectiveness of hard deletion, this section contrastively employs AMD operations in the MAC. Specifically, we will replace the Equation (5)-(8) with Equation (12)-(15): m′=k−1\n∑\ni=0m[i]∈[0, k]L(12) 470˜m[i]=⎧⎪⎪⎨⎪⎪⎩0, Ifm′[i]=0\n1, Otherwise(13) ˆm=m′−StopGrad (m′)+˜m∈{0,1}L(14) ˜Ah\ni=Ah\ni⊙ˆm, forhin1,2, ..., H (15) Where, kmeans the number of aspects, and m′represents the mask vector with a span of closed interval [0, k], ˆmindicates the calculated mask vector to multiply with attention score matrix. Here, we also use the Straight Through technique to bypass the non-differentiable problem. Experimental results are shown in Table 7. While using AMD, the rationalization and downstream performance are very poor. On the contrary, MARE-hard performs very well. In three aspects, the validation accuracy of MARE-hard was very close to BERT, and exceeded MARE-AMD by 3.5%, 4.8%, and 6.3%, respectively. Meanwhile, MARE-hard leads MARE-AMD by 23.1%, 23.4%, and 78.1% in rationalization performance, respectively. The reason is that AMD fails to effectively separate tokens corresponding to different aspects, leading to information leakage and hindering accurate rationale extraction. This indicates that AMD 7\nMethodsMemory Usage\n(MB)Training Time\n(minutes/epoch)Appearance Aroma Palate\nValAcc F1 ValAcc F1 ValAcc F1\nmulti-aspect collaborative training 24209 34.5 89.2 92.2 88.4 90.1 84.0 79.2\nmulti-task training 19877 25.8 89.2 92.5 89.1 92.0 84.7 82.0\nTable 6: Ablation study on different training strategies.\nMethodsAppearance Aroma Palate\nValAcc F1 ValAcc F1 ValAcc F1\nBERT 90.2 - 89.5 - 86.8 -\nMARE-AMD 85.7 69.4 84.3 68.6 78.4 3.9\nMARE-hard 89.2 92.5 89.1 92.0 84.7 82.0\nTable 7: Ablation study on different delete methods.\nis not suitable for multi-aspect collaborative coding, and also proves the necessity and effectiveness of using hard deletion. 6.2.3 Special Token Initialization To evaluate the impact of different initialization methods for special tokens on the model performance, this section explores three distinct initialization approaches: •random initialization: The first special token is initialized by [CLS], while all other special tokens are randomly initialized. •CLS initialization: All the special tokens are initialized by [CLS]. •sharing initialization: All the special tokens are shared and initialized by [CLS]. The performance comparisons are shown in Table 8. MARE-CLS is slightly better than MARErandom and the MARE-share performs the worst. We found that MARE share cannot distinguish the differences in sparsity between different aspects. MARE-CLS achieves the best performance because the special token [CLS] is a highly informative embedding after pre-training. By default, MARE uses the CLS initialization. 7 Conclusion This paper proposed a Multi-Aspect Rationale Extractor to solve the limitations of traditional uniaspect encoding models. MARE can collaboratively predict and interpret multiple aspects of text simultaneously. Additionally, MARE incorporated multi-task training, sequentially training on data from each aspect, thereby significantly reducing training costs. Extensive experimental results 525MethodsAppearance Aroma Palate\nACC F1 ACC F1 ACC F1\nMARE-random 85.7 87.1 85.4 90.7 87.0 80.9\nMARE-share 85.7 85.1 84.3 88.1 87.1 79.0\nMARE-CLS 85.6 92.5 86.0 92.0 88.0 82.0\nTable 8: Ablation study on different initialization strate-\ngies.\non two unsupervised rationale extraction datasets have shown that the rationalization performance of MARE is superior to all previous models. Ablation studies further demonstrated the effectiveness of our method. Limitations All of the above experiments have demonstrated the effectiveness of our method, but there are some limitations. MARE needs to prepend some special tokens in front of the input, which increases the computational overhead. Meanwhile, MARE can only adapted in encoder-based pre-trained language models. We are working hard to apply it to decoder-only models so that MARE can explain the predictions of LLMs. We will try to eliminate these limitations in our future work. 541",
        "aspect_review": "The experimental setup is comprehensive, utilizing two well-known unsupervised rationale extraction datasets, BeerAdvocate and Hotel Review, to validate the proposed model's effectiveness. The evaluation on multiple datasets, with both high-sparse and low-sparse settings, demonstrates the robustness and adaptability of the MARE model. MARE achieves state-of-the-art performance across multiple evaluation metrics, significantly improving token-level F1 scores compared to previous methods. The paper introduces a novel task of multi-aspect rationale extraction, addressing the limitations of uni-aspect models by enabling simultaneous predictions and interpretations for multiple aspects. The introduction of the Multi-Aspect Multi-Head Attention (MAMHA) mechanism and hard deletion for complete token removal are innovative and contribute to the model's superior performance.\n\nThe methodology could benefit from more detailed explanations of certain components, such as the Multi-Aspect Controller's exact functioning. While MARE outperforms existing models, the comparison could be strengthened by including a broader range of baselines, particularly those using different architectures. Some results, particularly on the Hotel Review dataset, show slightly inferior performance in certain aspects compared to YOFO, indicating room for improvement.",
        "general_review": "The paper introduces MARE, a Multi-Aspect Rationale Extractor, which is a novel advancement in the field of unsupervised rationale extraction. MARE addresses the limitations of traditional uni-aspect encoding models by enabling multi-aspect collaborative encoding through a Multi-Aspect Multi-Head Attention (MAMHA) mechanism. This is a significant methodological innovation that allows simultaneous prediction and interpretation of multiple aspects of text, enhancing the model's applicability and efficiency. The technical depth of the study is strong, with a robust multi-task training approach that significantly reduces training costs while maintaining high performance. The experimental validation is comprehensive, with MARE outperforming state-of-the-art methods on two benchmark datasets, demonstrating a notable improvement in token-level F1 scores. The practical impact is significant, as MARE provides a more efficient and effective solution for real-world scenarios where multiple aspects need to be considered simultaneously.\n\n1. The paper could benefit from minor improvements in writing clarity to ensure the methodology is easily understandable for a broader audience. 2. There is a need for slight clarification on the availability of the code to facilitate reproducibility and further research. 3. Some small details regarding hyperparameters and implementation specifics could be elaborated to aid in the replication of results. 4. Minor formatting issues in figures could be addressed to enhance visual clarity."
    },
    "43": {
        "paper": "Can we teach language models to gloss endangered languages?\nAnonymous ACL submission\nAbstract\nInterlinear glossed text (IGT) is a popular format in language documentation projects, where each morpheme is labeled with a descriptive annotation. Automating the creation of interlinear glossed text can be desirable to reduce annotator effort and maintain consistency across annotated corpora. Prior research (Ginn et al., 2023; Zhao et al., 2020; Moeller and Hulden, 2018) has explored a number of statistical and neural methods for automatically producing IGT. As large language models (LLMs) have showed promising results across multilingual tasks, even for rare, endangered languages (Zhang et al., 2024), it is natural to wonder whether they can be utilized for the task of generating IGT. We explore whether LLMs can be effective at the task of interlinear glossing with incontext learning, without any traditional training. We propose new approaches for selecting examples to provide in-context, observing that targeted selection can significantly improve performance. We find that LLM-based methods beat standard transformer baselines, despite requiring no training at all. These approaches still underperform state-of-the-art supervised systems for the task, but are highly practical for researchers outside of the NLP community, requiring minimal effort to use. 1 Introduction With thousands of endangered languages at risk of extinction, language documentation has become a major area of linguistic research (Himmelmann, 2006; Woodbury, 1999), aiming to produce permanent artifacts such as annotated corpora, reference grammars, and dictionaries. Furthermore, research has explored the potential for computational methods to aid in language documentation and revitalization (Palmer et al., 2009; Moeller and Hulden, 2018; Wiemerslage et al., 2022; Kann et al., 2022; Gessler, 2022; Zariquiey et al., 2022; Zhang et al., 2022; Flavelle and Lachler, 2023). 812101020304050Accuracy\n# ShotsRandomWord RecallWord PrecisionMax CoveragechrF ScoreMorpheme RecallFigure 1: Accuracy of an LLM-based glossing method\non Gitksan data, varying the number of provided exam-\nples and the strategy for selecting examples.\nIn particular, we study the task of generating interlinear glossed text (IGT), a line-by-line format for annotated text corpora that is commonly used in documentation projects. IGT generation has been studied using statistical (Palmer et al., 2009; Samardži ´c et al., 2015; McMillan-Major, 2020) and neural (Moeller and Hulden, 2018; Zhao et al., 2020; Barriga Martínez et al., 2021) methods. A key challenge when working with endangered languages is that, in nearly all cases,1there is very little labeled or unlabeled data available. This is particularly challenging for large neural models which depend on large, representative training data sets. Research has explored methods to overcome this challenge for IGT generation systems, such as crosslingual transfer (He et al., 2023; Okabe and Yvon, 2023; Ginn et al., 2024) and architectural modifications (Girrbach, 2023a), but these approaches struggle in very low-resource scenarios. In addition, previous approaches generally require expertise in model training, implementation, and deployment, as well as the computational resources 1As Liu et al. (2022) notes, not all endangered languages\nare low-resource (and vice versa), and such languages bear\ndifferent concerns when developing language technology.\n1\nneeded to serve large neural models. As large language models (LLMs) have demonstrated impressive performance on various natural language tasks, the question arises whether they can benefit language documentation. We seek to evaluate the ability of current LLMs to generate interlinear glossed text, compared with earlier state-of-the-art methods. This research can also shed light on the language-agnostic capabilities of LLMs, requiring the model to learn patterns in very rare languages which are unlikely to have significant presence in their training data. We study strategies for selecting in-context examples, finding significant impacts to performance. Our best-performing systems outperform transformer model baselines, despite involving no training whatsoever. They still underperform SOTA systems that induce morphological segmentation, but at the same time hold promise for offering a new approach to interlinear glossing for language documentation practitioners. 2 Background 2.1 Interlinear Glosed Text A typical example of IGT is shown in item 1. (1) nuhu’\nthistih-’eeneti-3i’\nwhen.PAST-speak-3PLheneenei3oobei-3i’\nIC.tell.the.truth-3PL“When they speak, they tell the truth.” (Cowell, 2020) The first line (transcription line) contains the text in the language being documented, and may be segmented into morphemes (as here). The second line (gloss line) provides a gloss for each morpheme in the transcription. Glosses may indicate grammatical function or a translation of the morpheme (for stems). The third line contains a translation into a high-resource language such as English. Producing each of these lines requires knowledge of the language and/or skilled linguistic analysis. Generally, automated IGT systems are trained to predict the gloss line given the transcription line (and sometimes the translation as in Zhao et al., 2020; Rice et al., 2024). The primary aim of such systems is to assist a human annotator, providing suggestions for common morphemes that are often glossed with the same label. These systems are not intended to replace human annotators, who are vital to the documentation process, annotating novel morphemes and interesting linguistic phenomena, as well as verifying automatically-produced labels. 1102.2 LLMs for Rare Languages Though LLMs generally have limited understanding of rare and low-resource languages (Ebrahimi et al., 2022), they can often achieve significantly better performance through crosslingual incontext learning (X-ICL), where a number of examples in the target language are provided directly in the prompt to a multilingual model (Winata et al., 2021; Lin et al., 2022; Cahyawijaya et al., 2024). We study X-ICL methods for using LLMs for the task of IGT generation, including complete IGT examples in the prompt. We hypothesize that this approach will leverage both the set of labeled training examples and the robust multilingual knowledge of the language model. In particular, we explore the effects of including an increasing number of examples in context (section 4) and using different strategies to select relevant examples (section 5). 2.3 Related Work A number of approaches have been used for IGT generation. Palmer et al. (2009) uses a maximum entropy classifier and represents the earliest work describing benefits of using automated glossing systems. A number of papers (Samardži ´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020) use statistical classifiers such as conditional random fields. Recent research explores neural models such as recurrent neural networks and transformers (Moeller and Hulden, 2018; Zhao et al., 2020; Barriga Martínez et al., 2021). Other approaches improve glossing performance using crosslingual transfer (He et al., 2023; Okabe and Yvon, 2023; Ginn et al., 2024), hard attention (Girrbach, 2023a), and pseudolabeling (Ginn and Palmer, 2023). IGT data is not only useful for preservation and revitalization projects, but also for downstream tasks such as machine translation (Zhou et al., 2019), developing linguistic resources like dictionaries (Beermann et al., 2020) and UMR (Uniform Meaning Representation) graphs (Buchholz et al., 2024), studying syntax and morphology (Bender et al., 2013; Zamaraeva, 2016; Moeller et al., 2020), and dependency parsing (Georgi et al., 2012). Given the cost and difficulty of obtaining IGT data, research has explored methods to scrape it from Latex documents (Schenner and Nordhoff, 2016; Nordhoff and Krämer, 2022) and even images (Round et al., 2020). Finally, work has attempted to standardize IGT conventions and formats, balancing consistency and expressiveness 2\nacross languages (Lehmann, 1982; Hughes et al., 2003; Nordhoff, 2020; Mortensen et al., 2023). 3 Methodology We study the IGT generation task described in Ginn et al. (2023). Given a transcription line and translation line, systems must predict the gloss line. We focus on the closed track setting, where the input words are not segmented into morphemes. This task is strictly more difficult than the setting where words are already segmented, as models must jointly learn segmentation and gloss prediction. As reported in Ginn et al. (2023), the SOTA on this task remains far weaker than the setting with segmented inputs, with up to a 40 point discrepency in SOTA performance. 3.1 Data We use the IGT corpora and splits from the 2023 SIGMORPHON Shared Task (Ginn et al., 2023), allowing us to directly compare to several other systems. We use the languages described in Table 1. # IGT Examples\nLanguage Train Dev Test\nGitskan [git] 74 42 31\nLezgi [lez] 705 88 87\nNatugu [ntu] 791 99 99\nUspanteko [usp] 9774 232 Table 1: Languages and data splits, originally from Ginn\net al. (2023)\nWe primarily focus on the lower-resource languages from the shared task, where neural methods tended to struggle due to limited training data. We use the data as formatted by Ginn et al. (2024). 3.2 Evaluation We evaluate using the same metrics as the shared task. We primarily report morpheme accuracy , which measures how many morpheme glosses match between the predicted and true glosses. Any predicted glosses beyond the length of the true gloss string are ignored. 3.3 Models We run preliminary experiments using Cohere’s Command R+ model,2a 104B parameter instruction-tuned language model with 128K token context that is designed for multilingual tasks. 2https://docs.cohere.com/docs/command-r-plus3.4 Prompting Though the exact prompt varies from experiment to experiment, all runs use the same base prompt, included in Appendix A. In the system prompt, we define the IGT generation task and desired output format and provide additional information such as the language and list of possible glosses. In the user prompt, we provide few-shot examples (if any) and the target example to be glossed. We run each experiment three times with temperature 0 and a different random seed, ensuring both the retrieval strategy and model API calls are reproducible. We report the average and standard deviation for performance. 4 Many-Shot Prompting Few-shot prompting, where a model is provided with a small number of examples in the context, has proven very effective at a variety of tasks (Brown et al., 2020; Winata et al., 2021; Lin et al., 2022; Cahyawijaya et al., 2024). Furthermore, as model context lengths have continued to increase, it has become possible to provide hundreds or even thousands of examples, and performance typically continues to improve (Bertsch et al., 2024). On the other hand, increasingly long prompts bear a high cost, and strategies to retrieve relevant examples can often achieve similar performance at a fraction of the cost (see section 5). 4.1 Experimental Settings For all experiments, we run two settings, one with just the base task description, and one where we include a list of possible glosses for functional morphemes. We scrape this list of glosses from all of the seen glosses in the training set. We instruct the model to only use these glosses for functional morphemes (while stem morphemes should still be glossed with their translation). We refer to this setting as [+ G LOSSLIST ], with an example gloss list in Appendix B. For each language, we experiment with varying number of examples. For all languages except Gitksan, we run experiments providing no examples (zero-shot) and 1, 2, 3, 5, 10, 30, 50, and 100 examples. Gitksan has fewer than 100 training examples, so we use all 74 for the final setting. For each example in our eval set, we randomly sample examples from the training set to be included in the prompt. In section 5, we compare this strategy to more intentional retrieval strategies that 3\n0510152020406080# Shots(a) Gitksan\n0510152020406080100# Shots (b) Lezgi\n0510152020406080100# Shots (c) Natugu\n0510152025303020406080100# Shots (d) Uspanteko\nFigure 2: Morpheme accuracy of LLM-based glossing on Gitksan, Lezgi, Natugu, and Uspanteko, varying the\nnumber of provided examples. Reported values are averages over three runs; error bars indicate standard deviation.\naim to select relevant examples. 4.2 Results We report results for our languages in Figure 2, with a full table of results provided in Appendix C. Generally, we see that the model has very weak performance in the zero-shot setting, indicating that the model has little knowledge of our chosen languages. In some cases, the zeroshot experiments produce results that are not even in the desired output format. Performance improves drastically for the first few shots added, showing smaller improvements as the number of shots increases. For Gitksan, performance levels up as the number of provided examples approaches the full training set. For the other languages with much larger training sets, performance shows continued improvement even around 100 shots, supporting the findings of Bertsch et al. (2024). We suspect that this trend would continue to some extent, but the cost of providing hundreds of examples quickly becomes infeasible. Relationship between Shots and Accuracy What sort of shape is formed by the curve in Figure 1 and Figure 2? The relationship appears to be roughly logarithmic, starting steep and leveling off. To quantify this relationship, we take thelog(# shots + 1) for each setting.3Figure 3 shows the transformed curve for Gitksan, which now shows a strong linear relationship. We compute the R2value over all settings and report it in Table 2. We observe extremely strong correlation values across all settings. This indicates that the logarithmic model is a good fit for the data, and predicts 3Adding 1 so the zero-shot setting is defined.\ny = 2.746x + 2.2512051015200246Accuracylog(# Shots + 1)Figure 3: Morpheme accuracy for Gitksan, where the\npredictor variable is the logarithm of the number of\nprovided examples (plus one).\nLanguage Base + Glosslist\nGitksan 0.962 0.Lezgi 0.934 0.Natugu 0.993 0.Uspanteko 0.952 0.Table 2: Coefficient of determination ( R2) computed\nbetween morpheme accuracy and log(# shots + 1)\n.\nthat maintaining steady performance improvements requires exponentially more examples. Effect of Gloss List We initially hypothesized that providing a complete list of possible glosses in the prompt could help the model better adhere to the desired glossing conventions. We report a summary plot of the difference in accuracy between the two settings across languages in Figure 4. The average difference is close to 0, well within a standard deviation in all cases, and thus there is little evidence to suggest that including the gloss list meaningfully affects performance. A possible explanation is that since the model has very limited prior knowledge of these languages, providing a list of glosses without any explanation or examples does not provide any useful information. 4\nAcc+Glosslist—AccBase-3-2-10GitksanLezgiNatuguUspantekoFigure 4: Difference in averaged accuracy between set-\ntings with and without a complete gloss list provided in\nthe prompt. We observe minimal differences.\nTo investigate whether including a gloss list changes the predictions at all, we measure the adherence percentage . This metric is computed by dividing the number of predicted (functional) glosses that adhere to the gloss list by the total number of predicted glosses. We report the distribution over languages and settings in Figure 5. /uni00000027/uni0000015d/uni0000019a/uni0000016c/uni00000190/uni00000102/uni00000176 /uni0000003e/uni0000011e/uni000001cc/uni00000150/uni0000015d /uni00000045/uni00000102/uni0000019a/uni000001b5/uni00000150/uni000001b5 /uni00000068/uni00000190/uni00000189/uni00000102/uni00000176/uni0000019a/uni0000011e/uni0000016c/uni0000017d/uni000003ec/uni000003ee/uni000003ec/uni000003f0/uni000003ec/uni000003f2/uni000003ec/uni000003f4/uni000003ec/uni000003ed/uni000003ec/uni000003ec/uni00000004/uni0000011a/uni0000015a/uni0000011e/uni0000018c/uni0000011e/uni00000176/uni00000110/uni0000011e/uni00000003/uni00000057/uni0000011e/uni0000018c/uni00000110/uni0000011e/uni00000176/uni0000019a/uni00000102/uni00000150/uni0000011e\n/uni00000011/uni00000102/uni00000190/uni0000011e /uni0000043d/uni00000027/uni0000016f/uni0000017d/uni00000190/uni00000190/uni0000016f/uni0000015d/uni00000190/uni0000019a\nFigure 5: Distribution of adherence percentages, across\nlanguages, comparing with and without the glosslist.\nWe observe that including the gloss list in the prompt is effective for increasing adherence compared to the base setting. While the experiments without the gloss list vary widely, the experiments with it nearly always use glosses from the list. On the other hand, we have observed no evidence that the gloss list improves performance, suggesting that the model may be predicting glosses from the list randomly. Furthermore, including a gloss list in the prompt carries a fixed cost of several hundred tokens for every prompt (e.g. for Uspanteko, the cost is 124 tokens). Since it provides negligible benefit, we opt to omit the glosslist for future experiments in 316order to reduce cost. 5 Retrieval Strategies While including a large number of in-context examples can certainly improve performance, long prompts carry a high cost that may be infeasible for real-world documentation projects. For example, running prompts with a thousand examples in Uspanteko costs roughly 10 cents per inference call, which can quickly add up over thousands of examples. Many LLMs still have limited context length, particularly among open-source models, and including many examples may not even be possible. Finally, Bertsch et al. (2024) suggests that the effectiveness of many-shot prompting is mainly due to the model seeing relevant examples, and ignoring many irrelevant ones. With this in mind, we consider a method inspired by retrieval-augmented generation (RAG, Lewis et al. 2020). RAG was originally used for knowledge-intensive tasks, using document embeddings to search for relevant documents to a given query and include them in prompt context. We apply a similar strategy in order to search for relevant IGT examples from our training corpus to include in our prompt. 5.1 Experimental Settings We consider several strategies for selecting examples that are relevant for the target sentence. Random As a baseline, we use the random strategy from the prior section, which simply samples nexamples randomly from the training corpus. Word Recall and Word Precision We hypothesize that a straightforward way to improve performance is by providing examples which have the same morphemes as the target sentence. Since our data is not segmented into morphemes, we instead look for matching words (which will nearly always be composed of the same morphemes). We split each example into words using whitespace, and compute the word recall for a target sentence T and candidate training sentence S. WORDRECALL =|unique (S)∩unique (T)|\n|unique (T)|\n(1) This computes the fraction of unique words in the target sentence that appear in the candidate sentence. We can also compute the word precision 5\nwith a slightly modified formula: WORDPRECISION =|S∩unique (T)|\n|S|(2) This metric rewards examples where the majority of words in the candidate are in the target sentence. Notice that we do not use the unique words ofS, instead weighting an example that uses the same word from Tseveral times more heavily. We select the examples with the highest word recall or precision, considering each example independently and breaking ties randomly. Aggregate Word Recall One limitation of the prior approach is that by considering each candidate individually, we can potentially select several redundent examples in few-shot scenarios. Instead, we can compute the aggregate word recall over a candidate sample of nexamples. Sagg=n[\ni=1unique (Si) (3) AGGWORDREC=|Sagg∩unique (T)|\n|unique (T)|(4) This metric rewards samples that jointly cover more of the words in the target. This is equivalent to the Maximum Coverage Problem , and as such is NP-Hard (Nemhauser et al., 1978). We use the greedy algorithm, which runs in polynomial time (Hochbaum, 1996). chrF A limitation of the previous strategies is that, by only considering atomic words, there is no way to select examples that may contain the same morphological units. One way we can attempt to capture morphological similarity is through using substring similarity metrics such as chrF (Popovi ´c, 2015) and chrF++ (Popovi ´c, 2017). These metrics compute the F-score of character n-gram matches (chrF++ also incorporates word n-grams), and have been shown to correspond more closely to human judgements for machine translation. Morpheme Recall Although we do not have segmented data, much research has explored methods to induce morphological segmentations from data in an unsupervised manner. In particular, we use Morfessor (Creutz and Lagus, 2005), a popular statistical method that seeks to find a segmentation that maximizes overall the probability of segmented words. 404We create silver segmentations using Morfessor and compute the recall metric as described earlier, but using morphemes rather than words. We train the segmentation model use the default parameters on the training data and use Viterbi inference to segment test examples. We use the Morfessor 2.0 library (Virpioja et al., 2013). 5.2 Results We report results across our four languages and six retrieval strategies in Figure 6. We run tests using 1, 2, 5, 10, 30, and 50 examples in each prompt. Comparison with Random Retrieval Across all languages, we observe clear and significant improvements over the random selection method described in the prior section (here indicated with a gray line). This is the case both with a small number of fewshot examples and as the number grows large. The only exception is the 50 example setting for Gitksan, at which point the provided examples make up a large fraction of the training corpus. This is an intuitive result, as the IGT generation task requires, at minimum, knowledge about the words of a language and their potential glosses. Even a simple baseline that glosses tokens with their most common gloss from the training set is often fairly effective (Ginn et al., 2023). This is particularly important since the LLM used seems to have very limited prior knowledge of the language, as evidenced by the poor zero-shot performance. Relationship between Shots and Accuracy As before, we generally see consistently improving performance as additional examples are added. However, there are several cases where performance drops going from 30 to 50 shots, as in Gitksan (Word Precision, Max Coverage, and Morpheme Recall) and Lezgi (chrF Score). Both of these languages have fairly small corpora, and it is possible that after a point these strategies run out of beneficial examples, and any additional examples simply contribute noise to the prompt. Effect of Different Granularities Many of the strategies perform very similarly, but there are some observable trends across granularity levels (word, morpheme, and substring). We observe that the chrF strategy is nearly always the most effective, outperforming the word- and morpheme-based strategies in most cases. We hypothesize that this strategy strikes a balance by selecting examples 6\nRandomWord RecallWord PrecisionMax CoveragechrF ScoreMorpheme Recall\n812101020304050Accuracy\n# Shots(a) Gitksan\n051015202530301020304050Accuracy\n# Shots (b) Lezgi\n051015202530301020304050Accuracy\n# Shots\n(c) Natugu\n010203040501020304050Accuracy\n# Shots (d) Uspanteko\nFigure 6: Morpheme accuracy of LLM-based glossing on four languages, varying the number of provided examples\nand using different strategies to select relevant examples. Reported values are averages over three runs.\nwith subword similarity, but not introducing error due to noisy morpheme segmentations. Word Recall vs Morpheme Recall We observe mixed results across the Word Recall and Morpheme Recall strategies. We observe a few settings where there appears to be a significant gap between the two (Gitksan at 30 shots; Lezgi at 50 shots), but generally the strategies are close. It is possible that the words in our evaluation examples often either are monomorphemic, or contain a combination of morphemes already observed in the training data, and thus selecting relevant examples according to morphemes has little benefit. Word Recall vs Word Precision While the Word Recall and Word Precision strategies both seek to quantify the word-level similarity between the target and candidate sentences, they are computed slightly differently and produce different results. The Word Recall strategy prioritizes candidate sentences that contain a large fraction of the word types in the target sentence, ignoring repeated words. 473Meanwhile, the Word Precision strategy selects candidates based on the fraction of words within the candidate that are also in the target. The Word Recall strategy consistently outperforms Word Precision, except for the two largest settings in Gitksan. This indicates that it is more important to provide examples which cover the words in the target than it is to provide several examples for a single word. Word Recall vs Max Word Coverage We experimented with the Max Word Coverage setting, where we consider the recall of the selected set of candidates as whole, rather than individually. We observe minimal benefits, in fact underperforming the Word Recall setting in many cases. 6 Comparison with SOTA Finally, we compare our best-performing strategies from the prior section with several previous baseline methods: •Thetoken classification transformer model 7\nof Ginn et al. (2023), which uses an encoder model to predict glosses word-by-word •Tü-CL from Girrbach (2023b), which uses hard attention to induce latent segmentations and predict glosses on segmented words For the LLM-based method, we select the chrF strategy and test with 30 examples for Gitksan and 100 examples for the other languages. We make some small prompt optimizations described in Appendix A, and raise the temperature to 0.2. We use the following language models: •Cohere’s Command R+ , which was used for preliminary experiments. •OpenAI’s GPT-4o , specifically the gpt4o-2024-05-13 checkpoint (OpenAI, 2024) •Google’s Gemini 1.5 Pro (Gemini Team, 2024) We run evaluation on the held out test set and report results in Figure 7. 6.1 Discussion We observe that the LLM based glossing strategies outperform a simple transformer in all languages, despite using no training whatsoever and using a small fraction of the training set as examples. Of the LLM models, Gemini performs best on three languages. However, we note that Gemini refuses to produce answers for many examples, which we count as completely wrong. If we omit such examples, Gemini’s performance is even higher, achieving 55.9%, 50.8%, and 63.9% accuracy on Lezgi, Natugu, and Uspanteko respectively. On the other hand, the LLM methods typically underperform the SOTA method of Girrbach (2023b), except for Gitksan, where the best LLM (Gemini) outperforms by 6.5 points. The Girrbach (2023b) approach explicitly models segmentation through a learned latent representation, which our strategy does not utilize. Future work with LLMbased methods could explore an analogous process, explicitly prompting the LLM to generate segmentations before producing final glosses. Furthermore, these methods will likely continue to improve as LLMs become more capable for rare (or even completely unseen) languages, as measured by benchmarks such as Tanzer et al. (2024). Most trivially, as LLMs with increasingly long contexts are developed, we can provide more examples in-context, which our results indicate will continue to provide benefits. 8.540.719.457.211.759.956.270.012.341.336.050.211.751.637.061.418.253.549.647.6020406080gitlezntuuspToken classTu-CLCohereGPT-4GeminiFigure 7: Morpheme accuracy results on test splits,\ncomparing several LLMs and baseline systems.\n7 Conclusion We find that SOTA large language models struggle to produce interlinear glosses for the endangered languages used in our research. However, by selecting relevant examples from a training corpus and providing them as part of the context for each example to be glossed, we can significantly improve performance. We find that the relationship between performance and the number of few-shot examples is roughly logarithmic. Performance improves by a wide margin when we select examples with a high chrF++ score relative to the target sentence. Our best systems outperform a standard transformer model, despite involving no explicit training and using a fraction of the training data. However, they still underperform the SOTA system for the glossing task on three out of four languages. Thus, for documentary linguists hoping to use automated glossing solutions, the use of LLMs may not achieve ideal accuracy. At the same time, LLMs may still be a preferrable choice for languages with very limited data comparable to Gitksan, and the use of an API is often far more accessible than training and hosting a neural model. Our results encourage further exploration of this approach. 8\nLimitations While we have selected a small set of languages that we believe give insight into the performance of automated glossing systems, they are certainly not representative of all the world’s languages. In particular, LLMs may struggle more with languages that use non-Latin writing scripts (Zhang et al., 2023). We use a single prompt template for the majority of experiments and do not conduct extensive prompt engineering. Frameworks such as DSPy (Khattab et al., 2024) have shown that prompt optimization can often greatly improve performance, so it is entirely possible that we could achieve better performance on this problem with the same models and strategies. We evaluate three popular closed-source LLMs, but results may vary across other models. In particular, we have not yet considered open-source, local LLMs due to resource constraints. Ethics Statement As our work involves documentation data produced through the combined efforts of documentary linguists and speakers of endangered languages, we strive to respect their desires and avoid treating data as merely a resource to train models with (Schwartz, 2022). We do not intend for automated glossing systems to replace human annotators, which would drastically impact the quality, novelty, and utility of annotated corpora, but rather to serve as a tool available to support documenters. Finally, we acknowledge that the use of large language models carries a high environmental cost, and make efforts to minimize unnecessary API calls and to track our usage. 603",
        "aspect_review": "The paper presents a novel approach to generating interlinear glossed text using large language models with in-context learning, bypassing the need for traditional training. This method is particularly useful for low-resource settings. The results demonstrate that LLM-based methods outperform standard transformer baselines without any training, offering a practical solution for researchers outside the NLP community.\n\nWhile the approach is innovative in its application of LLMs to IGT generation, it still underperforms compared to state-of-the-art supervised systems, which limits its novelty. The paper could benefit from a deeper exploration of the intuition and theoretical justification behind the proposed in-context learning strategies. The LLM-based methods, although practical, do not reach the performance levels of state-of-the-art systems, indicating room for improvement. The presentation of results could be enhanced with clearer figures and a more concise explanation of the experimental setup. The methodology, while sound, could be strengthened by incorporating more diverse evaluation metrics to capture different aspects of model performance. The paper lacks detailed discussion on the limitations and potential biases of using LLMs for endangered languages, which is crucial for understanding the broader impact of the work.",
        "general_review": "The paper presents a novel approach to generating interlinear glossed text (IGT) using large language models (LLMs) without traditional training, which is a significant advancement in the field of language documentation. The methodology is innovative, leveraging in-context learning and strategic example selection to improve performance. The research provides comprehensive experimental validation across multiple languages, demonstrating the practical applicability of the proposed methods. The findings have significant practical impact, offering a more accessible and less resource-intensive solution for language documentation practitioners, especially those working with endangered languages.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of experimental settings and results. 2. More detailed information on hyperparameters and implementation specifics would enhance reproducibility. 3. Some figures could be formatted more clearly to improve readability. 4. A slight clarification on code availability and access to the datasets used would be beneficial."
    },
    "44": {
        "paper": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\nModification\nAnonymous ACL submission\nAbstract Recent advances in fine-tuning VisionLanguage Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code will be released. 1 Introduction Large pre-trained Visual-Language Models (VLMs) have been developed a lot in recent years. For example, CLIP (Radford et al., 2021) and ALIG (Jia et al., 2021) demonstrated remarkable performance for various tasks, e.g., image recognition in a zero-shot fashion. To further improve the performance on the specific downstream tasks, prompt tuning (Lester et al., 2021; Yao et al., 2023; Zhu et al., 2023; Zhou et al., 2022a) and adapter tuning (Gao et al., 2023; Zhang et al., 2021) methods have been proposed. As shown in Fig. 1, prompt tuning methods proposed to introduce 042a set of learnable prompt vectors as the input of the text encoder while adapter tuning approaches adopted an additional bottleneck layer to learn new features. During the fine-tuning procedure, both of these two strategies keep CLIP’s parameters fixed. The performance of prompt tuning and adapter tuning methods are superior on various tasks (Zhou et al., 2022b; Gao et al., 2023), so research on fine-tuning the inherent parameters of VLMs has been barely touched. For language models, fully fine-tuning with downstream data can achieve promising results (Zaken et al., 2021; Liu et al., 2022). Moreover, recent works in language model fine-tuning (e.g., BitFit (Zaken et al., 2021)) have demonstrated that, without introducing any external parameters, finetuning only the bias terms in a pre-trained model can perform competitively on downstream tasks compared with fine-tuning the entire model. For VLMs, however, it is believed that fine-tuning the parameters of VLMs corrupts the inherent pretrained knowledge as fully fine-tuning degrades performance (Zhou et al., 2022b). In this paper, we revisit this viewpoint and ask if, without introducing any external parameters, fine-tuning the inherent parameters of VLMs can achieve competitive performance compared with prompt tuning. We start with directly applying BitFit to finetuning the CLIP model. We explore two strategies: (i) applying BitFit to the text encoder alone, and (ii) applying BitFit to both the text and image encoder. We found that both two strategies can acquire taskspecific knowledge but their performance to unseen class data can be poor (more discussed in Sec. 4.4), implying that directly fine-tuning the bias terms of a text or image encoder may harm the model’s generalization ability. These findings motivate us to develop more effective and efficient fine-tuning techniques for VLMs. In light of this, we propose CLIPFit, a simple yet effective method for efficiently fine-tuning VLMs. 1\nPrompt\n🔥[class]Text\tEncoderImage\tEncoder\n❄\n❄cos(a)\tPrompt\tTuning\tA\tphoto\tof\ta\t[class].Text\tEncoderImage\tEncoder\n❄\n❄cos(b)\tAdapter\tTuning\tAdapter\n🔥\nText\tEncoderImage\tEncodercos(c)\tCLIPFit–Our\tMethod\tA\tphoto\tof\ta\t[class].\n🔥/\n❄\n🔥/\n❄Figure 1: Comparison of (a) prompt tuning methods, (b) adapter tuning methods, and (c) our proposed CLIPFit\nmethod. Prompt tuning methods introduce a set of learnable external parameters as input to learn task-specific\nknowledge. Adapter tuning methods introduce extra learnable networks following the image encoder to learn\ntask-specific features. Unlike these two methods, our CLIPFit does not introduce external parameters and fine-tunes\nonly a small portion of the CLIP model.\nCLIPFit is orthogonal to previous prompt tuning and adapter tuning methods, as shown in Fig. 1 (c). For the text encoder, instead of fine-tuning all the bias terms, CLIPFit proposes to tune only the bias terms of projection linear layers in feed-forward networks (FFNs). Fine-tuning only the bias terms of projection linear layers in FFNs will reduce the number of training parameters compared with finetuning all the bias terms. Moreover, empirically, we discovered that our bias term tuning strategy can generalize better than BitFit (Zaken et al., 2021), as shown in Sec. 4.4. For the image encoder, as mentioned before, it may harm the model’s performance if directly applying BitFit. In the image encoder, layer normalization (LayerNorm) (Ba et al., 2016) aims to normalize the distributions of intermediate layers. Since the distributions of pretraining and downstream data might be divergent, pre-trained LayerNorm might lead to sub-optimal performance for downstream data inference. Therefore, CLIPFit proposes to further update only the parameters of the image encoder’s LayerNorm. Updating LayerNorm can yield a better image encoder for downstream data. Lastly, previous studies (Yao et al., 2023) have shown that generic pre-trained knowledge is easily forgotten in the fine-tuning stage. Therefore, we explored two different regularization strategies for alleviating forgetting: (i) using the knowledge distillation (KD) loss (Hinton et al., 2015) to guide CLIPFit to learn from the zero-shot CLIP; (ii) using the mean squared error (MSE) loss in bias terms to penalize changes in text encoder. We empirically found that both two strategies can alleviate forgetting problems and the KD loss performs better, thus we used the KD loss as the final solution for CLIPFit. Fine-tuning is an empirical and black-box pro- 120cess. So, understanding how fine-tuning affects the pre-trained models is important for uncovering the black-box fine-tuning process. Previous works (Zhou and Srikumar, 2022; De Vries et al., 2020; Merchant et al., 2020) explored this for language models fine-tuning. However, very little work explored the internal black-box fine-tuning process for VLMs. In this paper, we conducted an initial exploration to analyze VLM fine-tuning process of CLIPFit, focusing on changes in internal parameters and representations. We found that for bias terms in the FNN of the text encoder, as the number of layers increases, the change in bias decreases, which means that during the fine-tuning process, low-level features in the text encoder change more than high-level features. For LayerNorm in the image encoder, we found that the first layer (patch embedding) changes much more than other layers. Experimentally, we showed that more changed layers play a more important role in adapting downstream knowledge than less changed layers. Moreover, we explored how KD loss affects the finetuning process for alleviating forgetting. We found that KD loss will reduce the changes for the morechanged low-level bias terms and enhance changes in less-changed high-level layers, which implies that penalizing changes for low-level bias terms is important for avoiding overfitting. Lastly, we found that tuning LayerNorm will form a better image feature space compared with zero-shot CLIP. We conducted extensive experiments on 11 datasets in 4 different settings to show the effectiveness of the proposed CLIPFit. Overall, our main contributions can be summarized as follows: •We propose a CLIPFit method for efficiently fine-tuning the CLIP model to uncover the power of classic model fine-tuning on VLMs. 2\nUnlike existing prompt tuning or adapter tuning methods, CLIPFit does not introduce any external parameters and only fine-tunes a small specific subset of CLIP’s inherent parameters. •To analyze how CLIPFit affects the pretrained models, we conducted extensive analyses during the fine-tuning process, focusing on the changes in parameters and representations. These analyses help us better understand the black-box fine-tuning process. •We conducted extensive experiments on 11 datasets. Results show that CLIPFit brings a 7.33% improvement in harmonic mean accuracy compared with zero-shot CLIP on the 16shot base-to-new setting, demonstrating that CLIPFit is a promising alternative to prompt tuning and adapter tuning. 2 Related Works Visual-Language Models (VLMs). With largescale available web-crawled image-text pairs (Schuhmann et al., 2022), pre-training VLMs have been developed fast in recent years (Xu et al., 2021; Radford et al., 2021; Jia et al., 2021; Wang et al., 2022a) and achieved remarkable zero-shot performance in the downstream tasks, e.g., image classification. Despite the remarkable transfer ability, the potential of VLMs can be further stimulated by fine-tuning it with few-shot downstream data (Song et al., 2022; Zhang et al., 2021; Shen et al., 2021; Wang et al., 2022c,b; Chen et al., 2023). Parameter-efficient Fine-tuning (PEFT) on VLMs. There are mainly two categories of VLM parameter-efficient fine-tuning methods: prompt tuning (Zhou et al., 2022b,a; Chen et al., 2022; Yao et al., 2023; Zhu et al., 2023; Zhang et al., 2023; Khattak et al., 2023) and adapter tuning (Gao et al., 2023; Zhang et al., 2021). Prompt tuning methods for VLMs introduced a few learnable parameters (prompts) as input, which were inspired by language prompt tuning (Lester et al., 2021). Adapter tuning methods set an additional bottleneck layer following the text or image encoder to learn better features by a residual way. Both prompt tuning and adapter tuning methods boost CLIP’s performance, so research on fine-tuning the inherent parameters of CLIP seems to be overlooked. To explore classic model fine-tuning on VLMs, our CLIPFit proposes to fine-tune CLIP by modifying a small portion 206of the CLIP model’s inherent parameters without introducing any external learnable parameters. PEFT on Large Language Models. Fully finetuning language models (Radford et al., 2018; Devlin et al., 2018) can achieve promising results but is expensive. To efficiently fine-tune pre-trained language models, a lot of approaches have sought to fine-tune only a small number of parameters. For example, adapter methods (Bapna et al., 2019; Houlsby et al., 2019; Pfeiffer et al., 2020) and prompt tuning methods (Liu et al., 2023; Lester et al., 2021; Brown et al., 2020; Gao et al., 2020) introduce a set of learnable external parameters for adaptation to downstream tasks. Recently, BitFit (Zaken et al., 2021) demonstrated that, without introducing any new parameters, fine-tuning only the bias terms in pre-trained language models can perform competitively compared with fully fine-tuning. Moreover, to understand how finetuning affects pre-trained models, various works (Zhou and Srikumar, 2022; Mosbach et al., 2020; De Vries et al., 2020; Merchant et al., 2020) have explored this with LLM fine-tuning. However, very little work was attempted on the VLM side. In this paper, we attempt to bridge this gap by conducting an initial exploration to analyze the fine-tuning process in CLIPFit for VLMs, focusing on changes in internal parameters and representations. 3 Methodolgy In this section, we introduce CLIPFit. We first briefly review CLIP and then illustrate CLIPFit. 3.1 Review of CLIP We first briefly review CLIP (Radford et al., 2021). During pre-training, CLIP aims to align image features and text features in the joint embedding space to capture the relationship between images and texts. Let D={(xi,ti)}b\ni=1be the sampled batch, where xiis the input image, tiis the input text and bis the batch size. A CLIP model is comprised of two types of encoders: visual encoder EI(·,θI) and text encoder ET(·,θT). The visual encoder encodes image xiintofiand text tiintogi, i.e., fi=EI(xi,θI),gj=ET(ti,θT).(1) Then, a contrastive learning loss is applied to them for alignment. After pre-training, CLIP can perform zero-shot image recognition by comparing the image features with class weights {wi}K\ni=1, where Kis the number of classes. The class weight wiis generated 3\nA\tphoto\tof\ta\t[class].Word\tEmbed\tAttnLayer\tNormFFNText\tEncoderFFNImage\tEncoderEmbeding\nEmbedingLinearLayer×N\n×N\n🔥\n❄\nLayer\tNorm\n❄\nx1…x1·y1x1·y2x1·y3x1·yn…x2x3xny1y2y3ynx2·y1x2·y2x2·y3x2·ynx3·y1x3·y2x3·y3x3·ynxn·y1xn·y2xn·y3xn·yn………………………\n❄Attn\n❄\n❄Layer\tNorm\n🔥\nLayer\tNorm\n🔥Bias\n🔥\n❄EmbedingFFN\nImage\n❄Weight\nPatch\tEmbed\tcatdogbird…\n🔥\n❄TunedFrozenFigure 2: An overview of our CLIPFit. Unlike existing prompt tuning methods or adapter tuning methods, CLIPFit\ndoes not introduce any external parameters and fine-tunes specific inherent parameters of CLIP. For the text\nencoder, as shown in the upper part of the figure, CLIPFit fine-tunes only the bias terms of projection linear layers in\nfeed-forward networks. For the image encoder, as shown in the lower part of the figure, CLIPFit updates LayerNorm.\nby text encoder ET(·,θT)which takes the class descriptions (prompts) as input. These prompts usually take the form “a photo of a [CLASS].”, where the class token will be replaced by the i-th class name (e.g., cat) for weight wi. Formally, for an image feature f, the probability that it belongs to class iis calculated by p(y=i|x) =exp (cos ( wi,f)/τ)PK\nj=1exp (cos ( wj,f)/τ),(2) where τis a temperature parameter learned by CLIP during pre-training and cos(·,·)denotes the cosine similarity function. 3.2 CLIPFit The overall pipeline of the proposed CLIPFit is shown in Fig. 2. Without introducing any external parameters, CLIPFit involves fine-tuning only the bias terms of projection linear layers in FNNs of the text encoder and updating LayerNorm (Ba et al., 2016) in the image encoder. Text Encoder. For the text encoder, instead of fine-tuning all bias terms, CLIPFit fine-tunes only the bias terms of projection linear layers (i.e., second layers) in the FFNs of the text encoder. Fine-tuning only part of bias terms will reduce the number of training parameters compared with fine-tuning all bias terms. Moreover, Sec. 4.4 will empirically show that our bias tuning method can 281achieve better performance compared with finetuning all bias terms (Zaken et al., 2021). Image Encoder. As mentioned in Sec. 1, directly applying BitFit (Zaken et al., 2021) to the image encoder may cause a negative impact on the model’s performance. Instead of fine-tuning the bias terms of the image encoder, CLIPFit proposes to fine-tune LayerNorm. In LayerNorm, the two learniable parameters gain gand bias bare applied for affine transformation on normalized input vectors xfor re-centering and re-scaling, which are expected to enhance the expressive power by re-shaping the distribution (Ba et al., 2016). Different data distributions should produce different gains and biases in LayerNorm for distribution reshaping during the training process. So, if shifted gains and biases in LayerNorm are applied during inference, it may lead to a sub-optimal solution. Therefore, CLIPFit proposes to fine-tune LayerNorm in the image encoder. Loss function. Previous works (Yao et al., 2023) have verified that during the fine-tuning stage, generic pre-trained knowledge is easily forgotten. Therefore, we explore two different strategies for alleviating such forgetting. The first one is to use the knowledge distillation (Hinton et al., 2015; Yao et al., 2023) loss to guide CLIPFit to learn from the original zero-shot CLIP. Let {wclip\ni}K\ni=1and {wi}K\ni=1be the text features from original CLIP and text features from CLIPFit. The training loss 4\nand KD loss of CLIPFit are defined by L=Lce+βLkg, (3) Lkg=1\nKKX\ni=1cos(wclip\ni,wi), (4) whereLceis the cross entropy loss for classification (Zhou et al., 2022b,a) and βis a hyperparameter. The second strategy is using the MSE loss in bias terms to penalize changes in the text encoder. Let{bclip\ni}L\ni=1and{bi}L\ni=1be the unfixed text bias terms from pre-trained CLIP and unfixed text bias terms from CLIPFit, where Lis the number of unfixed bias layers. The MSE loss is defined as Lmse=1\nLLX\ni=1||bclip\ni−bi||2. (5) We found that both strategies can alleviate the forgetting problems and the KD loss performs better (as discussed in Sec. 4.3), thus we adopted the KD loss as the final solution for CLIPFit. 4 Experiments In this section, we show and discuss the experimental results. To evaluate the effectiveness of our proposed method, we conducted extensive experiments and analyses on 11 datasets. 4.1 Experimental Setup Datasets. Following CoOp, we conducted extensive experiments on 11 public classification benchmark datasets to evaluate CLIPFit. The datasets are ImageNet (Deng et al., 2009), Caltech101 (FeiFei et al., 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback and Zisserman, 2008), Food101 (Bossard et al., 2014), FGVCAircraft (Maji et al., 2013), SUN397 (Xiao et al., 2010), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), and UCF101 (Soomro et al., 2012). Implementation details. We implemented our method with PyTorch (Paszke et al., 2019). The experiments were based on the vision backbone with Vit-B/16 (Dosovitskiy et al., 2020). We followed CoOp to preprocess input images. We used a single text prompt for all experiments for a fair comparison. We used SGD optimizer with batch size set as 32, and set the learning rate as 0.002 (Zhou et al., 2022b). All results reported below are the average of three runs with different random seeds. The training epoch was set to 100 for all datasets except ImageNet and Food101. βwas set to 8 for 357Dataset CLIP CoOp CoCoOp Adapter KgCoOp MaPLe CLIPFit\nAverageBase 69.34 82.69 80.47 82.23 80.73 82.28 83.72\nNew 74.22 63.22 71.69 70.61 73.60 75.14 74.84\nHM 71.70 71.66 75.83 75.98 77.00 78.55 79.03\nImageNetBase 72.43 76.47 75.98 76.13 75.83 76.66 76.2\nNew 68.14 67.88 70.43 67.17 69.96 70.54 70.17\nHM 70.22 71.92 73.10 71.37 72.78 73.47 73.06\nCaltech101Base 96.84 98.00 97.96 97.40 97.72 97.74 98.3\nNew 94.00 89.81 93.81 93.23 93.70 94.36 93.7\nHM 95.40 93.73 95.84 95.51 96.03 96.02 95.94\nOxfordPetsBase 91.17 93.67 95.20 94.33 94.65 95.43 95.23\nNew 97.26 95.29 97.69 97.10 97.76 97.76 97.13\nHM 94.12 94.47 96.43 95.69 96.18 96.58 96.17\nStanford\nCarsBase 63.37 78.12 70.49 76.10 71.76 72.94 78.80\nNew 74.89 60.40 68.87 71.20 75.04 74.00 73.87\nHM 68.65 68.13 72.01 72.30 73.36 73.47 76.26\nFlowers102Base 72.08 97.60 94.87 97.23 95.00 95.92 96.83\nNew 77.80 59.67 71.75 69.27 74.73 72.46 73.53\nHM 74.83 74.06 81.71 80.90 83.65 82.56 83.59\nFood101Base 90.10 88.33 90.70 90.37 90.50 90.71 90.6\nNew 91.22 82.26 91.29 90.83 91.70 92.05 91.33\nHM 90.66 85.19 90.99 90.6 91.09 91.38 90.96\nFGVC\nAircraftBase 27.19 40.44 33.41 38.70 36.21 37.44 42.47\nNew 36.29 22.30 23.71 32.27 33.55 35.61 33.47\nHM 31.09 28.75 27.74 35.19 34.83 36.50 37.43\nSUN397Base 69.36 80.60 79.74 81.57 80.29 80.82 81.97\nNew 75.35 65.89 76.86 74.03 76.53 78.70 78.17\nHM 72.23 72.51 78.27 77.62 78.36 79.75 80.02\nDTDBase 53.24 79.44 77.01 79.53 77.55 80.36 81.97\nNew 59.90 41.18 56.00 51.67 54.99 59.18 63.5\nHM 56.37 54.24 64.85 62.64 64.35 68.16 71.56\nEuroSATBase 56.48 92.19 87.49 87.70 85.64 94.07 93.33\nNew 64.05 54.74 60.04 58.83 64.34 73.23 71.07\nHM 60.03 68.69 71.21 70.42 73.48 82.35 80.69\nUCF101Base 70.53 84.69 82.33 85.47 82.89 83.00 85.23\nNew 77.50 56.05 73.45 72.97 76.67 78.66 77.3\nHM 73.85 67.46 77.64 78.73 79.65 80.77 81.07\nTable 1: Accuracy comparison on Base-to-new generalization\nof CLIPFit with previous methods. Adapter: CLIP-Adapter.\nall datasets on the base-to-new and cross-dataset setting, and 2 for the distribution shift setting. For the few-shot setting, we set βto 2 for all datasets except SUN397 and DTD. More implementation details are provided in appendix B. Comparisons. We compared our method against state-of-the-art methods: zero-shot CLIP, prompt tuning methods: CoOp, CoCoOp (Zhou et al., 2022a), ProGrad (Zhu et al., 2023), KgCoOp (Yao et al., 2023), MaPLe (Khattak et al., 2023) and adapter tuning methods: CLIP-adapter, Tip-adapter (Zhang et al., 2021). Detailed introductions to these methods can be found in appendix C. 4.2 Comparisons with State-of-the-arts Results on base-to-new generalization setting Following Zhou et al. (2022b), we split each dateset into two disjoint groups: the base class dataset and the new class dataset. All compared methods and the proposed CLIPFit were trained on the base class dataset and evaluated on the new class dataset. We conducted 4/8/16-shot experiments, 5\nfollowing Yao et al. (2023). We reported base and new class accuracies (Base and New) and their harmonic mean accuracy (HM). The 16-shot results are shown in Table 1, and 4/8-shot results are provided in appendix G. As shown in Table 1, CLIPFit achieves 6 best HM accuracies among 11 datasets and the best average HM accuracy, which demonstrates that CLIPFit can not only learn well on seen base class data but also can generalize well to data from unseen new classes. A notable issue with previous methods like CoOp, CoCoOp, ProGrad, and KgCoOp is that they usually perform well only on either the base or new class. To alleviate this issue, MaPLe proposes a multi-modal prompt learning strategy for CLIP tuning which improves a lot over HM compared to previous methods. Compared to MaPLe, our CLIPFit achieves better HM accuracy and average performance on the base class, with slightly lower average performance on the new class. It is important to note that CLIPFit only needs to tune nearly 46K parameters while MaPLe needs to tune nearly 3.55M parameters for each task, which is 77 times more than CLIPFit, meaning that CLIPFit fine-tunes significantly fewer parameters and is much more efficient. Results on few-shot learning setting. To verify whether the proposed CLIPFit can learn taskspecific knowledge, we also compared CLIPFit with other existing methods on the few-shot learning setting. Following Zhou et al. (2022a), we used 1, 2, 4, 8, and 16-shot sets for training and reported accuracy performance. We report the results of the average accuracy of 11 datasets in Table. 2, and report all results on each dataset in appendix F. As shown in Table 2, compared with other methods, CLIPFit shows overall consistent improvements among all 1/2/4/8/16-shot settings. This demonstrates that CLIPFit can successfully learn taskspecific knowledge. It is worth noting that CLIPFit outperforms other methods by a large margin in 1/2/4-shot settings, demonstrating CLIPFit’s robust ability to learn with extremely few samples. Results on the robustness to distribution shift setting. Following Zhang et al. (2021), we evaluated the robustness under distribution shift of CLIPFit and other methods by first training models on the 16-shot ImageNet dataset and then evaluating on ImageNet-V2 (Recht et al., 2019) and ImageNetSketch (Wang et al., 2019). The label sets of two evaluating datasets are subsets of the label set of ImageNet. Although the label sets are compatible, the distributions of these three datasets are 430Table 2: Comparison of CLIPFit and other methods on\nthe few-shot learning setting. We report average accu-\nracy on 11 datasets for the 1/2/4/8/16-shot setting.\nMethodshot\n1 2 4 8 16\nCoOP 68.09 70.13 73.59 76.45 79.01\nCLIP-adapter 67.87 70.20 72.65 76.92 79.86\nCoCoOp 66.95 67.63 71.98 72.92 75.02\nProGrad 68.2 71.78 74.21 77.93 79.2\nKgCoOp 69.51 71.57 74.48 75.82 77.26\nTip-adapter 70.62 73.08 75.75 78.51 81.15\nCLIPFit 72.32 74.39 77.18 79.03 81.27\nTable 3: Comparison of our method against other meth-\nods on robustness to distribution shift.\nMethodSouce TargetAverageImageNet -V2 -Sketch\nCLIP 66.73 60.83 46.15 57.90\nCoOP 71.51 64.20 47.99 61.23\nCLIP-adapter 71.60 63.67 46.52 60.60\nTip-adapter 73.10 64.82 46.73 61.55\nCoCoOp 71.02 64.07 48.75 61.28\nProGrad 72.24 64.73 47.61 61.53\nKgCoOp 71.20 64.10 48.97 61.42\nCLIPFit 71.53 64.83 48.87 61.74\ndifferent from each other. The results are shown in Table 3. As shown in Table 3, while TIP-adapter achieves the best performance on the ImageNet dataset, CLIPFit can achieve better average performance compared to existing methods, effectively underlining the robustness of CLIPFit. Results on cross-dataset transfer setting is provided in appendix D. 4.3 Fine-tuning Analysis Analyzing parameter change. To understand the black-box fine-tuning process in CLIPFit, we first analyzed changes in the parameters of both the text encoder and image encoder. We computed the squared difference ||ppre−p||2for each layer, where ppreis the pre-trained parameter vector and pis the fine-tuned parameter vector. We conduct experiments on the DTD dataset. The results are shown in Fig. 3. As observed Fig. 3 (a), for bias terms in the FNN of the text encoder, when the number of layers increases, the change in bias decreases, which implies that low-level features in the text encoder change more than high-level features during the fine-tuning process of CLIPFit. From Fig. 3 (b), we found that for LayerNorm in the image encoder, the first layer (i.e., patch embedding layer) changes much more compared with other layers for both bias and gain, showing that tuning patch embedding LayerNorm is crucial for shifted downstream tasks. Moreover, the gain of the last several LayerNorm layers has much changed and the most intermediate layers change much less. The 6\n(a) Text encoder(b) Image encoder\nFigure 3: Visualization of changes in different layers.\ndifference in change between different layers may be caused by gradient difference. We visualize the squared sum of gradient from each text bias layer in Fig. 4 (a). As observed, the curve of the gradient sum is very similar to changes in parameters. To verify whether more changed layers are more important in fine-tuning, we conducted experiments by freezing less (or more) changed LayerNorm layers on the 4-shot setting. We found that when only updating the first LayerNorm layer and freezing other LayerNorm layers, the average accuracy is 76.22%. For comparison, the average accuracy is 74.93% when only updating the twelfth LayerNorm, and 75.06% when updating the last LayerNorm. Both are much less than the first layer and these two layers change much less than the first layer, as shown in Fig. 3 (b). Moreover, when updating the top 6 most changed LayerNorm layers, the average accuracy is 77.03%, which is only a 0.15% drop, while only tuning 23% parameters of CLIPFit. The phenomenon for text encoder is similar and can be found in appendix H. These results demonstrate that the more changed layers are more important for knowledge adapting. Analyzing regularization loss. We then analyze the two regularization losses: KD loss and bias term MSE loss. We found that both two losses can avoid overfitting and boost performance during fine-tuning. For the 4-shot learning task, finetuning w/ KD loss leads to a 77.18% average accuracy, and fine-tuning w/ KD loss leads to a 76.23% average accuracy. Both two performances are better than fine-tuning w/o regularization loss (76.13% average accuracy) and KD loss performs better. We then analyze how these two losses affect changes in parameters during fine-tuning of CLIPFit. The results are shown in Fig. 4 (b). As observed, KD loss will reduce the changes for the more-changed low-level bias terms and enhance changes in lesschanged high-level layers, which implies that penalizing changes for low-level bias terms is important in avoiding overfitting. Compared with KD loss, MSE loss directly applying to text bias terms (b) Change w/regulariza2on loss(a) Gradient sum\nFigure 4: Left: visualization of squared gradient sum.\nRight: visualization of change w/ regularization loss.\n(a) zero-shot CLIP(b) CLIPFit\nFigure 5: Visualization of learned image feature space\nfrom zero-shot CLIP and CLIPFit via t-SNE.\nreduces more changes in low-level layers. Analysing image encoder representations. We used t-SNE (Van der Maaten and Hinton, 2008) to visualize the image representation space of zeroshot CLIP and CLIPFit to analyze image encoder representations. We visualize the data from EuroSAT dataset. The visualization results are presented in Fig. 5. As observed, in high-dimensional classification feature space, CLIPFit has a much clearer separation of different class image features compared with zero-shot CLIP, which demonstrates that CLIPFit can better detect the similarities among images. These results verify that updating LayerNorm in the image encoder during fine-tuning will lead to a more separated and better similarity-detected image feature space. Updating LayerNorm can also benefit other methods. We show that updating LayerNorm can also benefit prompt tuning methods and adapter tuning methods. We re-implemented CoOp, KgCoOp, and CLIP-adapter with updating LayerNorm. The results are shown in Table 4. Table 4 shows that training with LayerNorm updating can boost the base class, new class, and harmonic mean performance for all three methods. For example, training KgCoOp with updating LayerNorm can bring 1.4%, 1.36%, and 1.38% improvements in the base class, new class, and Harmonic Mean (HM) accuracy, which demonstrates the effectiveness and wide validity of the proposed updating LayerNorm. 7\nTable 4: Comparison of prompt tuning and adapter tun-\ning methods w/ and w/o updating LayerNorm on the 16-\nshot base-to-new setting. +UL means training updating\nLayerNorm.\nMethod Base New H\nCoOp 82.63 67.99 74.60\n+UL 82.96(+0.33) 69.09(+1.10) 75.39(+0.79)\nKgCoOp 80.73 73.60 77.00\n+UL 82.13(+1.40) 74.96(+1.36) 78.38(+1.38)\nCLIP-adapter 82.23 70.61 75.98\n+UL 83.63(+1.40) 71.87(+1.26) 77.31(+1.33)\nTable 5: Comparison of different strategies of fine-\ntuning bias terms in CLIP.\nStrategy Base New H # param.\n(a) Text+Image bias 84.15 64.35 72.93 0.17M\n(b) Text bias 83.33 64.43 72.67 67.6K\n(c) FFNs bias (Text) 83.25 67.60 74.61 30.7K\n(d) Projection bias (Text) 83.23 67.58 74.59 6.1K\nMore detailed analyses about other datasets and other aspects are provided in appendix H. 4.4 Ablation Study Comparison of different strategies of fine-tuning bias terms. We give an in-depth exploration of how to apply BitFit to fine-tune the CLIP model. Original BitFit fine-tunes all bias terms in language models. We conduct 4 strategies for fine-tuning bias terms of CLIP: (a) fine-tuning all bias terms of the text and image encoder; (b) fine-tuning all bias terms of the text encoder; (c) fine-tuning bias terms of FFNs of the text encoder; (d) fine-tuning bias terms of projection linear layers in FFNs of the text encoder. We trained these four strategies on the 16-shot base-to-new setting with Lceand reported average accuracy. The results are shown in Table 5. As shown in Table 5, both strategy (a) and strategy (b) can boost seen base class performance but will decrease significantly unseen new class performance, which implies that directly applying BitFit to CLIP may be harmful to model’s generalization ability. Moreover, strategy (c) and strategy (d) can have similar performance among both the base and new class data, but strategy (d) fine-tunes only one-fifth of parameters compared with strategy (c), which speeds up training. Effectiveness of proposed components. We validated the effects of updating LayerNorm and KD loss by ablating them. The results are shown in Table 6. Fine-tuning bias terms with KD loss brings 2.35%, 1.22%, and 0.09% improvements for1/4/16-shot setting, respectively. Fine-tuning bias terms in the text encoder and LayerNorm in the image encoder brings 1.89%, 1.41%, and 1.51% improvements for 1/4/16-shot setting, re- 569ConfigurationsAccuracy(%)\n1-shot 4-shot 16-shot\nPorjection Bias 68.86 74.72 79.53\nw/ LayerNorm 70.75 76.13 81.04\nw/ KD loss 71.21 75.94 79.62\nw/ KD loss + LayerNorm 72.32 77.18 81.27\nTable 6: Ablation study of CLIPFit on the few-shot set-\nting with 1/4/16-shot. Projection Bias: fine-tuning bias\nterms of projection layer in FNNs of the text encoder.\nLayerNorm: updating LayerNorm in the image encoder.\nMethod Params time Base New HM\nCoOp 2048 0.44ms 82.69 63.22 71.66\nCoCoOp 35k 25.59ms 80.47 71.69 75.83\nKgCoOp 2048 0.44ms 80.73 73.60 77.00\nMaPLe 3.55M 2.1ds 82.28 75.14 78.55\nCLIPFit 44k 0.96ms 83.72 74.84 79.03\nTable 7: Comparison of training efficiency with other\nmethods over 11 datasets.\nspectively. Together, CLIPFit brings 3.46%, 2.46% and 1.74% improvements for 1/4/16-shot setting, respectively. These results demonstrate the effectiveness of each CLIPFit components. Training efficiency. We compare the training efficiency of CLIPFit and other methods w.r.t. parameters and training time per image (Yao et al., 2023). The results are shown in Table 7. It is noticed that CoOp and KgCoOp have the lowest number of training parameters and time. However, the performance of these two methods is not satisfactory. MaPLe improves accuracy performance compared with other methods but also increases the required tuning parameters to 3.55M, which is very time-consuming. CLIPFit achieves the best harmonic mean accuracy with only 44k parameters, which is much less than MaPLe. Also, the training time of CLIPFit is slightly higher than CoOp and KgCoOp. Given the large improvement of CLIPFit, a slight increase in training time is acceptable. 5 Conclusion In this paper, we presented CLIPFit for fine-tuning visual-language models. Unlike existing prompt tuning and adapter tuning methods, CLIPFit does not introduce any external parameters and finetunes CLIP by updating only bias terms of projection layers in FFNs of the text encoder and the image encoder’s LayerNorm. To understand the effect of CLIPFit fine-tuning on the pre-trained model, we conducted various analyses focusing on changes in internal parameters and representations. We conducted extensive experiments and analysis to evaluate CLIPFit on 11 datasets, whose performances show the superiority of our method. 8\n6 Limitations In this paper, we presented CLIPFit for VLM finetuning and conducted an exploration of how CLIPFit affects the pre-trained CLIP model. Our analyses found some interesting phenomena after finetuning, i.e., low-level bias terms in the text encoder change much more than high-level bias terms and the change in the first LayerNorm layer is much bigger than other LayerNorm layers in the image encoders. Moreover, we found that this may be caused by the difference in the magnitude of the gradient. Nevertheless, our analysis does not reveal why the difference in the magnitude of the gradient happens during fine-tuning. A deeper analysis of gradient back-propagation during fine-tuning is needed to understand this for future work. Furthermore, following previous works (Zhou et al., 2022a; Yao et al., 2023; Zhou et al., 2022b; Khattak et al., 2023), this paper focused on image classification for VLMs, so our study was constrained to classification tasks. Expanding CLIPFit for VLM fine-tuning to a broader range of tasks (e.g., image retrieval) could be the future work. 626",
        "aspect_review": "The paper is well-organized and clearly presents the novel concept of CLIPFit, providing a comprehensive overview of related work and situating its contribution within the existing literature. The study is robust, utilizing 11 diverse datasets to rigorously test the proposed method, demonstrating its versatility and effectiveness across different settings. CLIPFit introduces a groundbreaking approach to fine-tuning vision-language models by only modifying specific parameters, which is both innovative and efficient, significantly reducing computational overhead without sacrificing performance.\n\nWhile the paper covers a variety of datasets, it is primarily focused on image classification tasks, which may limit the perceived applicability of the method to other types of vision-language tasks. The experimental section, although comprehensive, could benefit from additional clarity in the presentation of hyperparameter settings and implementation details to facilitate replication.",
        "general_review": "The paper presents notable novelty by proposing CLIPFit, a method for efficiently fine-tuning Vision-Language Models (VLMs) without introducing external parameters. The technical depth and rigor are demonstrated through an innovative approach that fine-tunes specific bias terms and normalization layers, which is a significant departure from traditional methods. The paper offers comprehensive experimental validation across 11 datasets, showing an impressive 7.33% improvement in harmonic mean accuracy over zero-shot CLIP. This demonstrates the method's significant practical impact, offering a promising alternative to prompt and adapter tuning methods with fewer parameters and better efficiency.\n\n1. Minor writing or clarity improvements could be addressed to enhance readability. 2. Small details on hyperparameters or implementation specifics could be further clarified for replication purposes. 3. Very minor formatting issues in figures could be improved for better visual presentation. 4. Slight clarification on code availability would be beneficial to ensure accessibility for future research."
    },
    "45": {
        "paper": "PRompt Optimization in Multi-Step Tasks (PROMST):\nIntegrating Human Feedback and Heuristic-based Sampling\nAnonymous ACL submission\nAbstract\nPrompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework PROMST that incorporates human-designed feedback rules to automatically offer direct suggestions for improvement. We also use an extra learned heuristic model that predicts prompt performance to efficiently sample from prompt candidates. This approach significantly outperforms both human-engineered prompts and several other prompt optimization methods across 11 representative multi-step tasks (an average 10.6%29.3% improvement to current best methods on five LLMs respectively). We believe our work can serve as a benchmark for automatic prompt optimization for LLM-driven multi-step tasks. 1 Introduction The performance of large language models (LLMs) on a given task is sensitive to the prompt, so prompt engineering aims to create prompts that fully leverage the capabilities of LLMs. Due to the lack of access to model parameters for black-box LLMs, techniques for automatic prompt optimization have primarily focused on searching over the vast discrete space of tokenized language inputs (Cheng et al., 2023). Recent studies have shown that LLMs, combined with evolutionary algorithms, can help with this search by reasoning over errors made using existing prompts to suggest edits or generate 042new candidate prompts (Pryzant et al., 2023; Wang et al., 2023; Yang et al., 2023). These approaches have been evaluated on relatively simple one-step tasks, such as mathematical calculations (Roy and Roth, 2016; Cobbe et al., 2021), instruction induction (Honovich et al., 2022), and factual analysis (Wu et al., 2023b). The associated prompts are also relatively short, usually one to three sentences. In this work, we aim to optimize prompts for LLM-driven agents solving multi-step tasks and propose a method called PRompt Optimization in Multi-Step Tasks (PROMST). In these tasks, an LLM is used to decide a system’s actions (e.g., virtual software Wu et al. 2023a; Zhou et al. 2023a or real robots Chen et al. 2023a; Firoozi et al. 2023) as it interacts with an environment over multiple steps (Abdulhai et al., 2023). Engineering good prompts is hard due to the typical prompt length (300+ tokens) and individual task constraints and rules. The prompts needed for multi-step tasks are more complex to judge the long-horizon correctness of a single action. This difficulty hinders LLMs from automatically reasoning over errors and producing better prompts, which in turn reduces the effectiveness of current methods for automated prompt optimization. Prompt optimization in multi-step tasks is still an open challenge. Considering that humans excel in analyzing errors and incorporating relevant domain knowledge into feedback, we formalize PROMST as a framework involving human input, as shown in Figure 1. Here, during the multi-step agent-environment interactions, the agent (indicated by ’TaskLLM’ in Figure 1) sometimes makes errors and fails the task. While some work has used LLMs to evaluate errors, we instead use human-designed feedback rules constructed a priori that address different types of errors. Depending on the error, feedback is automatically generated and passed as additional context to an LLM that is responsible for producing a new set of candidate prompts (indicated by ’PromptLLM’ 1\nin Figure 1). A score is assigned to each prompt indicating the agent’s task performance given that prompt. Since the evaluation of many candidate prompts for multi-step tasks in environments can be expensive, we fine-tune a score prediction model online using prompt-score pairs which can be used as a heuristic to select a subset of the candidate prompts to evaluate. Our experiments in 11 tasks show that the integration of human feedback and the score model greatly improves the prompt optimization process (10.6%-29.3% relative improvements over all baseline methods across different LLMs). PROMST achieves the best performance on most tasks. PROMST has also been shown to perform better in multi-trial settings when combined with dynamic approaches. We further show that the humandesigned evaluation rules can be used to help align task performance with human preferences. Extensive experiments are conducted to validate the framework and investigate the underlying reasons why some prompts are more effective than others. In summary, our contributions are : (1) To our best knowledge, PROMST is the first to explore automatic prompt optimization in multi-step agent tasks. We release all codes and prompts for 11 multi-step environments, which may serve as a benchmark for future research. (2) We show that the integration of human feedback and a fine-tuned score model outperforms existing methods across various tasks and LLMs. (3) Our research indicates that PROMST is orthogonal and integrates well with established dynamic approaches. (4) We find that human-designed rules for task evaluation help align optimized prompts with human preferences. 2 Related Work Prompt Optimization To improve performance of black-box API models, it is useful to engineer the discrete prompts for downstream tasks. Various ’best practices’ have emerged for human-designed task prompts, such as including examples (Brown et al., 2020) or promoting reasoning chains (Kojima et al., 2022; Wei et al., 2022). However, manually designing prompts requires extensive human trial-and-error and is sub-optimal; thus, many recent works focus on automating this process. Some methods approximate the gradients (Diao et al., 2022) or emulate them via natural language (Pryzant et al., 2023). Others use edit operators to modify an initial prompt, driven either by rein- 133forcement learning (Zhang et al., 2023) or scoreguided search (Prasad et al., 2023). To help balance exploration and exploitation of prompts, several approaches have used LLM-driven evolution (Guo et al., 2023a; Fernando et al., 2023; Ma et al., 2023; Ye et al., 2023). In several works, LLMs are directly used to generate prompt candidates (Zhou et al., 2023b) often with feedback about parent prompts (Wang et al., 2023; Ma et al., 2023). Our work focuses on domains that include complex multi-step tasks, in which the evaluation and reflection processes are more challenging so that score prediction models and human feedback rules are introduced in order to mitigate this problem. LLM Based Agents for Multi-Step Tasks There are many recent works that use LLMs for multi-step planning. LLMs are used to interact with softwares and websites (Ma et al., 2024; Wu et al., 2023a; Zhou et al., 2023a), plan robot actions (Chen et al., 2023b; Ahn et al., 2022; Huang et al., 2022a; Ma et al., 2023; Aghzal et al., 2023), and connect to external tools (Liu et al., 2023; Chen et al., 2023a; Qin et al., 2023). Instead of careful design of lengthy prompts to capture all the constraints, our approach uses prompt optimization to transition from a simple initial human-provided prompt to a high-performing prompt. LLM Self-reflection from Feedback In planning domains, it is useful to provide feedback about syntactic errors (Silver et al., 2023; Skreta et al., 2023), potential infinite loops (Silver et al., 2023), failed action execution (Huang et al., 2022b), and generated trajectories (Chen et al., 2023a). Other recent work has shown that LLM-generated feedback via self-evaluation can improve performance on a variety of tasks (Yang et al., 2022; Welleck et al., 2022; Madaan et al., 2023), including prompt engineering (Wang et al., 2023) and reinforcement learning (Shinn et al., 2023; Ma et al., 2023). Compared to above works, our work combines LLM self-reflection with human-provided feedback templates to help improve performance on the more challenging multi-step tasks. Another type of methods is to utilize self-reflection for online dynamic feedback during task execution, such as Reflexion (Shinn et al., 2024). While Reflexion needs multiple trials for online optimization of action memory for each single test, our method PROMST only needs one trial for task execution and the prompt is optimized offline across multiple tests. In Section 4.4, we find that PROMST outperforms the offline variation of Reflexion and can perform bet2\nFigure 1: The PROMST framework. Given an initial human-designed prompt and the state of the\nenvironment for the current task, the TaskLLM iteratively generates an action and executes it until either\nan error occurs or the task is complete. Human-designed feedback rules automatically generate feedback\nabout errors that is then provided as context to the PromptLLM when generating new prompt candidates.\nThe task performance is scored according to a human-designed score function; this score can be used with\nthe prompt to train a score prediction model online. Given new prompt candidates, this score prediction\nmodel is used to select a subset of candidates to evaluate for the next generation.\nter when combining with online Reflexion in the multi-trial setting. 3 Methodology 3.1 Problem Formulation Given a base LLM Band a target task T, the goal of prompt optimization is to craft an optimized natural language prompt Pthat maximizes the performance of BonT. Here the prompt Pconsists of multiple components, such as a task description, scoring rules, and safety constraints. In multi-step tasks, the state information of the environment at each step will be transformed into a text string S and provided to the LLM Bto make decisions. The history of state ( S), action ( a), and environment feedback ( e) will also be reported to LLM. For the ithtesting trial on a particular task, the probability of an action sequence [ ai,1, ai,2, ..., a i,j] is: pB([ai,1, ai,2, . . . , a i,j]) =jY\nk=1pB(ai,k|Si,k, P,\nSi,k−1, ai,k−1, ei,k−1, . . . ,\nSi,1, ai,1, ei,1)\n(1) The sequence [ ai,1, ai,2, ..., a i,j] is executed in the task environment and assigned a score based on human-designed rules or functions R. The goal of prompt optimization is to find the optimal natural language prompt P∗that maximizes a score 208function R: P∗=arg max\nP∈AX\ni∈UR(pB([ai,1, ai,2, ..., a i,j])),\n(2) where Adenotes the vast and complex space of all possible natural language prompts and Udenotes the set of all the testing trials in a specific task. 3.2 PROMST Framework Figure 1 illustrates the general framework of PROMST. The goal is to more efficiently and strategically search over the vast space of possible prompts while integrating human-designed feedback of candidate prompt performance. LLMs are used in two key steps of PROMST: (1) the execution of the task via the current candidate prompt (’TaskLLM’) and (2) the generation of new candidate prompts given any available feedback about the current prompt’s performance on the task (’PromptLLM’). We refer to a single execution in a testing case of a task as a trial. In each trial, the TaskLLM executes the task over multiple rounds of interaction with the environment; for each round, the TaskLLM is provided both the current candidate prompt and the current trial’s execution history and generates the next action for the agent to take. Task execution terminates when an error is detected or the task is complete. The candidate prompt P is assigned a score for that trial via the humandesigned score function. Each candidate prompt is 3\nevaluated over multiple trials in which the initial environment state (e.g. number of objects, number of agents) is varied, resulting in a final average score calculated over all the trials. Once the candidate prompts have all been evaluated and assigned automatic feedback, the top performers are selected as parents for a new generation of candidate prompts. The PromptLLM uses each parent prompt and its feedback to generate new candidate prompts. This process is also described in Algorithm 1, 2, 3 in Appendix B. Score Prediction Model In general, producing more candidate prompts per generation allows for more exploration over the space of possible prompts; however, there is a trade-off between the number of candidates per generation and the cost of evaluation, and multi-step tasks can be much more expensive to evaluate (we query the TaskLLM for each next action). To help mitigate the evaluation cost for a generation, we learn a score prediction model online that functions as a heuristic with which to choose a subset of the generated candidate prompts for actual evaluation. Algorithm 2 in Appendix B shows the process\nof implementing the score prediction model as a\nheuristic for filtering candidate prompts. We fine-\ntune a task-specific bidirectional Longformer-base\n(148M) (Beltagy et al., 2020) model. The prompt-\nscore pairs on which we fine-tune are collected\nonline during early iterations of PROMST; there-\nfore, the score prediction model is not applied until\nsdthgeneration, where sdis a hyperparameter. We\ncontinue to update the learned model at each gen-\neration with the new prompt-score pairs. To miti-\ngate variance, we fine-tune multiple models on five\nrounds with the collected data following a random\n4:1 train/test split. The generated prompt candidate\np′will be selected for task evaluation if:\n(3)E[Mk(p′)] + Var[ Mk(p′)] + E[error k]\n≥hyper_M ×max( D.score ())\nwhere E [Mk(p′)]and Var [Mk(p′)are the mean and variance of predicted scores for p′from five models. The E [error k]is the average testing error of five score models. The max( D.score ())is the highest score of existing prompts. To balance efficient exploration and conservative filtering, we only filter prompt candidates when the score prediction model is sufficiently confident. When the variance of the prediction model is high, we want to be conservative in its application in order to reduce the chance 268that we filter out a good candidate. Similarly, we choose to be conservative in our filtering when the error is high. Equation 3 is therefore one formulation that incorporates these general ideas. The hyperparameter hyper_M allows this conservativeness to be tuned by users. Human-Designed Feedback Rules During task execution, the TaskLLM may encounter an error, resulting in the task being terminated. It is useful for the PromptLLM to have context about this error when generating new prompt candidates. Since automatic error analysis via LLMs is difficult for multi-step tasks (e.g. an agent stuck in an action loop), we instead use human-designed rules to automatically synthesize feedback, as shown in Figure 2. Some types of errors can be common across all tasks (e.g. syntactic errors), while others are task-specific. For types of human feedback in each task, see Appendix A. Note that the designing process of human feedback templates is simple and intuitive, without hard trial-and-error efforts, which is further discussed in Section 4.4. Figure 2: Eight examples of human-designed feed-\nback templates. The blue-colored text represents\nthe content specific to each instance of an error.\nCandidate Prompt Generator We produce new candidate prompts from a parent prompt in two steps: (1) summarizing feedback via an LLM (’SumLLM’) and (2) generating new prompt candidates via an LLM provided with the summarized feedback as context (’GenLLM’). In order to encourage exploration over more diverse candidate prompts, we randomly choose 10 instances of feed4\nback. This random selection also likely promotes more frequent errors. Given the selected feedback, SumLLM produces a summary that is included as context to GenLLM for generating new candidates. See Algorithm 3 in Appendix B for another description of this process and Appendix C for the meta-prompts used for SumLLM and GenLLM. 4 Experiments 4.1 Environments As shown in Figure 3, we test on 11 multi-step tasks requiring strong logical, geometrical, scientific, and commonsense reasoning capabilities (Zhou et al., 2023a; Shridhar et al., 2020; Wang et al., 2022; Chen et al., 2023b; Aghzal et al., 2023; Valmeekam et al., 2023). Each environment requires the LLM agent to determine the next action in the large discrete action space. Please refer to Appendix D for a complete description of all tasks. Figure 3: An illustration of the 11 environments\nused for multi-step task evaluation. See Ap-\npendix D for more details.\n4.2 Baselines We compare PROMST with six recent representative methods: Automatic Prompt Engineer (APE) (Zhou et al., 2023b), Automatic Prompt Optimization (APO) (Pryzant et al., 2023), PromptAgent (Wang et al., 2023), LLM-As-Optimizer (Yang et al., 2023), PromptBreeder (Fernando et al., 2023), and Evolutionary Prompt Optimizer (Guo et al., 2023b). All the methods under comparison involve iterative optimization of prompts. Some methods require error feedback through LLM selfreflection, while others do not. For methods that need error feedback, we randomly select 10 329instances of feedback, similar to the PROMST method but without the rules of human feedback. We also compare with the dynamic approach Reflexion (Shinn et al., 2024) by modifying it into an offline framework. 4.3 Experimental Setups For a fair comparison, all methods start the optimization from initial human-designed prompts; where possible, we use the provided publicly available prompts for each method. In all cases, we set the LLM sampling temperature to 0. For each method, we report the score of the best performing prompt on each task; in this case, the score is computed as: S=num(sub-goalsuccess )/num(sub-goalall),\n(4) where the score Sis the ratio of the number of successfully completed sub-goals/sub-steps to the total number of sub-goals/sub-steps, i.e., the task progress score. Note that the task completion score can also serve as the metric, while it may be sparse in some situations. Both types of scores are positively correlated as shown in Appendix Table 9. In Section 4.4, we also preliminarily test the impact of changing the scoring function S. Model Types One interesting feature of these methods is that the LLM used to execute the task (’TaskLLM’) and the LLM used to generate new candidate prompts (’PromptLLM’) do not need to be the same model. We mainly test two combinations of models. The first uses GPT-3.5 as the TaskLLM and GPT-4 (Achiam et al., 2023) as the PromptLLM, and the second uses GPT-4 for both the TaskLLM and PromptLLM. To verify the effectiveness of PROMST in varied models, we also evaluate it using Claude 3 Opus (Anthropic, 2024), Mixtral-8x7B, and Mixtral-Large (Jiang et al., 2024) as both TaskLLM and PromptLLM. Mixtral-8x7B is an open model, while all the others are closed. We also explore whether the optimized prompts specialized for one type of LLM can generalize better performance to other types of LLMs. Context Window Limit The constraint of the model’s context window is an issue for LLM-based agents, especially for longer multi-step tasks. Relying on the intuition that recency is important, in all the tested methods we use a sliding window of the history of state-action-feedback tuples, truncating the history by pruning older parts of the history 5\nthat extend beyond the window length, which is a common technique used in LLM agent researches. Hyperparameters For a fair comparison, we standardize all hyperparameters across methods, allowing each to explore the same number of prompt candidates at an equivalent level. The expansion number ncontrols the number of kid prompts generated based on each parent prompt, which is set to 20 in the first level and 8 for all additional levels. In each level, top k= 5 current prompts are selected as the parent prompts for further optimization. Search terminates once the recent three levels do not have any score improvements. In PROMST, we set hyper_M = 0.8 in Equation 3 to filter out prompts with low scores. 4.4 Results and Analysis Overall Better Performance Table 1 and Table 2 show the main experimental results. Note that BoxLift task is not included in Table 2 since we find GPT-4 can already achieve a full score with the initial human prompt. Table 3 shows the experimental results on other three types of LLMs. Due to limited computational resources, we only select four representative tasks and two strongest baseline methods (APO, PromptAgent) when evaluating other LLMs. Table 7 and Table 8 in Appendix E test the performance of the optimized prompts trained from one LLM with other types of TaskLLMs. The main takeaways are: 1) PROMST performs the best in most tasks. On average, PROMST outperforms strongest baseline PromptAgent with GPT-3.5-0613 (0.27 vs 0.32), GPT-4 (0.61 vs 0.69), Claude-3-opus (0.36 vs 0.46), Open-Mixtral-8x7B (0.12 vs 0.15), and Mixtral-large (0.30 vs 0.34). 2) When testing the best prompts trained from GPT3.5-0613 and GPT-4 with a different TaskLLM, we find that they still outperform human prompts. 3) However, each LLM does best with the prompts optimized on it. For example, the best prompts acquired when using GPT-3.5-0613 as the TaskLLM do not further improve performance when applied to GPT-4, and vice versa. 4) PROMST performs well when the TaskLLM and PromptLLM are the same LLM, showing that it does not rely on a stronger PromptLLM to pass extra knowledge into prompts, which can be regarded as cheating. Effects of Score Model To analyze the effects of the score model, we use BoxLift as a representative example, as shown in Figure 4. Figure 4a shows the distribution of prompt scores explored Figure 4: Several results inspecting the learned\nscore prediction model. (a) The distribution/ratio\nof prompt scores with/without the score prediction\nmodel. (b) The prediction error of the model on the\ntraining data and heldout test data as the amount of\ntraining data increases. (c) A plot of the predicted\nscore vs the actual score for various prompts; blue\nare the prompts that were chosen as parents for new\ncandidates. (d) The trend of the best performing\nprompt during optimization for increasing itera-\ntions both with and without using the learned score\nprediction model.\nin all the levels (1-8) with and without the score prediction model implemented, respectively. The implementation of the score prediction model truly makes the exploration more efficient since less lowscored prompts are explored. Figure 4b shows the training and testing errors of the score model versus different amounts of collected training data. The overfitting effect decreases with increasing data number. Figure 4c tests the fine-tuned score models on levels 5-8. We also evaluate the prompts that were filtered out by the score model and plot the predicted and actual scores. We find that nearly all chosen prompt candidates achieve scores higher than 0.4, and the filtered prompts have reliably low scores. We compare the evolution curves for PROMST with/without the score model, as shown in Figure 4d. The results show that both the training and testing paths converge faster and achieve better scores using the score model. The ablation experiments in other environments also have the same trend (shown in Appendix G). Overall, we find the score prediction models improves the efficiency and effectiveness of prompt search. Ablation on Methods of Score Model Instead of fine-tuning a pre-trained Longformer-base 6\nTable 1: Scores for initial (human) and optimized prompts on various multi-step tasks for different methods.\nP.Agent, LLMOP, P.Breeder, and P.Evolution refer to PromptAgent, LLM-As-Optimizer, PromptBreeder,\nEvolutionary Prompt Optimizer, respectively. GPT-3.5-0613 for TaskLLM and GPT-4 for PromptLLM.\nGPT-3.5-0613- AS-TASKLLM, GPT-4- AS-PROMPT LLM\nTASK HUMAN APE APO P.A GENT LLMOP P.B REEDER P.EVOLUTION PROMST\nWEBARENA 0.22 0.35 0.31 0.37 0.29 0.25 0.27 0.39\nALFWORLD 0.075 0.24 0.23 0.24 0.14 0.12 0.16 0.30\nSCIENCEWORLD 0.18 0.19 0.19 0.23 0.19 0.20 0.22 0.21\nBOXNET1 0.076 0.093 0.16 0.13 0.098 0.11 0.12 0.25\nBOXNET2 0.044 0.075 0.16 0.17 0.086 0.090 0.075 0.22\nBOXLIFT 0.31 0.69 0.70 0.74 0.55 0.58 0.62 0.90\nWAREHOUSE 0.0 0.012 0.012 0.036 0.008 0.008 0.004 0.GRIDWORLD 1 0.23 0.30 0.35 0.32 0.28 0.26 0.24 0.38\nGRIDWORLD 2 0.036 0.093 0.17 0.15 0.065 0.078 0.13 0.12\nBLOCKSWORLD 0.19 0.25 0.42 0.48 0.29 0.22 0.27 0.60\nLOGISTICS 0.083 0.083 0.12 0.12 0.083 0.083 0.12 0.18\nAVERAGE 0.13 0.22 0.26 0.27 0.19 0.18 0.20 0.32\nTable 2: Scores for initial (human) and optimized prompts on various multi-step tasks for different\nmethods. GPT-4 for both TaskLLM and PromptLLM.\nGPT-4- AS-TASKLLM, GPT-4- AS-PROMPT LLM\nTASK HUMAN APE APO P.A GENT LLMOP P.B REEDER P.EVOLUTION PROMST\nWEBARENA 0.57 0.59 0.64 0.60 0.58 0.58 0.59 0.62\nALFWORLD 0.45 0.49 0.50 0.53 0.50 0.47 0.49 0.57\nSCIENCEWORLD 0.70 0.72 0.74 0.76 0.71 0.73 0.76 0.81\nBOXNET1 0.65 0.72 0.72 0.77 0.74 0.67 0.70 0.79\nBOXNET2 0.34 0.38 0.36 0.35 0.40 0.37 0.40 0.42\nWAREHOUSE 0.16 0.18 0.27 0.34 0.30 0.25 0.22 0.51\nGRIDWORLD 1 0.73 0.78 0.82 0.89 0.83 0.76 0.80 0.86\nGRIDWORLD 2 0.26 0.50 0.44 0.41 0.41 0.31 0.29 0.60\nBLOCKSWORLD 0.71 0.74 0.83 0.87 0.76 0.75 0.77 0.95\nLOGISTICS 0.50 0.53 0.58 0.61 0.54 0.53 0.56 0.74\nAVERAGE 0.51 0.56 0.59 0.61 0.58 0.54 0.56 0.69\nFigure 5: (a) Comparison of score prediction errors\nfor few-shot GPT-4 vs finetuning Longformer for\nincreasing amount of few-shot examples or training\ndata, respectively. (b) An ablation study of the\nimpact of the human-designed feedback rules on\ntask performance for four multi-step tasks.\nmodel, another way to acquire score prediction models is few-shot learning via GPT-4. Figure 5a compares these two methods under varied training/example data number. GPT-4 is given randomly selected prompt-score pairs as examples during the study. We find that the performance of GPT-4 459few-shot learning cannot improve with the increasing number of examples. The fine-tuning method surpasses GPT-4 few-shot learning once the data number increases over 40. Ablation on SumLLM Component Since TaskLLM and GenLLM are necessary in the whole framework, we compare PROMST with/without SumLLM component in Table 4 to verify its effectiveness. The integration of SumLLM improves the performance on all the three representative tasks. Ablation on Human Feedback We compare the method with/without human feedback, both without the learned score model. As seen in Figure 5b, human feedback contributes to much higher scores across four tasks. In our work, the original human feedback templates did not require iterations via trial-and-error over possible versions. To demonstrate that designing human feedback rules is straightforward and requires minimal efforts, we test other four feedback templates in Table 5. The 7\nTable 3: Evaluation of different LLMs on a subset of the mutli-step tasks. The same LLM was used for\nthe TaskLLM and PromptLLM in each case.\nCLAUDE -3-OPUS -20240229 OPEN-MIXTRAL -8X7B\nTASK HUMAN APO P.A GENT PROMST HUMAN APO P.A GENT PROMST\nALFWORLD 0.32 0.40 0.42 0.49 0.055 0.074 0.071 0.10\nBOXNET2 0.42 0.47 0.44 0.53 0.078 0.21 0.20 0.24\nWAREHOUSE 0.21 0.26 0.27 0.36 0.020 0.093 0.13 0.16\nGRIDWORLD 2 0.13 0.29 0.34 0.44 0.013 0.038 0.075 0.10\nAVERAGE 0.27 0.36 0.37 0.46 0.041 0.10 0.12 0.15\nMIXTRAL -LARGE -2HUMAN APO P.A GENT PROMST\n0.28 0.33 0.33 0.45\n0.26 0.31 0.36 0.30\n0.16 0.23 0.28 0.31\n0.12 0.32 0.25 0.28\n0.21 0.30 0.30 0.34\nTable 4: Ablation studies on SumLLM.\nGPT-3.5- AS-TASKLLM, GPT-4- AS-PROMPT LLM\nTASK W S UMLLM WO S UMLLM\nALFWORLD 0.30 0.23\nBOXNET2 0.22 0.18\nBOXLIFT 0.90 0.85\nresults show that variability over the wording of the templates has little impact on the performance of PROMST. Thus, including the response in the feedback template is a useful task- and error-agnostic guiding principle. Appendix H articulates more specifically on the designing of four compared human feedback templates and the reasons why designing feedback rules is effortless. Preference Alignment via Score Function The choice of score functions impacts prompt optimization, in which humans may have different preferences for the same task. In Appendix J, we explore the impacts of varied score functions and find that PROMST can well align with human preferences by modifying score function formats. Explanability for Better Prompts We also try to dig out some mechanisms why the optimized prompts are better. In Figure 13, we plot prompt score vs. token length and perplexity, which implies some clues that longer prompts may be better. Meanwhile, when viewing through the discovered best prompts in Appendix M, we find some clues about better component emergence, i.e., the best prompts tend to list all the careful points one by one clearly. We conduct an ablation study by sum- 504Table 5: Ablation studies on the templates used for\nhuman feedback.\nGPT-3.5- AS-TASKLLM, GPT-4- AS-PROMPT LLM\nTASK ORIGINAL PARAPHRASED RANDOM\nBOXNET2 0.22 0.20 0.23\nBOXLIFT 0.90 0.93 0.97\nWO RESPONSE COMPONENT WO STUCK IN LOOP\n0.17 0.18\n0.73 0.87\nmarizing detailed careful points into varying token lengths using GPT-4 and evaluating their performance. The results indicate that task scores consistently decline as token lengths decrease, underscoring the importance of clearly listing detailed points. More specific discussion is shown in Appendix K. Comparison and Combination with Reflexion In Appendix L we find that PROMST outperforms the dynamic approach, Reflexion, in prompt optimization and achieves enhanced performance when Reflexion is integrated in a multi-trial setting. 5 Conclusion In this work we introduce an automatic prompt optimization framework for complex, multi-step agent tasks: PROMST. To handle the issues of task complexity, judging long-horizon correctness of individual actions, high prompt exploration cost, and human preference alignment, we propose the integration of human feedback, a learned score prediction model, and the modification of task score functions. Our approach generally outperforms six representative baselines on 11 different task environments over all the five LLMs. PROMST is orthogonal and combinatorial to existing dynamic approaches. The discovered best prompts have some inspiring characteristics for better performance. 8\n6 Limitations The limitations and potential societal risks of this work are as follows: Huge resource consumption of API calls Automatic prompt optimization requires significant computing resources and LLM API queries due to its search-based nature, which is a common issue in this research track. Though the introduction of score model makes the searching more efficient, the around 100 prompt candidate exploration is still a large burden. Score model increases computing demands of local devices The fine-tuned score prediction model trades-off the number of API queries for on-device computation by selecting good candidate prompts. Still, the training of extra score models increases the computing demands on local devices. Fine-tuning score model requires enough data points The fine-tuning process of score models typically requires around 100 prompt-score pairs, which is suitable for black box prompt searching since over 100 data points are truly needed for satisfying performance. However, the score model may not be suitable if in the future a more efficient searching method appears so that data points are not such much. 560",
        "aspect_review": "The paper presents a novel framework, PROMST, for optimizing prompts in multi-step tasks by integrating human feedback and heuristic-based sampling. This approach addresses the complexities of multi-step tasks where prompts are extensive and difficult to evaluate, providing a significant leap in the field of prompt optimization. The experiments are comprehensive, covering 11 representative multi-step tasks and demonstrating significant improvements over existing methods. The use of multiple LLMs for validation further strengthens the empirical results. The results show a 10.6%-29.3% improvement over existing methods, which is a substantial advancement in the field. The framework's ability to outperform human-engineered prompts and other optimization methods is a testament to its effectiveness. The paper introduces a benchmark for multi-step task prompt optimization, providing all codes and prompts for 11 environments. This contribution is valuable for future research and sets a new standard in the domain. PROMST's methodology is robust, combining human feedback with a learned score prediction model to efficiently sample from prompt candidates. The integration with dynamic approaches further highlights its adaptability and potential for broader application.\n\nThe paper could benefit from a more detailed discussion on the limitations of the experiments, such as the potential biases in task selection or the generalizability of the results to other domains. While the results are impressive, the paper could provide more insight into the specific reasons why PROMST outperforms other methods, beyond the integration of human feedback and score prediction models. Some figures and tables could be formatted more clearly to enhance readability and comprehension, particularly for readers who are not familiar with the specific tasks or methodologies used. Although the paper provides a benchmark, it would be helpful to include more detailed descriptions of the tasks and environments to better contextualize the results and facilitate replication. The paper could improve its clarity by providing more detailed explanations of the methodologies and algorithms used, particularly for readers who are not experts in prompt optimization or LLM-based agents.",
        "general_review": "The paper introduces a novel framework, PROMST, for optimizing prompts in multi-step tasks, which is an underexplored area in prompt optimization. The integration of human feedback and heuristic-based sampling demonstrates strong technical depth and rigor. The framework is validated through comprehensive experiments across 11 representative tasks, showing significant improvements over existing methods. The release of code and prompts for these environments adds significant practical impact, providing a benchmark for future research.\n\n1. The paper could benefit from minor writing improvements to enhance clarity, particularly in the explanation of the score prediction model. 2. Some small details on hyperparameters and implementation specifics are not fully fleshed out, which could aid in replicating the experiments. 3. Figures could be formatted slightly better for improved readability. 4. While the code availability is mentioned, a slight clarification on the accessibility and usability of the released resources would be beneficial."
    },
    "46": {
        "paper": "Evidence-Focused Fact Summarization for Knowledge-Augmented\nZero-Shot Question Answering\nAnonymous ACL submission\nAbstract\nRecent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challenging. Existing methods, such as triple-form orfree-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFS UM, an Evidence-focused Fact Sum marization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFS UM improves LLM’s zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary. 1 Introduction Large Language Models (LLMs) have shown remarkable zero-shot abilities but often produce factual errors, known as hallucinations , particularly in knowledge-intensive tasks like Question Answering (QA). This happens because the static knowledge within LLM parameters may be incomplete, incorrect, or outdated, failing to keep pace with evolving real-world knowledge. Recent studies remedy this by integrating external knowledge into LLMs (Karpukhin et al., 2020b; Min et al., 2019). As one form of external knowledge, Knowledge Graphs (KGs) have been considered as the knowledge source to augment LLMs for enhanced performance in knowledge graph QA (KGQA) (Baek et al., 2023a; Wu et al., 2023; Sen et al., 2023). The key challenge of utilizing KGs, which consist of a set of (head entity, relation, tail entity) triples, is to bridge the modality gap between graphs and text. Efforts on connecting the gap mostly fall into Pet SemataryStephen KingAuthorS: The author of Pet Sematary, Stephen King,was educated at the University of Maine …\n✅Density\n✅ClarityPet SemataryJeff BuhlerScreenwriterStephen KingUniversity of MaineEduca-ted atQ: Wheredidthe author ofPet Sematarygo to college?\nEFSumA: University of Maine\nKG\nLLM for QA\n⋮⋮Figure 1: The QA pipeline based on LLM prompting,\naugmented with relevant facts from KGs. Our fact sum-\nmarization improves both density and clarity of evidence\nwithin contextual knowledge for enhanced QA.\neither training additional layers to blend two representations, or verbalizing graphs into texts. While training fusion layers (Yasunaga et al., 2022a,b) can make models expressive on two different modalities, it takes expensive compuations, and needs to be trained when KGs are upated. On the other hand, recent studies proposed verbalizing KGs into text-form, without restirctions of trainining LLMs for QA. For instance, one strategy is simply concatenating the facts in their triple-form text (Baek et al., 2023a), and another entails converting them into semantically-coherent textual description (i.e., free-form text) through the distillation of LLM’s ability to generate text from KGs (Wu et al., 2023). Despite their remarkable efficacy, existing verbalization strategies for providing contextual knowledge exhibit critical limitations. (1) Low density of evidence : Both concatenation and linearization of the facts (Baek et al., 2023a; Wu et al., 2023) are highly likely to include duplicated entities or relations due to limited flexibility; this eventually degrades the density of useful evidence within the contextual knowledge, hindering the ability to answer questions effectively. (2) Low clarity of evidence : While contextual knowledge describes factual information, they often fail to highlight the evidence necessary for answering questions. This lack of focus can lead to noise from irrelevant facts, which can detrimentally impact the 1\nLLM’s ability to provide accurate answers. To address the aforementioned challenges, we introduce a novel EFS UMframework, Evidencefocused FactSum marization for enhanced QA with knowledge-augmented LLM prompting. The key idea is transforming the set of facts into plausible and coherent summary while highlighting evidence and filtering out noise given a question; this ensures that the summaries maintain a high density and clarity of evidence, facilitating effective QA (Figure 1). The most straightforward solution to this summarization is prompting LLMs with the detailed instruction. However, LLM-generated summaries often omit crucial evidence (such as answer spans), resulting in information loss , or include information that cannot be inferred from retrieved facts, leading to extrinsic hallucination . For enhanced summary quality, we optimize an open-source LLM as a fact summarizer in two steps: LLM distillation and preference alignment (Figure 3). During the first step, we train our fact summarizer by using the reference summaries obtained through LLM prompting. Subsequently, in the second step, we refine our summarizer to better align with the task-specific preference related to QA. To this end, we introduce two preference criteria for the summary candidates: helpfulness evaluates LLMs can correctly answer the question based on the summary, and faithfulness assesses the factual consistency of the summary in relation to the provided set of facts. By selecting pairs of preferred and dispreferred summaries based on these criteria, we further fine-tune the summarizer through direct preference optimization (DPO) (Rafailov et al., 2023). In the end, EFS UM is capable of generating summaries that are both helpful for QA and faithful to the given facts. Extensive experiments on two QA benchmark datasets validate the effectiveness of our evidencefocused fact summarization in improving LLM’s zero-shot QA performance. Our approach significantly improves the zero-shot QA performance of LLMs, while also enhancing the helpfulness and faithfulness of the generated fact summaries. For reproducibility, our codes are publicly available at https://github.com/anon809/EFSum 2 Preliminaries In this section, we introduce a KG-augmented zeroshot QA pipeline, and provide analyses on the verbalized facts obtained by existing methods. 1202.1 KG-Augmented LLM Prompting for QA We focus on a QA approach that leverages LLMs’ zero-shot capability for answering the question, enhanced with external knowledge from KGs. Fact retrieval from knowledge graph. The first stage aims to retrieve question-associated facts from KGs via entity linking, and then to select only top-Kones based on their semantic relevance to the input question. To select only the most relevant facts, recent studies utilize semantic similarities between each fact and the question, employing either a pretrained sentence encoder (Karpukhin et al., 2020a; Xiong et al., 2020) or one fine-tuned specifically for direct fact retrieval (Baek et al., 2023b). In this work, we utilized the former strategy (Song et al., 2020), if not stated. Fact verbalization into various form text. The second stage is fact verbalization, which refers to the task of transforming symbolic facts into textual strings, for feeding them into the LLM as the contextual knowledge. The linear verbalization simply concatenates the head, relation, and tail texts in the triple while keeping the structured format (i.e., triple-form text) (Baek et al., 2023a), or use manually-designed templates and heuristics for linearization (Oguz et al., 2022; Ma et al., 2022). On the other hand, the graph-to-text verbalization transforms the input facts into the plausible and coherent text by using a fine-tuned model (Ribeiro et al., 2021) or prompting LLMs (Wu et al., 2023). Fact injection for question answering. The last stage is prompting the LLM to generate the answer with the verbalized facts. This process, also known as knowledge-augmented LLM prompting for zeroshot QA, gathers the output as the predicted answer. To handle with insufficient evidence of knowledge, the detailed instructions are provided to allow the LLM to utilize its internal knowledge if needed. The prompts are in Appendix C.2. 2.2 Analysis on Verbalized Facts We provide a preliminary analysis of the contextual knowledge obtained by each fact verbalization method, evaluating their (1) density and (2) clarity of evidence. Note that our proposed verbalization method (denoted as EFS UM) summarizes the symbolic facts into free-form texts, prioritizing the evidence relevant to the question. In Figure 2 Upper, it is evident that the number of duplicated tokens is significantly higher in the lin2\nThe ratio of duplicated tokenswithin the verbalized facts\nAverage position of the gold answerwithin the verbalized factsSemantic similarity of the question to the verbalized factsThe length ratio of original factsto the verbalized facts\nKAPINGKG2TextRewriteEFSumprompt(ours)EFSumdistill(ours)Figure 2: Analysis on each fact verbalization method.\nearly verbalized texts (i.e., KAPING and Rewrite) compared to the others. This strongly indicates that their outputs contain redundant information stemming from the pre-defined relations within KGs. When considering the ratio of token lengths before and after verbalization (i.e., compression rate), KAPING and Rewrite either maintain or even increase the length, despite maintaining the same amount of information. Consequently, their evidence density remains low or may even decrease during verbalization. In Figure 2 Lower, the linear verbalization tends to scatter obvious evidence (i.e., answer span) randomly within the contextual knowledge. Their placement seems to rely on the rank obtained from fact retrieval, rather than being positioned at the forefront of the verbalized texts for emphasis. Moreover, the average semantic similarity1of their verbalized facts with the question is lower compared to other methods. This shows that their outputs may include noisy or irrelevant information, or they might not clearly highlight semantically relevant evidence. In essence, clarity of evidence is not adequately addressed in this approach. 3 EFS UM: Proposed Method In this section, we present an Evidence-focused FactSum marization framework, named EFS UM, which aims to effectively augment the set of relevant facts to LLMs for zero-shot QA tasks. 3.1 LLM Prompting for EFS UM The most straightforward implementation of evidence-focused fact summarization is prompting 1Average semantic similarity is calculated through the co-\nsine similarity between embeddings encoded via MPNet.LLMs to generate a summary sgiven a question q and its relevant facts F={fk}K\nk=1, focusing on the evidence to answer the question. Thus, we present EFS UMprompt to verbalize the facts with the help of LLM’s zero-shot capability on summarization; that is, s∼pLLM(·|tsum, q,F), where tsumis the prompt for summarization. Specifically, we instruct LLMs to turn the input facts into the summary for the scenarios where the summary serves as a context to facilitate QA task. We utilized GPT-3.5turbo for generating summary of EFS UMprompt . The detailed prompts are in Appendix C.2. 3.2 LLM Fine-Tuning for EFS UM To enhance the quality of summaries, we propose EFS UMdistill , a fact verbalization model based on an open-source LLM,2specifically fine-tuned for evidence-focused fact summarization. For helpful and faithful summary generation, EFS UMdistill is optimized in two steps: (1) LLM distillation and (2) preference alignment. Figure 3 illustrates the overall EFS UMdistill framework. 3.2.1 Distillation of Fact Summarization We first optimize our open-source model to generate diverse evidence-focused summaries given the question and the set of symbolic facts. For optimization, we augment the QA training dataset of question-answer pairs (q, a)intoX={(q, a,F)}, where F={fk}K\nk=1is the top- Kretrieved facts relevant to the question q. Reference summary generation. We utilize a closed-source LLM (i.e., GPT-3.5-turbo) to obtain reference fact summaries used for training our summarizer. For each tuple (q, a,F)of a questionqand the relevant facts F, we prompt the LLM to transform the set of facts Finto a concise textual description sthat highlights the evidence for the question q, which is same with EFSUMprompt . In the end, we construct the training dataset D={(q, a,F, s)}. The prompt used for summary generation is in Table 5 of Appendix C.2. Supervised fine-tuning. For each quadruplet (q,F, s, a)from the dataset D, our summarizer θ is optimized to generate sconditioned on qandF, by using the causal language modeling objective: LSFT=− E\n(q,a,F,s)∼Dlogpθ(s|q,F). (1) 2In this work, we choose Llama2-7B (Touvron et al., 2023)\nas the backbone open-source LLM for EFS UMdistill .\n3\nQuestionTeacher LLM\nBroad Summary⋯\nBroad SummaryBroad Summary\n✅Helpful\n✅Faithful\n❌Helpful\n❌FaithfulSpecific SummaryDiverse Summary GenerationHeadTailRelation⋮⋮HeadTailRelationHeadTailRelationBroad SummaryBroad SummaryPromptBroad Summary\nQuestionAnswer\n✅Helpful\n❌FaithfulSpecific Summary\nHelpfulness filterFaithfulness filter\nAnswer?QuestionSummaryFactsSummaryEntail?Step 2: Preference Alignment for Helpful and Faithful Summarization Step 1: Knowledge Distillation for Evidence-Focused Fact Summarization \nSupervised Fine-TuningPromptEFSum\n🤖Student LLM\n🤖\nDirect Preference OptimizationHeadTailRelation⋮⋮HeadTailRelationHeadTailRelation\n✅Helpful\n✅Faithful⋯⋯Specific Summary\n❌Helpful\n✅Faithful\nPreferredDispreferredFigure 3: The overall framework of EFS UMdistill . Our fact summarizer is trained to generate evidence-focused\nsummaries via LLM distillation, and then further optimized to align the QA-specific preference, which enhances the\nhelpfulness and faithfulness of its output summaries.\n3.2.2 Alignment with Summary Preference Our summarizer θis now able to generate evidencefocused summaries for a question and the relevant facts, but its output summaries might be unhelpful or unfaithful. Therefore, we additionally adopt preference tuning to enhance EFS UMdistill so that its summarization can align with the task-specific preference in the context of knowledge-augmented zero-shot QA; i.e., generating helpful and faithful summaries, while avoiding the counterparts. For preference tuning, we collect a set of preference pairs (q, a,F, s+, s−), where s+ands−respectively denote the preferred anddispreferred summaries. To identify the preferred and dispreferred summary for QA task, we first sample M summary candidates {s′\nm∼pθ(·|q,F)}M\nm=1for each tuple (q, a,F, s)∈ D using the summarizer θ. Then, for constructing the preference pairs, we adopt two summary filters (for checking helpfulness and faithfulness) and additional paraphrase process. We form a preference pair by selecting the answer-aware paraphrased candidate that passes both filter as preferred, and the candidate that cannot pass one of the filters as dispreferred. Helpfulness filter. The first filter examines the helpfulness of each summary candidate in terms of QA accuracy. That is, it checks whether the summary candidate s′is actually helpful to make the LLM to find the correct answer, by comparing 275the LLM’s generated answer a′∼pLLM(·|q, s′) with the gold answer a. Faithfulness filter. The second filter focuses on the faithfulness of each summary candidate. We use the G-Eval approach (Liu et al., 2023) to leverage LLM’s ability to evaluate the consistency between the input facts and the given summary in terms of hallucination. Precisely, the LLM is prompted to examine whether or not the summary contains unfaithful information, which cannot be inferred from the given symbolic facts. Please refer to the detailed prompt in Appendix C.2. Broad-to-specific paraphrasing. In addition, we adopt the paraphrase process guided by the gold answer a. This paraphrasing aims to obtain high-quality summary by refining a broad focus into a specific focus, using the given answer as the main evidence. We prompt the LLM to paraphrase the summary candidate s′intos′′;{s′′\nm∼ pLLM(·|tparaphrase , s′, a)}M\nm=1, where tparaphrase is the prompt for paraphrasing. This candidate s′′undergoes another pass through the helpfulness filter and the faithfulness filter. The resulting summary obtained in this manner is much more focused on the QA task than the initially obtained reference summary, called as the broad summary. The detailed prompt is provided in Appendix C.2 Direct Preference Optimization. Using the preference pairs P={(q, a,F, s+, s−)}, we apply 4\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) on our summarizer θto train a preference-tuned summarizer θ∗that minimizes the following objective: LDPO(θ∗;θ) =\n− E\n(q,a,F,s+,s−)∼Plogσ[r(q,F, s+)−r(q,F, s−)],(2)\nwhere r(q,F, s) =pθ∗(s|q,F)\npθ(s|q,F). By optimizing the model using preferred-dispreferred summary pairs, our obtained model θ∗is trained to be more biased towards helpful and faithful summary s+while avoiding unhelpful or unfaithful summary s−. Note thatθhas been specifically trained for each QA model, as there are different preferences regarding summary helpfulness across various QA models. The following is an example of an output summary. Question: where was george washington carver from?\nAnswer: Diamond\nFacts: (George Washington Carver, occupation, bi-\nologist),(George Washington Carver, interested in,\nbotany),(George Washington Carver, occupation, univer-\nsity teacher),(George Washington Carver, place of birth,\nDiamond),(George Washington Carver, given name,\nGeorge), (George Washington Carver, field of work,\npedagogy),(George Washington Carver, relative, Moses\nCarver),(George Washington Carver, place of birth,\nUnited States of America),(George Washington Carver,\nresidence, Tuskegee),(George Washington Carver, occu-\npation, inventor)\nSummary: George Washington Carver, the renowned\nbiologist, was born in Diamond, United States of Amer-\nica. He took great interest in botany and became a\nuniversity teacher, focusing on pedagogy. Additionally,\nCarver contributed significantly as an inventor. His rel-\native, Moses Carver, also had a close association with\nhim. Carver later resided in Tuskegee.\n4 Experiments In this section, we design our experiments to answer the following research questions: •RQ1: Does a high density of evidence in verbalized facts contribute to QA accuracy? •RQ2: Does a high clarity of evidence in verbalized facts contribute to QA accuracy? •RQ3: Can preference alignment enhance the generation of more helpful and faithful summaries? 4.1 Experimental Settings Datasets. WebQuestionsSP (WebQSP) (Yih et al., 2016) is a KGQA dataset that filters questions from the WebQuestions (Berant et al., 2013) dataset to include only those answerable via Freebase, and provides SPARQL queries for them. For convenience, we use WebQSP-WD (Sorokin and 334Gurevych, 2018), in which each question from WebQSP is pre-linked to the Wikidata KG. We use a test set comprising 1,033 examples for evaluation. Mintaka (Sen et al., 2022) is a QA dataset that encompasses eight different complexity types. Most question-answer pairs can only be solved by utilizing multi-hop reasoning or the attributes of multiple entities. We use a test set of Mintaka which has 4,000 examples for evaluation. LLMs for zero-shot QA. To measure the efficacy ofEFS UMand other fact verbalization methods, we utilized three different LLMs, GPT-3.5-turbo , Flan-T5-XL (Chung et al., 2022), and Llama2-7BChat (Touvron et al., 2023), for our zero-shot QA evaluation. Note that Flan-T5-XL and Llama2-7BChat are publicly available as open-source. We provide more details on the models in Appendix A.1. Baseline methods. As the main baselines for fact verbalization, we consider various approaches. •No knowledge does not pass any knowledge contexts, encouraging the LLMs to use their internal knowledge to answer the question. •KAPING (Baek et al., 2023a) simply linearizes top-Krelevant facts as the triple-form text. Triple-form text simply refers to the text that is composed by concatenating triplet strings in the form of (head, relation, tail). •Rewrite (Wu et al., 2023) transforms facts into the free-form text for each relation path with a LLM. We utilize GPT-3.5-turbo to convert triples into free-form text. •KG2Text (Ribeiro et al., 2021) employs an encoder-decoder model fined-tuned for the KGto-text task by using WebNLG (Gardent et al., 2017) dataset. We utilize a fine-tuned T5-large model3as our base KG2Text model. Evaluation metrics. Our task can be categorized as a generative KGQA. Following previous work (Baek et al., 2023a; Wu et al., 2023), we use accuracy as our evaluation metric. A score of 1 is assigned if at least one among multiple correct answers is present in the response text of the QA model; otherwise, the score is 0. Relevant fact retrieval. In retrieving questionrelated facts, as described in the Section 2.1, we employ MPNet (Song et al., 2020) to retrieve only the top- Ktriples among given KG, with the highest semantic similarity to the question representation, 3https://public.ukp.informatik.tu-\ndarmstadt.de/ribeiro/graph2text/webnlg-t5-large.ckpt\n5\nDataset MethodsGPT-3.5-turbo Flan-T5-XL Llama2-7B-Chat\nL=200 L=400 L=200 L=400 L=200 L=WebQSPNo knowledge 0.506 0.506 0.409 0.409 0.539 0.KAPING (Baek et al., 2023a) 0.507 0.538 0.391 0.439 0.517 0.KG2Text (Ribeiro et al., 2021) 0.476 0.476 0.316 0.321 0.439 0.Rewrite (Wu et al., 2023) 0.444 0.525 0.350 0.431 0.462 0.EFS UMprompt (Ours) 0.537 0.538 0.447 0.468 0.457 0.EFS UMdistill (Ours) 0.559 0.569 0.458 0.500 0.489 0.MintakaNo knowledge 0.540 0.540 0.228 0.228 0.440 0.KAPING (Baek et al., 2023a) 0.539 0.539 0.269 0.279 0.402 0.KG2Text (Ribeiro et al., 2021) 0.492 0.491 0.234 0.234 0.377 0.Rewrite (Wu et al., 2023) 0.515 0.521 0.280 0.288 0.394 0.EFS UMprompt (Ours) 0.496 0.491 0.312 0.321 0.423 0.EFS UMdistill (Ours) 0.474 0.449 0.326 0.338 0.405 0.Table 1: QA accuracy of the LLMs based on various fact verbalization. We limit the maximum token length of\ncontextual knowledge to L= 200 and400. The best and second-best results are in bold andunderlined , respectively.\nDatasets MethodsGPT-3.5-turbo Flan-T5-XL Llama2-7B-Chat\nRandom Popular MPNet Random Popular MPNet Random Popular MPNet\nWebQSPNo knowledge 0.506 0.506 0.506 0.409 0.409 0.409 0.539 0.539 0.KAPING (Baek et al., 2023a) 0.441 0.437 0.538 0.297 0.329 0.439 0.476 0.490 0.KG2Text (Ribeiro et al., 2021) 0.469 0.468 0.476 0.317 0.276 0.321 0.465 0.451 0.Rewrite (Wu et al., 2023) 0.473 0.445 0.525 0.323 0.348 0.431 0.458 0.439 0.EFS UMprompt (Ours) 0.542 0.534 0.538 0.443 0.442 0.468 0.477 0.472 0.EFS UMdistill (Ours) 0.475 0.539 0.569 0.500 0.505 0.500 0.457 0.488 0.MintakaNo knowledge 0.540 0.540 0.540 0.228 0.228 0.228 0.440 0.440 0.KAPING (Baek et al., 2023a) 0.553 0.516 0.539 0.201 0.198 0.279 0.417 0.398 0.KG2Text (Ribeiro et al., 2021) 0.505 0.500 0.492 0.220 0.235 0.234 0.421 0.389 0.Rewrite (Wu et al., 2023) 0.527 0.524 0.515 0.230 0.224 0.288 0.393 0.374 0.EFS UMprompt (Ours) 0.454 0.492 0.496 0.213 0.215 0.321 0.390 0.392 0.EFS UMdistill (Ours) 0.427 0.425 0.474 0.292 0.243 0.338 0.397 0.393 0.Table 2: QA accuracy of the LLMs based on various fact verbalization, with different fact retrieval strategies\n(i.e., random facts, popular facts, and question-relevant facts). We limit the maximum token length of contextual\nknowledge to L= 400 . The best and second-best results are in bold and underlined , respectively.\nfollowing Baek et al. (2023a). Since processing entire KGs is impractical, we focus on retrieving information from the n-hop neighbors of question entities within a KG. The value of nis given by the specific KGQA dataset to answer the question. In our experiment, nis set to 1 for WebQSP and 2 for Mintaka. When calculating semantic similarity, we use the linear verbalization approach, which involves combining the subject, relation, and object texts from the triple. 4.2 Effectiveness of Dense Evidence (RQ1) To examine the impact of dense evidence within verbalized facts on the final QA performance, we evaluate the LLM’s QA accuracy while imposing restrictions on the maximum token lengths Lof contextual knowledge. This implies that the num- 398ber of facts included in the contextual knowledge varies depending on fact verbalization methods. Effect of knowledge augmentation. First of all, we observe that knowledge augmentation for zeroshot QA does not always produce positive results. Knowledge augmentation cannot be helpful in two scenarios: where the model’s ability to ground input knowledge is lacking, or where the retrieved knowledge is noisy while the QA model’s internal knowledge is sufficient. In Table 1, when Llama27B-Chat was used as the QA model, it demonstrates higher performance under the “No knowledge” condition across both datasets (i.e., WebQSP and Mintaka) compared to other baselines. This is indicative of Llama2-7B-Chat’s limited capability in utilizing the provided knowledge. 6\nDataset MethodsGPT-3.5-turbo Flan-T5-XL Llama2-7B-Chat\nK=10 K=30 K=10 K=30 K=10 K=30\nWebQSPNo knowledge 0.617 0.607 0.498 0.451 0.646 0.KAPING (Baek et al., 2023a) 0.777 0.771 0.643 0.738 0.668 0.KG2Text (Ribeiro et al., 2021) 0.589 0.608 0.467 0.457 0.409 0.Rewrite (Wu et al., 2023) 0.628 0.728 0.533 0.664 0.594 0.EFS UMprompt (Ours) 0.788 0.755 0.629 0.711 0.497 0.EFS UMdistill (Ours) 0.786 0.783 0.644 0.741 0.599 0.MintakaNo knowledge 0.810 0.788 0.492 0.444 0.783 0.KAPING (Baek et al., 2023a) 0.912 0.869 0.723 0.673 0.832 0.KG2Text (Ribeiro et al., 2021) 0.879 0.768 0.536 0.491 0.799 0.Rewrite (Wu et al., 2023) 0.901 0.875 0.720 0.691 0.843 0.EFS UMprompt (Ours) 0.920 0.887 0.742 0.735 0.849 0.EFS UMdistill (Ours) 0.893 0.824 0.745 0.719 0.852 0.Table 3: QA accuracy of the LLMs based on fact verbalization methods. We fix the number of facts to K= 10 and\n30. The best and second-best results are in bold and underlined , respectively.\nComparison with other baselines. We first compare the performance of various fact verbalization methods at L= 200 and400. In Table 1, for most cases, EFS UMshows superior performance over the majority of baseline approaches. Considering the ratio of lengths before/after verbalization (in Figure 2), this clearly indicates that EFS UMcan encapsulate more intensive and useful information within shorter summaries. Notably, the effectiveness of our approach is more evident when the length of knowledge decreases from L= 400 to 200. This suggests that EFS UMremains highly effective even in contexts when the utilization of extremely concise knowledge is required. Compatibility with various retrievers. We investigate the QA performance using different types of fact retrieval, to assess the robustness across various knowledge qualities: randomly selected knowledge ( Random ), the knowledge possessing the most frequently occurring relation ( Popular ), and question-relevant knowledge ( MPNet ). In Table 2, EFS UMachieves the highest accuracy across most datasets and QA models, regardless of the retriever used, which implies that our method can exact useful facts from noisy input triples. 4.3 Effectiveness of Clear Evidence (RQ2) To examine the effectiveness of clear evidence within verbalized facts on the final QA performance, we assess the LLM’s QA accuracy only on the test tuples (q, a,F)where the facts Ffully contain the ground-truth answer span a. This experimental setup allows us to investigate how effectively each fact verbalization method converts F Figure 4: Summary-level and answer-level QA accu-\nracies with respect to the number of relevant facts on\nWebQSP (Upper) and Mintaka (Lower), respectively.\ninto a textual string without overlooking evidence a, thereby enabling correct answers. Comparison with other baselines. To examine the effectiveness of EFS UMin terms of evidence clarity, we evaluate the LLM’s zero-shot QA accuracy when the number of facts is K= 10 and30. In Table 3, EFS UMshows the highest performance across most datasets, showing consistently leading results for both Kvalues. Nevertheless, we notice the presence of uncontrolled model inclination. For example, KAPING consistently exhibits the highest performance when Llama2-7B-Chat is utilized as the QA model for the WebQSP dataset. This might be due to the model’s inclination towards a specific knowledge format. Robustness of EFS UMacross various K.To assess the robustness of EFS UM, we increase the retrieved number of facts Kfrom 10 to 150 and mea7\nKG2TextRewriteEFSumpromptEFSumdistillKG2TextEFSumpromptEFSumdistillRewriteFigure 5: Two quality metrics of verbalized facts.\nsure the performance of each baseline at the answerlevel andsummary-level . At the answer-level , we evaluate the extent to which the answers are contained within the responses generated by the QA model using summaries. In case of the summarylevel , we evaluate whether the answer is included within the verbalized knowledge produced by the method. In Figure 4, across both datasets and the majority of Kvalues, EFS UMdistill consistently outperforms, especially in answer-level accuracy, except for KAPING, which inherently achieves a summary level accuracy of 1 by simple linearization of facts. 4.4 Effect of Preference Alignment (RQ3) Helpfulness and faithfulness. We examine the helpfulness andfaithfulness of the verbalized facts generated by each verbalization method.4Note that helpfulness is calculated through summary-level accuracy, while faithfulness is calculated as 1 - (hallucination occurrence rate). That is, a method with a lower rate of hallucination occurrences possesses higher faithfulness. Figure 5 illustrates that EFSUMdistill achieves superior helpfulness and faithfulness compared to other baselines, including EFSUMprompt . In other words, EFS UMdistill is less likely to generate summaries of hallucination and is capable of incorporating more correct answers into the summaries compared to other baselines. Moreover, it validates that using the two filters in the broad-to-specific paraphrasing process definitively aids in improving both faithfulness and helpfulness. 5 Related Work 5.1 KG-Augmented LLM Prompting To supplement the incomplete internal knowledge of LLMs, recent research has been exploring knowledge augmentation methods using prompting. Various types of information can be utilized for prompting knowledge, notably KGs. Recently, Baek et al. 4We opt not to include KAPING in the plot, as its linear\nverbalization does not alter the content of the facts.(2023a); Wu et al. (2023) utilize triple-form facts from KGs for knowledge augmentation for LLMprompting. However, the knowledge injected into LLMs by these methods is not focused on the question. In other words, the semantics of the question are not considered during the process of deriving the final knowledge from the given triples. Consequently, the final knowledge derived through each method may inevitably have low density and clarity from an evidence perspective. 5.2 Knowledge Graph Question Answering Studies exploring the integration of additional knowledge extracted from KGs, represented as subgraphs, are divided into two main approaches. The first approach utilizes semantic parsing techniques (Bao et al., 2016; Luo et al., 2018), which enable the extraction of executable queries from the KG by using contextual information as a parsing reference. Alternatively, the information retrieval approach involves encoding assimilated information using techniques like Graph Neural Networks (GNNs). Several recent studies (Yasunaga et al., 2022a,b; Zhang et al., 2022) propose a learning framework that combines GNNs and LMs, allowing concurrent utilization of textual data and KG. In contrast to the conventional KGQA approaches that aim to directly identify the answer entity within KGs, in this work, we mainly focus on how the KGretrieved facts can be effectively utilized within the zero-shot QA capability of LLMs. 6 Conclusion In this paper, we explore methods to enhance the zero-shot QA performance of LLMs by augmenting knowledge from KGs. We introduce a novel summarization framework, called EFS UM, which transforms a set of facts into summary with high density and clarity of evidence for answering the question. To achieve this, we optimize an open-source LLM as a fact summarizer, leveraging a teacher LLM’s summarization capabilities and aligning its outputs with QA-specific preferences. Our experiments show that EFS UMsignificantly improve QA accuracy across various LLMs compared to other fact verbalization approaches. Furthermore, serving as an independent summarization module, it generates helpful and faithful summaries based on relevant facts and target questions. 8\n7 Limitation Despite our discoveries and improvements, we must acknowledge certain limitations in our work and potential areas for future research. To begin with, the accuracy, which is the metric used in our experiments has the potential to overestimate the correctness of responses, even if the response does not accurately convey the intended semantic meaning. This discrepancy can occur because the metric simply verifies the existence of the answer entity, regardless of whether it is contextually appropriate. Unlike semantic parsing KGQA, which involves retrieving entities from the KG, or multiple-choice KGQA, where the answer is chosen from several options, evaluating metrics for generative KGQA remains an open field that warrants further investigation. Secondly, there are cases where the tendency of LLMs to favor a certain fact verbalization method becomes overwhelming and difficult to manage. As demonstrated in Tables 2 and 3, with the example of Llama2-7B-Chat on the WebQSP dataset, it has been observed that certain models may have a inclination for specific knowledge formats in particular datasets. Consequently, while our summarizer generally demonstrates good performance, controlling performance may become challenging when a specific model has a strong inclination towards a particular knowledge format. Lastly, it is important to note that the performance of our proposed summarizer can be influenced by the performance of the retriever. As can be seen in Table 2, using a better retriever can lead to higher performance. While the off-the-shelf model we used (i.e. MPNet) demonstrates retrieving capabilities based on the semantic similarity between questions and facts, it’s difficult to assert that it is a flawless retriever. For example, in a 2-hop dataset(i.e. Mintaka), it tends to retrieve 1-hop neighbors more than 2-hops even if it is irrelevant. This is because an answer entity in 2-hop neighbors is unseen in given question, so that a retrieval model may measure question entities in 1-hop neighbors more similar. Therefore, we are currently conducting further research to propose not only a more powerful summarizer but also a more flawless retrieving method simultaneously. 8 Ethical Consideration Throughout our research, we thoroughly explore our methodology using an open-source dataset, 600chosen to ensure transparency and integrity in our work. It is important to acknowledge the inherent potential for biases within our summarizer, which relies on LLMs, and which may inadvertently reflect prevailing social biases. It is crucial to notice that our method is not intended to inflict harm upon any individuals or groups. 607",
        "aspect_review": "The paper introduces a novel Evidence-focused Fact Summarization (EFS UM) framework that effectively enhances the zero-shot QA performance of LLMs by transforming a set of facts into a coherent and evidence-dense summary. The methodology involves optimizing an open-source LLM as a fact summarizer through LLM distillation and preference alignment, showcasing strong technical depth and rigor. The extensive experiments conducted demonstrate that EFS UM significantly improves the zero-shot QA performance of LLMs across various datasets and models. The results validate the framework's ability to generate summaries that are both helpful and faithful, contributing to the clarity and density of evidence for QA tasks. The paper makes a significant contribution by addressing the challenges of low density and clarity in existing KG verbalization methods. It provides a comprehensive solution that enhances the practical impact of LLMs in knowledge-augmented QA, offering a valuable resource for future research in this area.\n\nThe paper could benefit from a more detailed discussion of related work, particularly in the context of existing fact verbalization strategies and their limitations. While the results are promising, the paper does not provide a detailed analysis of the potential biases in the summarization process and how they might affect the QA performance. The presentation of the experimental results could be improved with clearer visualizations and more concise tables to enhance readability. The reliance on specific datasets for evaluation may limit the generalizability of the findings. A broader range of datasets could provide a more comprehensive assessment of the framework's effectiveness. The analysis of the results could be expanded to include a more in-depth exploration of the factors contributing to the framework's success, particularly in comparison to baseline methods. While the paper compares EFS UM to several baselines, a more detailed comparison with state-of-the-art methods in related fields could strengthen the argument for its novelty and effectiveness. The methodology section could benefit from additional details on the implementation and hyperparameter settings used in the experiments to facilitate reproducibility.",
        "general_review": "The paper presents a novel and insightful methodology, EFS UM, which significantly enhances zero-shot QA performance of LLMs by utilizing Knowledge Graphs for evidence-focused fact summarization. The approach demonstrates strong technical depth and rigor by optimizing an open-source LLM through distillation and preference alignment, ensuring both helpfulness and faithfulness of summaries. Comprehensive experimental validation is provided through extensive experiments on two QA benchmark datasets, showcasing the framework's effectiveness in improving QA accuracy. The paper's contributions have significant practical impact, offering a robust solution for integrating external knowledge into LLMs to address factual errors and enhance performance in knowledge-intensive tasks.\n\n1. Minor Writing or Clarity Improvements: Some sections could benefit from clearer explanations, particularly in the methodology description. 2. Small Details on Hyperparameters or Implementation: The paper could provide more details on hyperparameters used during model training and evaluation. 3. Very Minor Formatting on Figures: A few figures could be formatted more clearly for better readability. 4. Slight Clarification on Code Availability: While the code is publicly available, the paper could provide more explicit guidance on how to access and utilize it."
    },
    "47": {
        "paper": "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and\nCreative Thinking through the Negation of Negation\nAnonymous ACL submission\nAbstract\nLarge language models (LLMs) exhibit powerful reasoning capacity, as evidenced by prior studies focusing on objective topics such as arithmetic, symbolic, and commonsense reasoning tasks. However, the reasoning of objective topics emphasizes more on logical thinking , but cannot effectively reflect the comprehensive, reflective, and creative thinking that is also critical for overall reasoning prowess of LLMs. In light of this, we build a dataset SJTP comprisingSubJective ToPics spanning diverse types and fields, as well as three evaluation indicators to fully explore the reasoning ability of LLMs. It has been observed that a sole emphasis on logical thinking falls short in effectively tackling subjective challenges. Therefore, we introduce a framework grounded in the principle of the Negation ofNegation (NeoN) to unleash the potential comprehensive, reflective, and creative thinking abilities of LLMs. Comprehensive experiments on SJTP demonstrate the efficacy of NeoN, and the enhanced performance on various objective reasoning tasks unequivocally underscores the benefits of stimulating LLM’s subjective thinking in augmenting overall reasoning capabilities. 1 Introduction Large language models (LLMs) have achieved remarkable performance in recent years (OpenAI, 2022, 2023; Touvron et al., 2023; Jiang et al., 2023) and have displayed formidable reasoning ability that validated on various objective topics, including arithmetic reasoning (Luo et al., 2023; Yang et al., 2023), symbolic reasoning (Wei et al., 2022a), and commonsense reasoning (Geva et al., 2021a; Talmor et al., 2019a), etc. Despite the remarkable reasoning capabilities, their evaluation still lacks comprehensiveness. Previous research primarily investigates LLMs based on objective topics with clear-out answers and logical reasoning path ( e.g.“3-2=1”, “Cat is herbivoThe density of pear is about 0.6 g/cm^3, the density of water is 1g/cm^3. \nYes or no:Would a pear sink in water?LogicalThinkingComprehensive ThinkingReflective ThinkingCreative Thinking\nThe pensity of pear is less than water.\nThe answer is NO.\nSocial media platforms have negative effects on mental health.\nWhat is the impact of social media on mental health?\nCognitive overload, self-esteem, anxiety, cyberbullying.All are negative impacts.\nHuman ThinkingComprehensive ThinkingReflective ThinkingCreative ThinkingObjective QuestionSubjective Question\nLogical thinkingComprehensive thinkingReflective thinkingCreative thinking\nLogical thinking performs well on objective questions. Missing yet vital for subjective questions.Figure 1: LLMs wich CoT prompting (Wei et al., 2022a)\nshowcase strong logical thinking ability, which is ade-\nquate to solve objective questions. Meanwhile, it fails\nto solve subjective questions and shows insufficiency of\ncomprehensive, reflective and creative thinking abilities.\nrous?”) (Mao et al., 2023). It is evident that reasoning on these topics heavily relies on logical thinking. The success of Chain-of-Thought (CoT) (Wei et al., 2022a) serves as compelling evidence, as it elicits logical thinking through prompts with a series of reasoning steps, instantly leading to significant improvement in solving objective questions. However, the ideal reasoning ability of LLMs transcends mere logical thinking, comprehensive, reflective, and creative thinking are indispensable when tackling complex tasks, which cannot be well reflected on objective topics, as shown in Figure 1. Consequently, the lack and difficulty in evaluating these abilities pose challenges for approaching human-like thinking ability of current LLMs. To enable analysis of LLMs in terms of comprehensiveness, reflection, and creativity and further enhance the overall reasoning ability, we first develop a benchmark SJTP to reflect these abilities, which consists of SubJective ToPics of three types and eight fields as examples shown in Table 1. Furthermore, we design three evaluation indicators based on six evaluation dimensions to assess the responses of subjective questions. The evaluation di1\nTopic Type Example SCR\nViewpoint Discourse What do you think about the impact of social media on mental health? 76.5\nBinary Dialectics Should school wear uniforms? 69.9\nPractical Analysis How can the preservation of cultural heritage contribute to the promotion\nof social cohesion and identity ?80.2\nTopic Filed ①Social and Ethics, ②History and Civilization, ③Literature and Arts,\n④Technology and Education, ⑤Environment and Health, ⑥Economy\nand Politics, ⑦Law and Human Rights, ⑧Psychology and Emotions\nEvaluation Dimensions ①Clarity of Viewpoint, ②Logicality of Argumentation, ③Correctness\nand Fidelity, ④Comprehensiveness and Diversity, ⑤Innovation and\nUniqueness, ⑥Depth and Essentially\nTable 1: Topic types, topic fields and evaluation dimensions for the construction and evaluation of SJTP dataset.\nThe value of SRC is the result by GPT-3.5 (average of the three evaluation indicators).\nmension includes both objective aspects(e.g., clarity, logicality) and subjective aspects(e.g., reflection, creativity) as listed in Table 1 and 9. We then briefly evaluate gpt-3.5 armed with the promising CoT prompts, which showing limited performance on tacking subjective questions. As shown in Figure 1, the reasoning pathway does not deliberately consider the full-sided perspectives and excavate the intrinsic causes or potential solutions about \" the impact of social media on mental health \", resulting in lack of depth and unbalanced analysis. To elicit the language models’ comprehensive, reflective and creative thinking abilities, we propose a structured framework inspired by the principle of the negation of negation, which is a philosophical principle proposed by Engels (Engels et al., 1954). The core insight is that the development and completion of things must go through negation and transcendence of themselves, which is in line with the spirit of \"abstraction-negationconcreteness\" proposed by Hegel (Pinkard, 1988). Through constant negation, we can emphasize the multifaceted and complex nature of problems, and break established thinking patterns. Please not that if there are no defects or incompleteness in the initial topic, the \"negation\" stage is not necessary, which is equivalent to classical logical thinking. Specifically, we proposed NeoN (Negation of negation), which casts an LLM as an agent of a negator instead of a logical reasoner. This agent will first determine whether to proceed with negation based on the similarity between the latest response and the existing ones. If so, the agent will be encouraged to generate unconsidered views, innovative ideas and deep insights by negating existing content. Note that the entire negation process is carried out upon maintaining correctness and fidelity. In the end, LLMs provides a refinement response by synthesizing all these answers. Further- 104more, this method is built on top of the prompting technique, thus there is no trainable module. Technically, the proposed NeoN comprises three stages: straight answer, recurrent negation, and unification reasoning. To guarantee basic objective reasoning ability, we first allow LLMs to generate answers directly with strict logical thinking. Then, a negation link to the previous responses is constructed with a simple judgment under the correct premise, which has been shown helpful to supplement and surpass the original response in a spiral upward manner. Finally, we enable LLMs to engage in reasoning based on both the original question and the unification of the diversified responses. We employ both API-based and open-source LLMs including GPT (OpenAI, 2023), InstructGPT (Ouyang et al., 2022), and LLaMA (Touvron et al., 2023), to validate the effectiveness of our framework. Experimental results show that NeoN leads to significant and consistent improvements under both subjective and objective topics, underscoring the necessity and effectiveness of unleashing the comprehensive, reflective and creative thinking for better reasoning ability. 2 Subjective Topic Dataset In this section, we aim to explore the comprehensiveness, reflection, and creativity of LLMs by constructing a SJTP benchmark, which includes diverse and sophisticated subjective topics along with reasonable scoring points and solutions. Besides, we further elaborately develop three evaluation indicators to assess the quality of responses. The overall illustration is shown in Figure 2. 2.1 Question Generation Topic Sampling. We first define and build a subjective topic pool that covers various types and 2\n#Type= {Viewpoint Discourse}# Field= {Technology and Education, Psychology and Emotions}# Themes= {technological impact, mental health}DataGenerationATopic Sampling\n#QuestionWhat do you think about the impact of social media on mental health?BQuestion Generation\n#Scoring PointsAwareness of both positive and negative impacts of social media on mental health.(20 points); Discussion of potential solutions for the negative impacts identified (20points); ...#SolutionIt's crucial to acknowledge the dual nature of social media's impact on mental health. On one hand…EvaluationCSolution GenerationSim. < 𝝐?#r𝒊: Social media platforms also offer numerous benefits, such as connecting people, sharing information, etc.i+1\n#R:Social media platforms have negative effectson mental health while offer numerous benefits. For example…Clarity of viewpoint: 85 (The answer provides clear viewpoints on…Logicality of argument: 95 (The answer presents a clear and logically structured argument…Correctness: 85…\nInference (NeoN)1.Topic 是从pool来2.关于solution，根据evaluationdimensions3.pool sample -> A, A B + eval dimensions 4.Inference 体现5.算rouge是当前的和前面的去算的6.加粗不够，加颜色7.Evaluation里，soluiton和r算的R和evaluationdimension得到的8.花哨Topic PoolSample\nLLMEvalution Dimensions\nNegate\n#r𝒊#𝟏: Social media platforms have negative effects on metal health …\nUnify\nCrietria:\nSolutionScoring PointsEvalution Dimensions\nEvalution Dimensions\nFigure 2: Illustration of the construction of SJTP dataset, the framework of NeoN, and the evaluation indicators.\nfields that be widely discussed, as shown in Table 1. Specifically, we group the subjective topics into three representative types, i.e., Viewpoint Discourse, Binary Dialectics, Practice Issues , which can reflect the comprehensiveness, reflection and creativity of the respondents. Additionally, each type particularly emphasizes one of the thinking abilities, e.g., the Practical Issues is more reflective of creative thinking. Besides, we define eight topic fields covering a broad scope of knowledge such as social and technology. Each field contains subdivided related themes to improve the diversity of problem generation, see Appendix A.1 for details . Given the topic pool, for each data, we randomly set the topic type and uniformly sample 1 or 2 themes from fields that with connection as seeds, and then generate corresponding type of subjective problems involving the themes by prompting LLM. This generation mechanism challenges the model to generate problems that join diverse topics while keeping the problems reasonable. Question Generation. Specifically, we incorporate the sampled topic type, fields, and themes into the prompts and utilize LLM’s excellent language comprehension ability to generate a specific question in a zero-shot manner. We do not include any exemplars or other manual interventions to avoid potential biases brought by the concepts within the exemplars and achieve a diverse generation. 1692.2 Solution Generation Evaluation dimensions. To generate qualitative solutions for the subjective topics, we first need to set standards for perfect answers. Considering both objective and subjective factors, we establish six evaluation dimensions and corresponding detailed requirements, as shown in Table 1 and Table 9. Scoring Points & Solution Generation. Given the generated questions, we prompts an LLM to generate the corresponding scoring points according to the evaluation dimensions. Since LLM is enforced to consider the requirements of evaluation standards and the knowledge in different fields, it will generate clear, organized, and reasonable scoring points that more meet our expectations than directly answer the question. Furthermore, we generate the corresponding complete solutions given both the question and scoring points by prompting LLM, to serve as a more intuitive reference. 2.3 Evaluation Indicator Given the inherent challenge in quantifying the quality of responses to subjective questions, we carefully craft three indicators to serve as robust measures: (1) SRC soltakes the generated solution as standard, which assesses the semantic similarity between the response and the reference solution; (2)SRC point takes the unique scoring points for each question as the standard, which measures to 3\nwhat extent the response meet the requirements of each scoring point; (2) SRC dimtakes the general evaluation dimensions as the standard, which assess the response from each dimension according to their detail standards respectively. The consistency presented by the three indicators in experiment results demonstrates their rationality. As a result, the LLM needs to complete the following three tasks: (T1) Generating a subjective problem that relates to the given type and themes; (T2) Providing corresponding scoring points and reference solution of the subjective problem with the prefix evaluation requirements; (T3) Performing evaluation according to the three evaluation indicators. In this work, we use GPT-4 (OpenAI, 2023) as the LLM for generating and evaluating. See prompt templates in Appendix A.2 and C.1. 3 NeoN Framework Our investigation into the reasoning performance of CoT prompting on the proposed SJTP dataset, as delineated in Table 1, reveals limitations of the current LLMs capacity for comprehensive, reflective, and creative thinking. To unleash these cognitive faculties in LLMs, we introduce the NeoN framework grounded on the principle of the negation of negation, allowing models to engage in a selfenhancing cycle of continuous improvement and transcendence. The overall framework of NeoN is schematically illustrated in Figure 2 and is characterized by two pivotal processes: iterative negation and integrative unification. TheNeoN framework first generates an initial solution through its inherent reasoning ability. Subsequently, it embarks on a process of negation, critiquing its earlier responses while maintaining a commitment to correctness and fidelity. This stage is designed to encourage the model to explore broader perspectives, uncover potential inconsistencies, engage in thorough analysis, and break away from established concepts. Finally, it obtains a refined response by methodically assimilating and integrating the preceding responses. Formally, given a subjective question Q, our goal is to let the LLM Msolve the question Q. Step 1: Direct Response. We first let LLM Mdirectly generate a reasonable response r0according to the question Q. Specifically, we have r0=M(Q ⊕ P 1), (1) where⊕denotes concatenation operation. P1is a 246prompt serving as a trigger sentence, for example, we can set P0as “Let’s generate the answer ”. Step 2: Negation of Negation. Then, building upon the principle of the negation of negation, i.e., things develop and progress in constant negation, we let LLM Mconstantly negate the previous responses, thereby facilitating the generation of novel and advanced content: rn=M(Q ⊕r0⊕ ··· ⊕ rn−1⊕ P 2),(2) where ndenotes the number of negation rounds. We terminate the negation process when the semantic similarity between the current response rn and previous responses r0⊕ ··· ⊕ rn−1exceeds a threshold ϵ, which implies that rnis approaching a state of refinement since M’s diminishing capacity to yield additional novel insights. The value of nis usually between 2∼3empirically. P2is a prompt for making reasonable negation, e.g., we can set P2 as “Negate the above responses to deduce a more perfect answer. ” Step 3: Integration and Unification. Finally, we take question Qand all the responses as the input, letting LLM Mgive the final response R: R=M(Q ⊕r0⊕ ··· ⊕ rn⊕ P 3), (3) whereP3is the last prompt leading to the final answer which can be set as “ Based on all the previous answers, generate a perfect answer. ”. Remark In step 2, the negation process terminates when the current response presents a highly consistency with preceding responses. This design is predicated upon a claim by Hegel (Pinkard, 1988), which asserts that “A truth with concreteness, comprehensiveness, and absoluteness must always go through the stage of negation in the process of completion. When a statement is sufficiently correct and consummate, further negation becomes futile, and reasoning is equivalent to logical reasoning. ” In alignment with this philosophical foundation, by constantly negating old concepts, the model will be propelled to explore new perspectives, excavate deeper essence, and challenge established opinions, thereby fostering the evolution and refinement of its solution. Besides, we implement NeoN without any exemplars to avoid potential limitation, and without any restriction on data types. 4 Experiment 4.1 Setups Datasets. We evaluate the performance of our framework on both subjective task and twelve ob4\nMethodViewpoint Discourse Binary Dialectics Practical Analysis\nAvg SCR solSCR point SCR dim SCR solSCR point SCR dim SCR solSCR point SCR dim\n(Acc.) (Acc.) (Acc.) (Acc.) (Acc.) (Acc.) (Acc.) (Acc.) (Acc.)\n(llama-2-70b )\nDirect Prompt 61.25 64 .72 78 .80 80 .52 44 .62 77 .30 76 .08 70 .82 81 .50 70 .64\nZero-Shot-CoT 66.70 67 .02 77 .63 79 .30 50 .27 74 .38 78 .82 68 .20 80 .42 70 .20\nSelf-Consistency 67.20 67.74 78 .96 81.76 50 .82 75 .72 80 .25 71.14 81.35 72.74\nRecite&Answer 66.01 68 .15 76.70 82 .50 51.27 76.19 78.30 69 .34 80 .76 71 .89\nNeoN (Ours) 71.33 70 .25 82 .70 84 .14 55 .10 81 .22 82 .15 72 .53 83 .20 75 .85\n(text-davinci-003 )\nDirect Prompt 62.33 64 .18 80 .91 83 .47 46 .20 78 .55 76 .68 69 .95 81 .15 71 .49\nZero-Shot-CoT 69.54 68 .24 79 .66 81 .21 54 .91 75 .10 81 .24 67 .62 80 .26 73 .09\nSelf-Consistency 71.17 71.26 80.78 84 .44 56 .74 76 .83 84.63 70.20 82 .94 75.44\nRecite&Answer 68.28 70 .41 82 .10 84.79 58.75 78.62 80.22 71 .58 82.27 75 .23\nNeoN (Ours) 77.66 75 .43 86 .30 89 .87 63 .70 82 .05 84.56 73.43 85 .35 79 .81\n(gpt-3.5-turbo-1106 )\nDirect Prompt 65.72 69 .21 84 .83 85 .66 43 .24 80 .69 87 .92 72 .50 84 .63 74 .93\nZero-Shot-CoT 71.86 72 .40 85 .26 88 .11 48 .35 73 .40 88 .24 70 .53 82 .89 75 .81\nSelf-Consistency 73.26 74.15 85.47 90.20 51.50 78 .85 89 .23 73.62 84 .18 77 .72\nRecite&Answer 72.60 66 .78 83 .46 89 .51 52 .68 82.07 85.70 77.86 85.25 77.31\nNeoN (Ours) 80.40 81 .82 88 .67 92 .50 60 .33 83 .21 89 .15 76.83 87.74 82 .29\nTable 2: Main results of methods on SJTP. The best result is in bold and the second-best is underlined .\njective tasks, to fully validate the effectiveness of NeoN and the importance of subjective thinking for LLM’s overall reasoning ability. For the subjective task, our proposed SJTP consists 500 data involving three types of subjective topics across eight fields. It aims to investigate LLM’s subjective thinking abilities such as comprehensive, reflective and creative thinking. For objective tasks, we consider six Arithmetic Reasoning datasets, two Commonsense Reasoning datasets, two Generic Reasoning datasets and two Symbolic Reasoning datasets. See details in Appendix B.1. Models. For LLMs, we evaluate on both API-based models including GPT-3.5 (OpenAI, 2023) and InstructGPT (Ouyang et al., 2022), and open-source model LLaMA-2-70B (Touvron et al., 2023). In particular, we use the released API version of gpt3.5-turbo-1106 and text-davinci-003 by OpenAI. We set the decoding temperature as 0.5 to obtain the diversity of the responses generated by LLMs. For AI evaluation, we take the sota LLM gpt-41106-preview released by OpenAI as the evaluator. Baselines. We compare NeoN mainly to zero-shot prompting methods to verify the effectiveness of its activation of subjective thinking abilities. Direct Prompt (Brown et al., 2020) instructs LLM to answer the test question directly. Zero-ShotCoT (Kojima et al., 2022) appends the prompt “Let’s think step by step” before reasoning. Selfconsistency (Wang et al., 2022) first samples multiple solutions from the LLM with CoT technique and then take the majority vote as the final result. claritylogicalitydepthcomprehensivenessinnovationcorrectness\nFigure 3: Categorized performance analysis for score in\ndifferent evaluation dimensions and topic types.\nRecite&Answer (Sun et al., 2023b) retrieves relevant passages first and then generates responses. 4.2 Main Results Main Results on SJTP. In Table 2, we show the main results of the baselines and our NeoN method. Direct Prompt method directly respond to questions and gives the average score of 70.64∼74193 across different models, showing relatively limited performance for subjective topics. By explicitly prompting LLMs to “ Let’s think step-by-step ” or “recite relevant passages then give answers ”, Zero-Shot-CoT and Recite&Answer surpass CoT in some cases thanks to their clear reasoning path or retrieved relevant information, but sometimes degrade possibly due to their limitation on diversity. Self-consistency generate multiple reasoning path via CoT prompt and make a synthesis that bene5\nArithmetic\nGSM8K SV AMP AQuA MultiArith SingleEq AddSub\nDirect Prompt 17.31 70.79 28.40 69.37 85.42 86.91\nZero-Shot-CoT 80.15 80.38 54.24 95.62 93.86 88.19\nSelf-Consistency 83.89 83.60 63.39 96.21 95.36 89.93\nRecite&Answer 76.25 79.33 55.91 96.67 93.81 89.27\nNeoN(Ours) 84.17 85.67 61.52 96.94 95.67 93.09\nCommonsense Generic Symbolic\nCSQA StrategyQA Date Understand Shuffled Objects Last Letter Coin Flip\nDirect Prompt 72.25 62.33 46.83 32.98 2.46 53.54\nZero-Shot-CoT 70.80 62.08 64.44 69.66 70.52 89.59\nSelf-Consistency 73.16 63.76 73.27 72.08 86.30 97.52\nRecite&Answer 71.81 61.34 70.34 72.76 71.34 87.93\nNeoN(Ours) 75.22 64.83 77.46 74.13 88.18 97.92\nTable 3: Main results of baseline methods and NeoN on twelve objective datasets (using gpt-3.5).\nfits the comprehensiveness of response, which improves performance to a limited extent restricted by depth and innovation. Regarding our NeoN method which involves multi-turn of negation, it consistently surpasses all the baselines across models and topic types. Compared to the second-best ones, our method improves absolutely by +3.11,+4.37, and +4.57by using LLaMa-2, InstructGPT, and GPT3.5 model, respectively, which further demonstrates the effectiveness of negation in solving subjective tasks. Note that the consistency between the scores evaluated under the three evaluation indicators further demonstrates the rationality of our proposed evaluation indicators To further verify that NeoN has unleashed the model’s comprehensiveness, reflection and creativity, we visualize the SCR dimscore of methods in each evaluation dimension in Fig 3. It is clear that NeoN surpass all the baselines in the three subjective evaluation dimensions by a large margin, while maintaining the quality in terms of the three objective criteria, demonstrating the effectiveness of NeoN in activating the comprehensiveness, reflection and creativity of LLMs. Besides, NeoN achieve consistently improvement on all the three topic types, showing its universality. Main Results on Objective Tasks. To substantiate the significance of unleashing comprehensive, reflective and creative thinking for enhancing the overall reasoning performance of LLMs, we further evaluate our NeoN on twelve objective reasoning datasets. The results are shown in Table 3. It can be clearly observed that NeoN surpasses the baselines on 11 of 12 datasets, indicating that unleashing subjective thinking abilities also benefits objective reasoning process, which also need comprehensive 380AddSub CSQA\nF2T( %) T2F( %) F2T( %) T2F( %)\nSelf-Consistency 18.42 0.79 13.33 2.91\nNeoN (ours) 25.64 0.13 17.98 0.99\nTable 4: The ratio to correct incorrect answers\n(F2T:True2False) and mislead models (T2F:True2False)\nof different methods on AddSub and CSQA dataset.\nconsideration, in-depth analysis, and constant reflection of the potential errors and omissions. We calculate the probability of NeoN correcting the initially incorrect answer and the proportion of misleading the original correct answer, as shown in Table 4, demonstrating that improve the subjective thinking abilities can alleviate the hallucination problem to a certain extent. 4.3 Analyses and Discussions Distribution of Topic Fields in SJTP. We presents the proportion of data involving different topic fields in the proposed SJTP dataset, as shown in the Figure 4. We can see that SJTP covers 8 topic fields fairly evenly, in which 41% data involve two topic across two fileds. This shows the diversity of subjective topics and a wide range of knowledge covered in SJTP, which enables SJTP to be used as Soc&Eth\n17.07%His&Civ\n10.98%Lit&Art\n8.54%\nTech&Edu20.73%\nEnv&Heal10.98%\nEco&Pol10.98%\nLaw&Right10.98%Psy&Emo9.76%\nFigure 4: Different fields of subjective topics in SJTP.\n6\nMethod SCR Sol SCR point SCR dim\nNeoN (Ours) 87.35 72 .99 86 .54\nw/oNG ↓2.33 ↓2.67 ↓2.69\nw/oUF ↓1.62 ↓1.78 ↓1.01\nw/oNG&UF ↓4.74 ↓5.17 ↓3.67\nTable 5: Ablation study of NeoN method on SJTP with\ngpt-3.5. NG: negation process. UF: Unification process.\na benchmark dataset for measuring the subjective thinking abilities of LLMs. Ablation study. We investigate the impact of negation process and unification process in our NeoN framework, as shown in Table 5. The full NeoN method, incorporating both steps, performs best on all indicators, highlighting the importance of both negation and unification. Removing negation process ( w/oNG) (i.e., generate nresponse randomly without explicit negation prompts) generally leads to a performance drop by around 2score, showing that it is crucial for constant negation to generate new insights. Removing unification process ( w/oUF) (i.e., take the last turn of response as the final answer) decreases the score by around 1, indicating that the analysis and synthesis of all contents benefit the refinement of response. Excluding both steps ( w/oNG&UF) leads to the worst performance. In summary, both the steps of negation and unification are important and the best performance is achieved when both of them are utilized. Rounds of Negation. We do not fix the number of negation round and the termination depends on when the current response with a high semantic similarity with the previous responses. For subjective reasoning, we evaluate the similarity by gpt-4 and set the termination threshold ϵ= 90 . For objective tasks that have definite answers, the similarity means whether the current answer is equal to the previous one. The average rounds of negation on different reasoning tasks are shown in Table 6, which are all around 2 ∼3, indicating that complement contents can be quickly induced by negation. View. Dis. Bi-Dial. Prac. Iss. Arith.\n#Rounds 2.87 3 .14 2 .68 1 .72\nComSens. Generic Symbol. A VG.\n#Rounds 2.72 2 .30 1 .46 2 .41\nTable 6: The average numbers or negation rounds of\nNeoN in different tasks.\nThen we specify the rounds of negation as 1∼5 to explore its effect on performance, the results are show in Figure 5a. We can see that as the rounds 0 2 4 6\nNegation Round7780828587SCRdim\nLlama-2-70B\nInstructGPT\nGPT-3.5-turbo\n0 2 4 6\nNegation Round5060708090SCRdim\narithmetic\ncommonsense\nsymbolic(a)\nViewpoint\n DiscourseBinary\n DialecticsPractical\n Issues\nLlama-2-70B456789Number of Viewpoints\nViewpoint\n DiscourseBinary\n DialecticsPractical\n Issues\nGPT-3.5-turbo456789Number of ViewpointsDirect Prompt 0-CoT Self-Consistency Relite&Answer NeoN(ours)\n(b)\nFigure 5: (a): Impact of the rounds of negation across\nmodels and reasoning types. (b): Comparison of the\nnumber of viewpoints in different methods.\nof negation increases, the performance of model will presenting an overall improvement, but when the turn increases qround 3, the improvement gradually slows down until stabilizing. This could be attributed to two primary factors: 1) LLM itself has passable basic reasoning ability, 2) our method only stimulates the potential performance that has not been released as much as possible by negation, rather than improving the essential reasoning ability. Therefore, when stimulated to a certain extent, the improvement will also reach a bottleneck. Case Study. To better demonstrate the SJTP dataset we constructed, the effectiveness of our NeoN method, and the evaluation flow, we conduct multiple study cases involving different topic types and topic fields. We can see the examples of data in SJTP from Table 11 to Table 13 in Appendix A.3. These cases clearly presented the data format and content in SJTP. For the responses of NeoN, we also present three cases including subjective reasoning, commonsense reasoning and arithmetic reasoning from Table 15 to Table 17 in Appendix B.2. The responses of each step show that NeoN can indeed explore new perspectives and thorough insights by negation, and can also rectify the wrong reasoning path via reflection. We also provide some evaluation examples from Table 21 to Table 22 in Appendix C.2, to present the details of AI evaluation and be more convinced. All these study cases demonstrate our rationality and effectiveness. Quality Comparison. To deeply analyze the 7\nAI Eval. HU Eval. Cohen’s κ\nSCR sol 96.2% 94 .7% 0.91\nSCR point 93.5% 96 .3% 0.87\nSCR dim 94.8% 95 .6% 0.94\nTable 7: Cross validation of AI evaluation indicators.\nimprovement of comprehensiveness by NeoN, we count the average number of perspectives involved in responses of different methods, as shown in Figure 5b. We can see that NeoN considers the most aspects, showing its comprehensiveness. Human Evaluation. To further validate the reliability of AI evaluation indicators, we compared whether AI and human choose NeoN as the best on 100 samples. The cohen’s κshown in Table 7 indicates a relatively high consistency between our evaluators and human evaluation, validating the rationality of our evaluation indicators. 5 Related Work 5.1 Chain of thought Style Prompting CoT prompting (Wei et al., 2022c) and its variants (Kojima et al., 2022; Zhang et al., 2023; Sun et al., 2023b) are widely used in augmenting the reasoning abilities of LLMs. These methods attempt to guide the models to think in a step-by-step manner by introducing reasoning path, which assists the model in logical thinking and has been shown effective for various objective tasks. Selfconsistency (Wang et al., 2022) extend the singlereasoning path to multi-path by sampling multiple solutions with cot prompt independently and the take the majority as the final solution. Another line of work proposes reasoning leveraging relevant passages (Sun et al., 2023b) or expert modeling (Xu et al., 2023). However, we show in experiments that these methods have limited performance in subjective tasks since they rely more on the logical thinking. Different from them, this paper aims to explore and further unleash the comprehensive, reflective and creative thinking abilities of LLMs. 5.2 Subjective Tasks Compared with objective tasks that have a clear solution or evaluation criteria that reflect logical thinking, subjective tasks have open and non-standard answers that can test more comprehensive thinking abilities (Kanclerz et al., 2023; Sun et al., 2023a). Traditional NLP studies have explored some subjective tasks including linguistic rhetoric, disambiguation, stance detection, etc. (Jentzsch and Ker- 507sting, 2023; Mao et al., 2023). Nevertheless, these tasks not only have been fewly explored in LLM researches, but also involve interpretation, judgment, and personal experiences (Rottger et al., 2022; Sun et al., 2023a) which emphasize on the ability to perceive context, language nuances, and emotions. Instead, the proposed SJTP consists of subjective topics across various types and fields, which can better evaluate the comprehensive, reflective and creative thinking abilities. Besides, the unsatisfactory performance of LLMs on SJTP suggests the challenge faced in subjective tasks and the significance of releasing the relevant thinking abilities. 5.3 Debate-based Reasoning There have been recent studies concern the debate between LLMs. A line of work aim to explore the limitations of LLMs in simulating human interactions(Taubenfeld et al., 2024), the interconsistency among multiple LLMs for collaboration(Wang et al., 2023), or the ability to resist misleading erroneous arguments and defend the truth (Xiong et al., 2023) by simulating debates. Furthermore, multi-agent debate framework is introduce for text evaluation (Chan et al., 2023) and mathematical and strategic reasoning(Liang et al., 2023; Du et al., 2023). In contrast, we aim to unleash the comprehensive, reflective and creative thinking abilities of LLM by conducting negation process on only one agent, which can be applied in both objective and subjective topics. Note that our framework needs no restriction on data types and preset debate positions, possessing more convenient and flexibility. 6 Conclusion We introduce SJTP , a subjective topic benchmark to explore the comprehensive, reflective, and creative thinking abilities of LLMs. We empirically observed that current methods that only focus on logical thinking fall short in solving subjective tasks. To alleviate this gap, we introduce NeoN , a framework based on the principle of negation of negation, allowing models to constantly improve and surpass previous responses. Experiments on both subjective and objective tasks across models show NeoN leads to significant and consistent improvement, indicating the impact of negation and the significance for unleashing the subjective thinking abilities, and shedding light on new directions for enhancing overall reasoning abilities of LLMs. 8\nLimitation Our method focuses on enhancing the comprehensive thinking, reflective thinking, and creative thinking abilities of LLMs, in complementary to logical reasoning. We implement our method NeoN on top of prompting, which enjoys efficiency for the training-free property. However, this leads to dependency on the pre-training (indicates all the training steps of LLMs not just the pertaining stage) of LLMs. If an LLM is not properly trained, it might inherently fall short in comprehensive thinking, reflective thinking, and creative thinking abilities, which also weaken the effectiveness of our method. In addition, the curated dataset could be expanded for better evaluation. Social Impact Large language models have a strong capacity to answer questions. Previous works enhance the logical reasoning of LLMs and improve the performance of objective questions, while our method enhances the LLMs’ performance of subjective questions. There is a potential risk that students leverage our methods to do their homework which impedes their regular learning of coursework. To remedy this, we plan to add a watermark to our method which prevent this kind of improper usage in future work. 582",
        "aspect_review": "The paper introduces a novel framework, NeoN, based on the philosophical principle of the negation of negation, which is groundbreaking in enhancing the comprehensive, reflective, and creative thinking abilities of LLMs. The study conducts comprehensive experiments on both subjective and objective datasets, demonstrating the effectiveness of the NeoN framework across multiple models and reasoning types. The paper provides a robust evaluation methodology with three distinct indicators that assess the quality of responses, ensuring thorough validation of the proposed approach. Significant improvements are observed in both subjective and objective reasoning tasks, showcasing the transformative impact of unleashing subjective thinking abilities in LLMs. The introduction of the SJTP dataset, which spans diverse subjective topics and fields, offers a valuable resource for evaluating comprehensive reasoning abilities of LLMs. The structured framework of NeoN, involving iterative negation and integrative unification, is methodologically sound and well-executed, providing a clear path for enhancing LLM reasoning capabilities.\n\nThe dependency on pre-trained models may limit the effectiveness of the NeoN framework if the underlying LLMs are not well-trained in comprehensive thinking abilities. While the study compares NeoN to several baselines, additional comparisons with more recent state-of-the-art methods could further strengthen the findings. The improvements in subjective reasoning tasks, although significant, may require further exploration to understand the underlying mechanisms and potential limitations of the NeoN approach.",
        "general_review": "The paper presents a novel framework, NeoN, grounded in the principle of the negation of negation, which significantly enhances the comprehensive, reflective, and creative thinking abilities of large language models (LLMs). The introduction of the SJTP dataset is a notable contribution, offering a benchmark for evaluating subjective reasoning capabilities across diverse fields and topics. The paper demonstrates strong technical depth and rigor through comprehensive experimental validation on both subjective and objective tasks, showcasing the practical impact of NeoN in improving reasoning abilities across various models. The results consistently show that NeoN outperforms existing methods, underscoring its effectiveness and potential for broader application.\n\n1. The paper could benefit from minor improvements in writing clarity, particularly in the explanation of the NeoN framework and its stages, to aid reader comprehension. 2. Details on hyperparameters and implementation specifics are somewhat limited, which could hinder reproducibility. 3. Some figures could be formatted more clearly to enhance readability and interpretation. 4. The availability of code and data is not explicitly addressed, which could be clarified to facilitate further research and application."
    },
    "48": {
        "paper": "Breaking the Boundaries: A Unified Framework for Chinese Named Entity\nRecognition Across Text and Speech\nAnonymous ACL submission\nAbstract In recent years, with the vast and rapidly increasing amounts of spoken and textual data, Named Entity Recognition (NER) tasks have evolved into three distinct categories, i.e., textbased NER (TNER), Speech NER (SNER) and Multimodal NER (MNER). However, existing approaches typically require designing separate models for each task, overlooking the potential connections between tasks and limiting the versatility of NER methods. To mitigate these limitations, we introduce a new task named Integrated Multimodal NER (IMNER) to break the boundaries between different modal NER tasks, enabling a unified implementation of them. To achieve this, we first design a unified data format for inputs from different modalities. Then, leveraging the pre-trained MMSpeech model as the backbone, we propose an Integrated Multimod alGeneration Framework (IMAGE ), formulating the Chinese IMNER task as an entity-aware text generation task. Experimental results demonstrate the feasibility of our proposed IMAGE framework in the IMNER task. Our work in integrated multimodal learning in advancing the performance of NER may set up a new direction for future research in the field. 1 Introduction Named Entity Recognition (NER) (Li et al., 2020a) is a fundamental and significant task in the field of Natural Language Processing and has been extensively studied to address the challenges posed by real-world text data. Chinese NER (CNER) (Liu et al., 2022), a significant subdomain of NER, specifically deals with challenges unique to Chinese, such as no clear word boundaries and the ambiguity from homophones and polyphones, drawing significant academic focus (Zhang and Yang, 2018; Li et al., 2020b; Ma et al., 2020). Traditionally, Named Entity Recognition (NER) tasks have concentrated on text-based NER TNER Model\nSNER Model\nMNER Model\nTNER \nResults\nSNER \nResults\nMNER \nResultsText Input\nSpeech Input\nText Input Speech InputTNER \nTask\nSNER \nTask\nMNER \nTask\nText Input Speech InputSpeech\nInputText\nInput\nIMNER ModelTNER \nResults\nSNER \nResults\nMNER \nResults(a) Existing NER Tasks\n(b) IMNER Task\nPseudo\nSpeech\nPseudo\nText\nFigure 1: Comparison of existing NER tasks with the\nIntegrated Multimodal NER (IMNER) task proposed in\nthis paper. In the figure, TNER represents text-based\nNER, SNER stands for Speech NER, MNER denotes\nmultimodal NER, Pseudo Speech refers to meaningless\nzero audio waveforms, and Pseudo Text indicates non-\nsensical text sequences.\n(TNER) (Gui et al., 2019b; Li et al., 2020b, 2022). However, as the volume of audio data increases, there has been a growing interest in Speech NER (SNER) (Yadav et al., 2020; Chen et al., 2022; Shon et al., 2022), which focuses on extracting named entities from speech, and Multimodal NER (MNER) (Sui et al., 2021; Liu et al., 2023), which involves extracting entities from both speech and text. Currently, data on the internet often appears in multiple modalities, such as user-generated content in social media and news reports in the media, which may be in text or audio formats, or a combination of both speech and its corresponding text. The key information extracted by NER tasks, such as persons and locations, can assist in search and recommendation in the news domain, as well as in analyzing trending topics and public opinion in social media. However, existing NER systems are usually designed for a single mode, either solely 1\nas SNER, TNER or MNER, as shown in Figure 1. These approaches face two significant issues. Issue 1: Treating SNER, TNER and MNER as three separate tasks overlooks the potential interconnections between them. Issue 2 : The need to design distinct models for each of the SNER, TNER and MNER tasks limits the versatility and overall efficiency of NER methods. Beyond the above issues, significant advances in speech processing have been achieved with Multimodal Pre-trained Models (MPMs) using data from both text and speech (Zhou et al., 2022; Ao et al., 2022). These models, trained on various tasks like speech-to-text and text-to-text generation, highlight the potential interconnections between different modal tasks. However, these MPMs face a challenge, identified here as Issue 3 : While MPMs benefit from a unified pre-training architecture across modalities, the need for task-specific fine-tuning for various downstream applications, to some extent,restricts their universality. To address these challenges, in this paper, we introduce a new NER task named Integrated Multimodal NER (IMNER ). The IMNER task aims to break the boundaries of traditional SNER, TNER and MNER by presenting a unified Named Entity Recognition (NER) framework capable of handling inputs from various modalities (text, speech, or both) to efficiently recognize Chinese named entities, as illustrated in Figure 1. Moreover, previous studies (Sui et al., 2021; Liu et al., 2023) have shown that features like pauses in speech signals can reduce ambiguities in Chinese NER tasks, which often arise from the lack of clear word delimitation or the presence of homophones. The IMNER approach, leveraging data from the SNER, TNER, and MNER tasks, possesses the potential to overcome the difficulties associated with the absence of natural word segmentation and the frequent occurrence of homophones in Chinese text. To solve the IMNER task, our approach begins with an original design of a data format unification method that transforms the data formats of TNER, SNER and MNER tasks into a unified data scheme. As illustrated in Figure 1, we treat TNER and SNER tasks as MNER tasks with missing speech and text modalities, respectively. For these “missing” modalities, we substitute Pseudo Speech and Pseudo Text. Based on the unified data format, and using the multimodal pretrained model MMSpeech (Zhou et al., 2022) as backbone, we introduce the Integrated Multimod alGeneration 113Framework ( IMAGE ), an encoder-decoder structure to execute the Chinese IMNER task. Specifically, inspired by the recent success of generative methods in NER tasks, we formulate the IMNER task as an entity-aware text generation task (Chen et al., 2022; Wang et al., 2023). Unlike previous works, our approach uniquely leverages the interrelations among the three different modalities of TNER, SNER and MNER tasks, facilitating the realization of the IMNER task. The main contributions of this work can be summarized as follows: •We introduce a new task, Integrated Multimodal NER (IMNER), aimed at breaking the boundaries between TNER, SNER and MNER tasks, enabling the model to uniformly handle inputs from various modalities. •From a novel perspective, we design a unified data format for TNER, SNER and MNER, establishing a bridge between these three tasks and serving as the basis for IMNER. •Utilizing the MMSpeech model as the backbone, we propose an Integrated Multimodal Generation Framework (IMAGE), formulating the Chinese IMNER task as an entityaware text generation task. Notably, our IMAGE framework is capable of handling both flat and nested entity scenarios. •Experimental results reveal that the IMAGE framework effectively exploits potential connections among TNER, SNER and MNER tasks, boosting their performance. IMAGE achieves competitive performance in these tasks, proving the viability of the IMNER task and incidenting the advantages of the IMAGE framework. 2 Related Work 2.1 Text-based Chinese NER (TNER) In Chinese NER, the lack of natural word boundaries and the existence of homophones introduce ambiguity in the text, posing challenges for Chinese NER. Therefore, in recent years, incorporating external lexicon resources to enhance Chinese NER performance has been proven to be an effective solution and has achieved significant success (Zhang and Yang, 2018; Gui et al., 2019a,b; Li et al., 2020b; Liu et al., 2021). Additionally, for the extraction of nested entities, recent work utilizing a 2\nunified NER framework (Li et al., 2022; Yan et al., 2021) to extract both flat and nested entities has shown promising results. 2.2 Speech NER (SNER) Speech NER (SNER), which is essential for Spoken Language Understanding (SLU) (Caubrière et al., 2020; Shon et al., 2022), initially adopts a two-stage pipeline approach (Cohn et al., 2019): converting speech to text with Automatic Speech Recognition (ASR) and then tagging named entities in the generated text. To overcome the error accumulation inherent in this approach, End-to-End (E2E) methods for languages like French (Ghannay et al., 2018), English (Yadav et al., 2020), and Chinese (Chen et al., 2022) have emerged, which incorporate entity-aware ASR, directly integrating entity tagging into the ASR decoding process. 2.3 Multimodal NER (MNER) With the rapid growth of multimodal data on the internet, leveraging multimodal information to enhance the performance of NER systems has attracted increasing academic attention. In the field of English NER, existing work has primarily focused on using data from text and image modalities to improve the performance of NER systems in social media contexts (Sun et al., 2021; Chen et al., 2021; Xu et al., 2022; Jia et al., 2023). Similarly, in the field of Chinese NER, Multimodal NER (MNER) that combines text with audio signals (Sui et al., 2021; Liu et al., 2023) has been introduced and achieved significant success. 2.4 MPMs Based on Text and Speech Recently, Multimodal Pretrained Models (MPMs) have received widespread attention in the field of speech processing. In English, models such as SpeechT5 (Ao et al., 2022) and STPT (Tang et al., 2022), which propose encoder-decoder pre-training using unlabeled text and speech data, have achieved significant success. Following this, in the Chinese Automatic Speech Recognition (ASR), MMSpeech (Zhou et al., 2022) makes great improvements through a multi-modal multi-task encoderdecoder pre-training framework. It is important to note that, in our work, although MMSpeech serves as the backbone, our model, named IMAGE, distinguishes itself from the aforementioned MPMs by overcoming the need for individual fine-tuning across different downstream tasks. Furthermore, while recent works like 210SpeechGPT (Zhang et al., 2023) and Qwen-Audio (Chu et al., 2023) have demonstrated the capability to handle both speech and text inputs in conversational tasks, to our knowledge, our work is the first attempt to explore integrated modality input capability within an NER system. 3 Methodology In this section, we first introduce the IMNER task definition. Then we detail the implementation of IMAGE, including its overal structure as depicted in Figure 2. 3.1 Task Definition Given the input X, which may be text {xtext}, speech {xspeech}, or a combination of both {xtext,xspeech}, the goal of IMNER is to find each entity in Xand then assign a label y∈Y, where Y is a predefined label types (e.g., PER, LOC, etc.). 3.2 Formulating IMNER into Text Generation Inspired by the success of generative methods in TNER (Wang et al., 2023) and SNER (Chen et al., 2022), we formulating the IMNER task as an entityaware text generation task. Illustrated by the sample in Figure 2, for the text “ 末阳市文联主席 张三” and its associated speech waveform, the Entity-aware Text Generation Target is designated as “<(末阳市)文联>主席[张三]”. Special tokens are incorporated into the vocabulary to annotate entities in the generated text, specifically, “[ ]” for PER, “( )” for LOC, and “< >” for ORG. We chose the entity-aware text generation task for generating entities primarily because this method allows for the simultaneous acquisition of entity span information and entity text content. 3.3 Details of the IMAGE Framework 3.3.1 Backbone Model In this paper, we employ a multimodal pre-trained model with an encoder-decoder structure, MMSpeech (Zhou et al., 2022), as our backbone model. The original MMSpeech structure primarily consists of: (1) a multi-layer Transformer-based MMSpeech encoder shared by text and speech modalities, equipped with a multi-layer convolutional and Transformer-based speech feature extractor, and a static word vector embedding for text feature extraction; (2) a decoder composed of multiple Transformer layers. Currently, MMSpeech is mainly used for downstream tasks with speech input, such as speech 3\nSpeech\nFeature\nExtractorText\nFeature\nExtractorSpeech FeatureText FeatureModality Absence \nFeatureMMSpeech Encoder\nConcat\nDecoder layers×NCross Multi-head \nAttention\nQK&V\n<S><(末阳市)文联>主席[张三<(末 张三?\nMMSpeech Decoder\n]\n丰\n强\n王\nOutputs (shifted righted)\nInput Speech Waveform Input Phoneme Input Char\n‘m o4 y ang2 sh i4 w en2 l i \nan2 zh u3 x i2 zh ang1 s an1’ ‘末 末 阳 阳 市 市 文 文 联 联 \n联 主 主 习 习 张 张 三 三’ \n‘$ $ $ $ $ $ $ $’ ‘# # # # # # # #’ \nPseudo SpeechValid Speech Waveform Valid Character Sequence\nPseudo Character SequenceValid Phoneme Sequence\nPseudo Phoneme SequenceRaw Text\n‘末阳市文联\n主席张三’ Phonemize\nOr OrMNER\nSNER\nTNER Entity Label‘<(末阳市)文联>主席[张三]’ Entity-aware Text Generation Target \n‘末阳市’--LOC\n‘末阳市文联’--ORG\n‘张三’ --PERWaveform\nFigure 2: Overall structure of IMAGE. The figure illustrates with the example of the text “ 末阳市文联主席张\n三”(Chairman Zhang San of the Moyang City Cultural Association) and its corresponding speech waveform. In the\nfigure, “<S>” denotes a special token indicating the start of the generated output, while “#” and “$” respectively\nrepresent meaningless phoneme tokens and Chinese character tokens. The purple, green, and blue arrows at the\nbottom right of the figure explain the composition of input data for the MNER, SNER, and TNER tasks within the\nIMAGE framework, respectively.\nrecognition. Besides speech data, MMSpeech was trained with a large volume (292GB) of text data, giving it a strong ability to model text. However, the capability of MMSpeech to handle text input tasks or dual input tasks with speech and text has been overlooked and not fully explored. This indicates the significant potential for expanding MMSpeech’s application across various modal scenarios. 3.3.2 Unified Integrated Modal Data Format The IMNER task comprises three sub-tasks: SNER, TNER and MNER, each involving different modal components in the input data. To transform the integrated modal inputs of IMNER task into a unified data format, maintaining data consistency and laying the groundwork for handling all three tasks with a uniform model structure, we adopt a novel perspective within the IMAGE framework. Here, we treat TNER and SNER tasks as MNER tasks with “missing” speech and text modalities, respec- 278tively. For these “missing” modalities, we substitute Pseudo Speech and Pseudo Text as illustrated in Figure 2. In the Unified Integrated Modal Data Format, the Input Speech Waveform is denoted as Xs= {xs\n1,···, xs\nNs}, where Nsrepresents the length of the speech waveform. When the speech modality is missing from the input, Xsrepresents a fixedlength sequence of all-zero signals. Because Chinese characters and their corresponding sounds are not tightly mapped to one another, the encoder in MMSpeech converts the original text inputXt=\b\nxt\n1,···, xt\nNt\t\ninto phoneme input P={p1,···, pNp}, where NtandNpdenote the sequence lengths of XtandP, respectively. However, as mentioned in Section 3.2, our entity generation scheme requires to restore all raw input text while generating special tokens for annotated entities in decoder. Through practice, we observed that with lots of homophones in Chinese, the decoder of original MMSpeech might not always ac4\ncurately restore the input text solely based on the input phoneme sequence sometimes. For example, the MMSpeech Decoder might restore the input “末阳市(Mo Yang Shi)” as “ 末杨市(Mo Yang Shi)” or “西安站(Xi An Zhan)” as “ 鲜站(Xian Zhan)”, leading to incorrect entity recognition. To address this issue, we introduce a sequence of Chinese characters C={c1,···, cNp}(where ciis the Chinese character corresponding to the phoneme pi) as auxiliary features to assist the model in accurately restoring the input text at the output end of MMSpeech Decoder. Specifically, for Pseudo Text, we choose to represent PandCusing semantically meaningless strings, such as \"########\" for Pand \"$$$$$$$$\" for C, as shown in Figure 2. 3.3.3 Feature Extractors Speech Feature Extractor : Consistent with MMSpeech, we first convert the raw speech waveform into Mel-filterbank features, and then use a multilayer convolutional network followed by a Transformer encoder (comprising multiple transformer layers with multihead self-attention (Vaswani et al., 2017)) as the speech feature extractor. The speech features Fsfor the input speech Xsare computed as follows: Fs=SFE (Xs) (1) where SFE (·)denotes the Speech Feature Extractor, Fs∈RLFs×dh,LFsis the length of the speech features, and dhis the dimension of the hidden features (consistent with all dhin this paper). Text Feature Extractor : Consistent with the pre-training phase of MMSpeech, we utilize static embeddings in our model to obtain the feature representation of the input phoneme sequence P: Fp=E(p)(P) (2) where E(p)(·)denotes the operation of static phoneme embedding, and Fp∈RNp×dh. Differing from the original MMSpeech, as described in Section 3.3.2, we introduce a character feature sequence Cas an auxiliary feature to enhance the model’s ability to perceive Chinese characters and improve its capability to accurately restore input Chinese characters at the decoding output. We also use static embeddings to obtain the feature representation of C: Fc=E(c)(C) (3) where E(c)(·)denotes the operation of static character embedding, and Fc∈RNp×dh. 347We then add FpandFctogether to derive the final text feature Ft: Ft=Fc+Fp(4) Modality Absence Feature : To enhance the model’s ability to detect the absence of a modality, thereby encouraging it to focus on inputs from present modalities and ignore inputs from missing ones, we introduce a learned embedding to every ft\ni(1⩽i⩽Np)andfs\ni(1⩽i⩽Ns)to incident whether that modality is missing: ¯ft\ni=(\nft\ni+mmissing ,if text is missing\nft\ni+mpresent,otherwise(5) ¯fs\ni=(\nfs\ni+mmissing ,if speech is missing\nfs\ni+mpresent,otherwise(6) where mmissing ∈Rdhandmpresent∈Rdhare the learned embeddings indicating the absence or presence of the modality, respectively. This approach allows the model to dynamically adapt its processing based on the availability of each modality. Ultimately, we obtain the final speech feature representation ¯Fs=\b¯fs\n1,···,¯fs\nNs\t\nand the final text feature representation ¯Ft=\b¯ft\n1,···,¯ft\nNp\t\n. 3.3.4 Encoder and Decoder of MMSpeech For¯Fsand¯Ft, we combine them through a concatenation operation to form the feature representation ¯Fthat is fed into the MMSpeech encoderdecoder structure: ¯F=¯Ft⊕¯Fs(7) where ¯F∈R(Np+Ns)×dh, and⊕denotes the concatenation operation. Encoder : IMAGE feeds the concatenated text and speech features representation ¯Finto the MMSpeech encoder, which is a multi-layer Transformer encoder, to obtain the hidden representation of the integrated modal input as follows: He=Encoder\u0000¯F\u0001\n(8) Decoder : Afterwards, Heis fed into the MMSpeech decoder, a multi-layer Transformer decoder, to model the probability distribution of the output texty. At the i-th step of decoding, the probability distribution p(yi)∈R|V|of the i-th output token yiinyis computed as follows: hyi=Decoder (He, y<i) (9) p(yi) =Softmax (Wlmhyi+blm) (10) 5\nDataset Data TypeEntity Type\nNumSentence Entity Type\ntrain dev test total PER. ORG. LOC. total\nCNERTA Text&Audio 3 34,102 4,440 4,445 42,987 8,034 12,047 16,876 36,AISHELL-NER Text&Audio 3 120,098 14,326 7,176 141,600 18,642 25,351 24,611 68,MSRA Text-only 3 46,539 - 4,380 73,321 18,565 21,804 32,952 73,Table 1: Statistics of the Datasets.\nwhere hyiis the hidden representation at the i-th decoding step, Wlm∈R|V|×dhandblm∈R|V|are learnable parameters in the language model (LM) head, and |V|represents the size of the vocabulary. 3.3.5 Training Strategy of IMAGE Loss Function : During the training phase, the parameters of IMAGE are optimized by minimizing the cross-entropy loss based on teacher forcing: L=−1\nMMX\ni=1|V|X\nk=1li,klogp(yi,k) (11) where li∈R|V|represents the ground-truth label distribution for decoding the i-th token, and M denotes the total number of tokens in the groundtruth label sentence. Training Data Creation : In this study, we utilize MNER datasets, containing both speech and text, to create training data for TNER and SNER by artificially removing certain modality data. During training, each sample in a batch is randomly assigned as an input for MNER, SNER or TNER. These input variations are depicted in Figure 2, illustrating the method for handling inputs with varying modality presence. 4 Experiments 4.1 Dataset & Evaluation Metrics In this study, we train and evaluate our IMAGE framework on the IMNER task using three datasets: the flat SNER dataset AISHELL-NER (Chen et al., 2022) , the nested MNER dataset CNERTA (Sui et al., 2021), and the flat TNER dataset MSRA (Levow, 2006). Both the AISHELL-NER and CNERTA datasets contain Chinese text with corresponding speech, where the Chinese text is annotated with entity information, while the MSRA dataset solely contains Chinese text with entity annotations. Detailed statistics of these datasets are available in Table 1 in in Appendix. Regarding evaluation metrics, we use the F1 score (F1), commonly employed in NER tasks, to assess the model’s effectiveness. For further implementation details, see Appendix Section A . 4294.2 Comparison Models We compare the performance of several strong baseline models on the TNER, SNER and MNER tasks using the benchmark datasets employed in this paper. The baseline models used fall into three main categories: (1) methods that only use the text modality (Text-only Methods), (2) methods that only use the speech modality (Speech-only Methods), and (3) multimodal methods that use both text and speech (Multimodal Methods). Due to page limitations, detailed introductions of each baseline model can be found in Appendix Section B . 4.3 Results and Analysis 4.3.1 Main Results We compare our proposed IMAGE model with several strong Text-only Baselines, Speech-only Baselines, and Multimodal Baselines, with the experimental results reported in Table 2. It is evident that, unlike existing baseline models that are limited to solving single-modality tasks, IMAGE not only breaks the boundaries between modalities by simultaneously addressing TNER, SNER and MNER tasks but also achieves highly competitive performance across these tasks. From the experimental results, we can further observe that: (1) Compared to baselines using MMSpeech trained on single task data (methods 6, 7, 9, 10, 16, 17 in Table 2), our proposed IMAGE method achieves significant performance improvements on TNER, SNER and MNER tasks. This demonstrates that our IMAGE method can effectively leverage the potential correlations among the three tasks across different input modalities, facilitating complementary benefits and jointly enhancing performance across all tasks. (2) On the SNER and MNER tasks, the performance of IMAGE with the MMSpeech-large backbone surpasses all baseline methods. Furthermore, IMAGE with the MMSpeech-base backbone, despite having only about 213M parameters, still remains competitive. This indicates that our proposed IMAGE framework can exploit the comple6\nModality MethodsCNERTA(nested) AISHELL-NER\nTNER SNER MNER TNER SNER MNER\nText-only\nMethods1.Bert-large-CRF(325M) 76.09ℏ- - 93.29ℏ- -\n2.FLAT(Bert-large) 79.31‡ℏ- - 93.57‡ℏ- -\n3.W2NER(Bert-large) 79.25‡- - 93.72‡- -\n4.Bart-large-ETG(407M) 76.84 - - 92.82 - -\n5.MT5-base-ETG(582M) 76.91 - - 92.94 - -\n6.MMSpeech-base-ETG(213M) 75.93 - - 90.23 - -\n7.MMSpeech-large-ETG(613M) 76.90 - - 92.81 - -\nSpeech-only\nMethods8.Conformer-ETG(E2E)(Chen et al., 2022) - 60.36‡- - 73.37℘-\n9.MMSpeech-base-ETG(E2E) - 67.21 - - 74.28 -\n10.MMSpeech-large-ETG(E2E) - 70.35 - - 75.42 -\n11.Conformer-ASR + Bert-large(Pipeline) - 60.92 - - 74.10 -\n12.MMSpeech-base-ASR + Bert-large(Pipeline) - 66.87 - - 73.14 -\n13.MMSpeech-large-ASR + Bert-large(Pipeline) - 69.76 - - 74.84 -\nMultimodal\nMethods14.Bert-USAF(Bert-base)(Liu et al., 2023) - - 76.73℘- - -\n15.Bert-M3T(Bert-large) - - 79.51‡ℏ- - 93.75‡ℏ\n16.IMAGE(MMSpeech-base,only MNER data) - - 76.68 - - 90.84\n17.IMAGE(MMSpeech-large,only MNER data) - - 80.79 - - 93.69\nIMNER\nMethods18.IMAGE(MMSpeech-base,213M) 76.61 68.27 76.96 90.95 75.35 91.10\n19.IMAGE(MMSpeech-large,613M) 80.83 71.17 81.10 93.49 76.33 93.83\nTable 2: F1-score (%) of the proposed IMAGE method and baselines on the TNER, SNER, MNER versions of\nthe test sets for two benchmark datasets. Here, “ETG” refers to models perform NER task using an entity-aware\ntext generation task. “(E2E)” and “(Pipeline)” respectively denote the end-to-end SNER methods and pipeline\nSNER methods. Superscript ‡indicates results obtained through official implementation. Superscript ℘denotes\nexperimental results reported from the original paper. Superscript ℏsignifies the use of the same Nested Structure\nLinearization method for annotating nested entities as in the M3T(Sui et al., 2021) work.\nmentarity between tasks across different modalities, enhancing modeling capabilities for both speechonly and speech-text multimodal tasks. (3) On the TNER task, our IMAGE method with MMSpeech-large backbone achieves performance comparable to the SOTA method W2NER. Moreover, the performance of the IMAGE framework exceeds all baselines based on an encoder-decoder structure using an entity-aware text generation task for entity annotation. This suggests that the IMAGE framework can improve text information encoding capabilities through joint training and modeling of tasks across different input modalities. (4) The performance of the IMAGE method on the MNER task surpasses its performance on the TNER task, indicating that multimodal inputs combining text and speech provide more effective information than text-only data, thus enhancing model performance on NER tasks. Additionally, the SNER task not only requires entity annotation but also the accurate transcription of speech to text, increasing the complexity of the task. Therefore, the performance of the IMAGE framework on the SNER task is notably lower than on the TNER task. 4964.3.2 Ablation Study To assess the impact of various components in IMAGE, we conducted ablation experiments on the CNERTA dataset, with the findings presented in Table 3. Our conclusions are as follows: (1) Removing the Character Feature Fcfrom IMAGE results in a performance decline, particularly in the TNER and MNER tasks. This indicates that the original MMSpeech model, which relies solely on phoneme input for the text modality, might introduce errors in the decoding of Chinese characters. Incorporating the Character Feature improves the model’s ability to interpret the input Chinese characters, thus alleviating this problem. (2) The removal of the Modality Absence Feature leads to reduced model performance, highlighting its role in enhancing IMAGE’s ability to discern valid modal information in the input. Additionally, eliminating either Pseudo Text Input or Pseudo Speech Input diminishes the model’s performance. These components are believed to capture global information across different modalities, fostering synergy among the tasks within IMAGE. (3) Excluding the training data for any one of the TNER, SNER, and MNER tasks during the training process results in a decline in the model’s performance across all three NER subtasks. This suggests 7\n6364656667686971000 2000 3000 4000 5000F1 score(%)\nNumber of Training Samples\nBert-large-crf W2NER(Bert-large) IMAGE(MMSpeech-base)63646566676861000 2000 3000 4000 5000F1 score(%)\nNumber of Training Samples\nBert-large-M3T IMAGE(MMSpeech-base)525456586061000 2000 3000 4000 5000F1 score(%)\nNumber of Training Samples\nMMspeech-base-ETG IMAGE(MMSpeech-base)\n(a) Results on the TNER Task (b) Results on the MNER Task (c) Results on the SNER TaskFigure 3: F1 Scores (%) of IMAGE (MMSpeech-base) on the three subtasks of the IMNER task in the CNERTA\ndataset with varying numbers of training samples. For each method depicted above, under different training data\nsizes, we chose identical hyperparameters and ran the experiments five times with different random seeds, averaging\nthe F1 scores to obtain the final results.\nTNER SNER MNER\nIMAGE 80.83 71.17 81.10\nw/o CF 78.51 71.10 79.04\nw/o MAF 80.12 70.61 80.57\nw/o PT 80.62 70.69 80.98\nw/o PS 80.25 71.02 80.89\nw/o PT&PS 80.21 70.59 80.81\nw/o TNER 40.35 70.35 80.86\nw/o SNER 80.67 36.27 80.91\nw/o MNER 80.59 70.21 42.21\nTable 3: An ablation study of the IMAGE (MMSpeech-\nlarge). F1 scores (%) were evaluated on the test sets of\nthree different tasks in CNERTA. “CF” stands for Char-\nacter Feature Fc. “MAF” represents Modality Absence\nFeature. “PT” and “PS” respectively denote Pseudo\nText Input and Pseudo Speech Input. The feature vec-\ntors corresponding to “PT” and “PS” are masked out in\nthe Transformer through the attention mask to negate\ntheir influence. The acronyms “TNER”, “SNER”, and\n“MNER” specifically refer to the training data for the\nrespective tasks.\nthat there is an intrinsic interconnection among the TNER, SNER, and MNER tasks, allowing them to mutually benefit from each other. This finding supports our IMNER approach, addressing Issue 1raised in the introduction of this paper, which concerns the overlooked potential interconnections between these tasks. 4.3.3 Performance on Low-resource Scenarios We conducted experiments with training data sets of 1000, 2000, 3000, 4000, and 5000 samples to evaluate the performance of IMAGE in lowresource scenarios, with results shown in Figure 3. The experimental results reveal that, with lim- 536ited training data resources, IMAGE maintains an advantage compared to baselines trained on singletask data. This demonstrates that within the lowresource context, the IMAGE framework can still effectively leverage the potential connections and complementarity among the three IMNER subtasks (i.e., TNER, SNER, MNER) to enhance the performance across these tasks. Notably, on the TNER task, when the training data ranges from 1000 to 4000 samples, the performance of the IMAGE method using MMSpeech-base (213M) surpasses that of the baseline method using a larger backbone model, Bert-large (325M). This underscores the potential of the IMAGE method in scenarios with limited training resources. Due to page limitations, additional experimental results and analysis have been included in the Appendix Section. 5 Conclusions In our study, we introduce the Integrated Multimodal NER (IMNER) task, bridging the gap between text-based NER, speech NER, and multimodal NER to enable a unified approach to these three distinct tasks. By designing a novel unified data format and leveraging the pre-trained MMSpeech as backbone, we introduced the IMAGE framework, transforming the Chinese IMNER task into an entity-aware text generation task. Experimental results reveal the effectiveness of IMAGE, marking a significant step forward in integrated multimodal learning for NER, which may shed light on future research in this research domain. 8\nLimitations In this section, we discuss two limitations of the IMAGE framework as follows: (1) Language Limitation: Currently, the IMAGE framework is designed to address the Chinese IMNER task exclusively. This restriction arises because the MMSpeech backbone, on which IMAGE relies, exhibits robust and balanced representation capabilities in both text and speech modalities only in Chinese. In contrast, English lacks multimodal pre-training models that perform equally well across both modalities. The available models, such as SpeechT5 (Ao et al., 2022) and STPT (Tang et al., 2022), have been pre-trained on limited text corpora, resulting in weaker text representation capabilities. Therefore, there is an urgent need to develop multimodal pre-trained models using extensive text and speech data in other languages, such as English, to support IMNER tasks in those languages. (2) Task Limitation: At present, the IMAGE framework has only been applied to the Chinese Integrated Multimodal Named Entity Recognition (IMNER) task. Future work will involve extending the IMAGE framework to other integrated multimodal information extraction tasks. This expansion aims to fully exploit the complementary nature of different modality tasks, enhancing the overall performance and applicability of the framework. 597",
        "aspect_review": "The paper introduces a novel task, Integrated Multimodal NER (IMNER), and a unified framework, IMAGE, which breaks the boundaries between text-based, speech, and multimodal NER tasks, showcasing notable novelty and strong technical depth. The authors design a unified data format that effectively integrates text and speech modalities, enabling the model to handle various inputs efficiently, which is a significant practical impact. The experimental results demonstrate that the IMAGE framework achieves competitive performance across TNER, SNER, and MNER tasks, validating its comprehensive experimental validation and transformative potential.\n\nSome minor details on hyperparameters and implementation are missing, which could slightly hinder reproducibility. The evaluation could benefit from additional baselines for a more comprehensive comparison, although the current results are strong. While the results are impressive, a more detailed analysis of the results could provide deeper insights. The ablation study is informative but could be expanded to explore more components. The comparison with state-of-the-art models is good but could include more recent models. Minor writing improvements could enhance clarity, particularly in the methodology section. The methodology section could benefit from more detailed explanations of certain components, although it is generally well-structured.",
        "general_review": "The paper presents a novel and groundbreaking approach to Named Entity Recognition (NER) by introducing the Integrated Multimodal NER (IMNER) task, which unifies text-based, speech, and multimodal NER into a single framework. This innovative method leverages a unified data format and the MMSpeech model, demonstrating strong technical depth and rigor. The experimental validation is comprehensive, showcasing competitive performance across TNER, SNER, and MNER tasks. The approach's significant practical impact lies in its ability to handle multimodal inputs efficiently, setting a new direction for future research in integrated multimodal learning.\n\n1. Minor improvements in writing clarity could enhance the paper's readability. 2. Additional details on hyperparameters and implementation specifics would be beneficial for reproducibility. 3. Some figures could benefit from minor formatting adjustments for better visual clarity. 4. Clarification on the availability of code and datasets would be helpful for the research community."
    },
    "49": {
        "paper": "Preﬁxing Attention Sinks can Mitigate Activation Outliers\nfor Large Language Model Quantization\nAnonymous ACL submission\nAbstract\nDespite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to ﬁnd a set of key-value cache, coined CushionCache , which mitigates outliers in subsequent tokens when inserted as a preﬁx. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and ﬁnd that it signiﬁcantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method. 1 Introduction Tremendous capabilities of large language models (LLMs) come with a tremendous computational cost. Modern language models often have over hundreds of billions of parameters, requiring signiﬁcant memory and computation for prediction and training. For instance, OPT-175B (Zhang et al., 2022), one of the most popular open-sourced language models, requires at least 350GB of memory and the order of 1018ﬂoating point operations to generate a new token1(Hoffmann et al., 2022). 1assuming the context length 2048Quantization is an effective strategy to reduce the computational cost of LLMs. Recent works demonstrate that the precision of LLM weight parameters can be greatly reduced by post-training quantization (PTQ), with minimal degradations in its generation quality. For example, Huang et al. (2024) shows that one can quantize the weights of the LLaMA3-70B to 4 bits, with less than 0.5%p drop in its zero-shot prediction accuracy. Roughly, the reduced precision translates into 4 \u0002increase in the generation throughput, and similar reduction in memory requirements (Lin et al., 2024). LLM activations, however, remain challenging to be quantized. The key obstacle is the activation outlier, i.e.,a small number of activations that are substantially larger than others (Bondarenko et al., 2021; Dettmers et al., 2022; Sun et al., 2024). Such outliers elongate the quantization range and ﬂattens out most non-outlier activations, leading to large performance losses even at W8A8 quantization. To address this issue, recent works propose to mitigate outliers based on various relaxations of the stringent static, per-tensor quantization. One line of work applies quantization separately to each channel depending on the outlier proneness (Bondarenko et al., 2021; Dettmers et al., 2022). These methods, however, are difﬁcult to be implemented on conventional hardwares. Another line of work reparameterizes the activations and weights in a way that the impact of outliers are amortized (Xiao et al., 2023; Ashkboos et al., 2024). These algorithms focuses on attaining high generative quality by adopting per-token or dynamic quantization, leaving the most hardware-friendly option—static per-tensor quantization—less explored. To ﬁll this gap, we take a novel approach for mitigating activation outliers in LLMs. In particular, we focus on answering the following key question: Can we ﬁnd a good preﬁx that mitigates the activation outliers in the subsequent 1\nFigure 1: Activation magnitudes in LLaMA2-7B, before and after CushionCache. CushionCache mitigates\nthe activation outliers in LLMs by inserting and tuning the several preﬁx tokens to the model, which acts as an\nattention sink. Adding such sink tokens alleviates outliers in the subsequent tokens and enables a better activation\nquantization of the model with coarse quantization granularities.\ntokens on a pretrained LLM? Our answer is positive; we develop a very simple yet effective method, coined CushionCache , to discover a preﬁx2which reduces the outlier in the following tokens processed by the given LLM. By inserting this preﬁx, one then can quantize the activations of the LLM with much smaller quantization error, leading to an improved generation quality. To design our method, we draw inspirations from a recent observation that the outliers may originate from attention sinks (Bondarenko et al., 2023)—the “no-operation” tokens that receive much attention from other tokens (Xiao et al., 2024). By adding sink-like tokens as a preﬁx, one may be able to separate out outlier activations as well, rendering the subsequent tokens outlier-free. In a nutshell, our method works in two steps. 1.Greedy initialization. We search for a sequence of sink-like prompt tokens in a greedy manner, so that the activations of the subsequent tokens are less prone to outliers (Section 4.1). 2.Quantization-aware preﬁx tuning. We train the greedily initialized preﬁx further to minimize the combined loss of the prediction loss and quantization error (Section 4.2). Our experiments demonstrate that the proposed CushionCache is highly effective in making LLMs more quantizable. The technique is versatile, consistently improving the quantized performance of LLMs under various scenarios, from per-token to per-tensor static quantization. The method can also 2more precisely, the key-value cache; we only care about\nthe keys and values, rather than the token itself,be seamlessly combined with existing quantization algorithms to further boost their performances. To summarize, we contribute the following. •We introduce CushionCache, a new preﬁx discovery method for mitigating LLM outliers to improve the quantization performance. •Through extensive experiments, we show that CushionCache can consistently improve the performance of quantized LLMs under a wide range of setup. In particular, we improve the prior state-of-the-art W8A8 per-tensor static range quantization of LLaMA3-8B over 30%p in zero-shot accuracy on downstream tasks. •Through our analysis, we demonstrate that CushionCache effectively replaces the role of attention sink tokens. 2 Related Work Outliers in LLMs. The fact that there exists usually large entries in LLM activations, or outliers, has been reported by multiple works. Kovaleva et al. (2021) and Timkey and van Schijndel (2021) report the existence of outliers in large transformerbased language models ( e.g., BERT), and ﬁnd that they appear mostly in a small number of channels and layers. Bondarenko et al. (2021) make a similar observation in the context of quantization, and ﬁnds that quantizing the activations lead to a large degradation in generation quality; the work also reports that semantically meaningless tokens can have higher tendencies to have outliers. 2\nDettmers et al. (2022) conﬁrm the same ﬁnding while quantizing GPT-scale models, and studies how the model scale affects the prevalence of outliers over tokens and layers. More recently, Sun et al. (2024) investigates a similar phenomenon in newer LLM variants and conﬁrms that certain tokens are more likely to suffer from outliers. Per-channel activation quantization. A line of work proposes to mitigate outliers in LLM activation quantization by applying different scaling factors or precision to each channel. Bondarenko et al. (2021) splits activation channels into several groups and perform quantization on each group. LLM.int8() (Dettmers et al., 2022) applies higher precision ( e.g., FP16) to a small number of outlierprone channels, while quantizing the other channels to lower bits ( e.g., INT8). These works, however, are difﬁcult to be implemented in conventional hardwares, as they requires scaling along the contracting dimension of matrix multiplication. Per-token, with reparameterization. Another line of work proposes to quantize the activations per-token to reduce the impact of outliers with better hardware acceleration. Many of these works adopt reparameterization of weights to mitigate the outliers further. ZeroQuant (Yao et al., 2022) applies per-tensor quantization and knowledge distillation to achieve reasonable INT8 quantization performance. SmoothQuant (Xiao et al., 2023), Outlier Suppression+ (Wei et al., 2023), and OmniQuant (Shao et al., 2024) migrates the activation magnitudes to the weights to normalize the scales of the activations. More recently, QuaRot (Ashkboos et al., 2024) rotates the activations so that the outlier magnitudes are distributed over multiple axes in the reparametrized space. While these methods are effective, per-token quantization are typically slower than per-tensor quantization at the same quantization precision as it requires larger scale which has a size of the number of tokens. Per-tensor quantization. Notably, Xiao et al. (2023) also provides two options for per-tensor activation quantization: one with dynamic quantization range, and another with static range. While these options tend to be faster than per-token (with static range being the fastest), their generation quality is much lower than per-token, especially on recent models such as LLaMA-3 (Touvron et al., 2023). Attention sinks and outliers. Recent works report an intriguing phenomenon in large transform- 190ers, termed attention sink . Xiao et al. (2024) ﬁnd that a small number of semantically meaningless tokens, usually at the beginning of the sequence, tend to receive unusually large attention. Darcet et al. (2024) make a similar observation for vision transformers, and show that training ViTs with additional meaningless tokens can help make the attention structures more semantically meaningful. Bondarenko et al. (2023) hypothesize that the sink tokens may be the root cause of the activation outliers, and propose a new architecture that prevents the outliers from emerging when pretrained from scratch. Our work shares a similar intuition, but critically differs in that we mitigate outliers by ﬁne-tuning the pretrained LLM. This means no modiﬁcation to the network architecture is needed and does not need to train the model from scratch. 3 Preliminaries Key-value cache. Modern language models, typically based on decoder-only architecture, are built as a sequence of transformer blocks which process a sequence of tokens to predict the next token (Vaswani et al., 2017). That is, at each decoding step, the transformer f(\u0001)performs: tn+1=f(t1;t2;:::;t n) (1) wheret1;\u0001\u0001\u0001;tnare the preceding tokens used as context. LLM iteratively applies Eq. (1) autoregressively to generate text as a sequence of tokens. As the context length grows, the computational cost to process all previous tokens also grows larger, slowing down the generation signiﬁcantly. A popular solution is to cache and reuse the keys and values of the preceding tokens computed during the previous iteration. This trick relies on the fact that preceding tokens affect the outcome of the current token only through their keys and values: (sn;1;:::;s n;n) = Attention( qn;k1:n) on=nX\ni=1vi\u0001sn;i whereqi;ki;videnotes the query, key, and value vectors for each token and si;jdenotes the attention score for the i-th token query on j-th token key. By storing and reusing the cached values (called KV cache), one only needs to process new tokens as: tn+l+1=f(tn+1:n+ljk1:n;v1:n); (2) 3\nwheretn+1:n+ldenotesltokens given at the current step and k1:n;v1:nare the keys and values of the preceding context of length n, computed during the previous iteration. During the preﬁll phase, l may be the length of the prompt, and during the decoding phase, we can simply use l= 1, processing only a single token at a time. Quantization. Quantization is an act of casting a high-precision tensor (typically FP) into a lowerprecision tensor (typically INT), to save the memory to store and computation to process the tensor. In neural network quantization, a popular choice is thelinear quantization , which performs Xint= round(( Xfp\u0000z)=s); (3) whereXint;Xfpdenotes the quantized and original tensors,z;s2Rdenote zero-point and scaling factor, and round(\u0001)denotes the rounding operation. The scaling factor is typically selected as s=max(Xfp)\u0000min(Xfp)\n2N\u00001\u00001; (4) whereNdenotes the number of bits for the integer format. The zero-point is determined as either z= min(Xfp)orz= 0, for the asymmetric and symmetric quantization, respectively. Activation quantization with static range. By quantizing both activation and weight matrices, one can avoid performing computation-heavy FP matrix multiplications. That is, for the case of symmetric quantization (for simplicity), we approximate: WfpXfp\u0019sWsX\u0001WintXint; (5) where the right-hand side can be computed using an integer matrix multiplication, and a single multiplication of FP16/32 quantities (for scaling factors). The combined scaling factor need not be multiplied back to the matrix immediately, and can be used in the subsequent operations directly. In many cases, the scaling factors sW;sXcan be pre-computed based on the validation set statistics. This method, called static-range quantization , enables more acceleration than computing these values dynamically during the inference. Outliers and complications. In LLMs, the activationXfptends to have a very large entry (Dettmers et al., 2022; Sun et al., 2024). In such case, the magnitude of max(Xfp)andmin(Xfp) will be very large, making the scaling factor sX 279very large. This leads to a high sparsity in the tensorXint, and a much degraded generation quality. This problem can be alleviated in various ways: One can change the scaling factor dynamically over time ( i.e., per-tensor dynamic quantization), or apply different scaling factors for each channel or token ( i.e., per-channel/token quantization). As these methods require on-the-ﬂy computations of scaling factors, the methods are typically slower. Granularity and the communication cost. The drawback of ﬁner quantization granularity becomes more signiﬁcant in the distributed setup, as it affect the communication cost between nodes. To see this, consider the case of multiplying matrices with tensor parallelism, e.g., Megatron-LM (Shoeybi et al., 2019). Comparing with the pertensor static quantization, per-tensor dynamic quantization requires an additional AllReduce operation over the nodes to aggregate the (high-precision) scaling factor. The overhead is even more significant for per-token dynamic quantization, as the number of scaling factors is multiplied by the number of tokens, increasing the cost of AllReduce. 4 Method We now describe CushionCache, an algorithm to ﬁnd a preﬁx which can mitigate activation outliers in the subsequent tokens, thereby alleviating the quality degradation from activation quantization. CushionCache aims to ﬁnd a set of preﬁx that minimizes the quantization error of the activations. More concretely, let Xidenote activation of a transformer block for the input token ti. Our goal is to minimize the squared difference between the original and the quantized activations, i.e., Lq(t1;:::;t n) =nX\ni=1kXi\u0000q(Xi)k2\n2;(6) whereq(\u0001)denotes the quantization function, speciﬁed asq(X) =s\u0001round((X\u0000z)=s) +z. In practice, we consider the summation of the errorLqof all transformer blocks, but we omit this for the notational simplicity. Similarly, we deﬁne Lq(t1:njp1:m)as the sum of squared error for t1:n given the preﬁx p1:n, where the scaling factor sand zero-pointzare determined for t1:nonly. We hypothesize that there exist preﬁx tokens p that can reduce the expected activation quantization error of the tokens. That is, we ﬁnd ^p1:m= arg min\np1:mE[Lq(t1:njp1:m)];(7) 4\nwhere the expectation is taken over the probability distribution of tokens t1:n. Once we ﬁnd such preﬁx ^p1:m, their keys and values are cached and reused at the inference time to avoid redundant computation: tn+1=f(t1:nj^k1:m;^v1:m); (8) where ^kand^vcorresponds to the key-value caches of the preﬁx ^p1:n, which we call CushionCache . We solve the minimization (eq. 7) with a strategy based on preﬁx tuning. This is done in two steps: Initializing preﬁxes based on greedily searched prompts (Section 4.1), and Quantization-aware Preﬁx tuning (Section 4.2). 4.1 Greedy Preﬁx Search We carefully initialize the preﬁx as the preﬁx tuning is known to be very sensitive to initial values. We follow Li and Liang (2021) to search for the preﬁx that are activations of hard prompt tokens, i.e., input tokens that correspond to real text. As the search complexity grows exponentially with respect to the embedding size, we propose to use a greedy search algorithm with tailored heuristics. In a nutshell, our method is a greedy search with early stopping . We add new tokens to the prompt one-by-one, selected to minimize the quantization error. If the new token does not decrease the error much, we stop adding to prevent overﬁtting and computational overhead from long prompts. Concretely, at each step, we ﬁrst draw a single sample text t1:nfrom the dataset; we use the C4 dataset (Raffel et al., 2020), which is commonly used for calibration or validation purposes, to draw a sentence of length n= 512 . Then, based on the current state of prompts p1:k, we search for the next prompt token pk+1by solving pk+1= arg min\np2ELq(t1:njp1:k;p); (9) whereEdenotes the embedding table; we can solve this problem rapidly by batched inference . If the discovered new token reduces the quantization error by some fraction \u001c >0,i.e., satisﬁes Lq(t1:njp1:k+1)<\u001c\u0001Lq(t1:njp1:k); (10) then we append this token to the prompt and proceed to the next iteration. Otherwise, or if the max length is met, we stop searching for a new token. We use\u001c= 0:5for all experiments, which consistently shows a good performance. 371Algorithm 1 Greedy preﬁx search\nRequire: validation dataset D, embedding table\nE, max length m, threshold\u001c\n1:p= [ ] .initialize the prompt\n2:while len(p)<m do\n3:t\u0018Unif(D) .draw a text\n4:p\u0003= arg minp02ELq(tjp;p0).\n5: ifLq(tjp;p\u0003)>\u001c\u0001Lq(tjp)then\n6: break\n7: end if\n8:p:append (p\u0003) .add new token\n9:end while\n10:returnp\nNote that this algorithmic design provides some ﬂexibility. More speciﬁcally, one can initialize the prompt with nonempty sequence before the search, as a heuristic that can help speed up the prompt search procedure. We ﬁnd that ﬁlling in nonsemantic words, e.g.,<bos> or\\n, is particularly useful; this observation is well-aligned with the ﬁndings of Bondarenko et al. (2021); Sun et al. (2024). 4.2 Quantization-aware Preﬁx Tuning Using the intermediate activations of the greedilysearched prompt as an initial preﬁx, we ﬁne-tune the CushionCache via preﬁx tuning (Li and Liang, 2021). Precisely, we freeze the model parameters and train the preﬁx with the loss L=Lpred+\u0015\u0001Lq (11) whereLpredis the cross entropy loss for the nexttoken prediction and \u0015is a hyperparameter that balances two losses. Here, we apply stop-grad to scaling factors and zero-points of the quantization function, as is typical in quantization-aware training literature (Jacob et al., 2018). By optimizing this loss function, we ensure that the CushionCache not only improves the prediction accuracy but also minimizes the quantization error. This tuning does not require excessive amount of memory, as we only train the preﬁx. 5 Experiments 5.1 Experimental Setup Models. We evaluate our method on ﬁve LLM models: LLaMA2 and 3 (Touvron et al., 2023), Mistral (Jiang et al., 2023), OPT (Zhang et al., 2022) and BLOOM (Le Scao et al., 2022). 5\nWikiText-2 ( #) LLaMA2-7B LLaMA3-8B Mistral-7B-v0.1 OPT-6.7B BLOOM-7B\nFP16 5.47 6.13 5.25 10.86 11.37\nPer-tensor Static 9250.33 9759.46 85.51 11.45 11.93\n+ CushionCache (ours) 5.98 (-99.9%) 7.41 (-99.9%) 5.84 (-93.2%) 11.00 (-3.9%) 11.50 (-3.6%)\nSmoothQuant-O3 15439.73 14022.91 618.27 10.85 11.55\n+ CushionCache (ours) 5.87 (-99.9%) 7.37 (-99.9%) 5.60 (-99.1%) 10.68 (-1.6%) 11.38 (-1.5%)\nPer-tensor Dynamic 8.01 23.86 67.86 11.73 11.81\n+ CushionCache (ours) 5.69 (-29.0%) 7.30 (-69.4%) 5.59 (-91.8%) 10.99 (-6.3%) 11.43 (-3.2%)\nSmoothQuant-O2 8.13 25.12 66.16 10.87 11.59\n+ CushionCache (ours) 5.66 (-30.4%) 7.29 (-71.0%) 5.56 (-91.6%) 10.68 (-1.7%) 11.39 (-1.7%)\nPer-token Dynamic 5.47 6.22 5.30 11.20 11.47\n+ CushionCache (ours) 5.37 (-1.8%) 6.15 (-1.1%) 5.21 (-1.7%) 10.77 (-3.8%) 11.37 (-0.9%)\nSmoothQuant-O1 5.49 6.19 5.27 10.86 11.38\n+ CushionCache (ours) 5.36 (-2.4%) 6.15 (-0.6%) 5.20 (-29.0%) 10.67 (-1.7%) 11.35 (-0.3%)\nTable 1: Perplexity of W8A8-quantized LLMs on raw-WikiText2. Green denotes the relative decrease.\n7 Zero-shot Tasks ( \") LLaMA2-7B LLaMA3-8B Mistral-7B-v0.1 OPT-6.7B BLOOM-7B\nFP16 65.63 68.83 69.14 60.50 56.20\nPer-tensor Static 36.37 35.86 48.83 57.94 55.87\n+ CushionCache (ours) 64.47 (+28.10) 67.85 (+31.99) 67.75 (+18.91) 59.85 (+1.91) 55.91 (+0.04)\nSmoothQuant-O3 36.32 36.22 37.45 60.61 55.96\n+ CushionCache (ours) 64.67 (+28.35) 66.99 (+30.77) 68.39 (+30.94) 60.87 (+0.26) 56.66 (+0.75)\nPer-tensor Dynamic 61.94 58.94 52.02 59.23 56.46\n+ CushionCache (ours) 65.34 (+3.40) 68.66 (+9.72) 69.02 (+17.00) 60.28 (+1.05) 58.47 (+2.01)\nSmoothQuant-O2 61.24 58.67 51.08 60.57 56.14\n+ CushionCache (ours) 65.65 (+4.41) 68.74 (+10.07) 69.15 (+18.07) 60.60 (+0.03) 58.99 (+2.85)\nPer-token Dynamic 65.43 68.92 68.90 59.48 56.55\n+ CushionCache (ours) 65.78 (+0.35) 68.58 (-0.34) 69.83 (+0.93) 60.65 (+1.17) 56.72 (+0.17)\nSmoothQuant-O1 65.64 68.64 69.09 60.55 56.35\n+ CushionCache (ours) 65.97 (+0.33) 68.78 (+0.14) 69.99 (+0.90) 61.01 (+0.46) 56.80 (+0.45)\nTable 2: Average zero-shot accuracies of W8A8-quantized LLMs. We average over LAMBADA, HellaSwag,\nPIQA, WinoGrande, OpenBookQA, RTE, and COPA. Green is the accuracy gain and red is the drop.\nDatasets. We measure the perplexity on the heldout set of WikiText-2 validation dataset (Merity et al., 2016). For zero-shot evaluation, we use seven tasks from the LM evaluation harness benchmark by EleutherAI (Gao et al., 2023). Precisely, we use LAMBADA, HellaSwag, PIQA, WinoGrande, OpenBookQA, RTE, and COPA datasets. Base algorithms. We apply CushionCache on two base activation quantization algorithms: Naïve activation quantization and SmoothQuant (Xiao et al., 2024). We consider three different scenarios: Per-tensor static, per-tensor dynamic, and pertoken dynamic quantization. Note that for each case, the SmoothQuant has a corresponding version, called O3, O2, and O1, respectively. Conﬁguration: Quantization. We mostly follow the setup of Li et al. (2024) and the TensorFlow default. We use symmetric group-wise quantization for model weights, and asymmetric quantization for the activations. For SmoothQuant, we use the migration strength \u000b= 0:8, which worked consis- 424tently well throughout our experiments. For static range quantization, we calibrate using the training split of WikiText-2 (Merity et al., 2016). Conﬁguration: Preﬁx tuning. We follow the setup of Li and Liang (2021) and tune for 2 epochs. We set the hyperparameter \u0015= 0:01. 5.2 Main Results: W8A8 Quantization In Tables 1 and 2, we provide the performance achieved by the quantized language models, quantized with and without the proposed CushionCache. We report the WikiText perplexity and zero-shot accuracy in the tables, respectively. For per-tensor static range quantization, CushionCache successfully improves the performance of the model; the boost is quite substantial in LLaMA and Mistral, often providing over 30%p gains in terms of zero-shot accuracies. Intriguingly, the gain is much more pronounced in LLaMA-style models, which adopt the pre-LayerNorm and gated linear units. For per-tensor dynamic range quantization, similarly, we make consistent improvements 6\nLLaMA3-8B Zero-shot acc. (%)\nFP16 68.83\nPer-tensor Dynamic 58.94\n+ Greedy-searched init. 67.78 (+8.84)\n+ Preﬁx tuning 68.13 (+0.35)\n+ Quantization-aware loss 68.66 (+0.53)\nTable 3: Ablation study. We compare the contribution\nof each algorithmic component by sequentially adding\nthem. We apply W8A8 per-tensor dynamic quantiza-\ntion on the LLaMA3-8B model.\nPer-Token Dyn. Perf. LLaMA3-8B Mistral-7B\nppl (#)6.13 5.25FP16acc. (\")68.83 69.14\nSmoothQuant-O1 ppl 6.93 5.49\n(W6A6) acc. 66.72 67.51\nppl 6.74 (-2.7%) 5.40 (-1.6%)+ CushionCacheacc. 67.60 (+0.88) 68.42 (+0.91)\nSmoothQuant-O1 ppl 130.32 18.57\n(W4A4) acc. 40.25 51.11\nppl 29.09 (-77.7%) 12.45 (-33.0%)+ CushionCacheacc. 48.78 (+8.53) 55.58 (+4.47)\nTable 4: W6A6/W4A4 quantization. We addition-\nally evaluate per-token quantization with lower bits, as\nW8A8 does not degrade much performance in general.\nover both vanilla quantization and SmoothQuant. For per-token dynamic quantization, the gain is somewhat marginal, as the base quantization algorithms already tend to achieve a close performance to the FP16 model; we revisit per-token case for lower precision in Section 5.4. 5.3 Ablation Study In Table 3, we sequentially add our key algorithmic components to validate their efﬁcacy. In particular, the components are (1) greedy-searched initial value, (2) preﬁx tuning, and (3) the quantizationerror-based regularizer. We observe that each component makes nontrivial contributions for achieving near-FP16 zero-shot accuracy. Interestingly, we ﬁnd that the greedysearched initialization is especially effective, contributing\u001891% of the accuracy gain. This suggests that our search mechanism can be used as a compute-light standalone method in the cases where it is difﬁcult to conduct preﬁx-tuning, due to a limited on-device memory. 5.4 4/6-bit Per-token Quantization To conﬁrm the effectiveness of CushionCache under per-token dynamic quantization, we additionally evaluate with a lower precision (Table 4). In particular, we use W6A6 and W4A4. 471Model Top-1 Top 10% Median\nLLaMA2-7B 2461.40 0.59 0.23\n+ CushionCache (ours) 25.83 0.59 0.24\nLLaMA3-8B 288.32 0.16 0.06\n+ CushionCache (ours) 4.94 0.16 0.06\nMistral-7B-v0.1 352.05 0.12 0.04\n+ CushionCache (ours) 3.51 0.12 0.04\nTable 5: Top-1, top 10%, and the median activation\nmagnitudes of three LLMs. Here, we inspect the in-\nput activations to the last transformer block.\nThe results conﬁrm that the proposed CushionCache is also effective in boosting the quantization performance of per-token activation quantization algorithms. In particular, CushionCache helps keeping the accuracy degradation quite low ( \u00181%p) for W6A6 quantization of both LLaMA3 and Mistral. 6 Analysis We now conduct a brief sanity check. In particular, we ask the following questions. • Did the outliers disappear? (Section 6.1) •Did the CushionCache really replaced the role of attention sink? (Section 6.2) •Will it be computationally viable to run CushionCache for large models? (Section 6.3) 6.1 Change of Activation Magnitudes In Table 5, we report various order statistics of the activation magnitudes that appear in LLaMA2/3 and Mistral. In particular, we focus on the input activations to the last transformer block of these models, and measure the top-1, top 10%, and median (i.e., top 50%) activation magnitude. We average over ten samples, with a sequence length 4096. The effect of CushionCache is quite dramatic. In particular, we ﬁnd that the CushionCache can reduce the scale of the activation outlier to 1-2% of the previous value. The ratio between the top-1 and the median decreases from roughly 10,000:1 to 100:1. We also note that the other order statistics, i.e., top 10% and median, remains roughly the same before and after the CushionCache. In Fig. 2, we visualize the top-1/2/3 activations and median for each layer of LLaMA3-8B. The left panel shows the magnitude of outliers that occur during the standard operation of LLaMA3-8B. The median is almost zero, indicating that most values are close to zero, with only a few signiﬁcantly large outliers. On the right panel, we present 7\nFigure 2: Top-1/2/3 and median activation magnitudes at each layer of LLaMA3-8B. The left panel shows the\nactivations without CushionCache, having signiﬁcant outliers except for initial layers. The right panel shows the\nactivation with CushionCache, having signiﬁcantly reduced outliers in every layers.\nFigure 3: Attention patterns before and after applying CushionCache in LLaMA3-8B and Mistral-7B. The\nﬁrst and third panels show the attention patterns in models without CushionCache, where the attention sinks are\nquite prevalent in the generated token sequence. The second and fourth panels illustrate the attention patterns after\ninserting CushionCache. By adding the CushionCache, the attention is redirected toward the CushionCache tokens,\npreventing the attention sink from arising in the subsequent tokens.\nthe same measurements after applying the CushionCache. We conclude that CushionCache effectively removes the activation outliers. 6.2 Attention on CushionCache In Fig. 3, we visualize the attention patterns of LLaMA2 and Mistral, before and after applying the CushionCache. Attention sinks, as identiﬁed by Xiao et al. (2024); Sun et al. (2024), are tokens that disproportionately attract attention. By inserting CushionCache, we observe that the CushionCache tends to dominate most of the attention from other tokens, removing the sinks in other tokens. 6.3 Time Needed to Search CushionCache In Table 6, we report the wall-clock time spent for performing the greedy search and preﬁx tuning of CushionCache. We observe that the greedy preﬁx search can be quite time-consuming, highly dependent on the side of the embedding table; LLaMA38B has a large embedding table. Another observation is that the quantization-aware preﬁx tuning step takes relatively small time for all models. 529Model Step 1 Step 2 Total Time\nLLaMA2-7B 2.68 hours 3.34 hours 6.02 hours\nLLaMA3-8B 12.09 hours 3.70 hours 15.79 hours\nOPT-7B 1.38 hours 2.71 hours 4.09 hours\nTable 6: Wall-clock time for the search. We use a\nserver with four NVIDIA A6000 GPUs.\n7 Conclusion In this paper, we present CushionCache, a novel approach for mitigating activation outliers in LLMs to improve activation quantization performance. Through extensive experiments, we demonstrate that CushionCache consistently enhances the performance of per-tensor activation quantization. Our analysis shows that CushionCache effectively reduces the magnitude of activation outliers and redirects attention sinks, leading to more uniform and quantization-friendly activations. In contrast with other approaches to faciliate activation quantization, CushionCache is the ﬁrst—up to our knowledge—to fundamentally alter the activation distribution itself without extensive training, making activations easier to quantize. 8\nLimitations A limitation of our study is that our method is designed for LLMs with the decoder-only transformer structure. An extension to encoder-decoder LLMs (Raffel et al., 2020) may require further modiﬁcations to the algorithm. Another limitation is the lack of a principled mechanism to determine the hyperparameter \u001c, which decides when to stop adding new tokens. An extensive tuning may incur a nonnegligible computational cost, especially when the target model is extremely large. Ethics statement All experimental results we provide in this paper is based on publicly available datasets and opensource models, whose intended use include research purposes. We have used an AI assistant for the grammar check. 562",
        "aspect_review": "The paper introduces a novel method, CushionCache, for mitigating activation outliers in large language models, which is a significant advancement in the field of quantization. The intuition behind using attention sinks to mitigate outliers is well-justified and validated through comprehensive experiments. CushionCache consistently improves the performance of quantized LLMs, achieving substantial gains in zero-shot accuracy, especially in LLaMA models. The method shows significant improvements over existing quantization techniques, such as SmoothQuant, across various scenarios. The paper is well-organized, providing clear explanations of the methodology and thorough experimental results.\n\nWhile the motivation is strong, the explanation of the hyperparameter selection could be more detailed. The evaluation is comprehensive, but the impact on encoder-decoder models is not addressed. The focus is primarily on decoder-only models, which limits the generalizability to other architectures. The greedy search process is time-consuming, which could be a limitation for very large models.",
        "general_review": "The paper presents a notable novelty in proposing CushionCache, a novel prefix discovery method that effectively mitigates activation outliers in large language models (LLMs), thus enhancing activation quantization performance. The technical depth and rigor are strong, with a well-defined method that includes greedy prefix search and quantization-aware prefix tuning. Comprehensive experimental validation is provided across multiple LLMs, demonstrating significant improvements in quantization performance, particularly in per-tensor static quantization scenarios. The practical impact is significant, as the method offers a hardware-friendly solution that can be seamlessly integrated with existing quantization algorithms, reducing computational costs without extensive retraining.\n\n1. Minor writing or clarity improvements could be made in the explanation of the greedy search algorithm and its initialization process. 2. Small details on hyperparameters, such as the choice of the threshold parameter 𝜉, could be further elaborated to aid reproducibility. 3. Very minor formatting on figures, such as ensuring consistent labeling and clarity in graphs, would enhance readability. 4. Slight clarification on code availability and whether it will be released for public use would be beneficial for the community."
    }
}