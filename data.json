{
    "0": {
        "paper_id": "6c75b626f9c7aac5a34b4486262ea62c268a2b87e11e0a381ec1516f9890d0cf4016c3f97279d418ef8cfa55c07eb3769ddd0babd08baab9c23ad11e88758a7b",
        "review_id": "918a30ad56af48da79702235d80d260018a1427411d8d2e3822f02cd60fe08fd0991534bcb481fd15896db6c7d7842eaa15916e90a8fdccbfb4ac1823982f434",
        "human": {
            "strengths": "- Focuses on highly understudied topic in RAG research i.e. the importance how information is structured is also critical\n- Introduces a training dataset which will be released for future research to enhance RAG models\n- Token reduction / compression rates of Refiner upto 89% which is beneficial for reducing real-world costs\n- The reduction and refinement into structured content also adds a notion of explainability\n- Ablation showing resilience to adding extra content, where Refiner is still able to maintain performance partly mitigating \"lost-in-the-middle\" problem\n- New STOA results for multi-hop QA datasets including: HotpotQA and 2Wiki with comparable results on single-hop datasets; its worth highlighting that the gains are significantly pronounced when compared to baseline ChatGPT-3.5-Turbo for multi-hop datasets",
            "weaknesses": "- The organization of paper is such that it becomes difficult to understand the methodology that is introduced by the work, e.g. sections 4.1 and 4.2 in less than a page mentions the main contribution of the paper i.e. multiple teacher KD-based dataset curations\n- Pertaining to above, plenty of effort went into analysis where Table 3 and 4 are too redundant in terms of reporting all numbers for +5 and +10 documents across all datasets to derive a conclusion for a given ablation study\n- Figure 4 in appendix gives detail about the training dataset construction and even it lacks several details like what is \"ParseGeneratedContent\" and what is \"V\"? similary there is a \"Counter\" and an \"assignSection\" not very clear\n- Based on these, it is difficult to assess the contribution of this work as the core contribution seems to be introducing this additional training data which is thin for a full paper"
        },
        "llm": "{\"strengths\": {\"Data/Task\": \"The paper addresses a significant limitation in Large Language Models (LLMs) by proposing a novel Retrieval-Augmented Generation (RAG) approach that effectively integrates external document chunks to enhance knowledge-intensive tasks.\", \"Ablation\": \"The ablation study demonstrates the effectiveness of the structured output in enhancing the performance of downstream LLMs, providing insights into the importance of sections in improving accuracy.\", \"Methodology\": \"The paper introduces Refiner, an end-to-end extract-and-restructure paradigm leveraging a single decoder-only LLM, which adaptively extracts query-relevant contents and sections them based on interconnectedness.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper provides a comprehensive explanation of the 'lost-in-the-middle' syndrome and effectively discusses how Refiner mitigates this issue by structuring content to maintain context and highlight information distinction.\"}, \"weaknesses\": {\"Analysis\": \"The analysis lacks depth in exploring the implications of the probabilistic nature of Refiner's output and its potential impact on real-world applications.\", \"Data/Task\": \"The robustness of Refiner has not been tested on alternative input structures such as table data or domain-specific documents, raising concerns about its generalizability.\", \"Ablation\": \"While the ablation study is thorough, it could benefit from further exploration of the impact of different output structures on downstream tasks.\", \"Contribution\": \"The paper could better articulate its contributions in the context of existing RAG systems, particularly in terms of novelty and advancement.\", \"Presentation\": \"The presentation of results, particularly in tables, could be improved for clarity and ease of interpretation.\", \"Methodology\": \"The methodology could benefit from a more detailed description of the training process and the specific challenges encountered during implementation.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "1": {
        "paper_id": "c938e233573f6d24cb977a140ecfad45d0e0717d4a59c9ffd7d4cb27242838371fab29bfc062557840fc0908fbce62435d113bbadcee6eb5d3315f74c1db777d",
        "review_id": "21ccee9e1dfae88701b269ef75c3a3eef4ae70638fb54c6c8f14a9f8ecf42d9cd60522af19c763a57a907d0a404f38b995e14083af4d8e8861604757cadea675",
        "human": {
            "strengths": "1. The paper is well-motivated to reduce the cost of data collection.\n2. The paper is easy-to-follow.",
            "weaknesses": "1. The performance and inadequate  baselines\n- In the zero-shot setting, models trained on NQ converge to the best F1 score very fast while models trained on QB need 600k data to achieve the best F1 score which is still worse than NQ. This questions the effectiveness of the proposed method. A handful of high-quality data is more useful than a large amount of QB data synthesized by the authors.\nBesides, the authors only compare the performance with the original NQ and its own variants using GPT and Llama to transform questions. Other data synthetic methods should be compared as well e.g. the series work of self-instruct. \n\n- In the supervised setting, from Figure 3, we can see the improvement brought by QB is marginal. Only QB-NQ-100 slightly outperforms NQ. It does not seem worth investing hundreds of hours to create such synthetic data for a minor improvement.\n\n2. The generalization of the proposed methods\n- The authors discuss the generalization of the proposed method in section 7.3. However, why is the performance evaluated with the converted data from other datasets on NQ? I think it should be tested on the test set from the original dataset to evaluate the efficacy.\n\n3. The format can be improved. For example, in L209-211, the version of GPT is not specified. The numbers on the x-axis in Figure 2-5 are overlapping or faded out."
        },
        "llm": "{\n    \"strengths\": {\n        \"Presentation\": \"The paper is well-structured, clearly outlining the motivation, methodology, and results. The use of diagrams to illustrate the NATURALIZATION process is helpful in understanding the transformation steps.\",\n        \"Data/Task\": \"The paper addresses the challenge of expensive and ambiguous natural question datasets by proposing a novel method to transform trivia questions into natural, information-seeking questions. This approach leverages existing high-quality datasets, making the task more affordable.\",\n        \"Intuition/Justification/Motivation/Validation\": \"The motivation to create a cheaper alternative to the Natural Questions dataset is well-justified, given the high costs and ambiguities associated with natural datasets. The validation through experimental results shows that the transformed dataset can achieve comparable performance to the original NQ dataset.\"\n    },\n    \"weaknesses\": {\n        \"Evaluation\": \"The paper lacks a thorough evaluation of the transformed questions' quality in terms of correctness and naturalness. While the F1 scores are presented, there is no systematic analysis of potential errors in the generated questions.\",\n        \"Result\": \"The results show that the transformed questions lag behind the original NQ dataset by about 6% on average. This gap indicates room for improvement in the transformation process.\",\n        \"Comparison\": \"The comparison with other methods, such as LLM-based transformations, is limited. More detailed analysis and comparison with other state-of-the-art methods could strengthen the claims.\",\n        \"Methodology\": \"The transformation process relies heavily on heuristic rules, which may not generalize well to other datasets or languages. The paper could benefit from exploring more robust and adaptive transformation techniques.\"\n    },\n    \"soundness\": \"3.5\",\n    \"overall assessment\": \"3.5\"\n}"
    },
    "2": {
        "paper_id": "a90e2ac318a5647b9977525bf7519456e1abcac68a931b7fba093c73b7e9f8b1855ca7b981e21e3b0d8f535a2ed982637a4709a44cbeaf71b869151fb0bab861",
        "review_id": "f9b925aad4d762041758862c0d9532d512cca1d36269adf92be80c65339c7bc6816a8a6bb5020eb0de51f5a2d31ea1b5cdd772ea15242478d7f9f3fbf914ae4d",
        "human": {
            "strengths": "- A new method combining visual speech information with LLMs for visual speech recognition and translation\n- Authors claim that their deduplication method can significantly reduces the computational costs with a good performance at the same time",
            "weaknesses": "- Compared to the submissions (https://openreview.net/forum?id=XBxEaLT3QlI), the authors did not solve mentioned weakness and did not discuss those concerns in the current version (e.g., novelty compared to related work [1,2,3], the necessity of using LLMs when AV-HuBERT already demonstrates a reduced WER to 26.9%, etc).\n- By reading both versions in detail, it is very strange why the experimental results of the proposed model are different to the previous version in Table 1. In the submitted response PDF, the authors mentioned that experimental results in the whole of section 4 were mistakenly reported on the LRS3-T dataset rather than the MuAViC dataset. However, the results of Table 1 are also different in both versions and Table 1 should be the results on LRS3 dataset. In addition, there are no mentions of which dataset was evaluated in those figures or tables' captions, and it is very hard to understand which dataset was used finally in the evaluation.\n\n\n[1] Lee A, Chen P J, Wang C, et al. Direct speech-to-speech translation with discrete units[J]. arXiv preprint arXiv:2107.05604, 2021.\n\n[2] Lee A, Gong H, Duquenne P A, et al. Textless speech-to-speech translation on real data[J]. arXiv preprint arXiv:2112.08352, 2021.\n\n[3] Popuri, Sravya, et al. \"Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation.\" arXiv preprint arXiv:2204.02967 (2022)."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel framework, VSP-LLM, for visual speech processing, which effectively integrates large language models (LLMs) to enhance context modeling capabilities, addressing the challenge of homophenes in visual speech recognition and translation.\", \"Result\": \"The VSP-LLM framework demonstrates state-of-the-art performance in visual speech recognition and translation tasks, outperforming models trained on significantly more data, indicating its efficiency and effectiveness.\"}, \"weaknesses\": {\"Related Work\": \"The paper provides a limited discussion on previous works integrating LLMs with visual speech processing, which could have been expanded to better contextualize the novelty of the approach.\", \"Novelty\": \"While the integration of LLMs is novel, the paper does not explore other potential applications of the VSP-LLM framework beyond visual speech recognition and translation, limiting its perceived impact.\", \"Experiment\": \"The experimental setup lacks diversity in datasets and tasks, focusing primarily on the MuA ViC benchmark, which may not fully demonstrate the generalizability of the proposed framework.\", \"Evaluation\": \"The evaluation metrics used are standard (WER and BLEU), but additional qualitative analyses on real-world applications could enhance the understanding of the framework's practical utility.\", \"Result\": \"While the results are impressive, the paper could benefit from a deeper analysis of the limitations and potential biases introduced by the reliance on LLMs.\", \"Data/Task\": \"The paper focuses on a specific set of tasks (VSR and VST) and datasets, which may limit the applicability of the findings to other visual speech processing scenarios.\", \"Presentation\": \"The presentation could be improved by providing clearer explanations of the deduplication process and its impact on computational efficiency.\", \"Methodology\": \"The methodology section could be more detailed in explaining the integration process of visual speech units and LLMs, which is crucial for understanding the framework's operation.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "3": {
        "paper_id": "2cabf44ebab6ff6297992953521445c153f4f2f085f83583a1aa0ca562bf2175c90187a82929ca314bbccc1679d0744c0ac22c9a5509061b239dcccc4b7a204d",
        "review_id": "da3adb3b7a6578c01a8646bbb2845cefaa8a5fee5dedac93a84e2d24c523450ca21e77891f2f2e63bab64004b36fcec473b4e8eae433fb4ace444275bcdda9fa",
        "human": {
            "strengths": "The motivation of the paper is strong. Using single vector is not enough to encode the long dialog history. Generative retrieval is a potential direction to exploit the LLMs to improve the traditional retrieval method. The experiments in this paper is comprehensive, which effectively demonstrates the usefulness of the proposed methods.",
            "weaknesses": "The writing structure of the paper should be further improved and some experiment results are missing. More details can be found in the comments below."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel DialogGSR model that integrates generative subgraph retrieval with dialog generation, leveraging language models for both tasks. The methodology effectively addresses the information bottleneck issue by using generative retrieval instead of encoder-based methods.\", \"Experiment\": \"DialogGSR demonstrates state-of-the-art performance on two benchmark datasets, OpenDialKG and KOMODIS, outperforming existing models in metrics like BLEU, ROUGE, and KQA.\", \"Intuition/Justification/Motivation/Validation\": \"The paper provides a strong motivation for addressing the limitations of existing methods, such as the inability to leverage pretrained language models and the information bottleneck issue. The proposed methods are validated through extensive experiments showing significant improvements over baselines.\"}, \"weaknesses\": {\"Experiment\": \"The experiments are limited to two datasets, which may not fully capture the model's performance across different types of knowledge graphs or dialog scenarios.\", \"Presentation\": \"The paper is dense with technical details, which might make it difficult for readers to follow without a deep understanding of the field. Some sections could benefit from clearer explanations or additional diagrams.\", \"Result\": \"While the model shows improvements, the gains in some metrics are relatively modest, and the paper does not explore the reasons behind these results in detail.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "4": {
        "paper_id": "a80085b38b0eea5859ae82b44525b8e49d35fabe494338dc386ef1f82ca850ee2b42d3c02a28a492eb8094426eda2fcd416cf0bedffec23e8e851c364b0ceab0",
        "review_id": "fa9b62cbbb3e2dae087d820c25edf05c120c7962c4f3ad6aa8c8101acf089daa3debf5cfdbf8798c29d4ca22ace1f18b7ed0bb498256ae2bfad5eacb9c8f4a36",
        "human": {
            "strengths": "The idea is very interesting and the proposed method is novel. The paper propose a framework that identifies the shared reasoning patterns underlying given tasks and customizes this logic for reasoning in individual instances. It effectively addresses the issue of reusing the same logic across different instances in algorithmic reasoning and reduces reliance on external compilers. \nAdditionally, the paper validate that using pseudocode to describe the logic for solving algorithmic reasoning tasks is more effectively than natural language.",
            "weaknesses": "The experiments appear to have been conducted independently on different task datasets, with no experiments or discussions regarding mixed-task results. For example, it is unclear whether the model can identify the type of an unknown task instance and then generate the corresponding task-specific pseudocode prompt. This could pose a challenge for applications in real-world scenarios."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The THINK-AND-EXECUTE framework introduces a novel approach to algorithmic reasoning by utilizing pseudocode to express task-level logic, allowing for efficient reuse across instances. This methodology significantly outperforms several strong baselines, including Chain-of-Thought and Program-of-Thought prompting.\", \"Data/Task\": \"The framework is tested on a diverse set of algorithmic reasoning tasks from Big-Bench Hard, demonstrating its applicability and effectiveness across various complex logical patterns.\", \"Novelty\": \"The use of pseudocode for task-level logic representation is a novel contribution that enhances the reasoning capabilities of LLMs beyond natural language or instance-specific code generation approaches.\", \"Intuition/Justification/Motivation/Validation\": \"The paper provides strong empirical validation through extensive experiments showing the superior performance of THINK-AND-EXECUTE over existing methods. The intuition behind using pseudocode is well-justified as it efficiently captures shared logic across task instances.\"}, \"weaknesses\": {\"Experiment\": \"While the experiments are extensive, the paper could benefit from additional analysis on the limitations of the framework in non-algorithmic reasoning tasks.\", \"Data/Task\": \"The focus is predominantly on algorithmic reasoning tasks, which may limit the perceived impact of the framework in broader NLP contexts.\", \"Methodology\": \"The approach relies heavily on the capabilities of specific LLMs like GPT-3.5-Turbo, which may not be accessible to all researchers, potentially limiting reproducibility.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more detailed explanations on how the pseudocode prompts are generated and the specific role of the Instructor and Reasoner LMs. Additionally, a more thorough discussion on the potential limitations and future applications of the framework would be beneficial.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "5": {
        "paper_id": "33d53cc1ce742d645a9cb480f017fcf9d7c9c7e89f217276ec11d9f3d2b1ae135944904285d845f5615b6958d3f63c1b52b34595211a93fe1fae5a82c52bde7d",
        "review_id": "7151a0d6d9c0da5a290cd0e5d575a8f01032bc975c088918f28c7a94611847a3c1ecf7989bb921c2b38e3fdb85349461a7f4b1257f55e766858977d041698efd",
        "human": {
            "strengths": "The paper investigates a relatively unexplored area of pixel-based language models, providing insights into their capabilities and limitations.\n\nThe study compares PIXEL with well-known models like BERT and VIT-MAE, clearly understanding where PIXEL stands in the spectrum.\n\nThe findings may potentially raise the interest of researchers in the same field",
            "weaknesses": "The paper focuses on a narrow field and does not provide a comprehensive understanding of Pixel-based Language Models through combining visual and linguistic capabilities, despite the author's claim to offer insights that aid in further development.\n\nMore experiments on visual knowledge are necessary, as results from just two tasks may not provide a concert assessment of PIXEL's visual capabilities.\n\nThe author needs to better explain the motivation behind evaluating PIXEL's visual capabilities. Although PIXEL processes text in a pixel format, it is not inherently clear why visual knowledge should be as crucial for PIXEL as it is for traditional vision models. Therefore, the significance and necessity of assessing its visual knowledge require clearer justification."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel approach by probing the PIXEL model using both linguistic and visual tasks, providing a comprehensive analysis of its capabilities across different layers.\", \"Comparison\": \"The study effectively compares PIXEL to BERT and VIT-MAE, highlighting its position on the vision-to-language spectrum and revealing insights into its linguistic and visual knowledge.\", \"Result\": \"The findings show that PIXEL learns surface-level linguistic information in the lower layers and more syntactic and semantic abstractions in the higher layers, offering valuable insights for future development of pixel-based language models.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"The paper could further clarify the motivation behind choosing specific probing tasks and rendering strategies, as well as the validation of their effectiveness.\", \"Experiment\": \"The experimental setup could benefit from additional experiments to explore the impact of different architectural modifications on PIXEL's performance.\", \"Evaluation\": \"The evaluation lacks a deeper analysis of the implications of PIXEL's performance gap with BERT, especially in semantic tasks.\", \"Significance\": \"While the study provides insights into PIXEL's capabilities, the overall significance and impact of these findings on the broader field of NLP and language modeling remain somewhat limited.\", \"Methodology\": \"The probing approach, while thorough, has inherent limitations that could affect the interpretation of the results, such as the potential biases in probing tasks.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "6": {
        "paper_id": "3994d1090bf46d6f09469ff302dbce0f0e70c9d55c008e60bacb21f5279bc433a8e3dd17f2b29c4533d269983ba4c5463ab94723e14584413bb13ea859f8a559",
        "review_id": "2f1e576bf8fc665474a972cd7c910492e43947d593a699604c6aa15c11b55d5907b62869236c41ef6c60a9fe26529a469882ce1b378e2fc936842bb2d36f1933",
        "human": {
            "strengths": "1. valuable and reasonably thorough study of potential impact of modulating sequence length of generations as a tool for efficiency. If novel and generalizable, the proposed framework could be simple to implement while having potential for high impact\n2. valuable analysis of efficiency/quality tradeoffs at different levels of conciseness\n3. good that the authors explicitly consider carbon footprint of the quality assessment\n4. Good set of comparisons in Section 4",
            "weaknesses": "1. Abstract needs more detail\n2. Claims of novelty are too strong in my opinion given multiple concurrent and previous works relating efficiency to sequence length or even treating it as a given that reduced sequence length would yield desirable properties including relating to efficiency (e.g. https://arxiv.org/abs/2405.13226v1, https://aclanthology.org/2023.emnlp-main.619/, https://aclanthology.org/2023.findings-emnlp.607/, https://aclanthology.org/2022.tsar-1.6/, https://arxiv.org/abs/2406.10278) Much of this is a matter of the writing, which could be edited to be more precise in general.\n3. missing detailed discussion of assumptions relevant to proposed Sprout framework's usage -- is it a single large server? How many models are being served? Can we assume many different users with varied uses, or **do we need to assume that the prompts within a contiguous time window tend to be similar to each other** in order for the proposed quality feedback approach to work as expected?\n4. paper would benefit from presentation of concrete examples of generated answers at different directive levels, that meet or do not meet certain quality constraints. This is especially important as the evaluator is completely automated.\n\nOverall, I believe the work done for this paper and the results presented could be considered to meet the standard for an ACL paper, but there are also multiple glaring weaknesses that I believe should be fixed before I can fully support its publication. I would be happy to raise my score if my concerns are addressed. 2. seems especially critical"
        },
        "llm": "{\n    \"strengths\": {\n        \"Novelty\": \"The paper introduces SPROUT, the first framework specifically aimed at reducing the carbon footprint of generative LLM inference, marking a novel contribution to sustainable AI practices.\",\n        \"Significance\": \"SPROUT demonstrates a significant reduction in carbon emissions by over 40%, aligning NLP with eco-friendly initiatives and addressing an urgent environmental concern.\",\n        \"Analysis\": \"The paper provides a thorough analysis of carbon emissions associated with LLM inference and explores the impact of generation directives on carbon savings and generation quality.\",\n        \"Comparison\": \"SPROUT is compared against various strategies and demonstrates superior performance in both carbon savings and generation quality, closely aligning with an oracle standard.\",\n        \"Methodology\": \"The methodology includes a dynamic optimization approach using generation directives and an offline quality evaluation mechanism, providing a comprehensive system-level solution.\"\n    },\n    \"weaknesses\": {\n        \"Novelty\": \"While SPROUT is novel in its focus on LLM inference, the concept of carbon-aware computing has been explored in other contexts, potentially limiting its novelty.\",\n        \"Related Work\": \"The paper could benefit from a more detailed discussion of related works in the area of sustainable AI and how SPROUT differentiates itself from existing solutions.\",\n        \"Result\": \"The evaluation is based on specific test conditions and may not fully capture the potential limitations or scalability challenges in real-world deployment scenarios.\",\n        \"Presentation\": \"The paper is dense with technical details which may hinder accessibility for readers not familiar with carbon-aware computing or optimization techniques.\",\n        \"Comparison\": \"Although SPROUT is compared to several strategies, the comparison might be strengthened by including more diverse models or configurations.\",\n        \"Methodology\": \"The methodology assumes access to specific API and infrastructure setups, which might limit its applicability in different environments or with closed-source models.\"\n    },\n    \"soundness\": \"4\",\n    \"overall assessment\": \"3.5\"\n}"
    },
    "7": {
        "paper_id": "95022640c7e6dc92e71f4ca71a165b84ef0f7ecc5e64fe6f3a30fb9ae4bc907ddfcd77bec8d2e5dc57757a8ed550f6f387c24b38bf87e4c29ca0984d7e238a79",
        "review_id": "aa37c057a5a41f84e6678785d1e7972d49b8e95077a09f15470cf76f9ef4ebbe379285d9282d52db7a5eb2081a6a3b6265e0a16a7c7ea227bdb7ae6857ad71a2",
        "human": {
            "strengths": "1. The paper provides in-depth analysis of key factors that may affect the performance of neural builders and corresponding improvements, including baseline model Jayannavar et al. (2020), the dataset and the evaluation metric. \n\n2. The paper provides comprehensive evaluation metrics, including shape accuracy, size accuracy, location specification accuracy, and location accuracy. These metrics allow for a thorough assessment of NeBuLa's performance in various aspects of shape construction and placement.\n\n3. The paper utilizes advanced language models such as Llama-2-7B, Llama-2-13B, and Llama-3-8B, which contribute to the model's ability to predict action sequences from natural language instructions. The finetuning of these models on the Minecraft Dialogue Corpus (MDC) results in nearly double the netaction F1 scores compared to previous models.\n\n4. The paper acknowledges and attempts to address the challenges posed by underspecified instructions, such as those involving location descriptions that could be interpreted in multiple ways. The model's ability to handle such ambiguities is a significant strength.",
            "weaknesses": "The organization of the paper could be improved. To fully grasp the paper's contribution, readers need to have a thorough understanding of the baseline method by Jayannavar et al. (2020). For example, at the beginning of Section 3, the author claims that Jayannavar et al.'s (2020) evaluation method for neural agents yields poor performance. However, the subsequent discussion does not seem to provide convincing support for this claim."
        },
        "llm": "{\"strengths\": {\"Evaluation\": \"The paper presents a novel evaluation metric that better captures the semantics of location expressions compared to previous methods, addressing the limitations of the net-action F1 metric.\", \"Result\": \"NeBuLa significantly improves the net-action F1 score, doubling the baseline score of prior work, demonstrating its effectiveness in predicting actions from conversations.\", \"Analysis\": \"The paper provides a detailed analysis of NeBuLa's performance, including error analysis and improvements through finetuning on synthetic datasets.\", \"Data/Task\": \"The use of the Minecraft Dialogue Corpus and Structured Dialogue dataset provides a rich and complex environment for testing language-to-action models, and the synthetic dataset helps in evaluating specific capabilities of the model.\", \"Methodology\": \"The paper introduces a discourse-aware model, NeBuLa, leveraging large language models fine-tuned on dialogue and action sequences, showing methodological innovation.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper thoroughly discusses the challenges of interpreting underspecified instructions and the role of discourse structure in improving action prediction.\"}, \"weaknesses\": {\"Evaluation\": \"The paper could benefit from additional evaluation on more diverse datasets to test the generalizability of NeBuLa beyond Minecraft scenarios.\", \"Result\": \"While the paper shows significant improvements in F1 scores, the impact of these improvements on real-world applications is not fully explored.\", \"Contribution\": \"The paper's contributions are somewhat limited to the specific task of Minecraft action prediction, which may narrow its perceived impact.\", \"Methodology\": \"The methodology could be enhanced by incorporating more complex discourse structures and exploring their effects in greater depth.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"Some parts of the paper, particularly the explanation of the new evaluation metric, could be clearer and more detailed to ensure broader understanding.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "8": {
        "paper_id": "b7fe809a20d7fea59fd2ad757dc37fb4cbd7e70522db3dd825d942d9a4a1a81750d9678b6d401e4630c486c81b6df3d4f2761dead1d46bd3106460f604720d34",
        "review_id": "01632e7885a52da2e39e74a3ae08818c7aa67a25627bc90655703b93c6897ad315ae98e284514ac84b4458661dbf6139841fec7753a257b6de014ad2ebdc2ab3",
        "human": {
            "strengths": "- The evaluation settings involve open-ended generation, which closely aligns with how users interact with LLMs. I believe this is a strength compared to other relevant benchmarks that mostly use multiple-choice questions (MCQs) as the task format.\n- ~~Even though the dataset is relatively small in number~~ (during the discussion period, the authors have clarified the total number of their dataset), the dataset covers a wide range of topics, making it a useful testbed and complementary to previous works.\n- The paper provides insightful analysis, which can serve as a basis for future work in designing better cultural competence evaluations.",
            "weaknesses": "I understand that designing an evaluation metric for open-ended text generation is a challenging problem, and I think this paper is an important first step in this direction. However, while the metrics used in the paper (i.e., lexical variation, top-n generated words, correlation) provide useful insights, they only scratch the surface. For example, the results show that the output varies and includes relevant words, but it is still unclear whether the outputs as a whole make sense or are accurate/valid. Consequently, the main question of whether LLMs have \"cultural competence\" cannot be fully answered using these metrics. The metrics can evaluate some aspects, but they are still limited."
        },
        "llm": "{\"strengths\": {\"Data/Task\": \"The paper introduces a novel focus on extrinsic evaluation of cultural competence in LLMs, specifically in the context of text generation tasks like story generation and open-ended question answering. It provides a large-scale evaluation across 195 nationalities, offering valuable insights into cultural adaptation in LLM outputs.\", \"Methodology\": \"The paper proposes both quantitative and qualitative evaluation methods to assess cultural competence, including lexical variance measurement and TF-IDF analysis for culturally relevant vocabulary. These methodologies are well-structured and offer a comprehensive approach to evaluating cultural competence.\", \"Evaluation\": \"The evaluation covers multiple models and tasks, providing a robust analysis of cultural competence across different contexts. The use of Kendall’s τ rank correlation to analyze the relationship between text similarity and cultural values is insightful.\", \"Analysis\": \"The analysis reveals interesting findings, such as the weak correlation between intrinsic and extrinsic cultural competence measures, and highlights the need for holistic evaluation approaches. It discusses the implications of cultural competence in user-facing applications, emphasizing the importance of comprehensive human evaluation.\"}, \"weaknesses\": {\"Methodology\": \"The reliance on nationality as a proxy for culture is a limitation, as it oversimplifies the complex nature of cultural identity. Additionally, the paper does not account for implicit cues of culture, such as dialect or topical differences, which could affect model outputs.\", \"Evaluation\": \"The paper lacks comprehensive human evaluation to assess the pragmatic correctness and user experience of culturally adapted outputs. This omission limits the understanding of how these adaptations affect real-world interactions.\", \"Result\": \"While the paper provides valuable insights into cultural competence, the weak correlation between text similarity and cultural values suggests that the evaluation methods may not fully capture the nuances of cultural adaptation. Further exploration is needed to understand the impact of these findings.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "9": {
        "paper_id": "67bd6ba7cd53aeb4998a1cad5c967e3f13ae085bbabc2dfc8c4aac64519ac69faf927b27789d2b96f74524d4c076e71fc35cbeaccaf9e054330a3c9984949186",
        "review_id": "301396c2cfa28c8a61792967398cf7469c77cd1a69f25824d9ec38d5bb8e0a85a066bc14901653a43b7293d608c39fd6367c316c51e96023672b828f524285a5",
        "human": {
            "strengths": "The author explore different ways for word sense representations, relying on methods in lexical semantics. \nThe evaluation of their method is comprehensive.\nI appreciated the exploration of out-of-terms vocabulary that is missing in similar works.",
            "weaknesses": "- Some relevant related works are missing (see Comments below) and differences and similarities wrt such works should be highlighted. What do the authors introduce wrt such previous work? How is hateful WIC different from dogwhistles or pejorative language?\n- Lack of expert annotations could introduce bias\n- The ablation study could have been done for this paper instead that leaving it for future work\n- The experimental pipeline is not super clear: it this a sentence or a word classification task? I think the whole pipeline should be better presented and visualized.\n- What's the motivation for this task? How will it be concretly used?"
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel task, HateWiC, focusing on the classification of hateful word meanings within specific contexts, which is an underexplored area in hate speech detection research. The methodology includes innovative approaches like incorporating sense definitions and annotator demographics, providing a comprehensive view of subjective and descriptive aspects of hateful word senses.\", \"Related Work\": \"The paper provides a thorough review of related work, discussing previous studies on hate speech detection at the word level, subjectivity in hate speech detection, and methods for modeling word senses. This contextualizes the research within existing literature, highlighting the novelty and significance of the HateWiC task.\", \"Evaluation\": \"The evaluation is extensive, utilizing cross-validation setups and comparing various models and embedding types. The paper also compares its methods with previous work, demonstrating improvements and providing insights into the effectiveness of incorporating definitions and annotator information.\"}, \"weaknesses\": {\"Related Work\": \"While the paper reviews related work extensively, it could further explore recent advancements in contextualized word embeddings and their application in similar tasks to strengthen its methodological choices.\", \"Intuition/Justification/Motivation/Validation\": \"The paper could provide more detailed justification for the choice of Wiktionary as the primary data source, addressing potential biases and limitations in user-contributed definitions.\", \"Presentation\": \"The presentation of results could be clearer, especially in tables where accuracy drops are discussed. More visual aids or graphs could enhance understanding of the performance differences.\", \"Data/Task\": \"The reliance on binary labels in the evaluation may limit the understanding of hate speech's multifaceted nature. A more nuanced class scheme could provide deeper insights.\", \"Ablation\": \"The paper could benefit from more detailed ablation studies to identify the most effective aspects of incorporating annotator demographics and definitions into the classification model.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "10": {
        "paper_id": "6d79284bd806e97605b4ee88aff1f03b561c3ec54735e057cddb7e4fc96dd5e60a4789891ac16e5fda34153a8e9a8d3ac7180e7a21577c2c143454457694c7b1",
        "review_id": "20607e0b969a691b1096fbed8286e39f815ce2b44de548a8c80be3ae379634657d352dc3ccc74cc4f7297ad01e2a884b4356ab5628439129c46f421b864d7f18",
        "human": {
            "strengths": "(1) The manuscript is easy to follow.\n\n(2) The idea of incorporating multimodal information is interesting.",
            "weaknesses": "(1) The accuracy improvement of MIND distilled LLMs over multiple baselines is minimal. The statement “The results are presented in Table 2, demonstrating significant improvements in both tasks when LLMs are fine-tuned on intentions generated by MIND.” can be misleading, suggesting that the improvement based on MIND is unique, however, many other baselines injected with other commonsense knowledge also achieved improvement. In some cases, they are even better than the LLMs fine-tuned on intentions generated by MIND.\n\n(2) The effectiveness of MIND is evaluated on the IntentionQA benchmark. Is there a potential data leakage? Were they both sourced from the Amazon product dataset?\n\n(3) For the analysis of the impact of human-centric filtering, a manual annotation of another randomly sampled 200 intentions that are NOT abandoned by MIND could be conducted to form the baseline.\n\n(4) The multimodality is framed as a key feature of MIND in the Introduction section. However, the benefit of incorporating multimodality is not explored thoroughly, as the manuscript then seems to focus on evaluating the effectiveness of MIND on the intentionQA benchmark. It would be better if the authors could conduct some in-depth analyses on the incorporation of multimodality.\n\n(5) The framework is only applied to one dataset (Amazon review), raising doubt on the generalizability.\n\n(6) Why is the co-buy relationship studied? In practice, I can imagine that customers often buy multiple things (more than 2) together. Does this study restrict to the purchasing records that only contain 2 items? If these 2-item records are obtained by combining every 2 items out of a multiple-item purchase, then this might lead to a bigger issue. Say a customer bought 4 items. Three of the items were bought together based on one intention. Would this cause any misinterpretation to the proposed framework? Another potential scenario is that there is no intention of purchase. For example, the two items bought together may be entirely unrelated. What is the proportion of such pairs?\n\n(7) Only 90 of 600 workers are deemed qualified. The internal agreement score is also not high. It seems that the annotation is a challenging task even for human. Can the authors further elaborate on the robustness of LLMs?\n\n(8) The prompts are manually designed. Would changing a few words of the prompts significantly impact the outcome? A verification could be conducted. For example, the authors can change the prompts a bit while maintaining the meaning and compute the cosine similarity between the original responses and the new responses."
        },
        "llm": "{\"strengths\": {\"Presentation\": \"The paper is well-organized and clearly presents the MIND framework, detailing each step in the process and providing a comprehensive overview of the methodology.\"}, \"weaknesses\": {\"Evaluation\": \"The evaluation relies heavily on human annotations, which may introduce bias and lacks detailed statistical analysis of the results.\", \"Result\": \"While the results are promising, the paper lacks a thorough comparison with more diverse baseline methods.\", \"Analysis\": \"The analysis could be deeper, particularly in exploring the reasons behind the performance of the proposed filtering mechanism.\", \"Data/Task\": \"The task and data are limited to specific domains (Electronics and Clothing), which may restrict the generalizability of the findings.\", \"Presentation\": \"Some sections of the paper, such as the introduction and related works, could be more concise to improve readability.\", \"Comparison\": \"The paper does not provide a comprehensive comparison with state-of-the-art methods beyond FolkScope.\", \"Methodology\": \"The methodology, while innovative, lacks detailed justification for the choice of LVLM and the specific prompting strategies used.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "11": {
        "paper_id": "2dfc1c1d488c857378f7f4bdf8cb2eff1518d74a785b1a2dd4e3a3da1ce8100fb4f10c02b66f38ed7c6e833e422a4697dd96b1cf8cb7c4c5129870d2d1ddf611",
        "review_id": "c846b199da1482283e145591a491e9196bbcadb4bdf114f47ee2b5c5435a9e359469e44f1441c2d2e383917a13c12b5d3edc2f8eb22698d45e33baaf5ddbe78b",
        "human": {
            "strengths": "The contributions are significant, where the FLAME Collection dataset could be a valuable resource for training future open-source evaluator LMs and the evaluation suite organized by this paper could be a good reference to test future open-source evaluator LMs.",
            "weaknesses": "(W1) The paper does not disclose the rating of each training task that is used to train the FLAME-RM model. Including such information could be valuable to determine which dataset is beneficial.\n\n(W2) There are a few datasets for math and coding which are all in pairwise ranking formats. The authors could show if FLAME or FLAME-RM could also perform pointwise ranking on math or coding evaluation as well.\n\n(W3) The example in Figure 1 shows that the evaluation criteria is \"Is all of the information in the summary fully attribute to the source article?\". How is the evaluation criteria designed for all the tasks inside the FLAME Collection? Also, how were the evaluation criteria devised for the tasks used for evaluating FLAME and FLAME-RM?\n\n(W4) For Best-of-N Sampling experiment, the authors only tested on a coding task. Were N^2 comparisons conducted to measure the winrate among responses when choosing the final prediction? More details of the experiment is needed and testing on a math benchmark could further highlight the effectiveness of the FLAME models.\n\n(W5) I really hope that the dataset and models could be released!"
        },
        "llm": "{\"strengths\": {\"Related Work\": \"The paper provides a comprehensive review of existing literature on autoraters, highlighting the novelty of FLAME in comparison to task-specific and LLM-AS-A-JUDGE autoraters.\", \"Evaluation\": \"FLAME demonstrates strong performance across a wide range of benchmarks, outperforming proprietary models like GPT-4 and CLAUDE on several tasks.\", \"Data/Task\": \"FLAME is trained on a large and diverse collection of 5M+ human judgments across 100 quality assessment tasks, curated from publicly available data.\", \"Contribution\": \"The introduction of FLAME and its novel tail-patch fine-tuning strategy significantly advances the field of LLM autoraters, offering a less biased and more effective solution.\", \"Methodology\": \"The paper presents a solid methodology for training LLM autoraters using supervised, multitask fine-tuning on standardized human evaluations.\"}, \"weaknesses\": {\"Experiment\": \"The paper does not sufficiently explore alternative training approaches like RLHF, which could enhance the robustness of FLAME.\", \"Evaluation\": \"While FLAME performs well on many tasks, it shows suboptimal performance on ExpertQA, indicating a potential gap in technical knowledge.\", \"Data/Task\": \"FLAME's training data is primarily in English, limiting its applicability in multilingual contexts.\", \"Comparison\": \"The paper could benefit from deeper comparisons with other state-of-the-art models in terms of bias and performance across additional benchmarks.\", \"Methodology\": \"The methodology primarily focuses on English data and short context lengths, which might not generalize well to long-context tasks.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "12": {
        "paper_id": "dc5aed41d95d0b2d731d6923ae17607d6bc418f164017615f03c209e02a02d759c07ff7055eca13012ad25abad1038f5326d6dd57410436d876c8c8e2ef59dd8",
        "review_id": "e7099d665f243ee5263c286182ab54829beaec952dc9c7477885bbd49ac5113677d1a0056b622da1ba573f7f08b009579ef420808794e758f85611eb4315d028",
        "human": {
            "strengths": "- The authors focused on speech-specific risk, such as Hostility, Malicious imitation and stereotypical biases, and built a taxonomy with them\n- The authors prepared synthesized speech data and evaluated LMM capability using the data",
            "weaknesses": "- There is no explanation of what the author did for avoiding \"position bias\" in multiple-choice setting. This is an important point if the experiments are feasible."
        },
        "llm": "{\"strengths\": {\"Data/Task\": \"The paper introduces a novel and comprehensive speech-specific risk taxonomy, focusing on paralinguistic cues that transform low-risk textual content into high-risk speech. This is a significant contribution to the field, addressing a largely unexplored area in multimodal models.\", \"Evaluation\": \"The authors create a high-quality synthetic speech dataset, meticulously curated and filtered by human annotators, to evaluate the capabilities of recent large multimodal models. The evaluation protocol is robust, testing several models and prompting strategies.\"}, \"weaknesses\": {\"Experiment\": \"The experiments are limited to a synthetic dataset due to the constraints of current TTS systems, which may not fully capture the complexities of human-generated speech.\", \"Significance\": \"While the taxonomy and dataset are novel, the paper's impact may be limited by the current inability to evaluate on real human speech data, reducing its immediate applicability.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could benefit from a clearer explanation of how the taxonomy and dataset can be applied in real-world scenarios, and more detailed discussion on the limitations of using synthetic data.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3\"}"
    },
    "13": {
        "paper_id": "879e44e2a97d6fd4de4a8ce0a04b47dff57d0bf241d5bd8ca7bab9cc835cb310bdea940720d2598c6ae5f590e4e98bf12b6d06d46c0541e4beac2f465a1c446d",
        "review_id": "f625103307b75c57bdd3928e6f56746559e927eda0a50cd6453988b87b6ad5b2e5ca8956b4a88a0cf39ba31c21e86bb9f3ddb0fcb96a94ed63b9b3792446ef70",
        "human": {
            "strengths": "1. Meaningful topic to investigate. The investigation addresses an important aspect of LLM application in software development, offering valuable insights that could influence future LLM construction and their practical applications.\n2. Novel metric design. The introduction of the Modularity Score (MoS) as a new metric to quantify code modularity provides a measurable and objective way to assess modularity in code snippets, advancing the field of code analysis.\n3. Comprehensive analysis. The paper presents a thorough investigation into the impact of modularity on code generation performance. This detailed analysis sheds light on a previously underexplored area, contributing to a deeper understanding of LLM capabilities.",
            "weaknesses": "1. Limited model diversity. The study could benefit from including a wider range of models, particularly closed-source models like GPT-4, Claude, and Gemini, to provide a more comprehensive understanding of how different LLM architectures respond to modularity.\n2. More programming languages are expected. The evaluation is limited to Python, which may reduce the generalizability of the findings. Expanding the analysis to include other programming languages could provide a more holistic view of modularity's impact across diverse coding environments."
        },
        "llm": "{\"strengths\": {\"Novelty\": \"The paper introduces a novel metric for quantifying code modularity, challenging conventional wisdom about the importance of modularity in code generation by LLMs.\", \"Evaluation\": \"The study conducts experiments with models of varying sizes and uses datasets like APPS and CodeContests to evaluate the impact of modularity on performance.\", \"Result\": \"Findings suggest that modularity may not be crucial for performance, providing a new perspective on code generation practices.\", \"Analysis\": \"The paper provides a detailed analysis of modularity's impact, including correlation studies and preference experiments, which help to understand the nuanced relationship between modularity and performance.\", \"Methodology\": \"The methodology is robust, introducing a quantitative metric for modularity and employing a controlled experimental setup to isolate the effects of modularity.\"}, \"weaknesses\": {\"Evaluation\": \"The experiments are limited to Python and a few LLM configurations, which may restrict the generalizability of the results.\", \"Result\": \"The performance differences observed are marginal, which might limit the perceived impact of the findings.\", \"Analysis\": \"The negative correlation between modularity and performance is weak, and the paper does not fully explore alternative explanations or confounding factors.\", \"Methodology\": \"The study relies heavily on the newly introduced metric, and its validity and reliability are not extensively validated against existing metrics or human judgment.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "14": {
        "paper_id": "0579894094a90fddcf940a02d601d4f016a0b3a1cf20de88684ab81f822821e7e1b3b0a9c38858480d206e5a5f850c60a9889c1a1c3ab6f3695b7a0bed1d579f",
        "review_id": "fd0270690d77c7417ddf0c85c7164893d4d1ea7c59f751325555bd7c891c1f314fe2eea61878f0fc54c199c84b5af87e136331056d289a4ee3de646f295b12dd",
        "human": {
            "strengths": "- The paper tackles a new task related to writing planning.\n\n- The paper shows extensive experiments, and the authors promise to make their classifiers available, which could be a good addition to the community.\n\n- The paper reveals some interesting insights (Section 5)",
            "weaknesses": "- The paper has room for improvement to enhance its clarity:\n\n    -  **Consistency**: For example, plan, schema, and approach explain the same thing - this is repeated for different things. \n\n    - In Figure 2, \"label set\" is used for labels, but in line 321, \"category\" is used.\n\n    -  What does *t* refer to in line 322?\n\n- An overview graph can help explain the different components of the paper.\n\n- The authors describe a \"plan\" as the set of labels in an article for one schema type (red circles or blue squares) (e.g., Affiliation, Stance), as depicted in Figure 1. What if a \"plan\" is a combination of both a square and a circle? \n\n- Two annotators annotated the dataset: Given that the agreement is moderate, how did they solve disagreements? \n\n- An overview (using descriptive statistics) can help understand the data, such as the distribution of schema labels per news article type in the \"silver-standard dataset\" and the dataset used for the main experiments."
        },
        "llm": "{\"strengths\": {\"Experiment\": \"The paper introduces novel metrics for evaluating latent plans in news articles and demonstrates their effectiveness in predicting source categorization schemes. It collaborates with professional journalists to validate the inferred plans, achieving a high F1 score, and explores the potential for computational journalism tools.\", \"Data/Task\": \"The paper releases a large corpus, NewsSources, with schema annotations for 4 million articles, providing a valuable resource for further study. It adapts and introduces schemas to describe sourcing criteria, contributing to understanding journalistic planning.\"}, \"weaknesses\": {\"Presentation\": \"The paper lacks clarity in exposition, with dense sections and complex methodologies that may be difficult for readers to follow. It could benefit from clearer explanations and structuring.\", \"Methodology\": \"The methodology relies heavily on supervised classifiers, which may not fully capture the nuances of latent planning. The paper acknowledges the limitations of the schemas used, indicating the need for more optimal approaches.\", \"Data/Task\": \"The paper only considers English-language sources, potentially limiting the applicability of findings to non-Western news outlets. The dataset may face extraction and labeling biases.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper's discussion of schemas and metrics is complex and may not be fully accessible to all readers. It could benefit from more detailed explanations and examples to illustrate key concepts.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "15": {
        "paper_id": "097df36ed7122c4d448a484b718564288388d7a9a87927248349217ff14d5a1a5577608e410ff757ea9dcf860d3d3e8c23676521fd59fb16fe65e51d613a9b1e",
        "review_id": "79ae194310b793e3ff643b36c44d6c442392503a7265cee194ce33a39078d6bffab13f061dd53e02071c1601c91e9d8caca7efbb02c3257bbd014fd09fdd61fa",
        "human": {
            "strengths": "- **Comprehensive experiments**: this paper conducts three types of lexical semantic tasks and shows the usefulness of their method. This wide range of validations is informative to the field of lexical semantics.\n - **Well-written paper**: The paper provides a clear introduction to the background of this field. This clear exposition makes the paper accessible not only to experts but also to newcomers in the field, facilitating a broader understanding and appreciation of the work.",
            "weaknesses": "**Misleading claims of state-of-the-art performance**: The paper claims state-of-the-art performance on three tasks. However, it becomes evident that the model achieves state-of-the-art performance on only one task (WiC). For the other two tasks (LSC and WSI), while the model performs well, it does not surpass the best existing models. This overclaiming undermines the credibility of the paper and can mislead readers and reviewers about the true contributions and performance of the proposed method."
        },
        "llm": "{\"strengths\": {\"Intuition/Justification/Motivation/Validation\": \"The paper introduces LlamaDictionary, a novel fine-tuned large language model designed to generate sense definitions for words in-context, achieving state-of-the-art results in definition generation and lexical semantics tasks.\", \"Experiment\": \"The paper demonstrates the effectiveness of LlamaDictionary and Flan-T5 as preprocessing tools for large-scale word meaning analysis, setting new state-of-the-art results in Word-in-Context, Word Sense Induction, and Lexical Semantic Change tasks.\", \"Presentation\": \"The paper is well-organized with a clear explanation of the methodology and results, making it accessible and informative for the readers.\", \"Methodology\": \"The paper employs instruction tuning with lexicographic resources and parameter-efficient fine-tuning techniques, enhancing the performance of large language models in generating sense definitions.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper provides a comprehensive discussion of the challenges in modeling lexical semantics and the advantages of using generated sense definitions for word usage representation.\"}, \"weaknesses\": {\"Result\": \"While achieving state-of-the-art results, the paper's reliance on generated definitions may lead to inconsistencies in style and wording, affecting the uniformity of definitions.\", \"Data/Task\": \"The paper is limited to English data due to the scarcity of available benchmarks for other languages, which might restrict its applicability across different linguistic contexts.\", \"Comparison\": \"The comparison with previous works is limited by different training data usage, particularly for unseen benchmarks, which might affect the fairness of the evaluation.\", \"Contribution\": \"Although the paper makes significant contributions, the novelty could be enhanced by exploring additional languages or integrating more diverse datasets.\", \"Methodology\": \"The paper's methodology is constrained by the high computational costs associated with fine-tuning large language models, potentially limiting broader experimentation.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "16": {
        "paper_id": "e8ebc5efec5ffb0aeac12a36b45c55c083f8734568c9f8dc24efc1dee4200c287b01ed316d7ad2a7e4b33722ffa8d8e047ce846203dccec12b8475ec3940f1c6",
        "review_id": "733f35bdd2261104e11373f459f644ea5673355fc1361f95a75c90fe7f5183412ad7e0461c99798369b65d49afeadd0290fd5aef585ef8ab3fa010ae38270b9d",
        "human": {
            "strengths": "1.The motivation of this paper are clear and reasonable.\n2.The study provides thorough empirical evaluations, demonstrating the superiority of the SMILE method over baseline approaches and confirming the optimal performance of GPT-4o in generating multi-turn dialogues.",
            "weaknesses": "1.The approach heavily relies on the capabilities of GPT-4o, which may not be accessible to all researchers due to potential cost and availability constraints. This dependency could hinder reproducibility and wider adoption of the method.\n2.While the study evaluates the basic characteristics and diversity of the dialogues, it lacks a comprehensive assessment of the emotional and contextual accuracy of the generated conversations, which is crucial for mental health support systems.\n3.The use of automated methods to generate dialogues may introduce or perpetuate biases present in the training data of the language models, which could affect the quality and fairness of the mental health support provided."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces SMILE, a novel method leveraging ChatGPT to convert single-turn dialogues into multi-turn conversations, addressing data scarcity in mental health support.\", \"Evaluation\": \"Empirical evaluations demonstrate that MeChat, trained on the SMILE CHAT dataset, excels in generating empathic, professional, helpful, and safe responses.\", \"Intuition/Justification/Motivation/Validation\": \"The motivation to create a large-scale, diverse, and high-quality multi-turn chat dataset for mental health is well-justified, given the challenges in gathering real-life data due to privacy concerns.\", \"Result\": \"The resulting SMILE CHAT dataset, consisting of 13k dialogues, is diverse and high-quality, and the fine-tuned models show improved performance in mental health support.\"}, \"weaknesses\": {\"Data/Task\": \"The dataset is generated from LLMs and thus may inherit biases or limitations inherent in these models.\", \"Methodology\": \"While the method is novel, it relies heavily on the capabilities of ChatGPT, which may not fully capture the nuances of real-life multi-turn dialogues.\", \"Evaluation\": \"The evaluation is limited to expert reviews and lacks comprehensive automatic evaluation metrics tailored for mental health support.\", \"Result\": \"The paper does not provide a detailed comparison with existing real-life datasets, which could further validate the practical utility of SMILE CHAT.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "17": {
        "paper_id": "e44ac2911e12a96bb30c0bedc500d61361abe7e1d7b6c5c540cef584f4e4c9cbc208580a893cec5dffaaf2148f6ef3841c86b3a8dd186f7236d4606f2e131fc0",
        "review_id": "12e8bd0f9f1b6990744cb37063db88bc49d375b9fb9a4c14764eb035a37a38683819c7bb199578cfeb3066f5b4e17c98d0ae26e25de1ef19faefe641f49471ed",
        "human": {
            "strengths": "1) A novel methodology in prompt tuning, addressing the instability and environmental dependency of RL methods.\n2) The method is shown to be effective on agent models larger than 7B, indicating its scalability.\n3) Applicability across various tasks and model sizes suggests a broad impact and utility in the field of natural language processing.",
            "weaknesses": "The paper does not cover experiments beyond the scope of medical and legal domains, which limits the generalizability of the results."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces StablePrompt, a novel RL-based method for prompt tuning using Adaptive Proximal Policy Optimization (APPO), which mitigates the instability of RL and ensures training stability while maintaining a flexible search space.\", \"Novelty\": \"StablePrompt is the first RL-based approach that effectively works with agent models larger than 7B, addressing the inherent instability and environmental dependency of RL in prompt tuning.\", \"Significance\": \"StablePrompt achieves State-of-The-Art performance across various tasks, including text classification, question answering, and text generation, demonstrating robustness to different model types and sizes.\"}, \"weaknesses\": {\"Experiment\": \"While the paper presents strong results on several datasets, it lacks experiments in domains significantly beyond the scope of prior learning, such as medical and legal fields.\", \"Methodology\": \"The paper's methodology could be seen as complex, with the introduction of an anchor model and adaptive updates, which may require careful tuning and understanding for effective implementation.\"}, \"soundness\": \"4.5\", \"overall assessment\": \"4\"}"
    },
    "18": {
        "paper_id": "836ea4c8f43f70f4a1a199bf1ea5983bdb728376e0fd901c7ecee68789155bac043dfa2eecf0a372086d006057f356a483171da718951a1aa27141579d7721f2",
        "review_id": "b6ece5fbe8b61e040b8d3010840569c23a26531bb5a9344a4782e0f53c1f4d242abe9d58494debc67e805e1dde642898096dc3e1fa29f566eff3ed8c2b50aab4",
        "human": {
            "strengths": "1. The paper introduces a creative way to enhance the performance of large, pre-trained language models without fine-tuning them. By generating focused prompts using a smaller model, the approach maximizes the utility of LLMs in detecting communities.\n2. The method is rigorously evaluated on various tasks, including community detection, bot detection, and news media profiling, using data from different platforms (Reddit and Twitter).",
            "weaknesses": "1. Social media has a large community base, and the most important goal is to find similar communities within big data. However, the experiments in the article are not convincing enough. First, the test samples are too small (550, 232, 155), which lacks sufficient persuasive power. In such cases, fluctuations in a small number of data points can greatly impact the metrics. Secondly, the maximum user number set in the article is 6, which is too small for community detection. The authors did not prove their method's effectiveness with larger user numbers. Lastly, the additional time cost of generating focus areas during inference does not significantly improve results with the proposed method.\n\n2. The article lacks other baselines, such as using simple similarity methods to cluster user information."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel approach using reinforcement learning to enhance the prompting of large language models (LLMs) for community detection on social media, which is a significant methodological advancement.\", \"Evaluation\": \"The evaluation is comprehensive, covering multiple datasets including Reddit and Twitter for intrinsic and downstream task assessments, showing the robustness of the proposed method.\", \"Data/Task\": \"The paper addresses the challenging task of community detection in emerging news events settings, which is highly relevant and impactful in social media analysis.\", \"Result\": \"Results show significant improvements in community detection and downstream tasks when using the proposed focus areas, demonstrating the effectiveness of the method.\"}, \"weaknesses\": {\"Experiment\": \"The experiments are limited to English and specific social media platforms, which may restrict the generalizability of the findings.\", \"Evaluation\": \"The evaluation relies heavily on the performance of the smaller LLM and the specific reward functions used, which could be further explored or optimized.\", \"Result\": \"While improvements are noted, the paper lacks detailed statistical analysis of the results, which would strengthen the claims.\", \"Data/Task\": \"The paper uses a fixed number of users (six) for community detection, which may not reflect real-world scenarios where community sizes vary.\", \"Comparison\": \"There is limited comparison with other state-of-the-art methods in community detection, which could provide more context on the significance of the improvements.\", \"Methodology\": \"The paper treats LLMs as black-boxes, which might limit understanding of how focus areas influence model behavior.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "19": {
        "paper_id": "fe19894e94c9a13244ce3a9a177c6a55f46abea52231ba756b6846aa34639022d65d3ff552096e657551d496cb061fadadb7a6c87828be9ba6cdba85daab9472",
        "review_id": "8adb8dcb1022cd71971a5eb983784546b3ba2aaf963d1dafdf387a89862005d3309e56362340aa0a9da5af15e1b91a6f9f3c78fe90ef0bff2c768e8e0099285e",
        "human": {
            "strengths": "1. The paper introduces IMAGINE, a novel method to utilize imagined images to mitigate the reporting bias.\n2. The paper creates synthetic training data to jointly train on both text and visual data.\n3. The method achieves good performance compared with SOTA baselines across different benchmarks.\n4. The paper is written in a good structure.",
            "weaknesses": "1. Since the paper claims using both visual and textual information to answer the commonsense reasoning questions, I think it should engage some state-of-the-art vision-language models as baselines.\n2. Lacks model performance on larger scale models like LLaMA2 or 3.\n3. Although the method improves the PLM’s performance on zero-shot commonsense reasoning, it is unknown whether it can be a general approach that can be engaged into general PLMs."
        },
        "llm": "{\"strengths\": {\"Result\": \"IMAGINE demonstrates superior performance in zero-shot commonsense reasoning tasks, outperforming existing methods and even some large language models.\", \"Data/Task\": \"The synthetic VQA dataset created for training effectively combines textual and visual signals, enhancing reasoning capabilities across diverse benchmarks.\", \"Comparison\": \"IMAGINE consistently surpasses state-of-the-art zero-shot reasoning models, showcasing its effectiveness.\", \"Presentation\": \"The paper is well-organized, clearly presenting the methodology and experimental results.\", \"Methodology\": \"The integration of a text-to-image generator and visual encoder with PLMs is novel and addresses human reporting bias effectively.\"}, \"weaknesses\": {\"Methodology\": \"The reliance on machine-generated images can lead to misalignment with textual inputs, potentially affecting inference accuracy.\", \"Comparison\": \"While IMAGINE performs well, the comparison with LLMs is limited to specific tasks, and further exploration on larger models could be beneficial.\", \"Result\": \"The paper highlights the limitations in computational efficiency due to the additional processing required for visual signals.\"}, \"soundness\": \"4.5\", \"overall assessment\": \"4\"}"
    },
    "20": {
        "paper_id": "377fbe8b0690a89fc7312404e7a3359d5b68cfd12ba17282826f3bfb482e2b5856afdc1e67e9bc9ff9f64b6f045fdd167d783ca69952ff67ebaefeb6e6d4c94b",
        "review_id": "f85557b6a4c8669c75de3ae17f6d88050c02de332d4e2197fae6bf8f6bb1ac61fe866103fe10b09f1960e345457efbfdf5a9a9f962470187f7cbae67a51a1456",
        "human": {
            "strengths": "1. The introduction of HAST, which combines the strengths of self-training and active learning, represents a novel approach to reducing labeled data requirements in text classification.\n2. The paper thoroughly reproduces four existing self-training approaches, providing a solid foundation for comparison.\n3. The experiments demonstrate that HAST achieves competitive results on three out of four datasets using significantly less labeled data.",
            "weaknesses": "1. Self-training with pseudo-labels inherently risks propagating errors from incorrect pseudo-labels. While the authors introduce regularization techniques to mitigate this, the paper does not fully address how noise in pseudo-labels can impact long-term model performance and stability.\n2. Some elements of the proposed method, such as the use of KNN for pseudo-labeling, may overlap with existing techniques in semi-supervised learning and active learning. What are the unique contributions and advantages of HAST over these existing methods?\n3. The effectiveness of self-training heavily relies on the quality of the unlabeled data used to generate pseudo-labels. If the unlabeled data is noisy or not representative of the target domain, the performance of the proposed method may degrade."
        },
        "llm": "{\"strengths\": {\"Novelty\": \"The paper introduces HAST, a novel self-training strategy that effectively complements active learning with pseudo-labels, demonstrating significant improvements in sample efficiency for text classification tasks.\", \"Experiment\": \"Extensive experiments are conducted on four text classification benchmarks, showcasing the effectiveness of HAST compared to four existing self-training approaches.\", \"Result\": \"HAST achieves competitive results using only 25% of the data compared to previous methods, demonstrating its efficiency in reducing labeled data requirements.\", \"Data/Task\": \"The paper evaluates its approach on established text classification benchmarks, ensuring the relevance and applicability of the results to real-world scenarios.\", \"Comparison\": \"A fair comparison is made with reproduced self-training approaches under consistent settings, providing a clear understanding of HAST's relative performance.\", \"Methodology\": \"The integration of self-training with active learning is well-motivated and methodologically sound, leveraging contrastive representation learning to enhance pseudo-label effectiveness.\"}, \"weaknesses\": {\"Result\": \"The reliance on pseudo-labels may introduce noise, and the effectiveness of HAST is somewhat dependent on the model's ability to handle this noise.\", \"Data/Task\": \"The study focuses solely on single-label classification tasks, limiting its applicability to multi-label scenarios.\", \"Comparison\": \"While the paper reproduces existing approaches, it does not explore the impact of varying hyperparameters extensively, potentially affecting the generalizability of the findings.\", \"Contribution\": \"The paper's contribution is somewhat limited by the computational feasibility of larger models, restricting its exploration of recent large language models.\", \"Methodology\": \"The use of fixed confidence thresholds in previous approaches highlights a limitation that could have been addressed more comprehensively in the proposed method.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "21": {
        "paper_id": "1392430b924a4c6d0cc50769b258c7ea0dedd9db07f11de661310db1a708ec96501266b505c601131cfd645b1b2149a1b0374146345d6ca990cf6d4ebf929e2b",
        "review_id": "a85515f74c49ddc8b6476a8d358b0d328c615bae32318be55031efb888d8babee0da3d895b71481a980c5c118e8f5cfd0119a2ed5cbb152d492e8fa2518478aa",
        "human": {
            "strengths": "The paper reads very well and it is invaluable to check and raise awareness about the common practice in NLP research that is taken for granted but might not be suitable, especially in a niche, interdisciplinary field like sign language processing.\n\nI enjoy reading the linguistic part as well as the empirical part, although the two do not correspond perfectly, it is reasonable as explained by the authors in an honest way in subsection 4.2.\n\nAlso, the analysis of individual interpreters is informative and inspiring, which could be further developed into another key insight of the paper, i.e., translation of different signing modes (non-native interpretation vs. native signing).",
            "weaknesses": "Although the authors well justify their main argument of reconsidering sentence-level sign Language translation, they do not show clearly how its natural alternative, document-level translation, could come to the rescue (except for slightly increased but still low BLEU/BLEURT scores). \n\nIn general, the paper provides great observations and reflections but not a clear way going forward (which is of course harder).\n\n(below also as concrete comments and questions)\n\n- In the last setup in subsection 4.1, is there a reason to provide the annotator with full-text context instead of full-video context? Isn’t the video/signing context more about what we should expect as input for ASL-to-English translation? In addition, when discussing document-level MT, we should also include the context after the current clip, i.e., the whole document. The authors later realized and wrote about it.\n\n- On line 567, \"for 33.3% of test instances, the annotators judged that they were unable to understand key details of the signed content from the sentence in isolation which they later understood from additional context\": natural questions would be, what are the exact percentage numbers of understanding when adding the context step by step as designed in the experimental setups? Do they also increase monotonically like BLEU and BLEURT? It is not quite clear what you mean by “they later understood from additional context”, does it mean 100% understood given the last setup? The claims would be stronger if you could provide these numbers."
        },
        "llm": "{\"strengths\": {\"Presentation\": \"The paper is well-structured with a clear organization of related literature, providing a comprehensive background on sign language translation and its challenges.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper thoroughly discusses the limitations of sentence-level sign language translation, providing detailed examples of linguistic phenomena that require discourse-level context.\", \"Analysis\": \"The paper presents a novel human baseline study, highlighting the necessity of context in sign language translation and offering a disaggregated analysis by interpreter.\"}, \"weaknesses\": {\"Experiment\": \"The study is limited to a single dataset and language pair, which may not generalize to other sign languages or domains.\", \"Presentation\": \"The paper could benefit from clearer visual examples to illustrate the described linguistic phenomena.\", \"Intuition/Justification/Motivation/Validation\": \"The motivation for focusing on sentence-level translation is not thoroughly challenged; more justification for the chosen experimental setup could strengthen the argument.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "22": {
        "paper_id": "5c58ff3e7d18e959e791267310c0979f81c58670fdb79beb8ce1616ed475ba7ad3c6d0d0bb2c360ff7807adb5eb80b82ae5b168e9c3a82425376d8eb6d08fb8a",
        "review_id": "995a4e252af351eaf52dcc6f605ab92e307975281e024ab9c130b7d76d91a2ea21862f916ebc6b864045a30df354ac920151beb7b2c0d2690ccc04d5885d54f7",
        "human": {
            "strengths": "The proposed method can retain or improve multi-hop QA performance (EM, F1) while achieving high compression rate. Compared to the naiive approach in which raw context is given to LLM, the proposed method can improve performance more effectively with increased size of contexts by condensing relevant information into a short text before answering questions.",
            "weaknesses": "* (W1) **Resolved** While the proposed method can achieve high compression rate, its inference speed is slower than the naiive baseline with raw uncompressed contexts (Table 5). In this sense, compression rate seems not a good metric to measure efficiency of a QA system unlike computation time, amount of processed tokens, or GPU memory usage which directly contribute to the inference cost. Since the authors argue high compression rate as one of key contributions of the proposed method, more discussion is needed to support importance of compression rate in QA task.\n\n* (W2) Missing Baselines:\n  - While the paper compares the proposed method against long-context LLM and compression based methods, it does not compare against recent methods that incorporates multi-hop information seeking. Since question-conditioned sequential compression text generation in the proposed method is analogous to the iterative information seeking of these method, I think these methods can be good baselines for the proposed method.\n  - Examples of multi-hop information seeking approaches:\n    - [Knowledge Graph Prompting for Multi-Document Question Answering](https://arxiv.org/abs/2308.11730)\n    - [Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading](https://arxiv.org/abs/2310.05029)\n    - [A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts](https://arxiv.org/abs/2402.09727)\n  - Also, the problem of creating a summary of relevant information to a question (or query) is also considered in the field of query-focused summarization task, and the task has been recently extended to multi-document scenario (query-focused multi-document summarization). The field seems directly relevant to the proposed method.\n    - You can find relevant papers in the Related Work section of [Open domain multi document summarization A comprehensive study of model brittleness under retrieval](https://arxiv.org/abs/2212.10526)."
        },
        "llm": "{\n  \"strengths\": {\n    \"Methodology\": \"The paper introduces COMP ACT, a novel framework employing an active strategy for document compression. It effectively captures essential context from long documents and integrates information across multiple documents, which is crucial for complex QA tasks.\",\n    \"Result\": \"The experiments demonstrate significant improvements in compression rate and QA performance on multi-hop question-answering datasets, achieving a high compression rate of 44x, which indicates cost-efficiency.\"\n  },\n  \"weaknesses\": {\n    \"Related Work\": \"The paper could provide a more comprehensive comparison with existing compression methods and frameworks, detailing how COMP ACT stands out or improves upon them.\",\n    \"Evaluation\": \"The inference time for compression is lengthy, which could limit practical applications. The paper acknowledges this but does not offer concrete solutions or alternative strategies.\",\n    \"Data/Task\": \"The training data is limited to HotpotQA, raising concerns about generalizability across other datasets without retraining or additional fine-tuning.\",\n    \"Comparison\": \"While the paper compares COMP ACT with several baselines, it lacks a detailed analysis of the trade-offs between compression rate and QA performance, especially concerning inference latency.\",\n    \"Contribution\": \"The paper's contribution is significant but could be more impactful if the framework's inference speed is optimized or if it addresses the completeness judgment errors more robustly.\",\n    \"Methodology\": \"The methodology relies heavily on the assumption that active compression strategies will always lead to better QA performance, which may not hold in all scenarios.\",\n    \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could benefit from more detailed explanations of the framework's components and the rationale behind certain design choices, such as the early termination condition.\"\n  },\n  \"soundness\": \"3.5\",\n  \"overall assessment\": \"3.5\"\n}"
    },
    "23": {
        "paper_id": "d4b7c0f43de4e733fa9be1e4b306bef452d970ce153ccaf482e4404c7ba96a9419a985879bc4da0348120ac7640b570d9de04c82e7fed5df58ac4979e41a5a4e",
        "review_id": "f2d1ef28bc398577395054ec5ef2fdf4aede57361c10b41bc47a25b0a37f81a3750683ec129d20472ca03960a73d87db1f4ba06487a99bab2d5a5d84b1727f3b",
        "human": {
            "strengths": "- I find the paper to be well written and that most of the design choices are reasonable.\n- The paper introduced a dataset with rich annotation, which could be of value to people working in this field. Also, the experimental results of using the dataset in supervised fine-tuning setting also support the effectiveness of the data.\n- The paper includes detailed comparison with previous works, the contribution of each reward components, etc.",
            "weaknesses": "- **unrealistic dataset:** Since the dialogue in the dataset is mainly generated with LLM, the curated dataset is likely to be unrealistic. For instance:\\\n(1) The persona might not be consistent within a dialogue, which is a phenomenon that has been observed in LLMs.\n(2) The dialogue flow is randomly generated, which could result in discrepancy between real-world negotiation dialogues and the generated ones. although the human intervention might be helpful in removing unnatural dialogue flows, the overall distribution will be different from real-world ones.\\\nIt is perhaps a good idea to conduct analysis on the quality of the generated dataset, and if possible, compare the LLM-generated data to human-generated ones.\n\n- **Persona analysis:** While \"persona\" is one of the main focus of this research, I do not see much discussion regarding persona (and it's affect on negotiation) in the results/analysis part. It would be nice if you can add some more analysis regarding persona."
        },
        "llm": "{\"strengths\": {\"Related Work\": \"The paper provides a comprehensive review of existing negotiation dialogue datasets and systems, highlighting gaps and positioning the current work within the broader research context.\", \"Experiment\": \"Extensive qualitative and quantitative analyses demonstrate the effectiveness of TRIP NEGOTIATOR in generating personalized responses during negotiation.\", \"Result\": \"The proposed model achieves superior performance across various automatic and human evaluation metrics compared to several baselines, indicating its effectiveness.\", \"Data/Task\": \"The introduction of DEAL, a novel dataset for personalized integrative negotiation in tourism, is a significant contribution to the field.\", \"Comparison\": \"The paper includes a thorough comparison with multiple baseline models, showing clear improvements in performance metrics.\", \"Contribution\": \"The work introduces a new task of personalized integrative negotiation and develops a novel dialogue system, TRIP NEGOTIATOR, that effectively generates persona-aware responses.\", \"Presentation\": \"The paper is well-structured, with clear explanations of the methodology, experimental setup, and results.\", \"Methodology\": \"The use of reinforcement learning with Proximal Policy Optimization and the design of unique rewards for generating persona-aware responses are innovative aspects of the methodology.\"}, \"weaknesses\": {\"Result\": \"The model's performance might be affected in scenarios involving continuous, brief, or direct responses, leading to repetitive or inconsistent outputs.\", \"Analysis\": \"The analysis could benefit from additional insights into the model's behavior in diverse negotiation scenarios.\", \"Data/Task\": \"The dataset creation process involves manual correction due to hallucinations from the language model, which could impact scalability.\", \"Comparison\": \"While the model outperforms baselines, the comparison could be strengthened by including more recent state-of-the-art models.\", \"Methodology\": \"Optimizing reward weights is complex and may extend training and validation times, indicating potential inefficiencies.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more detailed explanations of the reward function and its components to enhance understanding.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "24": {
        "paper_id": "bd92716c26dae40a70455295cae151b27ccfd01d594c430e49de73efd8d78a2b053b3b08ef50dc2c2f84fcafef5490951035cd61295582d5f9e9e2e3740a1987",
        "review_id": "cdd0f7a2c2e7d17b1e4b0f1fef66c6defe5baabda946cc1dcff66666ee229e0b0adec4abe26e95db580a7edfe0ceac651b21a7dd087b00accbc81729b0db1753",
        "human": {
            "strengths": "1. The paper tackles an important and timely topic, as hallucination in foundation models is a critical challenge hindering their widespread adoption.\n2. It provides a structured taxonomy and clear definitions, which helps in understanding the various types and aspects of hallucination across modalities.\n3. The survey covers a broad range of recent works and benchmarks in detecting and mitigating hallucination for language, vision, and video models.",
            "weaknesses": "1. Limited analysis and insights: The paper mostly presents a collection of related works but lacks deeper analysis and insights that would make it a more valuable contribution. For example, it could provide a comparative analysis of the strengths and limitations of different approaches, discuss trade-offs and practical considerations, or offer a more unified perspective that cuts across modalities.\n\n2. The listed contributions in the introduction are somewhat generic and not well-reflected in the body of the paper. For example, the paper does not really deliver on \"identifying key factors and mechanisms that contribute to hallucination\" beyond what is already known in the cited works. The contributions need to be more specific and clearly demonstrated.\n\n3. Missing discussion on open challenges: Although the introduction states that the paper will highlight open challenges and future research directions, this is currently missing from the body of the paper. A section on open problems, limitations of current approaches, and promising directions would make the paper more impactful."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper presents a comprehensive and structured taxonomy for classifying hallucination in foundation models across multiple modalities, which is a novel approach.\", \"Data/Task\": \"It covers a wide range of domains including text, image, video, and audio, providing a holistic view of the hallucination problem in foundation models.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper offers a detailed explanation of the mechanisms, detection methods, and mitigation strategies for hallucination, making it a valuable resource for researchers and practitioners.\"}, \"weaknesses\": {\"Related Work\": \"The paper acknowledges that it may miss some relevant work in the field, indicating a potential gap in the coverage of existing literature.\", \"Significance\": \"While the paper provides a comprehensive overview, the practical impact of the proposed taxonomy and findings on real-world applications is not clearly articulated.\", \"Analysis\": \"The paper lacks a critical analysis of the effectiveness of the discussed detection and mitigation strategies, which could limit the understanding of their practical implications.\", \"Presentation\": \"The presentation of the paper is dense, which may make it challenging for readers to extract key insights quickly.\", \"Contribution\": \"Although it aims to fill the gap in multimodal hallucination research, the paper does not provide new empirical results or theoretical advancements.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "25": {
        "paper_id": "8d0ca389bda68aa60d01eb4050c87d688a279220bd4dcd6d21b315a97f030054c3b8bfc9078e4ac85d234160ee82bc86e51c1b004084465e1eb80044b34832cf",
        "review_id": "3f801e6d82098597437626949bd7dab4be3a8cbd7cb9fd4f905f1d9de2fbe6441e5f012c4a87b063c3fef082f24ddcdc840706571dac47786f8ac8d27175d8fc",
        "human": {
            "strengths": "Using DP to construct a text segmentation is an interesting method for the long text task.",
            "weaknesses": "1. Do the compared methods, i.e., Longformer, ToBERT, CogLTX, and ChunkBERT use the external knowledge? If not, the results presented in Table 2 seem to be not fair. It would be better to contain a clear explanation. \n2. As presented in Table 2, the performance gain of the proposed method on EURLEX and Book summary is marginal, e.g., 0.09% on EURLEX and 0.13% on Book summary compared to simple Bert+Random baseline. \n3. In Table 4, the performance gain for the KG_p is also marginal. Even on the EURLEX dataset, the KG_s method excels to KG_p by a large margin, e.g., 2.79%."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel text segmentation algorithm and a comprehensive approach that integrates dynamic segmentation, knowledge fusion, and representation learning to improve multi-label long text classification. The proposed method effectively addresses the input length limitation of PLMs and incorporates external knowledge to enhance text representations.\", \"Data/Task\": \"The study conducts extensive experiments on various benchmark datasets, demonstrating the effectiveness of the proposed model in unraveling intricate correlations between texts and labels.\"}, \"weaknesses\": {\"Result\": \"While the paper claims improvements, the actual gains in performance metrics over existing methods are relatively modest, which may limit the perceived impact of the findings.\", \"Data/Task\": \"The experiments are limited to a few datasets, and the paper does not explore the generalizability of the approach to other types of long text datasets.\", \"Comparison\": \"The paper lacks a detailed comparison with state-of-the-art models in terms of computational efficiency and resource requirements, which are crucial for practical applications.\", \"Presentation\": \"The exposition of the methodology, especially the algorithms and mathematical formulations, could be clearer and more concise to enhance reader comprehension.\", \"Methodology\": \"The approach relies heavily on external knowledge and specific segmentation techniques, which may not be easily adaptable to all domains or datasets.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could benefit from a more thorough discussion on the limitations of the proposed method and potential avenues for future work.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "26": {
        "paper_id": "49a9ec763d4bdf6467c203c52c219c1209153953772aa26dc4b448eec9e2cc5d80ad4c9b02506b767f32c22e78ca07f9f6a852a0dbf7fb9b2475be25f3b021a4",
        "review_id": "dea4239bc390ccbd6a484a0292d92796d919b8f32fd4db8764176265fce05432369d6e0f63beccc08f6c192e499a118235ee60a202d136aeebaaedb3cb8f19a2",
        "human": {
            "strengths": "- well written paper\n- convincing argument for use of offline RL (instead of online LLMs), and transferral of knowledge from LLM via reward to RL agent\n- thorough evaluation setup with many state of the art baselines, and ablations\n\nI think the paper does establish convincingly that the augmentation towards a dense reward helps learn an agent. But it does not go very far in establishing why that may be so, or what information exactly these rewards convey.",
            "weaknesses": "- given that the \"consistency guidance\" is sold as the main point (and even taken into the title), there is fairly little explanation of what exactly makes the various prompt-induced functions contextually, structurally, and temporally consistent. More room should have been given to introducing the intuitions behind why this is needed, and the way how the LLM-based reward functions, and the way that they are ensembled, ensures this consistency.\n- the dataset for training and evaluation seems somewhat small, starting with only 25 trajectories?"
        },
        "llm": "{\"strengths\": {\"Evaluation\": \"The paper demonstrates COREN significantly outperforms other offline RL agents and achieves comparable performance to state-of-the-art LLM-based agents despite using fewer parameters.\", \"Ablation\": \"Thorough ablation studies highlight the importance of spatio-temporal consistency in reward estimation, showcasing the benefits of the proposed ensemble method.\", \"Presentation\": \"The paper is well-organized, clearly presenting the novel framework and its components with detailed explanations and diagrams.\", \"Methodology\": \"Introduces a novel consistency-guided reward ensemble framework (COREN) for embodied agent learning, leveraging LLMs for reward estimation in offline RL.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"The paper could further elaborate on the intuition behind using LLMs for reward estimation and the specific challenges addressed by COREN.\", \"Evaluation\": \"While the paper provides extensive evaluation, it could benefit from more diverse benchmarks beyond VirtualHome to test generalizability.\", \"Data/Task\": \"The paper focuses on a specific domain (household tasks in VirtualHome), which may limit the perceived impact across broader embodied agent applications.\", \"Methodology\": \"The dependency on LLM capabilities for reward estimation is a noted limitation, especially in dynamic environments where domain knowledge may differ.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could improve in detailing how COREN can adapt to continuously changing environments, which is acknowledged as a limitation.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "27": {
        "paper_id": "6b135b6a1ac604fc303e56c6bb022d7762dfa774d5a44a1c0fc5ead5e73ff103f23311f21a02be3ecfcfc998a32f965a53fe61fd2230d81bf57187e3b0a44cae",
        "review_id": "77ea825f049f6af69f4cea534f4cfce73da143e60e85ba941236606fc045e38b5cb29965cff6c6a7afcaa6a28971c91afff680f3be31f793f03a1cfc754b0d30",
        "human": {
            "strengths": "- Compared with the previous version, the paper structure is better and the expression is clearer.\n- For similar tasks and webpages, AUTOSCRAPER can first generate Scraper from Seed Webpages through Progressive Generation and Synthesis, and then use Scraper to extract information from all webpages, which is more efficient than directly using LLM to traverse all webpages.\n- Comprehensive experiments on three available datasets with 7 LLMs.",
            "weaknesses": "- The description of seed webpages is still unclear. I think it is an important point in the paper. LLMs have to traverse all N_w webpages to find information. However, AutoScraper can build a scraper using n_s seed webpages and apply it to N_w webpages. This might involve reuse of XPath. (I noticed that this issue was also mentioned in ARR April, but it seems unresolved.)\n- Q1 mention that the construction of the scraper needs to be reusable. This problem is also discussed in the paper. I believe this issue is not only related to the type of webpage but also to the type of instruction. What I mean is, suppose there are 100 instructions, and they all have the form \"What's the average rebound of xxx?\" In that case, it makes sense to generate an AutoScraper using 3-5 seed webpages. But if there are other types of instructions, such as \"Comparing the last five games of xxx, which one has the highest rebound?\" then it would require a different XPath or action sequence. What if the dataset consists of a variety of instruction types? So, I am skeptical about the applicability of this method on other datasets.\n- Compared to the previous version, I am not clear why the details of the prompts were removed. This gives me a sense of incompleteness. After reviewing the reviewer comments, it seems that the reviewer wanted a clear explanation of the correspondence between the prompt and the algorithm rather than direct deletion. \n- I believe Efficiency Analysis is an important insight, but I still have doubts about it. It would be helpful to test the actual time using 3-5 examples and report the results, rather than just providing theoretical explanations. In particular, I am not sure if the estimation *T_g ≈ d_max \\* T_d* is accurate. From the algorithm's perspective, Top-down has 2 steps (*T_d + T_e*), and step-back has *N_repeat* steps. The determination of \"h contains value\" in while loop involves LLM call, so it should be *N_repeat \\* (T_d + T_e)*. Therefore, in my view it might be, *T_g ≈ d_max \\* (N_repeat + 1) \\* (T_d + T_e)*.\n- There may be limitations in the online web environment, where webpages can be updated frequently. However, this could be a point for future research"
        },
        "llm": "{\"strengths\": {\"Presentation\": \"The paper is well-organized with clear sections detailing the methodology and experiments.\", \"Data/Task\": \"The introduction of a new executability metric and comprehensive evaluation on diverse datasets is a strong point.\", \"Methodology\": \"The two-stage framework is novel and leverages the hierarchical structure of HTML effectively.\", \"Experiment\": \"Extensive experiments demonstrate the effectiveness of AUTOSCRAPER, outperforming state-of-the-art methods.\"}, \"weaknesses\": {\"Significance\": \"The paper is limited to vertical webpage information extraction and lacks exploration of broader applications.\", \"Evaluation\": \"While the executability metric is new, traditional metrics are still used, which may not fully capture the task's complexity.\", \"Result\": \"The results are promising but rely heavily on the performance of backbone LLMs.\", \"Analysis\": \"Error analysis is provided but lacks depth in addressing the non-generalizability issue.\", \"Data/Task\": \"The datasets used are limited to specific domains and may not represent broader web scraping challenges.\", \"Theory\": \"The paper lacks a theoretical exploration of the underlying challenges of LLMs in handling HTML.\", \"Presentation\": \"Some sections are dense and could benefit from additional clarification, particularly the methodology.\", \"Methodology\": \"The synthesis module's reliance on multiple web pages may limit its applicability in scenarios with fewer pages.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The explanation of the synthesis module's impact on other methods is not sufficiently detailed.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "28": {
        "paper_id": "32f47ac5decf92cd29815a2c81fa1b361707b3daa361f1b4f89c0401510b1dbd62e83eb89dc2bc84f598a446143d30291a0a7bcf8bf9cb10819180e70bac43f6",
        "review_id": "c06ea6df9c96f854afe67ec4877f304b5bca19840463115a4d28d389f3ee804f7a20ec94271568540f68779c605f4b1b265db9cfd01289e28c3ce6122dd8ab16",
        "human": {
            "strengths": "Leverages the structured nature of SQL queries to guide LLMs, similar to how humans learn by focusing on specific question types.\n\nOutperforms state-of-the-art methods like DIN-SQL on the Spider and BIRD datasets.\n\nThe proposed method shows its advantages in the Chatgpt and GPT4, as well as the deepseek coder, which we can be sure is useful, at least on the top of state-of-the-art models. The experiments are extensive and persuasive.",
            "weaknesses": "The proposed method is working well but is technically relatively simple."
        },
        "llm": "{\"strengths\": {\"Experiment\": \"The paper demonstrates that PTD-SQL can surpass or match previous state-of-the-art methods on Spider and BIRD datasets, showing significant improvements in reasoning capabilities of LLMs.\", \"Methodology\": \"The novel Partitioning and Targeted Drilling approach mirrors human learning processes, allowing LLMs to focus on specific problem types, enhancing their reasoning abilities.\", \"Result\": \"PTD-SQL achieves superior execution accuracy and efficiency metrics compared to existing frameworks, indicating its effectiveness in improving LLM performance in Text-to-SQL tasks.\"}, \"weaknesses\": {\"Methodology\": \"The paper does not address the exploration of effectiveness on larger-scale databases with broader domain spans, and the impact of different types of divisions in SQL statements is not thoroughly investigated.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "29": {
        "paper_id": "9f7532078998e336b085fa0ec5d1c02b35161ea1a0245f08a58c9f8d55692679deecb66c92e0bc976e2527a6f22cfc55c1cbce541f0f712e1c967f9356cbfe4c",
        "review_id": "1451f0a9245a9771bd3f64bc71d3d3e02c189d952e62a59a82b192e3d1e9d8c606534d56efc40754ee898edb920274b3d291dba8b854716368ed94d59cd3bccf",
        "human": {
            "strengths": "1. The authors present a new benchmark for assessing the multi-turn conversational abilities of LLMs, a topic that has been infrequently explored in the era of LLMs.",
            "weaknesses": "1. While the authors introduce a new benchmark on a rarely studied topic, the design of the benchmark is not sufficiently robust. With only four tasks, the underlying rationale for this specific selection is unclear. Are these four tasks sufficient to model the multi-turn conversational abilities? Is there any analysis of the coverage of these tasks within their data source LMSYS-1M or in real-world human conversations? I believe that four tasks are insufficient to comprehensively evaluate LLM performance in multi-turn conversations. Furthermore, the benchmark is relatively small, consisting of only 168 multi-turn conversations, and lacks comparison with other multi-turn benchmarks. As an independent paper, this work is inadequate for publication.\n\n2. The writing quality could be improved. For example:\n\n    2.1. Maintaining the same writing structure and task order in Section 3.1 as in Section 3 would enhance readability.\n\n    2.2. In Figure 1, the user icon is misaligned.\n\n3. Given that this paper is motivated by the LMSYS-1M dataset, why not construct the benchmark directly from LMSYS-1M? This approach seems more reasonable than constructing it from GPT-4 and simple NLP tasks.\n\n4. The experimental results are somewhat outdated, as the most advanced models, such as GPT-4, Qwen2, and LLaMA-3, are not included.\n\n5. The paper lacks references to some related works. The authors should provide a more comprehensive literature review and discussion, including:\n\n    [1] MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues. ACL 2024.\n\n    [2] DialogBench: Evaluating LLMs as Human-like Dialogue Systems. NAACL 2024.\n\n    [3] BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues. Arxiv 2023."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces MT-Eval, a novel benchmark specifically designed to evaluate the multi-turn conversational capabilities of large language models (LLMs), addressing a significant gap in existing evaluation frameworks.\", \"Data/Task\": \"MT-Eval categorizes interaction patterns into four types: recollection, expansion, refinement, and follow-up, and constructs multi-turn queries for each, offering a comprehensive evaluation of LLMs in real-world scenarios.\"}, \"weaknesses\": {\"Related Work\": \"The paper provides limited comparison with existing multi-turn evaluation frameworks, which could have strengthened the justification for MT-Eval.\", \"Intuition/Justification/Motivation/Validation\": \"While the motivation for multi-turn evaluation is clear, the paper could benefit from more detailed justification on the choice of interaction patterns and their relevance.\", \"Experiment\": \"The exclusion of larger open-source models like Llama-2-chat-70B due to computational limits is a notable limitation in the experimental setup.\", \"Evaluation\": \"The reliance on GPT-4 for evaluation may introduce bias, and the paper acknowledges potential bias towards its generated outputs.\", \"Result\": \"The paper highlights performance discrepancies between single-turn and multi-turn settings but does not explore solutions to mitigate these issues.\", \"Analysis\": \"The analysis of error propagation and noncompliance with earlier instructions is insightful but lacks depth in exploring potential solutions or improvements.\", \"Presentation\": \"The presentation of results could be enhanced with clearer visualization to better convey the performance differences and analysis insights.\", \"Comparison\": \"The paper mainly compares closed-source and open-source models without deeper exploration of the reasons behind performance differences.\", \"Methodology\": \"The methodology for constructing single-turn instances for comparison lacks detailed explanation, especially for the Follow-up task.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more detailed explanations and interpretations of the findings, especially in the context of improving LLM performance in multi-turn dialogues.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "30": {
        "paper_id": "a17abd3e046f4b03b74e27f9bcc102a8dcab36d01960391913d625267b8c81afdbe43ab7a7d570c0c278f89c6875c4958793ebd3f227b411826b25ba4b238f19",
        "review_id": "2b0f4ed9e3ab6308283fc9827daa6083230be5ec22315919cd30b23972c5c0b13746fb8262dd9ae979ae872eb885c7112c35984133d8aa79471c40ce18752924",
        "human": {
            "strengths": "* The authors create a novel database of various gender-identity YouTubers\n* The paper is well-written and clear",
            "weaknesses": "* The results (Fig 6) indicate that the models are more effective at identifying transgender individuals than cisgender individuals. The models even manage to predict non-binary better than CIS-woman and with similar success as CIS-men. Thus, contradicting the conclusion/statement that models will misgender marginalized groups, they will likely misgender the CIS group.\n* While SAGE somewhat supports the claim that genders are not determined by the specific \"wording\" of YouTubers, authorship can still be identified through other lexical properties, such as idiolects. Additionally, many YouTubers likely follow a structured template for their videos, and these properties might assist the model in making its decision about the gender/author."
        },
        "llm": "{\"strengths\": {\"Presentation\": \"The paper is well-organized and clearly presents its objectives, methodology, and findings. It includes a comprehensive overview of related literature, providing context and justification for the study.\", \"Data/Task\": \"The creation of the TRAN SCRIPT corpus is a significant contribution, offering a novel dataset that includes language samples from individuals across diverse gender identities. This dataset fills a gap in existing resources and is crucial for studying gender identity representation in PLMs.\"}, \"weaknesses\": {\"Methodology\": \"The methodology, while thorough, relies heavily on the representation of diverse gender identities on Wikidata, which may introduce bias due to demographic underrepresentation. Additionally, the diarization step might misidentify speakers, leading to noise in the data.\", \"Comparison\": \"The study is limited to the domain of YouTube transcripts and does not compare its findings across different domains, which limits the generalizability of the results.\", \"Result\": \"While the study identifies discrepancies in model behavior across gender identities, it lacks a detailed sociolinguistic analysis to explain these findings, which could provide deeper insights into the observed biases.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "31": {
        "paper_id": "94a4c00725512ee784d5e7190853969ac3b79a9148f17b3d0ea603738e16d69e2b73991968cf881a387643196bc294d7e4d0675eda11463455c5d123adc239d5",
        "review_id": "541a63725268f1054e96faa56591f1cb277233a98fcf78dd1c9a64e54883fbdd30a91bab850662f8cbb801a3622e9a6300890622a01094e526c85a34cf10baea",
        "human": {
            "strengths": "Overall, the ideas in this paper are interesting enough.\n\n* It might be thought that many people have probably noticed that label noise appears due to its influence on true labels, \nbut it is useful knowledge for readers to have actually demonstrated this through experiments and incorporated it into the model.\n\n* The proposed LeD framework using this knowledge actually performed well in multi-label classification, demonstrating its effectiveness.",
            "weaknesses": "The idea is interesting enough, but the experiments and explanations to support it are not enough.\n\n* In the experimental results shown in tables 2 and 3, \nthe experiment was conducted under conditions in which noise was added to the existing data set, \nbut it is assumed that the noise satisfies LDN, which is the authors' hypothesis and background of the proposed method, so it is an advantageous condition for the proposed method.\n* Also, the main results (table 2 and 3) should be done without adding noise labels. The datasets authors used were naturally contain noise labels, so if LeD outperforms baselines in datasets without adding noise labels, it will be good support for your idea.\n* There is no experiments to show how well the noise label identification itself works, which is a key feature in the proposed method.\n   * (Added after author response) It is mentioned in appendix. I'm sorry for missed it. Also, authors gave the experimental results without correction, and it showed that there approaches are effective enough.\n* In the Ablation study, the performance without filtering is not shown, so it is difficult to know how effective the filtering methods themselves are.\n\nIn addition, some important descriptions are missing, as listed below, making it difficult to understand and reproduce the content.\n\n* Section 2.2 is an important section, but difficult to understand.\nIn the experiment using the Riedel dataset, did authors use the annotated label and original label in the paper by Chen et. al (2019) \nfor the gold label and noisy label, respectively?\nWithout information about what the noise label is, it is difficult to judge the content just by looking at the results.\n\n* When calculating Centroid Distance, it is unclear how to calculate it if none of the samples included in $\\mathcal{H}_j$ appear in batch.\nThe frequency of appearance of labels is biased, and this should happen with low-frequency labels.\n  * (Added after author response) In the authors' comment, I understood that there are no such cases in their experiments, but it may be a potential risk to occur such situations. It may be better to consider how to deal such situations.\n\n* I think the calculating cost becomes very high in model training using LeD framework \nbecause the loss is calculated once and then noise is filtered using multiple metrics.\nIn particular, when calculating Centroid Distance, the calculation cost must be high \nbecause the scores of all labels are calculated to create $\\mathcal{H}_j$, and the cosine similarity is calculated from it.\nIt is also necessary to compare the computational cost in model training.\n  * (Added after author response) Authors gave computational costs as comments, and I understood that the calculation cost of the proposed method is not significantly different from conventional methods."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel label-specific denoising framework that effectively addresses label-dependent noise in multi-label text classification. The holistic selection metric combines ranking-enhanced loss and centroid distance, offering a comprehensive approach to identify and correct noisy labels.\", \"Data/Task\": \"The study validates the presence of label-dependent noise in real-world datasets, challenging the class-conditional noise assumption. It provides a method to generate controllable label-dependent noise, which is crucial for advancing research in noisy multi-label text classification.\"}, \"weaknesses\": {\"Experiment\": \"While extensive experiments demonstrate the effectiveness of the proposed method, the computational intensity of calculating the holistic selection metric may limit scalability for large datasets.\", \"Result\": \"The paper does not explicitly validate the capability of the framework to handle instance-dependent noise (IDN), which may limit its applicability in certain scenarios.\", \"Presentation\": \"The paper could benefit from clearer exposition in some sections, particularly in explaining the complex methodologies and experimental setups.\", \"Ablation\": \"Although an ablation study is conducted, further detailed analysis of the individual contributions of different components could strengthen the empirical evidence.\", \"Comparison\": \"The comparison with existing methods is comprehensive, but the paper could include more discussion on the limitations of these methods in handling label-dependent noise.\", \"Methodology\": \"The reliance on the memorization effect limits the applicability of the framework to deep learning-based approaches only, excluding traditional machine learning methods.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper provides a detailed theoretical background, but some sections could benefit from more intuitive explanations to enhance understanding.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "32": {
        "paper_id": "00a5158786a2cc18993c5fc180c25bf407bf132fca0d19ced10912943c4952fba94052e2cea7931ca49911f8ff27b90909ae05bb7c1efc926983f22b31396ad4",
        "review_id": "4fa25672d8ecc79f5ddc1669daeb96a2e056e337d6e0e8571231439c81d4f4ef664ac3f6264877b3d819e5c8dd31d37ae89755a3915d17117d208662697ebc22",
        "human": {
            "strengths": "1. This paper addresses one known limitation of the LoRAHub paper. The proposed method is intuitive and effective. \n2. The experimental setting closely follows the original LoRAHub paper, with proper ablation studies on two important dimensions, which improves the overall soundness of the paper.",
            "weaknesses": "1. Novelty. The major novelty compared to LoRAHub [1] comes from using the embedded task instances as anchors to retrieve LoRA modules. While this exact problem setting is not studied in previous work, it arguably shares the same fundamental idea with the prompt retrieval literature [2,3,4].\n2. The writing lacks clarity with too many key details omitted. For instance, no details regarding the non-gradient optimization method (line 207) is given. Does the paper simply follow the search algorithm used in LoRAHub? \n3. In line 223, are the retrieved composition weights merged with each other, or only the weight corresponding to the most similar e_j is used? In the latter case, will there be more overfitting compared to the original LoRAHub setting, where the weights are optimized wrt all few-shot examples? In this case, more analysis on the stability to the number of few-shot example should be provided. \n4. Statistical significance should be provided in Table 4 to support the claim on line 275. \n5. How does the proposed method address the limitation mentioned in line 76, that retrieving tasks that are overly similar can cause “poor stability”?\n6. Related work on fine-grained LoRA composition should be cited, such as [5]. \n\nReferences\n\n[1] LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition\n\n[2] Learning To Retrieve Prompts for In-Context Learning\n\n[3] Learning to retrieve in-context examples for large language models\n\n[4] In-context Learning with Retrieved Demonstrations for Language Models: A Survey\n\n[5] LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks"
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces an instance-level dynamic LoRA composition method that selects appropriate task LoRAs for each input instance, dynamically determining composition weights. This approach addresses the limitations of task-level LoRA combination by considering the specificity between different instances, enhancing cross-task generalization.\", \"Experiment\": \"The experiments conducted on publicly available datasets show that the proposed method outperforms the typical method, LoraHub, in 16 out of 27 tasks, demonstrating its effectiveness. The use of the Flan-T5-Large model and the Big-Bench Hard benchmark provides a robust experimental setup.\", \"Ablation\": \"The paper includes an ablation study that highlights the efficacy of capturing input nuances through instance-specific adaptation, showing a 0.9% performance enhancement over task-level methods.\"}, \"weaknesses\": {\"Related Work\": \"The related work section could be improved by providing more detailed comparisons with existing methods and highlighting the distinct contributions of the proposed approach.\", \"Novelty\": \"While the instance-level approach is a novel extension of existing LoRA methods, the overall novelty may be perceived as incremental due to its reliance on existing techniques such as semantic similarity for module selection.\", \"Significance\": \"The significance of the findings is somewhat limited by the fact that the method still cannot compete with full fine-tuning (FFT) on certain highly structured tasks, indicating room for improvement.\", \"Analysis\": \"The analysis could be more comprehensive, particularly in discussing why the method outperforms others on certain tasks but not on complex tasks like 'date understanding' and 'dyck language'.\", \"Presentation\": \"The presentation of the methodology and results could be clearer, with more detailed explanations of the experimental setup and retrieval strategies used.\", \"Methodology\": \"The methodology incurs higher computational costs and requires additional preliminary preparations, which could limit its practical applicability in scenarios with limited resources.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3.5\"}"
    },
    "33": {
        "paper_id": "dcbb6259a9880645362ec5a25723c209976d67711f2762635ab9d3b70672a726d7919979bb75b34411ab082006db6751da341574e2302a2a94576060dea3b8dd",
        "review_id": "4d09f8803c4720d044540cab409761f9d3889a978180a21c61006e2f57fd46a514823db3003c3f6f51b5d794104f3f253924942305373011c41ece63773f95f3",
        "human": {
            "strengths": "1. The multi-prompt MBR decoding technique is creative and leverages the strengths of LLMs' sensitivity to prompts.\n2. Experiments cover a broad range of tasks, including text simplification, code generation, and machine translation, providing robust evidence of its effectiveness.\n3. Improvements are statistically validated, ensuring that observed gains are not due to random chance.",
            "weaknesses": "1. The major concern is the complexity of implementation: Multi-prompt MBR decoding involves additional computational steps such as sampling from a prompt bank and using a trained value metric, which might be complex to implement and computationally intensive, especially during inference time."
        },
        "llm": "{\"strengths\": {\"Intuition/Justification/Motivation/Validation\": \"The paper introduces a novel concept of multi-prompt decoding to improve Minimum Bayes Risk (MBR) decoding, motivated by the observation that relying on a single prompt cannot capture all approaches to a generation problem. This intuition is validated through comprehensive experiments across different tasks and models.\", \"Experiment\": \"The paper provides extensive experiments across three distinct generation tasks: text simplification, machine translation, and code generation, demonstrating the effectiveness of multi-prompt MBR decoding.\", \"Result\": \"Multi-prompt MBR consistently improves generation quality over single-prompt MBR across all tasks and models, with significant performance gains reported.\", \"Data/Task\": \"The research covers a broad range of tasks, including text simplification, machine translation, and code generation, providing a comprehensive evaluation of the proposed method.\", \"Methodology\": \"The methodology includes the use of a prompt bank at inference time and various strategies for prompt selection and sampling, which are well-justified and contribute to the overall improvement in performance.\"}, \"weaknesses\": {\"Methodology\": \"While the methodology is novel, the paper does not explore the impact of prompt formats, in-context example ordering, or example selection on multi-prompt performance, which could provide further insights into the effectiveness of the approach.\"}, \"soundness\": \"4.5\", \"overall assessment\": \"4\"}"
    },
    "34": {
        "paper_id": "8864dac7d81e30de3daa8ff95ece57990c47ec319808a97db28873a53d07c2ba449c1b77ea352b57985246a8a8b3fbce7fea5915b543069297efde246656fb44",
        "review_id": "7847f8ac00bbd30a8ebc12e65d0cc47e7012ead9d29a12d5e38c8bc600d5b5ea999bade7ba5616399a62f6669e8a508084ba50411dd63569c6af98553a625efa",
        "human": {
            "strengths": "- The paper shows that the models enhanced with DEFT show improved alignment capabilities and generalization abilities compared to previous methods. This suggests that DEFT can effectively maintain or improve a model’s performance while better adhering to desired human values.\n\n- One significant advantage noted is the reduction in training time when using DEFT. This can make the process more efficient \n\n- Data filtering is quite important and this paper proposes new method on automatic filtering instead of labor intense human filtering",
            "weaknesses": "- How are the positive and negative distributions, Q_{+} and Q_{-}, derived? Are these distributions based on word frequencies from the data? Can you provide a more detailed explanation?\n\n- It would be beneficial to elaborate on how word frequency effectively addresses distributional rewards. Considering that the context in which words are used may be more critical than the words themselves, do you have any insights or statistical evidence to support this approach?\n\n- I wonder if the proposed framework is more effective when applied to reinforcement learning-based alignment methods such as PPO, beyond just comparing it with DPO or PRO.\n\n- As mentioned in Appendix A, the inference length is set to 128. Were the MT Bench results also based on this length? It seems that a length of 128 might be insufficient for obtaining good score in the MT Bench evaluation.\n\n- Have you considered using larger or more advanced reward models? If so, is there any notable difference in performance?"
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel alignment framework, DEFT, which efficiently enhances preference learning by integrating data filtering and distributional guidance, offering a significant improvement over existing methods.\", \"Data/Task\": \"The use of a high-quality subset filtered from the raw data allows for more efficient training, reducing costs and maintaining or enhancing generalization capabilities.\", \"Result\": \"Experimental results demonstrate that DEFT-enhanced methods outperform original methods in alignment capability and generalization ability with reduced training time.\"}, \"weaknesses\": {\"Evaluation\": \"The paper relies heavily on automated metrics and a single human preference dataset, which may not fully capture the effectiveness of the proposed method across diverse scenarios.\", \"Result\": \"While the DEFT framework shows improvements, the gains in some metrics are incremental, and the impact on broader tasks is not extensively validated.\", \"Data/Task\": \"The study is limited to the HH-RLHF dataset, focusing only on harmlessness and helpfulness, which may not generalize to other preference datasets.\", \"Comparison\": \"The paper could benefit from a more comprehensive comparison with a wider range of existing alignment methods.\", \"Methodology\": \"The description of the distribution reward calculation and its integration into the training process could be more detailed to enhance reproducibility.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper lacks a detailed discussion on the limitations and potential biases introduced by the data filtering process and the assumptions in the discrepancy distribution.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "35": {
        "paper_id": "95e29435ec1a2a1adfac11dc4aafd5f3550e957c8f0cec07236d3ff596f21239f66c225f6196b46e4017dfb62491a8ccde278c3444caea9e35909e0c2bd5f43a",
        "review_id": "ba6da21af99a3983e78b42e1161d3150990ea5f1d20e20fead043fb86da0aa71dcebd7253cd59b0e41a347b6e7666bae036d0ab75895e197f5d92523887d47e0",
        "human": {
            "strengths": "1.\tThe paper is easy to follow.\n2.\tThe proposed method obtains competitive results on multiple performance metrics, compared with baseline models. The improvements for topic and average correctness are significant.",
            "weaknesses": "1.\tHyperparameters such τ and λ are introduced, but the value/effects are not stated. \n2.\tThe parameters are weakly defined and the calculation process is hard to follow. The dimensions for the parameters are missing. In addition, it is not clear whether the conditional probability vector W is an overall relation representation or dynamically calculated based on the current input. \n3.\tLimited efficiency. It seems one attribute model is required for each attribute. \n4.\tThe focus of this paper is to incorporate both prohibitive and promotive relations between provided attributes during generation. It seems such relations are captured by conditional probability vector W in dynamic text generation strategy. How does this eventually affect the logits of individual tokens when attributes are prohibitively and promotively related? The effect is unclear and the motivation is weakly supported.\n5.\tInsufficient experiments. \na)\tLacks ablation study regarding the use of a frozen base quality control model. How are the logits from frozen PLM and weight W from dynamic generation strategy balanced? If effect of W is too significant, semantically incorrect text might be generated. \nb)\tThe proposed framework is tested using two attributes (one sentiment attribute and one topic attribute). More experiments may be required to prove the effectiveness of TARA in more complex settings where more attributes are present."
        },
        "llm": "{\"strengths\": {\"Presentation\": \"The paper is well-organized, clearly outlining the novel methodology and its contributions to multi-attribute CTG.\", \"Methodology\": \"TARA introduces a token-level attribute relation adaptation method, including attribute-adaptive prefix tuning and dynamic text generation strategy, which is innovative and enhances multi-attribute control.\", \"Result\": \"Experimental results demonstrate TARA's competitive control ability and comparable text quality and diversity over baseline methods.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"The paper could provide deeper insights into the intuition behind the token-level approach and its broader implications.\", \"Experiment\": \"While the experiments show competitive results, additional datasets and scenarios could strengthen the validation of the method.\", \"Analysis\": \"The analysis of results could be more comprehensive, particularly in explaining why certain methods outperform others in specific metrics.\", \"Ablation\": \"The ablation study is helpful but could be expanded to cover more components of the proposed method.\", \"Presentation\": \"Some sections could benefit from clearer explanations, particularly the technical details of the methodology.\", \"Methodology\": \"The methodology lacks detailed comparison with other potential approaches for handling attribute relations.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "36": {
        "paper_id": "8d928c3b54e5bfef5195daa2075402da9a24ecfd19635d211c138de6f4ea026860cea3cb895b93285547a5e6e1a6f11cfd4510e01972b50ea1e276aaff1c5c97",
        "review_id": "2dd7f623d70b6692217b228911d7d9f6e632f32dfd043b9d0b369245cb1eeb5e16266629895c477b95f134fafb7508fa7014a6c1597574226c607ae1b5a7a497",
        "human": {
            "strengths": "- This work points out a potential issue when simulating students by large language models causing regression in factual knowledge. The motivation is clear and the solution of introducing additional tags sound reasonable.\n- Experiments sound consistent in that simulating students by LLMs degrades performance in various natural language related tasks when compared with LLMs simulating tutors.",
            "weaknesses": "- If my understanding is correct, large language models are fine-tuned to follow instructions with dialogue history comprising student-tutor instructions. Thus, it is not clear why not running an experiment for tutor-hal, which simulate a tutor by LLMs but the responses by students are augmented by special tags.\n \n  * The authors comments resolved my misunderstanding.\n\n- The drop in performance for tutor when compared with vicuna for TruthfulQA in Table 1 is not clear to me. It might be caused by student responses in the dialogue histories when tuning a tutor model.\n\n  * Thank you for the explanation."
        },
        "llm": "{\"strengths\": {\"Intuition/Justification/Motivation/Validation\": \"The paper identifies and explores the 'Student Data Paradox,' a critical challenge in training LLMs for personalized education. The motivation to address the trade-off between modeling student misconceptions and preserving factual accuracy is timely and relevant.\", \"Experiment\": \"The authors conducted a comprehensive set of experiments using state-of-the-art LLMs trained on a realistic student-tutor dialogue dataset and evaluated across multiple benchmarks, demonstrating the regressive side effects of the Student Data Paradox.\", \"Result\": \"The introduction of 'hallucination tokens' showed substantial improvements in mitigating the regressive effects, indicating a promising strategy to balance student behavior modeling with factual accuracy.\", \"Data/Task\": \"The use of a student-tutor dialogue dataset from the CLASS framework provides a realistic representation of student learning patterns, enhancing the study's relevance to educational contexts.\", \"Methodology\": \"The methodology is well-structured, involving data preparation, model training, and the innovative incorporation of hallucination tokens to address identified issues.\"}, \"weaknesses\": {\"Experiment\": \"While the experiments are thorough, the introduction of hallucination tokens does not fully restore baseline performance, indicating that further research is needed to address the complexity of the issue.\", \"Result\": \"The results, although improved with hallucination tokens, still show a significant drop in performance compared to baseline, highlighting the need for more effective solutions.\", \"Methodology\": \"The proposed solution of hallucination tokens, while novel, is only partially effective, suggesting that the methodology could benefit from additional techniques or refinements.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more in-depth discussion on the long-term implications of the Student Data Paradox on personalized learning and potential pedagogical strategies to integrate LLMs into educational practices.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "37": {
        "paper_id": "a29c29b4fda7f386e854524f327259ec8a521281e369d11bf25a89f68ab39334ec06c1d07fe4e04045283419854093f3b7f795f46a7e07d36359c6ed531c9689",
        "review_id": "d62dea79e84379626cf3aff3dd34f68e2f1b474fb0a8709674f9206f377c9aa60f3b72c5a8cabf958001735a0d6488fdee6ff9f66ed1bd438242f4abe449c3e0",
        "human": {
            "strengths": "- The way of utilize the attribution analysis to reduce the redundancy of dependency tree is interesting, there are works to do pruning, but the dynamic way is the first.\n\n- The ideas are communicated in a comprehensible way, though the phrasing and syntax may not be as refined or sophisticated as more polished academic writing. \n\n- The experiment that was conducted was quite thorough and comprehensive in its scope and methodology.",
            "weaknesses": "- Although effective, it's pre-LLM era work, pretty old fashion.\n\n- Aspect-level sentiment classification is an easier task when compared with other ABSA tasks, I wonder how well the LLMs can handle it. It would be better to have LLM-based baselines included.\n\n- The other contributions are added to the initial motivation of reducing the redundancy of the semantic structure of the dependency tree for increasing the workload of this paper, such as the \"Multi-granularity\", they do contribute to the performance, but adding them does not make any sense.\n\n\n- What do you mean by \"sentiment knowledge at a data-specific level\"\n\n- the figures need to be revised, for instance, \n  - the example and highlight in figure 1(b) making people feel confusing, \n  -  figure 2 contains the architecture of the proposed DMAN, which consists of three modules, but I cannot see the annotation in the figure.\n\n- the generalization of pruning the semantic structure could be explored, such as other more complicated tasks like events extraction or aspect-level quadruple extraction."
        },
        "llm": "{\"strengths\": {\"Analysis\": \"The paper introduces a novel approach to aspect-based sentiment analysis by incorporating attribution analysis, which provides data-specific insights for reasoning sentiment polarity.\", \"Methodology\": \"The use of Integrated Gradients for attribution analysis and the multi-granularity approach are innovative and well-implemented, allowing for dynamic capture of semantic and syntactic information.\", \"Presentation\": \"The paper is well-organized and clearly presents the motivation, methodology, and results, making it accessible to readers.\", \"Experiment\": \"Extensive experiments on five benchmark datasets demonstrate the effectiveness and interpretability of the proposed method, showing improvements over existing models.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"While the paper presents a novel approach, the justification for choosing specific hyperparameters and the impact of certain design choices could be further elaborated.\", \"Result\": \"The paper does not always provide accurate attributions for sentences with overly complex content and structure, which is acknowledged as a limitation.\", \"Data/Task\": \"The datasets used are standard benchmarks, but the paper could explore more diverse datasets to test the generalizability of the method.\", \"Comparison\": \"Although comparisons are made with existing models, the paper could benefit from deeper analysis of why specific models perform differently under certain conditions.\", \"Contribution\": \"While the contributions are significant, the paper could explore further applications or extensions of the proposed method beyond ABSA.\", \"Presentation\": \"Some sections could benefit from clearer explanations, particularly the technical details of the methodology.\", \"Methodology\": \"The complexity of the model structure results in increased computational costs, which could be a concern for practical applications.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "38": {
        "paper_id": "5807721cad675a80979e028af256ad05c4f94f1987ace71dd32b4328713b4b7e72e30e34b284508784e0159ebb8e380da73d3c494e849d8ee0a9d297f8381484",
        "review_id": "84d8afb159506074a1bc35be97322ec0ac79aa4b42f6f05cf68da9fd350c7dfe4d669412d461cbcf3dcddfe87d29c857d3f043df8eba3a906ab0be5731c73e3e",
        "human": {
            "strengths": "1. A combination of infuser and adaptor is proposed to alleviate the cataphoric forgetting issue in Section 3.3. An MLP block was used to produce a learnable score, controlling the relative impact of the added knowledge adapter. By employing a pre-training stage before jointly fine-tuning using the knowledge triplets that LLM has already known, the learned MLP block can be effective for judging the fusion of the new adapter.\n\n2. Evaluation in Section 4 confirms the effectiveness of the proposed InfuserKI pipeline, and Figure 6 validates that the Infusion score correctly reflects designed usage, although the detailed ablations are missing.",
            "weaknesses": "1. Incomplete ablation studies: L341, L358, L365, L385 and L393 suggest that the entire InfuserKI pipeline learns from multiple supervisions, and the effectiveness of those losses remains unclear in the submission. The training process also consists of three different stages, but is the relation classification loss necessary in stage 3?\n\n2. While the authors have provided an explanation of why they do not directly compare with RAG in the Response PDF & Justification For Not Keeping Action Editor Or Reviewers, and claimed their method is orthogonal to the RAG method, Appendix A.6 only compares with the pure RAG method."
        },
        "llm": "{\"strengths\": {\"Intuition/Justification/Motivation/Validation\": \"The paper addresses a novel problem of efficiently integrating unknown knowledge into LLMs without causing catastrophic forgetting of existing knowledge, which is a significant challenge in enhancing LLM performance in knowledge-intensive tasks.\", \"Evaluation\": \"Extensive evaluations on UMLS and MetaQA datasets demonstrate the effectiveness of the proposed InfuserKI framework, showing substantial improvements over state-of-the-art baselines in reducing knowledge forgetting.\", \"Ablation\": \"The paper includes thorough ablation studies that highlight the importance of each component of the InfuserKI framework, such as the Infuser loss and relation classification task.\", \"Methodology\": \"The InfuserKI framework is well-designed, leveraging transformer internal states to determine when to enhance LLM outputs with additional information, and employs knowledge adapters to preserve original model parameters while integrating new knowledge.\"}, \"weaknesses\": {\"Methodology\": \"The paper could benefit from a more detailed explanation of the infuser's mechanism and its integration process to enhance clarity.\", \"Ablation\": \"While the ablation study is comprehensive, it does not explore the impact of varying adapter positions extensively, which could provide further insights into optimizing the framework.\", \"Comparison\": \"The paper lacks a comparison with more diverse baseline methods, particularly those that employ alternative strategies for mitigating knowledge forgetting, which could strengthen the evaluation results.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "39": {
        "paper_id": "6aa84d5e2353121a89dec87403f67a89fe60af83383e1cc31d9445d7fc4064c4d91ff6cd2208e1cf4bd0633948fdaf49ff83f5a2597249f13312c2784c144710",
        "review_id": "9dd3882310b6903c78aee9af8d83a74d7011729d20c7d1c099dee7bd02d08c6c8c1abd9b3283b3535590aec26b9382ad75792a16918210570ee00a9f57728403",
        "human": {
            "strengths": "1. Dual Encoder Approach: Separating the semantic and acoustic encoding of speech is a novel and potentially effective way to capture the multi-faceted nature of the speech signal. This could improve the model's ability to understand and process speech in more nuanced ways.\n\n2. Curriculum Learning and Prompt-Aware Adaptation: The two-stage training approach, starting from simple tasks and progressing to complex ones, along with the prompt-aware LoRA adapter, seem to be successful strategies for enhancing generalization and robustness to diverse instructions.\n\n3. Strong Empirical Performance: The paper demonstrates state-of-the-art results on a range of single-task benchmarks, as well as significant improvements over prior models on challenging multi-task and Chain-of-Thought evaluations. This suggests the proposed WavLLM framework is a promising direction for advancing speech-based large language models.",
            "weaknesses": "1. Out-of-domain evaluation: The experiments mainly focus on test sets that are drawn from the same data distributions as the training data, such as the test sets of LibriSpeech, CoVoST2, VoxCeleb, etc. While this provides a good indication of the model's performance on seen tasks and datasets, it does not necessarily reflect how well WavLLM would generalize to completely new, out-of-domain speech data and tasks. Evaluating on external, out-of-domain speech benchmarks could help uncover any potential weaknesses or biases in the model. This could include tasks like speech recognition or understanding on accented speech, specialized domains like medical or technical speech, or even cross-lingual scenarios. The lack of such out-of-domain evaluations is a potential limitation of the paper. \n\n2. Concatenation of outputs from Dual encoders: The paper states: \"The outputs of both adapters are concatenated together before feedforwarding into the linear projector.\" However, it does not explicitly specify whether this concatenation happens at the dimension level or the time level. If the concatenation is done at the time level, this would effectively double the sequence length, which could indeed limit the overall effective batch size during training due to memory constraints. The paper does not seem to address this potential issue or provide details on how they managed the computational and memory requirements with the concatenated encoder outputs. It would have been helpful if the authors had clarified the exact concatenation strategy used, as well as any strategies or considerations they had to account for to ensure efficient training and inference with the increased sequence length.\n\n3. Overfitting / effectiveness of dual encoders: The results in Table 7 show that when the WavLM encoder is removed, the performance drop is more significant for the ASR task compared to the speaker verification (SV) task. This seems contradictory to the claim that the WavLM encoder is primarily responsible for capturing the acoustic information, while the Whisper encoder handles the semantic content. One would expect the removal of the WavLM encoder to impact the SV task more, since it is more reliant on the acoustic/speaker characteristics. However, the results indicate the ASR task suffers a larger performance degradation. A possible explanation could be that the adapters are overfitting to the training datasets, and the ASR task is more sensitive to this overfitting compared to the SV task. The paper does not provide a clear justification for this observed discrepancy. The authors should have addressed this in the analysis and discussion section to better explain the role and importance of each encoder component for the different speech tasks."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel and robust methodology by integrating dual encoders, Whisper and WavLM, within a two-stage curriculum learning framework, enhancing the model's generalization capabilities.\", \"Novelty\": \"The introduction of a prompt-aware LoRA weight adapter is innovative, allowing adaptive tuning for different tasks, which is a significant advancement in speech LLMs.\", \"Data/Task\": \"The paper utilizes a diverse set of datasets and tasks, including universal speech benchmarks and specialized datasets, ensuring comprehensive evaluation.\", \"Result\": \"The results demonstrate state-of-the-art performance across a range of speech tasks, showing robust generalization and CoT capabilities.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"While the paper introduces a novel approach, the intuition behind the choice of specific model components and their integration could be better justified.\", \"Evaluation\": \"The evaluation could benefit from additional baselines and ablation studies to further validate the effectiveness of the proposed components.\", \"Result\": \"The paper lacks a detailed analysis of failure cases or scenarios where the model underperforms, which could provide insights into its limitations.\", \"Presentation\": \"Some sections of the paper are dense and could be better structured to improve readability and comprehension.\", \"Data/Task\": \"The paper primarily focuses on English speech tasks, which might limit its applicability to other languages without further adaptation.\", \"Analysis\": \"The analysis of results could be more thorough, particularly in terms of explaining why the proposed model outperforms others in certain tasks.\", \"Methodology\": \"The methodology, while novel, could be more clearly explained, particularly the integration of the prompt-aware LoRA weight adapter.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more detailed explanations and discussions on certain technical aspects, such as the specific role and impact of the Whisper and WavLM encoders.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "40": {
        "paper_id": "47ea300921f3ea34e615ba73b9458bd72040be5de2c6b69fc0910e3f1c4430635042a4a748393775f693707cfb9d7a3fb9c44986fb3f2aba2e4cd2401e02af77",
        "review_id": "4452ae28a8d6673fc9b31af4a0b6ef4d566935d32dc1bbdd7016804ff5c257b7bd978a0c33ea6de4f2dbb3297bb3acf767cbe6d7ec9529412408e22a0b1ce3e2",
        "human": {
            "strengths": "The paper addresses an understudied topic (substance use and stigma) in NLP/CSS. I'm only aware of a small handful of papers on this subject, none of which are at *CL venues, despite related concepts of toxicity, hate speech, model bias, etc. being very popular here. As outlined in the paper, SU is an important public health concern, and thus, could/should be addressed by CL methods. \n\nI like the approach of Informed + Stylistic LLMs. This takes the standard stigma detection task two steps further than other stigma detection papers. \n\nThe labeled data set will be very helpful for those studying stigma in substance use, as there are few resources here.",
            "weaknesses": "The paper could be tighter. First, the research questions are not really discussed after the introduction. I think RQ1 corresponds to Section 5.1, but this should be said explicitly. Most of the paper is centered on RQ2, so it was surprising to see Results Section 5.1 (since this wasn't mentioned in the methods section). We see mentioned of RQ1 later, but it seems like this is in the context of RQ2 (section 4.2), but again this is not clear. Finally, neither research questions are mentioned in the Discussion. \n\nSimilarly, the methods section could be better explained. I had several questions (see below), some clarifying and some essential to understanding the methods. For example, it is unclear where definitions of \"Directed\", \"Self\", \"Structural\" come from or what they are exactly. I see definitions in the Appendix in Figure 3, where you show your prompt, but not in the main text. (Even in this figure Directed stigma is not defined.)"
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper presents a novel computational framework for analyzing and de-stigmatizing online content using large language models (LLMs). The methodology is well-structured, leveraging a comprehensive pipeline that includes filtering, detection, explanation, and transformation of stigmatizing language.\", \"Data/Task\": \"The study utilizes a large dataset of over 1.2 million Reddit posts, which provides a robust basis for analyzing stigma in online communities. The task of transforming stigmatizing language into empathetic expressions is highly relevant and addresses a significant gap in existing research.\"}, \"weaknesses\": {\"Result\": \"While the paper shows promising results in reducing stigma, the evaluation metrics are not fully aligned with traditional benchmarks, making it difficult to compare with existing models.\", \"Presentation\": \"The paper is dense and could benefit from clearer exposition, especially in detailing the experimental setup and results.\", \"Methodology\": \"The study relies heavily on LLMs, which can be prone to biases inherent in their training data. The ethical implications of using these models in sensitive domains are acknowledged but not fully addressed.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more detailed explanations and discussions on the implications of the findings, especially regarding the broader impact on social media discourse and potential applications in other domains.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "41": {
        "paper_id": "723eb1395def7ac19bd2991baca754e4022011163699692017ca5e1b2704e4cc0a52cdd704917dea2c457f7e08abf33a5aa58da0c58c098c6b463adca0be5cea",
        "review_id": "eddaf277e1583f87bc60dd418686918b4201cb86db768ebe414e8e0cdb863fe81199ad8cfc80fdfc53bdce769c4f52ecc1a0b3753508a003b592dcb7decc2a36",
        "human": {
            "strengths": "1. The paper studies the important problem of knowledge editing, which is shown to be important in various practical scenarios. The challenge of knowledge editing comes with the unavailability of up-to-date information that is consistently generated in the real world. As such, it becomes paramount to constantly update knowledge in machine learning models, especially LLMs.\n\n2. The paper is carefully organized. The authors include clear figures to illustrate the overall process, which helps readers understand the mechanism.\n\n3. The authors conduct extensive experiments to evaluate the effectiveness of the proposed method. Moreover, the performance is evaluated under various metrics to demonstrate the capability of the method.",
            "weaknesses": "1. The authors mainly use relatively small LLMs for performance evaluation. I understand that the proposed method is proposed for white-box LLMs and requires parameter updates, which makes it difficult to generalize to larger models without sufficient computational resources. This appears as a crucial disadvantage of the work, as its capability in larger models is still not validated. Especially when the black-box LLMs are unavailable, the authors should be more concerned about performance on larger models, as it is not prevalent in practice to deploy a 7-b model and update it.\n\n2. As mentioned in Weakness 1, the method is not applicable to black-box LLMs, which limits its application scope."
        },
        "llm": "{\"strengths\": {\"Experiment\": \"The paper conducts extensive empirical experiments across various datasets and LLM architectures, demonstrating the effectiveness of the proposed KDPO method.\", \"Evaluation\": \"The evaluation uses multiple metrics to assess the performance of knowledge editing, ensuring comprehensive analysis of edit success, locality, portability, and fluency.\", \"Result\": \"KDPO consistently achieves state-of-the-art or comparable results across datasets and models, particularly excelling in locality, indicating precise editing without affecting unrelated parts of the LLM.\", \"Presentation\": \"The paper is well-organized and clearly presents the methodology, experiments, and results, making it accessible to readers interested in LLM knowledge editing.\", \"Methodology\": \"The introduction of KDPO, a novel variation of DPO optimized for incremental knowledge modifications, is a significant methodological contribution.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"While the paper provides a strong methodological approach, the motivation behind treating KE as an LLM alignment problem could be further elaborated for better understanding.\", \"Methodology\": \"The paper relies on maintaining a copy of the model for KDPO, which increases memory footprint and may limit practical applicability.\", \"Evaluation\": \"The evaluation primarily focuses on sequential edits; additional experiments on diverse editing scenarios could further validate the method's robustness.\", \"Result\": \"Although KDPO performs well, the paper could discuss potential limitations or trade-offs in performance across different model sizes and datasets.\"}, \"soundness\": \"4.5\", \"overall assessment\": \"4\"}"
    },
    "42": {
        "paper_id": "a8658bb0e89862c301c94a66cd74ac60d6758b0d8962279ba1d1cd59a2f1705d06d33a2b36bcec5e8a52349f687f95e47c5b2c5c3d3448ee9d4474d109bab0e0",
        "review_id": "efe6f46ceca69c8cf1f30f96acee69c62f5430d596cc1d90048ed49b356255cfbe253a4d45b9dbcfcfcf38789de2c8b040a37c58b4c8c9cb1f27525a05702e0c",
        "human": {
            "strengths": "- Innovative Multi-Aspect Encoding: MARE introduces a Multi-Aspect Multi-Head Attention mechanism, which allows the model to encode multiple aspects of the text simultaneously. This is a significant improvement over traditional uni-aspect models.\n\n- The experimental results show that MARE achieves state-of-the-art performance on unsupervised rationale extraction benchmarks. Specifically, it outperforms previous methods in token-level F1 scores and accuracy metrics.\n\n- The multi-task training strategy reduces memory usage and training time by 17.9% and 25.2%, respectively, compared to multi-aspect collaborative training. This efficiency gain is crucial for practical applications.",
            "weaknesses": "- The results of baseline methods are all collected from YOFO, while YOFO has not been formally publised yet. This can lead to some potentially inconsistent setups, which can produce unfair comparisons. For example, the authors are encouraged to reimplement BERT-RNP themselves.\n\n- There are some details that are in need of proofreading, for example, I do not find the experiment the authors mention in L330-332."
        },
        "llm": "{\"strengths\": {\"Experiment\": \"The paper presents extensive experiments on two unsupervised rationale extraction benchmarks, demonstrating the superiority of the proposed MARE model over state-of-the-art methods.\", \"Evaluation\": \"The evaluation metrics used, such as token-level F1 and accuracy, are standard and appropriate for rationale extraction tasks. The results show significant improvements in performance across different aspects.\", \"Result\": \"MARE achieves state-of-the-art performance with notable improvements in token-level F1 scores, indicating effective multi-aspect rationale extraction.\", \"Data/Task\": \"The paper addresses the task of unsupervised rationale extraction with a focus on multi-aspect scenarios, which is a relevant and challenging problem in NLP.\", \"Methodology\": \"The introduction of the Multi-Aspect Multi-Head Attention (MAMHA) mechanism and multi-task training are novel contributions that allow the model to encode multiple aspects simultaneously and reduce training costs.\"}, \"weaknesses\": {\"Methodology\": \"The paper relies heavily on the MAMHA mechanism, and while effective, it may increase computational overhead due to the need for special tokens.\", \"Comparison\": \"While MARE shows improvements, the paper could benefit from a more detailed comparison with other multi-aspect models, if available, to better highlight its unique advantages.\", \"Result\": \"Despite the strong results, the paper does not fully explore the limitations of MARE, such as its applicability to decoder-only models, which could impact its generalizability.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    },
    "43": {
        "paper_id": "4136dbf7520dca31bcf2bf24ee71cef97d64e313657f2aefffbb7f51939105d7c9ea13d03556e3201dbed773044b14e63dfd5bc7a20579709e1353faadaa2b4d",
        "review_id": "7389ecc46ade0d9d45a31ba07005f8ce04df3778c6253365e7dc7166c8aee8d018387517bb18a82ca4f2cb4d64fc4e3fb1c1dc791a669aadbae5f252349bc3df",
        "human": {
            "strengths": "The proposed method shows performance improvement on the gloss prediction task, validating the effectiveness of the proposed method.",
            "weaknesses": "The weaknesses of this paper are listed as follows:\n\n1. I don't see any novelty in the proposed method. To me, it's just a in context learning without any additional techniques used.\n\n2. The practical utility of the proposed setting is limited. In the paper's setting, the transcription line and translation line are provided, and then the authors would like to predict the gloss line. However, in most scenarios, the gloss line is provided to achieve a better text generation quality. If the author did one further step to incorporate this predicted gloss line into a text generation model such as the endangered language translation model and validated that the results are better than the original translation line, then I feel this method would be more practically useful. \n\n3. The writing of this paper is pretty bad, and many details of this paper are quite unclear, making the implementation details of this paper unknown. For example, the authors keeps saying to explore whether LLMs can be effective at the task of interlinear glossing with in context learning, without any traditional training. What LLMs are used in this paper? In Figure 2, what do these two lines represent? There is no description in the figure."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper proposes novel approaches for selecting in-context examples, which significantly improve performance of LLM-based methods for interlinear glossing without traditional training.\", \"Result\": \"LLM-based methods outperform standard transformer baselines despite requiring no training, offering practical solutions for researchers outside the NLP community.\"}, \"weaknesses\": {\"Novelty\": \"The approaches still underperform state-of-the-art supervised systems for the task, indicating limited novelty in terms of achieving state-of-the-art results.\", \"Intuition/Justification/Motivation/Validation\": \"The paper lacks strong justification for the choice of retrieval strategies and does not fully explore alternative methods for improving performance.\", \"Result\": \"While LLM methods outperform simple baselines, they generally underperform compared to the best supervised systems, limiting their impact.\", \"Presentation\": \"The paper could benefit from clearer presentation of experimental results and more detailed explanations of retrieval strategies.\", \"Methodology\": \"The methodology does not incorporate segmentation explicitly, which is a key factor in achieving state-of-the-art performance.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper could provide more detailed discussion on the limitations of the proposed methods and potential improvements, especially regarding prompt engineering and exploring open-source models.\"}, \"soundness\": \"3.5\", \"overall assessment\": \"3\"}"
    },
    "44": {
        "paper_id": "2238d6aa620f89ee25aa77e98fe4e493f13577706a7e15e900958ad91dac682e239a811f100eaa5bdb3d0b35d601365c7e39bd5a89fe6432adcc601fffadd432",
        "review_id": "a9bd72536aaf3e9babd41bd71a76b32dee396ce4a4b2b8faff761b318d95a47567c679e7d91e4d210a61fea2164eb31d56dfbb0b3f1626fc829e48e953449444",
        "human": {
            "strengths": "* The paper is clearly written, well-illustrated, and straightforward to understand.\n* CLIPFit effectively balances overfitting and downstream task adaptation. It is good to see that there is still room for improvement in this research domain.",
            "weaknesses": "* CLIPFit is somewhat a combination of existing PEFT studies. The main concern is that the components appear to be selected empirically, in a data-driven manner. Either clear logic or extensive experiments would be needed to support the design choices."
        },
        "llm": "{\"strengths\": {\"Presentation\": \"The paper is well-organized and clearly presents the novel methodology of CLIPFit, making it easy for readers to understand the differences between existing methods and the proposed approach.\", \"Data/Task\": \"The paper evaluates the proposed method on 11 diverse datasets, demonstrating its effectiveness across various tasks and settings.\", \"Methodology\": \"The paper introduces CLIPFit, a novel fine-tuning approach that modifies only specific inherent parameters of CLIP without introducing external parameters, which is efficient and effective compared to existing methods.\"}, \"weaknesses\": {\"Data/Task\": \"While the paper focuses on image classification tasks, it does not explore other potential applications of VLMs such as image retrieval or captioning, limiting its perceived impact.\", \"Experiment\": \"The paper does not provide a detailed analysis of the gradient differences during fine-tuning, which could help understand the underlying reasons for parameter changes.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "45": {
        "paper_id": "a6971a903bf2443c8d32935fa294dcfee39a4bbe14dd909d41617a568104f0759cf3d1c272b1eaf487ac8b61c2eb22f634959e9f2e5af79ac0f456cf74e4e0de",
        "review_id": "0c994a1351936fa0686a50d8dffa5181d9db9568a7c32c9a9d7dcad588d98529bb7149741af986d54ee5e853b415e5686d3787dc344fdd11fd95aa7f72fea260",
        "human": {
            "strengths": "1. This paper tackles a novel and important problem of prompt optimization in multi-step tasks.\n2. The proposed method, PROMST, is well-motivated by important problems of multi-step prompt optimization, such as the difficulty of prompt engineering and automatic reasoning over errors.\n3. The experimental results are satisfactory in quality and quantity, as the authors provide results on multiple tasks and LLMs compared to a sufficient number of baseline methods.",
            "weaknesses": "I don't have any significant concerns in this paper that would stop the paper from appearing in an ACL event, but I would like to mention some minor weaknesses.\n\n1. The formatting for both the text and the visualizations has room for improvement. I have described a few of them in the suggestions, and I would suggest carefully reviewing the paper to fix them.\n2. It seems that some descriptions and experimental results solely rely on the Appendix, which may confuse the readers of this paper. \n3. Some explanations on tasks that PROMPST underperforms baseline methods would be helpful."
        },
        "llm": "{\"strengths\": {\"Intuition/Justification/Motivation/Validation\": \"The paper introduces PROMST, a novel framework for automatic prompt optimization in multi-step tasks, addressing challenges like prompt complexity and human preference alignment. The integration of human feedback and heuristic-based sampling is well-motivated, showing significant improvements over existing methods.\", \"Experiment\": \"Extensive experiments across 11 multi-step tasks demonstrate the effectiveness of PROMST, with substantial performance gains over baseline methods. The use of various LLMs for validation further strengthens the empirical evidence.\", \"Result\": \"PROMST achieves impressive relative improvements (10.6%-29.3%) over baseline methods, indicating strong empirical results. The framework consistently outperforms other prompt optimization methods across different tasks and LLMs.\", \"Data/Task\": \"The paper covers a diverse set of 11 multi-step tasks, providing a comprehensive evaluation of the proposed framework. This variety ensures that the results are broadly applicable and not limited to specific task types.\", \"Methodology\": \"PROMST combines human feedback, score prediction models, and heuristic sampling in a coherent framework, offering a systematic approach to prompt optimization for complex tasks.\"}, \"weaknesses\": {\"Experiment\": \"The experiments rely heavily on LLM API calls, which may limit accessibility due to resource constraints. The paper could benefit from discussing alternative approaches to reduce computational costs.\", \"Result\": \"While the results are strong, the paper could provide more insights into the specific characteristics of optimized prompts that lead to better performance.\", \"Presentation\": \"The paper is dense with technical details, which may hinder readability for some audiences. Simplifying explanations and providing clearer diagrams could improve comprehension.\", \"Data/Task\": \"The paper does not sufficiently explore the impact of different scoring functions on task performance, which could be crucial for aligning with human preferences.\", \"Definition/Description/Detail/Discussion/Explanation/Interpretation\": \"The paper lacks detailed discussion on the limitations and potential societal impacts of the proposed framework. More thorough exploration of these aspects would enhance the overall contribution.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "46": {
        "paper_id": "88ace629b9391388b017c723101a03bd89445ad6b4370c2d099f88036bd72b3de6ec3311a1f3eb95f2fb748694e0bc3e53686ed33fd17b164161dcb42bd92f2d",
        "review_id": "4b2950758108cf56a4c33755cb58d6d9a92c657dea751f0a2b1c7ceb4363577802e52de6ac313ed725c6ec58d8ebe090ef180271aa99f46ffb492a98cff8b2c7",
        "human": {
            "strengths": "The work contributes to a line of research (Retrieval-Augmented Generation QA, RAG-QA) that has the potential for addressing and mitigating the hallucination and factuality shortcomings of current LLMs. The proposed approach, involving preference tuning to improve helpfulness and factuality, brought about improved performance on most of the models tested on (except Llama-2-7B-Chat, see comments in Summary of Weaknesses below).",
            "weaknesses": "- The approach appears to require meaningful resources to train (obtaining the candidate reference summaries, filtering them, passing through a paraphrasing step, and then filtering them again, 1 phase of supervised fine-tuning and then another  phase of preference tuning). Since EFSUM models have been tuned for two datasets (WebQSP and Mintaka), it would useful to apply the models across datasets to investigate how well the proposed EFSUM model can generalise to new datasets. \n\n- The applicabilty of the proposed approach may be constrained to certain LLMs (see the poorer results obtained on Llama-2-7B-Chat in Tables 1 and 2 for EFSUM compared to \"no knowledge\")\n\n- It is not clear whether the work surpasses SOTA on the WebQSP dataset. Although, the authors used WebQSP-WD in their work, i.e. it is not directly comparable, this is a simply a 1,033-instance subset of the 4,737-instance original WebQSP, that has been successfully mapped to Wikidata.  SOTA (Tan et al, 2023) [1] reports accuracy of 67.68 (GPT3) and 79.60 (GPT3.5v2) on WebQSP -- see Table 3 of (Tan et al, 2023). As an indication, EFSUM would need to get at least 85% of the remaining instances of WebQSP correct in order to hit accuracy of 78.93, i.e. `(1033*0.569 + (4737-1033)*0.85)/4733`. Fortunately, since the code for the SOTA work is publicly released https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family, it should be relatively easy to run it on the same 1,033 WebQSP-WD instances to obtain results for a baseline. The findings there could help strengthen the case for the benefits of EFSUM\n\n**Citation**\n- [1] [Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family](https://arxiv.org/pdf/2303.07992)"
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel Evidence-focused Fact Summarization framework (EFS UM) that optimizes an open-source LLM for fact summarization through distillation and preference alignment, providing a robust approach to enhancing zero-shot QA performance.\", \"Result\": \"Extensive experiments demonstrate that EFS UM significantly improves LLM's zero-shot QA performance, achieving superior helpfulness and faithfulness in generated fact summaries compared to existing methods.\", \"Contribution\": \"The paper makes a valuable contribution by addressing challenges in structured KG verbalization and improving evidence density and clarity, ultimately enhancing QA accuracy with knowledge-augmented LLMs.\"}, \"weaknesses\": {\"Related Work\": \"The paper could benefit from a more comprehensive comparison with existing verbalization strategies and their limitations.\", \"Result\": \"While the results are promising, they rely heavily on the performance of the retriever, which may not always be flawless, potentially affecting the robustness of the approach.\", \"Presentation\": \"The paper is dense with technical details that may be challenging for readers to follow, especially in sections explaining the methodology and experiments.\", \"Data/Task\": \"The paper does not address potential biases in the dataset or the LLMs used, which could impact the generalizability of the findings.\", \"Analysis\": \"The analysis lacks depth in exploring the reasons behind the model's inclination towards specific knowledge formats, which could provide insights into improving model performance.\", \"Comparison\": \"The paper does not sufficiently compare EFS UM with other state-of-the-art methods on a broader range of datasets or tasks.\", \"Methodology\": \"The reliance on a teacher LLM for summarization capabilities might limit the applicability of the approach to scenarios where such resources are unavailable.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "47": {
        "paper_id": "fb13d93d08b48cf261f5f3047485d5b129d4bf7450262673179e22f77a7ae6017f06540bab2f10b9714e16820fe53b6188d65a1bb91eaa232eb4592e4267f7dc",
        "review_id": "f44ae6185d860604ab6950cb72573259d40150657518fcb9ccf5f02a5d4a797b5b88c9ffe8af23a19461ec7edd5e51b311cf72b53c022738dfe3e9ba24fd5798",
        "human": {
            "strengths": "1. Objective topics evaluation of LLMs is novel. This paper develops a new benchmark, which explores subjective topics rather than objective tasks, while existing evaluations lack comprehensiveness and only focus on logical thinking.\n2. To improve the performance of LLM in both subjective and objective topics, they propose NeoN, casting the LLM as an agent of a negator and experimental results show that the effectiveness of NeoN.",
            "weaknesses": "1. More baselines and more LLMs should be compared. This work only compares the performance of NeoN with CoT, self-consistency, recite and answer. More agent debating baseline should be added. More LLMs should be compared, including Mistral, GPT-4, Claude, Baichuan, Qwen ect."
        },
        "llm": "{\"strengths\": {\"Novelty\": \"The paper introduces a novel framework, NeoN, based on the philosophical principle of negation of negation to enhance the comprehensive, reflective, and creative thinking abilities of LLMs.\", \"Experiment\": \"Comprehensive experiments are conducted on both subjective and objective tasks, validating the effectiveness of NeoN across various models.\", \"Evaluation\": \"The paper proposes three robust evaluation indicators to assess the reasoning abilities of LLMs, demonstrating consistency and rationality.\", \"Result\": \"NeoN consistently outperforms baseline methods across different models and topic types, indicating significant improvements in reasoning abilities.\", \"Data/Task\": \"A diverse SJTP dataset is constructed, covering various subjective topics and fields, providing a benchmark for evaluating non-logical reasoning abilities.\", \"Methodology\": \"The structured framework of NeoN with iterative negation and integrative unification is well-defined and systematically implemented.\"}, \"weaknesses\": {\"Methodology\": \"The dependency on pre-trained LLMs may limit the effectiveness of NeoN if the models are not properly trained in comprehensive thinking abilities.\", \"Comparison\": \"While NeoN shows improvements, the paper lacks a detailed comparison with other advanced reasoning frameworks beyond zero-shot prompting methods.\", \"Result\": \"The improvement in subjective reasoning tasks reaches a bottleneck after a certain number of negation rounds, which may indicate limitations in stimulating further cognitive faculties.\"}, \"soundness\": \"4.5\", \"overall assessment\": \"4\"}"
    },
    "48": {
        "paper_id": "45d4a799176e1eb404c411664c2ec1d6a032dbe3f447ad538f3b3ea95300829156bd7b50b97ee3eb85950f8e8ca1e9735b9e5cd1c4d19fc41e522e1c50ea6f51",
        "review_id": "609a5a4ca2ac6c6c0ed7e24b5a4138bd9b733f615627d9fa5b55739485e785c1304ab3675bf76ec1956d32d7a6b450fb2c9d9220816327d70d6fa200dd71c44a",
        "human": {
            "strengths": "* Previously, separate models were required for text-based NER, speech NER, and multimodal NER. \nIMNER, which is a task that the authors proposed in this paper, handles them in one model, which is considered to be a good task with great practical benefits.\n\n* The performance of Chinese NER\nwas improved by using a IMNER method compared with task-specific methods.\n\n* Proposed method can perform NER with sufficient performance even when only single-modal data is available. \nThis is very good feature considering the practical conditions that it is not always possible to use both speech and text data.\n\n* The method is relatively simple; \nit is based on the multimodal NER model and uses a pseudo sequence to compensate for missing data, \nso it is easy to implement and the computational cost is not expected to be that high.",
            "weaknesses": "* The method relies heavily on the existing MMSpeech and appears to be an incremental method.\n  * In the ablation study, pseudo text/speech input, which is one of the most distinctive parts of the method, \n    does not contribute much to performance improvement.\n    * (Updated after reading authors' comments) Even the performance is large compared to the others in the table, I have confirmed that there is certainly an improvement in performance by using pseudo text/speech.\n  * The experimental results in Table 2 compare the performance of the proposed method with TNER and MNER using MMSpeech,\n    so I understand that the performance improved when applied to IMNER. \n    However, even if the performance is not good, if the authors evaluate the performance of other backbone models in the same way, \n    it may be able to see the characteristics of the proposed method itself rather than the performance of MMSpeech.\n    * By comparing the performance using other backbone models, \n      the possibilities for application for other tasks will expand, \n      and I think readers will be more interested in this paper.\n  * (Updated after reading authors' comments) MMSpeech is a general model like BERT etc., so the above does not seem to be a big problem.\n    * However, it was not clearly written in the paper, and I think there are many readers who are not familiar with Chinese speech recognition, so it may be better to explain what kind of model MMSpeech is, etc., as the authors wrote in the comment. I think it would be easier for readers to understand if you added relevant information to the appendix.\n\n* Although it is mentioned in the introduction that Chinese NER is difficult due to the lack of clear word delimitation or the presence of homophones,\n  there is no evaluation of how much the proposed method can solve these problems.\n  * For example, it may become clear by experimenting and comparing the sequential labeling-based method for tokenized Chinese."
        },
        "llm": "{\"strengths\": {\"Methodology\": \"The paper introduces a novel task, Integrated Multimodal NER (IMNER), which unifies text-based, speech, and multimodal NER tasks. It proposes a new framework, IMAGE, leveraging a pre-trained multimodal model, MMSpeech, and formulates the task as an entity-aware text generation problem.\", \"Data/Task\": \"The IMNER task is well-defined, addressing the limitations of treating NER tasks separately and proposing a unified approach for handling different modalities. The paper also provides a unified data format for these tasks.\", \"Result\": \"Experimental results demonstrate the effectiveness of the IMAGE framework, showing competitive performance across TNER, SNER, and MNER tasks, and highlighting the potential connections among these tasks.\"}, \"weaknesses\": {\"Experiment\": \"The experiments lack diversity in datasets, focusing only on Chinese data. This limits the generalizability of the findings to other languages.\", \"Evaluation\": \"The evaluation primarily focuses on F1 scores without discussing other potential metrics or qualitative analysis that could provide deeper insights into the model's performance.\", \"Result\": \"The results, while competitive, do not significantly outperform state-of-the-art methods in all tasks, particularly in TNER.\", \"Ablation\": \"The ablation study, though insightful, could be expanded with more detailed analysis on the impact of different components.\", \"Comparison\": \"Comparative analysis with more diverse baseline models across different languages and tasks could strengthen the paper's claims.\", \"Presentation\": \"The paper is dense and could benefit from clearer exposition, particularly in the methodology section, to enhance readability and understanding.\", \"Methodology\": \"The reliance on a single pre-trained model, MMSpeech, which is specific to Chinese, limits the applicability of the proposed framework to other languages.\"}, \"soundness\": \"4\", \"overall assessment\": \"3.5\"}"
    },
    "49": {
        "paper_id": "1c7c702390ae35f658a8f96168ce7d416503853e1fbc0d7b163908e7be4503f7de04c86f86d719da74636d9eb668f9c98d48c27166f16d1244bacbcf682968b1",
        "review_id": "58ffdfff0f7b78b97f2910b583aacec2bf9e0b00bb4a9bd31bfdf25778612a1be2710da480f8954a1c340e2f16c6088cbd6c9cb4a1cc7a1630bb1c8bf586a112",
        "human": {
            "strengths": "1: This paper is well-organized and well-written. It's easy to understand.\n\n2: The PPL results seem promising. It's much better than existing works. \n\n3: The proposed method is somehow novel. Although, I'm not fully convinced by the motivation.",
            "weaknesses": "1: My major concern  is about the motivation. Because there is a quantization-aware fine-tuning procedure, of course, the error spanning over other tokens would be small. I mean, if you put such modules somewhere else, this might also work.\n\n2: There is a lack of real-world tasks such as GSM8K. Many existing quantization works such as GEAR[1] construct evaluation on real tasks.\n\n3: From the system perspective, there is no evaluation considering the real efficiency improvement, such as throughput, etc ..\n\n[1]GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"
        },
        "llm": "{\"strengths\": {\"Novelty\": \"The paper introduces CushionCache, a novel method for mitigating activation outliers in LLMs, improving activation quantization.\", \"Intuition/Justification/Motivation/Validation\": \"The method is motivated by the observation that activation outliers originate from attention sinks, and it effectively addresses this by inserting sink-like tokens as a prefix.\", \"Result\": \"CushionCache significantly improves the performance of quantized LLMs, particularly in per-tensor static range quantization, with over 30% improvements in zero-shot accuracy for some models.\", \"Comparison\": \"The paper provides extensive comparisons with existing methods like SmoothQuant, showing consistent improvements across various quantization scenarios.\", \"Presentation\": \"The paper is well-organized, with detailed explanations of the method, experiments, and analyses, making it easy to follow.\"}, \"weaknesses\": {\"Intuition/Justification/Motivation/Validation\": \"The method is primarily validated on decoder-only LLMs, limiting its applicability to other architectures without further modifications.\", \"Evaluation\": \"While the method shows significant improvements, the evaluation lacks exploration of its impact on encoder-decoder models.\", \"Data/Task\": \"The experiments focus on a limited set of models and tasks, which may not fully represent the broader applicability of the method.\", \"Methodology\": \"The hyperparameter selection process for the greedy prefix search lacks a principled approach, potentially requiring extensive tuning for different models.\"}, \"soundness\": \"4\", \"overall assessment\": \"4\"}"
    }
}